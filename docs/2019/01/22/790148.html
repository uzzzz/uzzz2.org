<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Kubernetes部署（十二）：helm部署harbor企业级镜像仓库 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Kubernetes部署（十二）：helm部署harbor企业级镜像仓库" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="相关内容： Kubernetes部署（一）：架构及功能说明Kubernetes部署（二）：系统环境初始化Kubernetes部署（三）：CA证书制作Kubernetes部署（四）：ETCD集群部署Kubernetes部署（五）：Haproxy、Keppalived部署Kubernetes部署（六）：Master节点部署Kubernetes部署（七）：Node节点部署Kubernetes部署（八）：Flannel网络部署Kubernetes部署（九）：CoreDNS、Dashboard、Ingress部署Kubernetes部署（十）：储存之glusterfs和heketi部署Kubernetes部署（十一）：管理之Helm和Rancher部署Kubernetes部署（十二）：helm部署harbor企业级镜像仓库 harbor简介 harbor官方github：https://github.com/goharborHarbor是一个用于存储和分发Docker镜像的企业级Registry服务器。Harbor通过添加用户通常需要的功能（如安全性，身份和管理）来扩展开源Docker Distribution。使registry更接近构建和运行环境可以提高图像传输效率。Harbor支持在registry之间复制映像，还提供高级安全功能，如用户管理，访问控制和活动审计。 特征 云原生注册表：Harbour 支持容器镜像和Helm chart，可用作本地云环境（如容器运行和业务流程平台）的注册表。 基于角色的访问控制：用户和存储库通过“项目”进行组织，用户可以对项目下的镜像拥有不同的权限。 基于策略的映像复制：可以基于具有多个过滤器（存储库，标记和标签）的策略在多个注册表实例之间复制（同步）映像。如果遇到任何错误，Harbor将自动重试进行复制。非常适合负载平衡，高可用性，多数据中心，混合和多云场景。 漏洞扫描：Harbor定期扫描镜像并警告用户漏洞。 LDAP / AD支持：Harbor与现有企业LDAP / AD集成以进行用户身份验证和管理，并支持将LDAP组导入Harbor并为其分配适当的项目角色。 镜像删除和垃圾收集：可以删除图像，并可以回收它们的空间。 公证：可以确保镜像的真实性。 图形用户门户：用户可以轻松浏览，搜索存储库和管理项目。 审计：跟踪存储库的所有操作。 RESTful API：适用于大多数管理操作的RESTful API，易于与外部系统集成。 易于部署：提供在线和离线安装程序。 先决条件 Kubernetes集群 1.10+ helm 2.8.0+ Harbor部署 1. 添加域名解析。 将h.cnlinux.club和n.cnlinux.club的A记录解析到我的负载均衡IP 10.31.90.200，用于创建ingress。 2. 下载harbor的chart包 [root@node-01 harbor]# wget https://github.com/goharbor/harbor-helm/archive/1.0.0.tar.gz -O harbor-helm-v1.0.0.tar.gz 3. 修改配置文件 提取harbor-helm-v1.0.0.tar.gz文件中的values.yaml文件，并放到和harbor-helm-v1.0.0.tar.gz同一级的目录中。 修改values.yaml，我的配置修改了如下几个字段： 需要说明的是如果k8s集群中存在storageclass就可以直接用storageclass，在几个persistence.persistentVolumeClaim.XXX.storageClass中指定storageclass名就可以了，会自动创建多个pvc，但是我这里为了防止创建多个pvc增加管理难度，我在部署前创建了一个pvc，harbor下所有的服务都使用这一个pvc，具体每个字段的作用请查看官方文档https://github.com/goharbor/harbor-helm。 expose.ingress.hosts.core xpose.ingress.hosts.notary externalURL persistence.persistentVolumeClaim.registry.existingClaim persistence.persistentVolumeClaim.registry.subPath persistence.persistentVolumeClaim.chartmuseum.existingClaim persistence.persistentVolumeClaim.chartmuseum.subPath persistence.persistentVolumeClaim.jobservice.existingClaim persistence.persistentVolumeClaim.jobservice.subPath persistence.persistentVolumeClaim.database.existingClaim persistence.persistentVolumeClaim.database.subPath persistence.persistentVolumeClaim.redis.existingClaim persistence.persistentVolumeClaim.redis.subPath expose: type: ingress tls: enabled: true secretName: &quot;&quot; notarySecretName: &quot;&quot; commonName: &quot;&quot; ingress: hosts: core: h.cnlinux.club notary: n.cnlinux.club annotations: ingress.kubernetes.io/ssl-redirect: &quot;true&quot; nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot; ingress.kubernetes.io/proxy-body-size: &quot;0&quot; nginx.ingress.kubernetes.io/proxy-body-size: &quot;0&quot; clusterIP: name: harbor ports: httpPort: 80 httpsPort: 443 notaryPort: 4443 nodePort: name: harbor ports: http: port: 80 nodePort: 30002 https: port: 443 nodePort: 30003 notary: port: 4443 nodePort: 30004 externalURL: https://h.cnlinux.club persistence: enabled: true resourcePolicy: &quot;keep&quot; persistentVolumeClaim: registry: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;registry&quot; accessMode: ReadWriteOnce size: 5Gi chartmuseum: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;chartmuseum&quot; accessMode: ReadWriteOnce size: 5Gi jobservice: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;jobservice&quot; accessMode: ReadWriteOnce size: 1Gi database: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;database&quot; accessMode: ReadWriteOnce size: 1Gi redis: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;redis&quot; accessMode: ReadWriteOnce size: 1Gi imageChartStorage: type: filesystem filesystem: rootdirectory: /storage imagePullPolicy: IfNotPresent logLevel: debug harborAdminPassword: &quot;Harbor12345&quot; secretKey: &quot;not-a-secure-key&quot; nginx: image: repository: goharbor/nginx-photon tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} portal: image: repository: goharbor/harbor-portal tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} core: image: repository: goharbor/harbor-core tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} adminserver: image: repository: goharbor/harbor-adminserver tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} jobservice: image: repository: goharbor/harbor-jobservice tag: v1.7.0 replicas: 1 maxJobWorkers: 10 jobLogger: file nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} registry: registry: image: repository: goharbor/registry-photon tag: v2.6.2-v1.7.0 controller: image: repository: goharbor/harbor-registryctl tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} chartmuseum: enabled: true image: repository: goharbor/chartmuseum-photon tag: v0.7.1-v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} clair: enabled: true image: repository: goharbor/clair-photon tag: v2.0.7-v1.7.0 replicas: 1 httpProxy: httpsProxy: updatersInterval: 12 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} notary: enabled: true server: image: repository: goharbor/notary-server-photon tag: v0.6.1-v1.7.0 replicas: 1 signer: image: repository: goharbor/notary-signer-photon tag: v0.6.1-v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} database: type: internal internal: image: repository: goharbor/harbor-db tag: v1.7.0 password: &quot;changeit&quot; nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} redis: type: internal internal: image: repository: goharbor/redis-photon tag: v1.7.0 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} 4. 创建存储卷 因为harbor需要使用到mysql，为防止mysql在调度过程中造成数据丢失，我们需要将mysql的数据存储在gluster的存储卷里。 [root@node-01 harbor]# vim pvc-harbor.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-harbor spec: storageClassName: gluster-heketi accessModes: - ReadWriteMany resources: requests: storage: 50Gi [root@node-01 harbor]# kubectl apply -f pvc-harbor.yaml 5. 安装harbor [root@node-01 harbor]# helm install --name harbor harbor-helm-v1.0.0.tar.gz -f values.yaml 如果安装不成功可以用helm del --purge harbor删除重新安装。 6. 演示 在一段时间后可以看到harbor所有相关的pod都已经运行起来了，我们就可以访问了，默认用户密码是admin/Harbor12345,可以通过修改values.yaml来更改默认的用户名和密码。 [root@node-01 ~]# kubectl get pod NAME READY STATUS RESTARTS AGE harbor-harbor-adminserver-7fffc7bf4d-vj845 1/1 Running 1 15d harbor-harbor-chartmuseum-bdf64f899-brnww 1/1 Running 0 15d harbor-harbor-clair-8457c45dd8-9rgq8 1/1 Running 1 15d harbor-harbor-core-7fc454c6d8-b6kvs 1/1 Running 1 15d harbor-harbor-database-0 1/1 Running 0 15d harbor-harbor-jobservice-7895949d6b-zbwkf 1/1 Running 1 15d harbor-harbor-notary-server-57dd94bf56-txdkl 1/1 Running 0 15d harbor-harbor-notary-signer-5d64c5bf8d-kppts 1/1 Running 0 15d harbor-harbor-portal-648c56499f-g28rz 1/1 Running 0 15d harbor-harbor-redis-0 1/1 Running 0 15d harbor-harbor-registry-5cd9c49489-r92ph 2/2 Running 0 15d 接下来我们创建test的私有项目用来测试。 因为我们创建的harbor仓库是https的所以在docker pull或者push镜像之前，需要先把证书加到docker对应的配置目录里，不然docker是无法登录harbor的。 进入test项目，点解“注册证书”下载harbor的CA证书。 在每个node节点创建目录（以后可能会在master上传镜像，所以此次我的master节点也都一起创建了） for n in `seq -w 01 06`;do ssh node-$n &quot;mkdir -p /etc/docker/certs.d/h.cnlinux.club&quot;;done #将下载下来的harbor CA证书拷贝到每个node节点的etc/docker/certs.d/h.cnlinux.club目录下 for n in `seq -w 01 06`;do scp ca.crt node-$n:/etc/docker/certs.d/h.cnlinux.club/;done 在node节点上功登录harbor,登录成功后的信息保存在当前用户家目录下的.docker/config.json里。 [root@node-06 ~]# docker login h.cnlinux.club Username: admin Password: Login Succeeded [root@node-06 ~]# cat .docker/config.json { &quot;auths&quot;: { &quot;h.cnlinux.club&quot;: { &quot;auth&quot;: &quot;YWRtaW46SGFyYm9yMTIzNDU=&quot; } } } 在官方docker仓库pull一个nginx镜像，任何打上tag，push到harbor仓库，如下就可以看到harbor的test项目下已经有nginx的镜像了 [root@node-06 ~]# docker pull nginx:latest [root@node-06 ~]# docker tag nginx:latest h.cnlinux.club/test/nginx:latest [root@node-06 ~]# docker push h.cnlinux.club/test/nginx:latest 问题：如果我的k8s集群很多的node节点是不是每个node节点都要上去登录才能pull harbor仓库的镜像？这样是不是就非常麻烦了？ 其实在k8s里有一种secret的类型是kubernetes.io/dockerconfigjson就是用来解决这种问题的。 首先将docker的登录信息转换成base64格式 [root@node-06 ~]# cat .docker/config.json |base64 ewoJImF1dGhzIjogewoJCSJoLmNubGludXguY2x1YiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9IgoJCX0KCX0sCgkiSHR0cEhlYWRlcnMiOiB7CgkJIlVzZXItQWdlbnQiOiAiRG9ja2VyLUNsaWVudC8xOC4wNi4xLWNlIChsaW51eCkiCgl9Cn0= 创建secret apiVersion: v1 kind: Secret metadata: name: harbor-registry-secret namespace: default data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJoLmNubGludXguY2x1YiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9IgoJCX0KCX0sCgkiSHR0cEhlYWRlcnMiOiB7CgkJIlVzZXItQWdlbnQiOiAiRG9ja2VyLUNsaWVudC8xOC4wNi4xLWNlIChsaW51eCkiCgl9Cn0= type: kubernetes.io/dockerconfigjson [root@node-01 ~]# kubectl create -f harbor-registry-secret.yaml secret/harbor-registry-secret created 创建nginx demo并使用harbor上的nginx镜像。并将nginx.cnlinux.club解析到负载均衡10.31.90.200。 apiVersion: apps/v1 kind: Deployment metadata: name: deploy-nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: h.cnlinux.club/test/nginx:latest ports: - containerPort: 80 imagePullSecrets: - name: harbor-registry-secret --- apiVersion: v1 kind: Service metadata: name: nginx spec: selector: app: nginx ports: - name: nginx protocol: TCP port: 80 targetPort: 80 type: ClusterIP --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx annotations: # nginx.ingress.kubernetes.io/rewrite-target: / kubernetes.io/ingress.class: nginx spec: rules: - host: nginx.cnlinux.club http: paths: - path: backend: serviceName: nginx servicePort: 80 可以看到3过node节点上的nginx已经运行，证明harbor上的镜像已经成功pull下来。 [root@node-01 ~]# kubectl get pod -o wide|grep nginx deploy-nginx-647f9649f5-88mkt 1/1 Running 0 2m41s 10.34.0.5 node-06 &lt;none&gt; &lt;none&gt; deploy-nginx-647f9649f5-9z842 1/1 Running 0 2m41s 10.40.0.5 node-04 &lt;none&gt; &lt;none&gt; deploy-nginx-647f9649f5-w44ck 1/1 Running 0 2m41s 10.46.0.6 node-05 &lt;none&gt; &lt;none&gt; 最后我们访问http://nginx.cnlinux.club,至此所有的都已完成。后续会陆续更新所有的k8s相关文档，如果你觉得我写的不错，希望大家多多关注点赞，如有问题可以在下面给我留言，非常感谢！ 转载于:https://blog.51cto.com/billy98/2345517" />
<meta property="og:description" content="相关内容： Kubernetes部署（一）：架构及功能说明Kubernetes部署（二）：系统环境初始化Kubernetes部署（三）：CA证书制作Kubernetes部署（四）：ETCD集群部署Kubernetes部署（五）：Haproxy、Keppalived部署Kubernetes部署（六）：Master节点部署Kubernetes部署（七）：Node节点部署Kubernetes部署（八）：Flannel网络部署Kubernetes部署（九）：CoreDNS、Dashboard、Ingress部署Kubernetes部署（十）：储存之glusterfs和heketi部署Kubernetes部署（十一）：管理之Helm和Rancher部署Kubernetes部署（十二）：helm部署harbor企业级镜像仓库 harbor简介 harbor官方github：https://github.com/goharborHarbor是一个用于存储和分发Docker镜像的企业级Registry服务器。Harbor通过添加用户通常需要的功能（如安全性，身份和管理）来扩展开源Docker Distribution。使registry更接近构建和运行环境可以提高图像传输效率。Harbor支持在registry之间复制映像，还提供高级安全功能，如用户管理，访问控制和活动审计。 特征 云原生注册表：Harbour 支持容器镜像和Helm chart，可用作本地云环境（如容器运行和业务流程平台）的注册表。 基于角色的访问控制：用户和存储库通过“项目”进行组织，用户可以对项目下的镜像拥有不同的权限。 基于策略的映像复制：可以基于具有多个过滤器（存储库，标记和标签）的策略在多个注册表实例之间复制（同步）映像。如果遇到任何错误，Harbor将自动重试进行复制。非常适合负载平衡，高可用性，多数据中心，混合和多云场景。 漏洞扫描：Harbor定期扫描镜像并警告用户漏洞。 LDAP / AD支持：Harbor与现有企业LDAP / AD集成以进行用户身份验证和管理，并支持将LDAP组导入Harbor并为其分配适当的项目角色。 镜像删除和垃圾收集：可以删除图像，并可以回收它们的空间。 公证：可以确保镜像的真实性。 图形用户门户：用户可以轻松浏览，搜索存储库和管理项目。 审计：跟踪存储库的所有操作。 RESTful API：适用于大多数管理操作的RESTful API，易于与外部系统集成。 易于部署：提供在线和离线安装程序。 先决条件 Kubernetes集群 1.10+ helm 2.8.0+ Harbor部署 1. 添加域名解析。 将h.cnlinux.club和n.cnlinux.club的A记录解析到我的负载均衡IP 10.31.90.200，用于创建ingress。 2. 下载harbor的chart包 [root@node-01 harbor]# wget https://github.com/goharbor/harbor-helm/archive/1.0.0.tar.gz -O harbor-helm-v1.0.0.tar.gz 3. 修改配置文件 提取harbor-helm-v1.0.0.tar.gz文件中的values.yaml文件，并放到和harbor-helm-v1.0.0.tar.gz同一级的目录中。 修改values.yaml，我的配置修改了如下几个字段： 需要说明的是如果k8s集群中存在storageclass就可以直接用storageclass，在几个persistence.persistentVolumeClaim.XXX.storageClass中指定storageclass名就可以了，会自动创建多个pvc，但是我这里为了防止创建多个pvc增加管理难度，我在部署前创建了一个pvc，harbor下所有的服务都使用这一个pvc，具体每个字段的作用请查看官方文档https://github.com/goharbor/harbor-helm。 expose.ingress.hosts.core xpose.ingress.hosts.notary externalURL persistence.persistentVolumeClaim.registry.existingClaim persistence.persistentVolumeClaim.registry.subPath persistence.persistentVolumeClaim.chartmuseum.existingClaim persistence.persistentVolumeClaim.chartmuseum.subPath persistence.persistentVolumeClaim.jobservice.existingClaim persistence.persistentVolumeClaim.jobservice.subPath persistence.persistentVolumeClaim.database.existingClaim persistence.persistentVolumeClaim.database.subPath persistence.persistentVolumeClaim.redis.existingClaim persistence.persistentVolumeClaim.redis.subPath expose: type: ingress tls: enabled: true secretName: &quot;&quot; notarySecretName: &quot;&quot; commonName: &quot;&quot; ingress: hosts: core: h.cnlinux.club notary: n.cnlinux.club annotations: ingress.kubernetes.io/ssl-redirect: &quot;true&quot; nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot; ingress.kubernetes.io/proxy-body-size: &quot;0&quot; nginx.ingress.kubernetes.io/proxy-body-size: &quot;0&quot; clusterIP: name: harbor ports: httpPort: 80 httpsPort: 443 notaryPort: 4443 nodePort: name: harbor ports: http: port: 80 nodePort: 30002 https: port: 443 nodePort: 30003 notary: port: 4443 nodePort: 30004 externalURL: https://h.cnlinux.club persistence: enabled: true resourcePolicy: &quot;keep&quot; persistentVolumeClaim: registry: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;registry&quot; accessMode: ReadWriteOnce size: 5Gi chartmuseum: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;chartmuseum&quot; accessMode: ReadWriteOnce size: 5Gi jobservice: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;jobservice&quot; accessMode: ReadWriteOnce size: 1Gi database: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;database&quot; accessMode: ReadWriteOnce size: 1Gi redis: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;redis&quot; accessMode: ReadWriteOnce size: 1Gi imageChartStorage: type: filesystem filesystem: rootdirectory: /storage imagePullPolicy: IfNotPresent logLevel: debug harborAdminPassword: &quot;Harbor12345&quot; secretKey: &quot;not-a-secure-key&quot; nginx: image: repository: goharbor/nginx-photon tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} portal: image: repository: goharbor/harbor-portal tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} core: image: repository: goharbor/harbor-core tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} adminserver: image: repository: goharbor/harbor-adminserver tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} jobservice: image: repository: goharbor/harbor-jobservice tag: v1.7.0 replicas: 1 maxJobWorkers: 10 jobLogger: file nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} registry: registry: image: repository: goharbor/registry-photon tag: v2.6.2-v1.7.0 controller: image: repository: goharbor/harbor-registryctl tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} chartmuseum: enabled: true image: repository: goharbor/chartmuseum-photon tag: v0.7.1-v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} clair: enabled: true image: repository: goharbor/clair-photon tag: v2.0.7-v1.7.0 replicas: 1 httpProxy: httpsProxy: updatersInterval: 12 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} notary: enabled: true server: image: repository: goharbor/notary-server-photon tag: v0.6.1-v1.7.0 replicas: 1 signer: image: repository: goharbor/notary-signer-photon tag: v0.6.1-v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} database: type: internal internal: image: repository: goharbor/harbor-db tag: v1.7.0 password: &quot;changeit&quot; nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} redis: type: internal internal: image: repository: goharbor/redis-photon tag: v1.7.0 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} 4. 创建存储卷 因为harbor需要使用到mysql，为防止mysql在调度过程中造成数据丢失，我们需要将mysql的数据存储在gluster的存储卷里。 [root@node-01 harbor]# vim pvc-harbor.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-harbor spec: storageClassName: gluster-heketi accessModes: - ReadWriteMany resources: requests: storage: 50Gi [root@node-01 harbor]# kubectl apply -f pvc-harbor.yaml 5. 安装harbor [root@node-01 harbor]# helm install --name harbor harbor-helm-v1.0.0.tar.gz -f values.yaml 如果安装不成功可以用helm del --purge harbor删除重新安装。 6. 演示 在一段时间后可以看到harbor所有相关的pod都已经运行起来了，我们就可以访问了，默认用户密码是admin/Harbor12345,可以通过修改values.yaml来更改默认的用户名和密码。 [root@node-01 ~]# kubectl get pod NAME READY STATUS RESTARTS AGE harbor-harbor-adminserver-7fffc7bf4d-vj845 1/1 Running 1 15d harbor-harbor-chartmuseum-bdf64f899-brnww 1/1 Running 0 15d harbor-harbor-clair-8457c45dd8-9rgq8 1/1 Running 1 15d harbor-harbor-core-7fc454c6d8-b6kvs 1/1 Running 1 15d harbor-harbor-database-0 1/1 Running 0 15d harbor-harbor-jobservice-7895949d6b-zbwkf 1/1 Running 1 15d harbor-harbor-notary-server-57dd94bf56-txdkl 1/1 Running 0 15d harbor-harbor-notary-signer-5d64c5bf8d-kppts 1/1 Running 0 15d harbor-harbor-portal-648c56499f-g28rz 1/1 Running 0 15d harbor-harbor-redis-0 1/1 Running 0 15d harbor-harbor-registry-5cd9c49489-r92ph 2/2 Running 0 15d 接下来我们创建test的私有项目用来测试。 因为我们创建的harbor仓库是https的所以在docker pull或者push镜像之前，需要先把证书加到docker对应的配置目录里，不然docker是无法登录harbor的。 进入test项目，点解“注册证书”下载harbor的CA证书。 在每个node节点创建目录（以后可能会在master上传镜像，所以此次我的master节点也都一起创建了） for n in `seq -w 01 06`;do ssh node-$n &quot;mkdir -p /etc/docker/certs.d/h.cnlinux.club&quot;;done #将下载下来的harbor CA证书拷贝到每个node节点的etc/docker/certs.d/h.cnlinux.club目录下 for n in `seq -w 01 06`;do scp ca.crt node-$n:/etc/docker/certs.d/h.cnlinux.club/;done 在node节点上功登录harbor,登录成功后的信息保存在当前用户家目录下的.docker/config.json里。 [root@node-06 ~]# docker login h.cnlinux.club Username: admin Password: Login Succeeded [root@node-06 ~]# cat .docker/config.json { &quot;auths&quot;: { &quot;h.cnlinux.club&quot;: { &quot;auth&quot;: &quot;YWRtaW46SGFyYm9yMTIzNDU=&quot; } } } 在官方docker仓库pull一个nginx镜像，任何打上tag，push到harbor仓库，如下就可以看到harbor的test项目下已经有nginx的镜像了 [root@node-06 ~]# docker pull nginx:latest [root@node-06 ~]# docker tag nginx:latest h.cnlinux.club/test/nginx:latest [root@node-06 ~]# docker push h.cnlinux.club/test/nginx:latest 问题：如果我的k8s集群很多的node节点是不是每个node节点都要上去登录才能pull harbor仓库的镜像？这样是不是就非常麻烦了？ 其实在k8s里有一种secret的类型是kubernetes.io/dockerconfigjson就是用来解决这种问题的。 首先将docker的登录信息转换成base64格式 [root@node-06 ~]# cat .docker/config.json |base64 ewoJImF1dGhzIjogewoJCSJoLmNubGludXguY2x1YiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9IgoJCX0KCX0sCgkiSHR0cEhlYWRlcnMiOiB7CgkJIlVzZXItQWdlbnQiOiAiRG9ja2VyLUNsaWVudC8xOC4wNi4xLWNlIChsaW51eCkiCgl9Cn0= 创建secret apiVersion: v1 kind: Secret metadata: name: harbor-registry-secret namespace: default data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJoLmNubGludXguY2x1YiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9IgoJCX0KCX0sCgkiSHR0cEhlYWRlcnMiOiB7CgkJIlVzZXItQWdlbnQiOiAiRG9ja2VyLUNsaWVudC8xOC4wNi4xLWNlIChsaW51eCkiCgl9Cn0= type: kubernetes.io/dockerconfigjson [root@node-01 ~]# kubectl create -f harbor-registry-secret.yaml secret/harbor-registry-secret created 创建nginx demo并使用harbor上的nginx镜像。并将nginx.cnlinux.club解析到负载均衡10.31.90.200。 apiVersion: apps/v1 kind: Deployment metadata: name: deploy-nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: h.cnlinux.club/test/nginx:latest ports: - containerPort: 80 imagePullSecrets: - name: harbor-registry-secret --- apiVersion: v1 kind: Service metadata: name: nginx spec: selector: app: nginx ports: - name: nginx protocol: TCP port: 80 targetPort: 80 type: ClusterIP --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx annotations: # nginx.ingress.kubernetes.io/rewrite-target: / kubernetes.io/ingress.class: nginx spec: rules: - host: nginx.cnlinux.club http: paths: - path: backend: serviceName: nginx servicePort: 80 可以看到3过node节点上的nginx已经运行，证明harbor上的镜像已经成功pull下来。 [root@node-01 ~]# kubectl get pod -o wide|grep nginx deploy-nginx-647f9649f5-88mkt 1/1 Running 0 2m41s 10.34.0.5 node-06 &lt;none&gt; &lt;none&gt; deploy-nginx-647f9649f5-9z842 1/1 Running 0 2m41s 10.40.0.5 node-04 &lt;none&gt; &lt;none&gt; deploy-nginx-647f9649f5-w44ck 1/1 Running 0 2m41s 10.46.0.6 node-05 &lt;none&gt; &lt;none&gt; 最后我们访问http://nginx.cnlinux.club,至此所有的都已完成。后续会陆续更新所有的k8s相关文档，如果你觉得我写的不错，希望大家多多关注点赞，如有问题可以在下面给我留言，非常感谢！ 转载于:https://blog.51cto.com/billy98/2345517" />
<link rel="canonical" href="https://uzzz.org/2019/01/22/790148.html" />
<meta property="og:url" content="https://uzzz.org/2019/01/22/790148.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-22T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"相关内容： Kubernetes部署（一）：架构及功能说明Kubernetes部署（二）：系统环境初始化Kubernetes部署（三）：CA证书制作Kubernetes部署（四）：ETCD集群部署Kubernetes部署（五）：Haproxy、Keppalived部署Kubernetes部署（六）：Master节点部署Kubernetes部署（七）：Node节点部署Kubernetes部署（八）：Flannel网络部署Kubernetes部署（九）：CoreDNS、Dashboard、Ingress部署Kubernetes部署（十）：储存之glusterfs和heketi部署Kubernetes部署（十一）：管理之Helm和Rancher部署Kubernetes部署（十二）：helm部署harbor企业级镜像仓库 harbor简介 harbor官方github：https://github.com/goharborHarbor是一个用于存储和分发Docker镜像的企业级Registry服务器。Harbor通过添加用户通常需要的功能（如安全性，身份和管理）来扩展开源Docker Distribution。使registry更接近构建和运行环境可以提高图像传输效率。Harbor支持在registry之间复制映像，还提供高级安全功能，如用户管理，访问控制和活动审计。 特征 云原生注册表：Harbour 支持容器镜像和Helm chart，可用作本地云环境（如容器运行和业务流程平台）的注册表。 基于角色的访问控制：用户和存储库通过“项目”进行组织，用户可以对项目下的镜像拥有不同的权限。 基于策略的映像复制：可以基于具有多个过滤器（存储库，标记和标签）的策略在多个注册表实例之间复制（同步）映像。如果遇到任何错误，Harbor将自动重试进行复制。非常适合负载平衡，高可用性，多数据中心，混合和多云场景。 漏洞扫描：Harbor定期扫描镜像并警告用户漏洞。 LDAP / AD支持：Harbor与现有企业LDAP / AD集成以进行用户身份验证和管理，并支持将LDAP组导入Harbor并为其分配适当的项目角色。 镜像删除和垃圾收集：可以删除图像，并可以回收它们的空间。 公证：可以确保镜像的真实性。 图形用户门户：用户可以轻松浏览，搜索存储库和管理项目。 审计：跟踪存储库的所有操作。 RESTful API：适用于大多数管理操作的RESTful API，易于与外部系统集成。 易于部署：提供在线和离线安装程序。 先决条件 Kubernetes集群 1.10+ helm 2.8.0+ Harbor部署 1. 添加域名解析。 将h.cnlinux.club和n.cnlinux.club的A记录解析到我的负载均衡IP 10.31.90.200，用于创建ingress。 2. 下载harbor的chart包 [root@node-01 harbor]# wget https://github.com/goharbor/harbor-helm/archive/1.0.0.tar.gz -O harbor-helm-v1.0.0.tar.gz 3. 修改配置文件 提取harbor-helm-v1.0.0.tar.gz文件中的values.yaml文件，并放到和harbor-helm-v1.0.0.tar.gz同一级的目录中。 修改values.yaml，我的配置修改了如下几个字段： 需要说明的是如果k8s集群中存在storageclass就可以直接用storageclass，在几个persistence.persistentVolumeClaim.XXX.storageClass中指定storageclass名就可以了，会自动创建多个pvc，但是我这里为了防止创建多个pvc增加管理难度，我在部署前创建了一个pvc，harbor下所有的服务都使用这一个pvc，具体每个字段的作用请查看官方文档https://github.com/goharbor/harbor-helm。 expose.ingress.hosts.core xpose.ingress.hosts.notary externalURL persistence.persistentVolumeClaim.registry.existingClaim persistence.persistentVolumeClaim.registry.subPath persistence.persistentVolumeClaim.chartmuseum.existingClaim persistence.persistentVolumeClaim.chartmuseum.subPath persistence.persistentVolumeClaim.jobservice.existingClaim persistence.persistentVolumeClaim.jobservice.subPath persistence.persistentVolumeClaim.database.existingClaim persistence.persistentVolumeClaim.database.subPath persistence.persistentVolumeClaim.redis.existingClaim persistence.persistentVolumeClaim.redis.subPath expose: type: ingress tls: enabled: true secretName: &quot;&quot; notarySecretName: &quot;&quot; commonName: &quot;&quot; ingress: hosts: core: h.cnlinux.club notary: n.cnlinux.club annotations: ingress.kubernetes.io/ssl-redirect: &quot;true&quot; nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot; ingress.kubernetes.io/proxy-body-size: &quot;0&quot; nginx.ingress.kubernetes.io/proxy-body-size: &quot;0&quot; clusterIP: name: harbor ports: httpPort: 80 httpsPort: 443 notaryPort: 4443 nodePort: name: harbor ports: http: port: 80 nodePort: 30002 https: port: 443 nodePort: 30003 notary: port: 4443 nodePort: 30004 externalURL: https://h.cnlinux.club persistence: enabled: true resourcePolicy: &quot;keep&quot; persistentVolumeClaim: registry: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;registry&quot; accessMode: ReadWriteOnce size: 5Gi chartmuseum: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;chartmuseum&quot; accessMode: ReadWriteOnce size: 5Gi jobservice: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;jobservice&quot; accessMode: ReadWriteOnce size: 1Gi database: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;database&quot; accessMode: ReadWriteOnce size: 1Gi redis: existingClaim: &quot;pvc-harbor&quot; storageClass: &quot;&quot; subPath: &quot;redis&quot; accessMode: ReadWriteOnce size: 1Gi imageChartStorage: type: filesystem filesystem: rootdirectory: /storage imagePullPolicy: IfNotPresent logLevel: debug harborAdminPassword: &quot;Harbor12345&quot; secretKey: &quot;not-a-secure-key&quot; nginx: image: repository: goharbor/nginx-photon tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} portal: image: repository: goharbor/harbor-portal tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} core: image: repository: goharbor/harbor-core tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} adminserver: image: repository: goharbor/harbor-adminserver tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} jobservice: image: repository: goharbor/harbor-jobservice tag: v1.7.0 replicas: 1 maxJobWorkers: 10 jobLogger: file nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} registry: registry: image: repository: goharbor/registry-photon tag: v2.6.2-v1.7.0 controller: image: repository: goharbor/harbor-registryctl tag: v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} chartmuseum: enabled: true image: repository: goharbor/chartmuseum-photon tag: v0.7.1-v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} clair: enabled: true image: repository: goharbor/clair-photon tag: v2.0.7-v1.7.0 replicas: 1 httpProxy: httpsProxy: updatersInterval: 12 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} notary: enabled: true server: image: repository: goharbor/notary-server-photon tag: v0.6.1-v1.7.0 replicas: 1 signer: image: repository: goharbor/notary-signer-photon tag: v0.6.1-v1.7.0 replicas: 1 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} database: type: internal internal: image: repository: goharbor/harbor-db tag: v1.7.0 password: &quot;changeit&quot; nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} redis: type: internal internal: image: repository: goharbor/redis-photon tag: v1.7.0 nodeSelector: {} tolerations: [] affinity: {} podAnnotations: {} 4. 创建存储卷 因为harbor需要使用到mysql，为防止mysql在调度过程中造成数据丢失，我们需要将mysql的数据存储在gluster的存储卷里。 [root@node-01 harbor]# vim pvc-harbor.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-harbor spec: storageClassName: gluster-heketi accessModes: - ReadWriteMany resources: requests: storage: 50Gi [root@node-01 harbor]# kubectl apply -f pvc-harbor.yaml 5. 安装harbor [root@node-01 harbor]# helm install --name harbor harbor-helm-v1.0.0.tar.gz -f values.yaml 如果安装不成功可以用helm del --purge harbor删除重新安装。 6. 演示 在一段时间后可以看到harbor所有相关的pod都已经运行起来了，我们就可以访问了，默认用户密码是admin/Harbor12345,可以通过修改values.yaml来更改默认的用户名和密码。 [root@node-01 ~]# kubectl get pod NAME READY STATUS RESTARTS AGE harbor-harbor-adminserver-7fffc7bf4d-vj845 1/1 Running 1 15d harbor-harbor-chartmuseum-bdf64f899-brnww 1/1 Running 0 15d harbor-harbor-clair-8457c45dd8-9rgq8 1/1 Running 1 15d harbor-harbor-core-7fc454c6d8-b6kvs 1/1 Running 1 15d harbor-harbor-database-0 1/1 Running 0 15d harbor-harbor-jobservice-7895949d6b-zbwkf 1/1 Running 1 15d harbor-harbor-notary-server-57dd94bf56-txdkl 1/1 Running 0 15d harbor-harbor-notary-signer-5d64c5bf8d-kppts 1/1 Running 0 15d harbor-harbor-portal-648c56499f-g28rz 1/1 Running 0 15d harbor-harbor-redis-0 1/1 Running 0 15d harbor-harbor-registry-5cd9c49489-r92ph 2/2 Running 0 15d 接下来我们创建test的私有项目用来测试。 因为我们创建的harbor仓库是https的所以在docker pull或者push镜像之前，需要先把证书加到docker对应的配置目录里，不然docker是无法登录harbor的。 进入test项目，点解“注册证书”下载harbor的CA证书。 在每个node节点创建目录（以后可能会在master上传镜像，所以此次我的master节点也都一起创建了） for n in `seq -w 01 06`;do ssh node-$n &quot;mkdir -p /etc/docker/certs.d/h.cnlinux.club&quot;;done #将下载下来的harbor CA证书拷贝到每个node节点的etc/docker/certs.d/h.cnlinux.club目录下 for n in `seq -w 01 06`;do scp ca.crt node-$n:/etc/docker/certs.d/h.cnlinux.club/;done 在node节点上功登录harbor,登录成功后的信息保存在当前用户家目录下的.docker/config.json里。 [root@node-06 ~]# docker login h.cnlinux.club Username: admin Password: Login Succeeded [root@node-06 ~]# cat .docker/config.json { &quot;auths&quot;: { &quot;h.cnlinux.club&quot;: { &quot;auth&quot;: &quot;YWRtaW46SGFyYm9yMTIzNDU=&quot; } } } 在官方docker仓库pull一个nginx镜像，任何打上tag，push到harbor仓库，如下就可以看到harbor的test项目下已经有nginx的镜像了 [root@node-06 ~]# docker pull nginx:latest [root@node-06 ~]# docker tag nginx:latest h.cnlinux.club/test/nginx:latest [root@node-06 ~]# docker push h.cnlinux.club/test/nginx:latest 问题：如果我的k8s集群很多的node节点是不是每个node节点都要上去登录才能pull harbor仓库的镜像？这样是不是就非常麻烦了？ 其实在k8s里有一种secret的类型是kubernetes.io/dockerconfigjson就是用来解决这种问题的。 首先将docker的登录信息转换成base64格式 [root@node-06 ~]# cat .docker/config.json |base64 ewoJImF1dGhzIjogewoJCSJoLmNubGludXguY2x1YiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9IgoJCX0KCX0sCgkiSHR0cEhlYWRlcnMiOiB7CgkJIlVzZXItQWdlbnQiOiAiRG9ja2VyLUNsaWVudC8xOC4wNi4xLWNlIChsaW51eCkiCgl9Cn0= 创建secret apiVersion: v1 kind: Secret metadata: name: harbor-registry-secret namespace: default data: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJoLmNubGludXguY2x1YiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9IgoJCX0KCX0sCgkiSHR0cEhlYWRlcnMiOiB7CgkJIlVzZXItQWdlbnQiOiAiRG9ja2VyLUNsaWVudC8xOC4wNi4xLWNlIChsaW51eCkiCgl9Cn0= type: kubernetes.io/dockerconfigjson [root@node-01 ~]# kubectl create -f harbor-registry-secret.yaml secret/harbor-registry-secret created 创建nginx demo并使用harbor上的nginx镜像。并将nginx.cnlinux.club解析到负载均衡10.31.90.200。 apiVersion: apps/v1 kind: Deployment metadata: name: deploy-nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: h.cnlinux.club/test/nginx:latest ports: - containerPort: 80 imagePullSecrets: - name: harbor-registry-secret --- apiVersion: v1 kind: Service metadata: name: nginx spec: selector: app: nginx ports: - name: nginx protocol: TCP port: 80 targetPort: 80 type: ClusterIP --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx annotations: # nginx.ingress.kubernetes.io/rewrite-target: / kubernetes.io/ingress.class: nginx spec: rules: - host: nginx.cnlinux.club http: paths: - path: backend: serviceName: nginx servicePort: 80 可以看到3过node节点上的nginx已经运行，证明harbor上的镜像已经成功pull下来。 [root@node-01 ~]# kubectl get pod -o wide|grep nginx deploy-nginx-647f9649f5-88mkt 1/1 Running 0 2m41s 10.34.0.5 node-06 &lt;none&gt; &lt;none&gt; deploy-nginx-647f9649f5-9z842 1/1 Running 0 2m41s 10.40.0.5 node-04 &lt;none&gt; &lt;none&gt; deploy-nginx-647f9649f5-w44ck 1/1 Running 0 2m41s 10.46.0.6 node-05 &lt;none&gt; &lt;none&gt; 最后我们访问http://nginx.cnlinux.club,至此所有的都已完成。后续会陆续更新所有的k8s相关文档，如果你觉得我写的不错，希望大家多多关注点赞，如有问题可以在下面给我留言，非常感谢！ 转载于:https://blog.51cto.com/billy98/2345517","@type":"BlogPosting","url":"https://uzzz.org/2019/01/22/790148.html","headline":"Kubernetes部署（十二）：helm部署harbor企业级镜像仓库","dateModified":"2019-01-22T00:00:00+08:00","datePublished":"2019-01-22T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/01/22/790148.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Kubernetes部署（十二）：helm部署harbor企业级镜像仓库</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix" data-track-view="{&quot;mod&quot;:&quot;popu_307&quot;,&quot;con&quot;:&quot;,https://blog.csdn.net/weixin_34337265/article/details/91613957,&quot;}" data-track-click="{&quot;mod&quot;:&quot;popu_307&quot;,&quot;con&quot;:&quot;,https://blog.csdn.net/weixin_34337265/article/details/91613957&quot;}"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-cd6c485e8b.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-cd6c485e8b.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="con artical-content editor-preview-side">
   <h3>相关内容：</h3> 
   <p><a href="https://blog.51cto.com/billy98/2334654" rel="nofollow">Kubernetes部署（一）：架构及功能说明</a><br><a href="https://blog.51cto.com/billy98/2334685" rel="nofollow">Kubernetes部署（二）：系统环境初始化</a><br><a href="https://blog.51cto.com/billy98/2334704" rel="nofollow">Kubernetes部署（三）：CA证书制作</a><br><a href="https://blog.51cto.com/billy98/2334706" rel="nofollow">Kubernetes部署（四）：ETCD集群部署</a><br><a href="https://blog.51cto.com/billy98/2335056" rel="nofollow">Kubernetes部署（五）：Haproxy、Keppalived部署</a><br><a href="https://blog.51cto.com/billy98/2335161" rel="nofollow">Kubernetes部署（六）：Master节点部署</a><br><a href="https://blog.51cto.com/billy98/2335575" rel="nofollow">Kubernetes部署（七）：Node节点部署</a><br><a href="https://blog.51cto.com/billy98/2335580" rel="nofollow">Kubernetes部署（八）：Flannel网络部署</a><br><a href="https://blog.51cto.com/billy98/2336724" rel="nofollow">Kubernetes部署（九）：CoreDNS、Dashboard、Ingress部署</a><br><a href="https://blog.51cto.com/billy98/2337874" rel="nofollow">Kubernetes部署（十）：储存之glusterfs和heketi部署</a><br><a href="https://blog.51cto.com/billy98/2338415" rel="nofollow">Kubernetes部署（十一）：管理之Helm和Rancher部署</a><br><a href="https://blog.51cto.com/billy98/2345517" rel="nofollow">Kubernetes部署（十二）：helm部署harbor企业级镜像仓库</a></p> 
   <pre><code></code></pre> 
   <h3>harbor简介</h3> 
   <p><strong>harbor官方github</strong>：<a href="https://github.com/goharbor" rel="nofollow">https://github.com/goharbor</a><br>Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器。Harbor通过添加用户通常需要的功能（如安全性，身份和管理）来扩展开源Docker Distribution。使registry更接近构建和运行环境可以提高图像传输效率。Harbor支持在registry之间复制映像，还提供高级安全功能，如用户管理，访问控制和活动审计。</p> 
   <h3>特征</h3> 
   <ul>
    <li><strong>云原生注册表</strong>：Harbour 支持容器镜像和Helm chart，可用作本地云环境（如容器运行和业务流程平台）的注册表。</li> 
    <li><strong>基于角色的访问控制</strong>：用户和存储库通过“项目”进行组织，用户可以对项目下的镜像拥有不同的权限。</li> 
    <li><strong>基于策略的映像复制</strong>：可以基于具有多个过滤器（存储库，标记和标签）的策略在多个注册表实例之间复制（同步）映像。如果遇到任何错误，Harbor将自动重试进行复制。非常适合负载平衡，高可用性，多数据中心，混合和多云场景。</li> 
    <li><strong>漏洞扫描</strong>：Harbor定期扫描镜像并警告用户漏洞。</li> 
    <li><strong>LDAP / AD支持</strong>：Harbor与现有企业LDAP / AD集成以进行用户身份验证和管理，并支持将LDAP组导入Harbor并为其分配适当的项目角色。</li> 
    <li><strong>镜像删除和垃圾收集</strong>：可以删除图像，并可以回收它们的空间。</li> 
    <li><strong>公证</strong>：可以确保镜像的真实性。</li> 
    <li><strong>图形用户门户</strong>：用户可以轻松浏览，搜索存储库和管理项目。</li> 
    <li><strong>审计</strong>：跟踪存储库的所有操作。</li> 
    <li><strong>RESTful API</strong>：适用于大多数管理操作的RESTful API，易于与外部系统集成。</li> 
    <li><strong>易于部署</strong>：提供在线和离线安装程序。 <h3>先决条件</h3></li> 
    <li>Kubernetes集群 1.10+</li> 
    <li>helm 2.8.0+</li> 
   </ul>
   <h2>Harbor部署</h2> 
   <h3>1. 添加域名解析。</h3> 
   <p>将<code>h.cnlinux.club</code>和<code>n.cnlinux.club</code>的A记录解析到我的负载均衡IP <code>10.31.90.200</code>，用于创建ingress。</p> 
   <h3>2. 下载harbor的chart包</h3> 
   <pre><code>[root@node-01 harbor]# wget https://github.com/goharbor/harbor-helm/archive/1.0.0.tar.gz -O harbor-helm-v1.0.0.tar.gz</code></pre> 
   <h3>3. 修改配置文件</h3> 
   <ul>
    <li>提取<code>harbor-helm-v1.0.0.tar.gz</code>文件中的<code>values.yaml</code>文件，并放到和harbor-helm-v1.0.0.tar.gz同一级的目录中。</li> 
    <li> <p>修改values.yaml，我的配置修改了如下几个字段：</p> 
     <blockquote> 
      <p>需要说明的是如果k8s集群中存在storageclass就可以直接用storageclass，在几个persistence.persistentVolumeClaim.XXX.storageClass中指定storageclass名就可以了，会自动创建多个pvc，但是我这里为了防止创建多个pvc增加管理难度，我在部署前创建了一个pvc，harbor下所有的服务都使用这一个pvc，具体每个字段的作用请查看官方文档<a href="https://github.com/goharbor/harbor-helm" rel="nofollow"></a><a href="https://github.com/goharbor/harbor-helm" rel="nofollow">https://github.com/goharbor/harbor-helm</a>。</p> 
     </blockquote> 
     <ul>
      <li><code>expose.ingress.hosts.core</code></li> 
      <li><code>xpose.ingress.hosts.notary</code></li> 
      <li><code>externalURL</code></li> 
      <li><code>persistence.persistentVolumeClaim.registry.existingClaim</code></li> 
      <li><code>persistence.persistentVolumeClaim.registry.subPath</code></li> 
      <li><code>persistence.persistentVolumeClaim.chartmuseum.existingClaim</code></li> 
      <li><code>persistence.persistentVolumeClaim.chartmuseum.subPath</code></li> 
      <li><code>persistence.persistentVolumeClaim.jobservice.existingClaim</code></li> 
      <li><code>persistence.persistentVolumeClaim.jobservice.subPath</code></li> 
      <li><code>persistence.persistentVolumeClaim.database.existingClaim</code></li> 
      <li><code>persistence.persistentVolumeClaim.database.subPath</code></li> 
      <li><code>persistence.persistentVolumeClaim.redis.existingClaim</code></li> 
      <li><code>persistence.persistentVolumeClaim.redis.subPath</code></li> 
     </ul></li> 
   </ul>
   <pre><code>expose:
  type: ingress
  tls:
    enabled: true
    secretName: ""
    notarySecretName: ""
    commonName: ""
  ingress:
    hosts:
      core: h.cnlinux.club
      notary: n.cnlinux.club
    annotations:
      ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      ingress.kubernetes.io/proxy-body-size: "0"
      nginx.ingress.kubernetes.io/proxy-body-size: "0"
  clusterIP:
    name: harbor
    ports:
      httpPort: 80
      httpsPort: 443
      notaryPort: 4443
  nodePort:
    name: harbor
    ports:
      http:
        port: 80
        nodePort: 30002
      https: 
        port: 443
        nodePort: 30003
      notary: 
        port: 4443
        nodePort: 30004
externalURL: https://h.cnlinux.club
persistence:
  enabled: true
  resourcePolicy: "keep"
  persistentVolumeClaim:
    registry:
      existingClaim: "pvc-harbor"
      storageClass: ""
      subPath: "registry"
      accessMode: ReadWriteOnce
      size: 5Gi
    chartmuseum:
      existingClaim: "pvc-harbor"
      storageClass: ""
      subPath: "chartmuseum"
      accessMode: ReadWriteOnce
      size: 5Gi
    jobservice:
      existingClaim: "pvc-harbor"
      storageClass: ""
      subPath: "jobservice"
      accessMode: ReadWriteOnce
      size: 1Gi
    database:
      existingClaim: "pvc-harbor"
      storageClass: ""
      subPath: "database"
      accessMode: ReadWriteOnce
      size: 1Gi
    redis:
      existingClaim: "pvc-harbor"
      storageClass: ""
      subPath: "redis"
      accessMode: ReadWriteOnce
      size: 1Gi
  imageChartStorage:
    type: filesystem
    filesystem:
      rootdirectory: /storage
imagePullPolicy: IfNotPresent
logLevel: debug
harborAdminPassword: "Harbor12345"
secretKey: "not-a-secure-key"
nginx:
  image:
    repository: goharbor/nginx-photon
    tag: v1.7.0
  replicas: 1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
portal:
  image:
    repository: goharbor/harbor-portal
    tag: v1.7.0
  replicas: 1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
core:
  image:
    repository: goharbor/harbor-core
    tag: v1.7.0
  replicas: 1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
adminserver:
  image:
    repository: goharbor/harbor-adminserver
    tag: v1.7.0
  replicas: 1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
jobservice:
  image:
    repository: goharbor/harbor-jobservice
    tag: v1.7.0
  replicas: 1
  maxJobWorkers: 10
  jobLogger: file
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
registry:
  registry:
    image:
      repository: goharbor/registry-photon
      tag: v2.6.2-v1.7.0
  controller:
    image:
      repository: goharbor/harbor-registryctl
      tag: v1.7.0
  replicas: 1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
chartmuseum:
  enabled: true
  image:
    repository: goharbor/chartmuseum-photon
    tag: v0.7.1-v1.7.0
  replicas: 1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
clair:
  enabled: true
  image:
    repository: goharbor/clair-photon
    tag: v2.0.7-v1.7.0
  replicas: 1
  httpProxy:
  httpsProxy:
  updatersInterval: 12
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
notary:
  enabled: true
  server:
    image:
      repository: goharbor/notary-server-photon
      tag: v0.6.1-v1.7.0
    replicas: 1
  signer:
    image:
      repository: goharbor/notary-signer-photon
      tag: v0.6.1-v1.7.0
    replicas: 1
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
database:
  type: internal
  internal:
    image:
      repository: goharbor/harbor-db
      tag: v1.7.0
    password: "changeit"
    nodeSelector: {}
    tolerations: []
    affinity: {}
  podAnnotations: {}
redis:
  type: internal
  internal:
    image:
      repository: goharbor/redis-photon
      tag: v1.7.0
    nodeSelector: {}
    tolerations: []
    affinity: {}
  podAnnotations: {}</code></pre> 
   <h3>4. 创建存储卷</h3> 
   <p>因为harbor需要使用到mysql，为防止mysql在调度过程中造成数据丢失，我们需要将mysql的数据存储在gluster的存储卷里。</p> 
   <pre><code>[root@node-01 harbor]# vim pvc-harbor.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-harbor
spec:
  storageClassName: gluster-heketi
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi</code></pre> 
   <pre><code>[root@node-01 harbor]# kubectl apply -f pvc-harbor.yaml </code></pre> 
   <h3>5. 安装harbor</h3> 
   <pre><code>[root@node-01 harbor]# helm install  --name harbor harbor-helm-v1.0.0.tar.gz -f values.yaml </code></pre> 
   <blockquote> 
    <p>如果安装不成功可以用<code>helm del --purge harbor</code>删除重新安装。</p> 
   </blockquote> 
   <h3>6. 演示</h3> 
   <p>在一段时间后可以看到harbor所有相关的pod都已经运行起来了，我们就可以访问了，默认用户密码是admin/Harbor12345,可以通过修改values.yaml来更改默认的用户名和密码。</p> 
   <pre><code>[root@node-01 ~]# kubectl get pod
NAME                                           READY     STATUS    RESTARTS   AGE
harbor-harbor-adminserver-7fffc7bf4d-vj845     1/1       Running   1          15d
harbor-harbor-chartmuseum-bdf64f899-brnww      1/1       Running   0          15d
harbor-harbor-clair-8457c45dd8-9rgq8           1/1       Running   1          15d
harbor-harbor-core-7fc454c6d8-b6kvs            1/1       Running   1          15d
harbor-harbor-database-0                       1/1       Running   0          15d
harbor-harbor-jobservice-7895949d6b-zbwkf      1/1       Running   1          15d
harbor-harbor-notary-server-57dd94bf56-txdkl   1/1       Running   0          15d
harbor-harbor-notary-signer-5d64c5bf8d-kppts   1/1       Running   0          15d
harbor-harbor-portal-648c56499f-g28rz          1/1       Running   0          15d
harbor-harbor-redis-0                          1/1       Running   0          15d
harbor-harbor-registry-5cd9c49489-r92ph        2/2       Running   0          15d</code></pre> 
   <p><img src="https://s1.51cto.com/images/blog/201901/22/b7ebcae705fd8b56036d32164d762b9f.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt="Kubernetes部署（十二）：helm部署harbor企业级镜像仓库"></p> 
   <ul>
    <li> <p>接下来我们创建test的私有项目用来测试。<br><img src="https://s1.51cto.com/images/blog/201901/22/d44d493c31e1977062c293ea351eedeb.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt="Kubernetes部署（十二）：helm部署harbor企业级镜像仓库"></p> </li> 
    <li>因为我们创建的harbor仓库是https的所以在docker pull或者push镜像之前，需要先把证书加到docker对应的配置目录里，不然docker是无法登录harbor的。</li> 
    <li>进入test项目，点解“注册证书”下载harbor的CA证书。<br><img src="https://s1.51cto.com/images/blog/201901/22/ce6a0c7d0a42de922d803d15e5d00bae.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt="Kubernetes部署（十二）：helm部署harbor企业级镜像仓库"></li> 
    <li>在每个node节点创建目录（以后可能会在master上传镜像，所以此次我的master节点也都一起创建了）</li> 
   </ul>
   <pre><code>for n in `seq -w 01 06`;do ssh node-$n "mkdir -p /etc/docker/certs.d/h.cnlinux.club";done
#将下载下来的harbor CA证书拷贝到每个node节点的etc/docker/certs.d/h.cnlinux.club目录下
for n in `seq -w 01 06`;do scp ca.crt node-$n:/etc/docker/certs.d/h.cnlinux.club/;done</code></pre> 
   <ul>
    <li>在node节点上功登录harbor,登录成功后的信息保存在当前用户家目录下的<code>.docker/config.json</code>里。</li> 
   </ul>
   <pre><code>[root@node-06 ~]# docker login h.cnlinux.club
Username: admin
Password: 
Login Succeeded

[root@node-06 ~]# cat .docker/config.json 
{
        "auths": {
                "h.cnlinux.club": {
                        "auth": "YWRtaW46SGFyYm9yMTIzNDU="
                }
        }
}</code></pre> 
   <ul>
    <li>在官方docker仓库pull一个nginx镜像，任何打上tag，push到harbor仓库，如下就可以看到harbor的test项目下已经有nginx的镜像了 <pre><code>[root@node-06 ~]# docker pull nginx:latest
[root@node-06 ~]# docker tag nginx:latest h.cnlinux.club/test/nginx:latest
[root@node-06 ~]# docker push h.cnlinux.club/test/nginx:latest</code></pre> <p><img src="https://s1.51cto.com/images/blog/201901/22/79c49aa961339db1d6c4a5861b0c6a74.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt="Kubernetes部署（十二）：helm部署harbor企业级镜像仓库"></p></li> 
   </ul>
   <p><strong>问题：如果我的k8s集群很多的node节点是不是每个node节点都要上去登录才能pull harbor仓库的镜像？这样是不是就非常麻烦了？</strong></p> 
   <ul>
    <li>其实在k8s里有一种secret的类型是<code>kubernetes.io/dockerconfigjson</code>就是用来解决这种问题的。</li> 
    <li>首先将docker的登录信息转换成base64格式</li> 
   </ul>
   <pre><code>[root@node-06 ~]# cat .docker/config.json |base64
ewoJImF1dGhzIjogewoJCSJoLmNubGludXguY2x1YiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9IgoJCX0KCX0sCgkiSHR0cEhlYWRlcnMiOiB7CgkJIlVzZXItQWdlbnQiOiAiRG9ja2VyLUNsaWVudC8xOC4wNi4xLWNlIChsaW51eCkiCgl9Cn0=</code></pre> 
   <ul>
    <li>创建secret <pre><code>apiVersion: v1
kind: Secret
metadata:
name: harbor-registry-secret
namespace: default
data:
.dockerconfigjson: ewoJImF1dGhzIjogewoJCSJoLmNubGludXguY2x1YiI6IHsKCQkJImF1dGgiOiAiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9IgoJCX0KCX0sCgkiSHR0cEhlYWRlcnMiOiB7CgkJIlVzZXItQWdlbnQiOiAiRG9ja2VyLUNsaWVudC8xOC4wNi4xLWNlIChsaW51eCkiCgl9Cn0=
type: kubernetes.io/dockerconfigjson</code></pre> <pre><code>[root@node-01 ~]# kubectl create -f harbor-registry-secret.yaml 
secret/harbor-registry-secret created</code></pre></li> 
    <li>创建nginx demo并使用harbor上的nginx镜像。并将nginx.cnlinux.club解析到负载均衡<code>10.31.90.200</code>。</li> 
   </ul>
   <pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-nginx
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template: 
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: h.cnlinux.club/test/nginx:latest
        ports:
        - containerPort: 80
      imagePullSecrets:
        - name: harbor-registry-secret
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports:
  - name: nginx
    protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP 

---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx
  annotations:
    # nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: nginx
spec:
  rules:
    - host: nginx.cnlinux.club
      http:
        paths:
          - path: 
            backend:
              serviceName: nginx
              servicePort: 80</code></pre> 
   <ul>
    <li>可以看到3过node节点上的nginx已经运行，证明harbor上的镜像已经成功pull下来。 <pre><code>[root@node-01 ~]# kubectl get pod -o wide|grep nginx 
deploy-nginx-647f9649f5-88mkt                  1/1     Running            0          2m41s   10.34.0.5      node-06   &lt;none&gt;           &lt;none&gt;
deploy-nginx-647f9649f5-9z842                  1/1     Running            0          2m41s   10.40.0.5      node-04   &lt;none&gt;           &lt;none&gt;
deploy-nginx-647f9649f5-w44ck                  1/1     Running            0          2m41s   10.46.0.6      node-05   &lt;none&gt;           &lt;none&gt;</code></pre> <p>最后我们访问<code>http://nginx.cnlinux.club</code>,至此所有的都已完成。<br><img src="https://s1.51cto.com/images/blog/201901/22/c0179eccafaabc8586b69544d546b4f3.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt="Kubernetes部署（十二）：helm部署harbor企业级镜像仓库"><br><strong><br>后续会陆续更新所有的k8s相关文档，如果你觉得我写的不错，希望大家多多关注点赞，如有问题可以在下面给我留言，非常感谢！</strong></p></li> 
   </ul>
  </div> 
  <p>转载于:https://blog.51cto.com/billy98/2345517</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
