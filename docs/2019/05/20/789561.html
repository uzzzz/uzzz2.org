<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>CEPH 不同类型磁盘 加入不同pool | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="CEPH 不同类型磁盘 加入不同pool" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="版本：(L版之前 需要手动修改CRUSH map) # ceph --version ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable) 因是测试环境 只在虚拟机上加了两块磁盘做OSD 将class修改了 一块为hdd 一块为ssd #ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.03998 root default -2 0 host 192.168.60.99 -5 0.03998 host control01 1 hdd 0.01999 osd.1 up 1.00000 1.00000 0 ssd 0.01999 osd.0 up 1.00000 1.00000 # ceph osd crush class ls [ &quot;ssd&quot;, &quot;hdd&quot; ] 创建两条rule host分别指定为ssd/hdd # ceph osd crush rule create-replicated rule-ssd default host ssd # ceph osd crush rule create-replicated rule-hdd default host hdd 修改pool绑定的rule # ceph osd pool set images crush_rule rule-hdd # ceph osd pool set backups crush_rule rule-hdd # ceph osd pool set volumes crush_rule rule-ssd # ceph osd pool set vms crush_rule rule-ssd 查看绑定后信息 # ceph osd pool ls detail pool 1 &#39;images&#39; replicated size 1 min_size 1 crush_rule 3 object_hash rjenkins pg_num 128 pgp_num 128 last_change 60 flags hashpspool stripe_width 0 application rbd removed_snaps [1~3] pool 2 &#39;volumes&#39; replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 128 pgp_num 128 last_change 64 flags hashpspool stripe_width 0 application rbd removed_snaps [1~3] pool 3 &#39;backups&#39; replicated size 1 min_size 1 crush_rule 3 object_hash rjenkins pg_num 128 pgp_num 128 last_change 62 flags hashpspool stripe_width 0 application rbd pool 4 &#39;vms&#39; replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 128 pgp_num 128 last_change 66 flags hashpspool stripe_width 0 application rbd removed_snaps [1~3] 上传一个镜像文件到images pool中 测试是不是到指定的OSD # glance image-show 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 +------------------+----------------------------------------------------------------------------------+ | Property | Value | +------------------+----------------------------------------------------------------------------------+ | checksum | ecc9a5132e7a0f11a4c585f513cd0873 | | container_format | bare | | created_at | 2019-05-16T17:13:42Z | | direct_url | rbd://20660ae0-3a39-40b4-9cbe-97cc046cbe40/images/1cf5aa5a-cc08-45cf-9d1e- | | | 5d9b77cd4e63/snap | | disk_format | qcow2 | | id | 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 | | locations | [{&quot;url&quot;: &quot;rbd://20660ae0-3a39-40b4-9cbe-97cc046cbe40/images/1cf5aa5a-cc08-45cf- | | | 9d1e-5d9b77cd4e63/snap&quot;, &quot;metadata&quot;: {}}] | | min_disk | 0 | | min_ram | 0 | | name | cirros520 | | owner | 7447893a665043fda4dcf573c5061173 | | protected | False | | size | 15731712 | | status | active | | tags | [] | | updated_at | 2019-05-16T17:13:43Z | | virtual_size | None | | visibility | shared | +------------------+----------------------------------------------------------------------------------+ # rbd ls images |grep 1cf5a 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 # ceph osd map images 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 osdmap e68 pool &#39;images&#39; (1) object &#39;1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63&#39; -&gt; pg 1.70ef4ae7 (1.67) -&gt; up ([1], p1) acting ([1], p1) &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="版本：(L版之前 需要手动修改CRUSH map) # ceph --version ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable) 因是测试环境 只在虚拟机上加了两块磁盘做OSD 将class修改了 一块为hdd 一块为ssd #ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.03998 root default -2 0 host 192.168.60.99 -5 0.03998 host control01 1 hdd 0.01999 osd.1 up 1.00000 1.00000 0 ssd 0.01999 osd.0 up 1.00000 1.00000 # ceph osd crush class ls [ &quot;ssd&quot;, &quot;hdd&quot; ] 创建两条rule host分别指定为ssd/hdd # ceph osd crush rule create-replicated rule-ssd default host ssd # ceph osd crush rule create-replicated rule-hdd default host hdd 修改pool绑定的rule # ceph osd pool set images crush_rule rule-hdd # ceph osd pool set backups crush_rule rule-hdd # ceph osd pool set volumes crush_rule rule-ssd # ceph osd pool set vms crush_rule rule-ssd 查看绑定后信息 # ceph osd pool ls detail pool 1 &#39;images&#39; replicated size 1 min_size 1 crush_rule 3 object_hash rjenkins pg_num 128 pgp_num 128 last_change 60 flags hashpspool stripe_width 0 application rbd removed_snaps [1~3] pool 2 &#39;volumes&#39; replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 128 pgp_num 128 last_change 64 flags hashpspool stripe_width 0 application rbd removed_snaps [1~3] pool 3 &#39;backups&#39; replicated size 1 min_size 1 crush_rule 3 object_hash rjenkins pg_num 128 pgp_num 128 last_change 62 flags hashpspool stripe_width 0 application rbd pool 4 &#39;vms&#39; replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 128 pgp_num 128 last_change 66 flags hashpspool stripe_width 0 application rbd removed_snaps [1~3] 上传一个镜像文件到images pool中 测试是不是到指定的OSD # glance image-show 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 +------------------+----------------------------------------------------------------------------------+ | Property | Value | +------------------+----------------------------------------------------------------------------------+ | checksum | ecc9a5132e7a0f11a4c585f513cd0873 | | container_format | bare | | created_at | 2019-05-16T17:13:42Z | | direct_url | rbd://20660ae0-3a39-40b4-9cbe-97cc046cbe40/images/1cf5aa5a-cc08-45cf-9d1e- | | | 5d9b77cd4e63/snap | | disk_format | qcow2 | | id | 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 | | locations | [{&quot;url&quot;: &quot;rbd://20660ae0-3a39-40b4-9cbe-97cc046cbe40/images/1cf5aa5a-cc08-45cf- | | | 9d1e-5d9b77cd4e63/snap&quot;, &quot;metadata&quot;: {}}] | | min_disk | 0 | | min_ram | 0 | | name | cirros520 | | owner | 7447893a665043fda4dcf573c5061173 | | protected | False | | size | 15731712 | | status | active | | tags | [] | | updated_at | 2019-05-16T17:13:43Z | | virtual_size | None | | visibility | shared | +------------------+----------------------------------------------------------------------------------+ # rbd ls images |grep 1cf5a 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 # ceph osd map images 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 osdmap e68 pool &#39;images&#39; (1) object &#39;1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63&#39; -&gt; pg 1.70ef4ae7 (1.67) -&gt; up ([1], p1) acting ([1], p1) &nbsp; &nbsp; &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/05/20/789561.html" />
<meta property="og:url" content="https://uzzz.org/2019/05/20/789561.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-20T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"版本：(L版之前 需要手动修改CRUSH map) # ceph --version ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable) 因是测试环境 只在虚拟机上加了两块磁盘做OSD 将class修改了 一块为hdd 一块为ssd #ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.03998 root default -2 0 host 192.168.60.99 -5 0.03998 host control01 1 hdd 0.01999 osd.1 up 1.00000 1.00000 0 ssd 0.01999 osd.0 up 1.00000 1.00000 # ceph osd crush class ls [ &quot;ssd&quot;, &quot;hdd&quot; ] 创建两条rule host分别指定为ssd/hdd # ceph osd crush rule create-replicated rule-ssd default host ssd # ceph osd crush rule create-replicated rule-hdd default host hdd 修改pool绑定的rule # ceph osd pool set images crush_rule rule-hdd # ceph osd pool set backups crush_rule rule-hdd # ceph osd pool set volumes crush_rule rule-ssd # ceph osd pool set vms crush_rule rule-ssd 查看绑定后信息 # ceph osd pool ls detail pool 1 &#39;images&#39; replicated size 1 min_size 1 crush_rule 3 object_hash rjenkins pg_num 128 pgp_num 128 last_change 60 flags hashpspool stripe_width 0 application rbd removed_snaps [1~3] pool 2 &#39;volumes&#39; replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 128 pgp_num 128 last_change 64 flags hashpspool stripe_width 0 application rbd removed_snaps [1~3] pool 3 &#39;backups&#39; replicated size 1 min_size 1 crush_rule 3 object_hash rjenkins pg_num 128 pgp_num 128 last_change 62 flags hashpspool stripe_width 0 application rbd pool 4 &#39;vms&#39; replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 128 pgp_num 128 last_change 66 flags hashpspool stripe_width 0 application rbd removed_snaps [1~3] 上传一个镜像文件到images pool中 测试是不是到指定的OSD # glance image-show 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 +------------------+----------------------------------------------------------------------------------+ | Property | Value | +------------------+----------------------------------------------------------------------------------+ | checksum | ecc9a5132e7a0f11a4c585f513cd0873 | | container_format | bare | | created_at | 2019-05-16T17:13:42Z | | direct_url | rbd://20660ae0-3a39-40b4-9cbe-97cc046cbe40/images/1cf5aa5a-cc08-45cf-9d1e- | | | 5d9b77cd4e63/snap | | disk_format | qcow2 | | id | 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 | | locations | [{&quot;url&quot;: &quot;rbd://20660ae0-3a39-40b4-9cbe-97cc046cbe40/images/1cf5aa5a-cc08-45cf- | | | 9d1e-5d9b77cd4e63/snap&quot;, &quot;metadata&quot;: {}}] | | min_disk | 0 | | min_ram | 0 | | name | cirros520 | | owner | 7447893a665043fda4dcf573c5061173 | | protected | False | | size | 15731712 | | status | active | | tags | [] | | updated_at | 2019-05-16T17:13:43Z | | virtual_size | None | | visibility | shared | +------------------+----------------------------------------------------------------------------------+ # rbd ls images |grep 1cf5a 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 # ceph osd map images 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63 osdmap e68 pool &#39;images&#39; (1) object &#39;1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63&#39; -&gt; pg 1.70ef4ae7 (1.67) -&gt; up ([1], p1) acting ([1], p1) &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/05/20/789561.html","headline":"CEPH 不同类型磁盘 加入不同pool","dateModified":"2019-05-20T00:00:00+08:00","datePublished":"2019-05-20T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/05/20/789561.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>CEPH 不同类型磁盘 加入不同pool</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-cd6c485e8b.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-cd6c485e8b.css"> 
 <div class="htmledit_views" id="content_views"> 
  <pre class="has">
<code>版本：(L版之前 需要手动修改CRUSH map)
# ceph --version
ceph version 12.2.5 (cad919881333ac92274171586c827e01f554a70a) luminous (stable)

因是测试环境 只在虚拟机上加了两块磁盘做OSD  将class修改了 一块为hdd  一块为ssd
#ceph osd tree
ID CLASS WEIGHT  TYPE NAME              STATUS REWEIGHT PRI-AFF 
-1       0.03998 root default                                   
-2             0     host 192.168.60.99                         
-5       0.03998     host control01                             
 1   hdd 0.01999         osd.1              up  1.00000 1.00000 
 0   ssd 0.01999         osd.0              up  1.00000 1.00000 


# ceph osd crush class ls
[
    "ssd",
    "hdd"
]


创建两条rule host分别指定为ssd/hdd
# ceph osd crush rule create-replicated rule-ssd default  host ssd
# ceph osd crush rule create-replicated rule-hdd default  host hdd


修改pool绑定的rule
# ceph osd pool set images crush_rule rule-hdd
# ceph osd pool set backups crush_rule rule-hdd
# ceph osd pool set volumes crush_rule rule-ssd
# ceph osd pool set vms crush_rule rule-ssd

查看绑定后信息
# ceph osd pool ls detail 
pool 1 'images' replicated size 1 min_size 1 crush_rule 3 object_hash rjenkins pg_num 128 pgp_num 128 last_change 60 flags hashpspool stripe_width 0 application rbd
	removed_snaps [1~3]
pool 2 'volumes' replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 128 pgp_num 128 last_change 64 flags hashpspool stripe_width 0 application rbd
	removed_snaps [1~3]
pool 3 'backups' replicated size 1 min_size 1 crush_rule 3 object_hash rjenkins pg_num 128 pgp_num 128 last_change 62 flags hashpspool stripe_width 0 application rbd
pool 4 'vms' replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 128 pgp_num 128 last_change 66 flags hashpspool stripe_width 0 application rbd
	removed_snaps [1~3]




上传一个镜像文件到images pool中 测试是不是到指定的OSD

# glance image-show 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63
+------------------+----------------------------------------------------------------------------------+
| Property         | Value                                                                            |
+------------------+----------------------------------------------------------------------------------+
| checksum         | ecc9a5132e7a0f11a4c585f513cd0873                                                 |
| container_format | bare                                                                             |
| created_at       | 2019-05-16T17:13:42Z                                                             |
| direct_url       | rbd://20660ae0-3a39-40b4-9cbe-97cc046cbe40/images/1cf5aa5a-cc08-45cf-9d1e-       |
|                  | 5d9b77cd4e63/snap                                                                |
| disk_format      | qcow2                                                                            |
| id               | 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63                                             |
| locations        | [{"url": "rbd://20660ae0-3a39-40b4-9cbe-97cc046cbe40/images/1cf5aa5a-cc08-45cf-  |
|                  | 9d1e-5d9b77cd4e63/snap", "metadata": {}}]                                        |
| min_disk         | 0                                                                                |
| min_ram          | 0                                                                                |
| name             | cirros520                                                                        |
| owner            | 7447893a665043fda4dcf573c5061173                                                 |
| protected        | False                                                                            |
| size             | 15731712                                                                         |
| status           | active                                                                           |
| tags             | []                                                                               |
| updated_at       | 2019-05-16T17:13:43Z                                                             |
| virtual_size     | None                                                                             |
| visibility       | shared                                                                           |
+------------------+----------------------------------------------------------------------------------+


# rbd ls images |grep 1cf5a
1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63


# ceph osd map images 1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63
osdmap e68 pool 'images' (1) object '1cf5aa5a-cc08-45cf-9d1e-5d9b77cd4e63' -&gt; pg 1.70ef4ae7 (1.67) -&gt; up ([1], p1) acting ([1], p1)
</code></pre> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
