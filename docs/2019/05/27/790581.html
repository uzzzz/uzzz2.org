<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>诊断修复 TiDB Operator 在 K8s 测试中遇到的 Linux 内核问题 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="诊断修复 TiDB Operator 在 K8s 测试中遇到的 Linux 内核问题" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="原文链接： https://juejin.im/post/5ceb78736fb9a07ecd3d3a69 作者：张文博 Kubernetes（K8s）是一个开源容器编排系统，可自动执行应用程序部署、扩展和管理。它是云原生世界的操作系统。 K8s 或操作系统中的任何缺陷都可能使用户进程存在风险。作为 PingCAP EE（效率工程）团队，我们在 K8s 中测试 TiDB Operator（一个创建和管理 TiDB 集群的工具）时，发现了两个 Linux 内核错误。这些错误已经困扰我们很长一段时间，并没有在整个 K8s 社区中彻底修复。 经过广泛的调查和诊断，我们已经确定了处理这些问题的方法。在这篇文章中，我们将与大家分享这些解决方法。不过，尽管这些方法很有用，但我们认为这只是权宜之策，相信未来会有更优雅的解决方案，也期望 K8s 社区、RHEL 和 CentOS 可以在不久的将来彻底修复这些问题。 Bug #1: 诊断修复不稳定的 Kmem Accounting 关键词：SLUB: Unable to allocate memory on node -1 社区相关 Issue: github.com/kubernetes/… github.com/opencontain… support.mesosphere.com/s/article/C… 问题起源 薛定谔平台是我司开发的基于 K8s 建立的一套自动化测试框架，提供各种 Chaos 能力，同时也提供自动化的 Bench 测试，各类异常监控、告警以及自动输出测试报告等功能。我们发现 TiKV 在薛定谔平台上做 OLTP 测试时偶尔会发生 I/O 性能抖动，但从下面几项来看未发现异常： TiKV 和 RocksDB 的日志 CPU 使用率 内存和磁盘等负载信息 只能偶尔看到 dmesg 命令执行的结果中包含一些 “SLUB: Unable to allocate memory on node -1” 信息。 问题分析 我们使用 perf-tools 中的 funcslower trace 来执行较慢的内核函数并调整内核参数 hung_task_timeout_secs 阈值，抓取到了一些 TiKV 执行写操作时的内核路径信息： 从上图的信息中可以看到 I/O 抖动和文件系统执行 writepage 有关。同时捕获到性能抖动的前后，在 node 内存资源充足的情况下，dmesg 返回的结果也会出现大量 “SLUB: Unable to allocate memory on node -1” 的信息。 从 hung_task 输出的 call stack 信息结合内核代码发现，内核在执行 bvec_alloc 函数分配 bio_vec 对象时，会先尝试通过 kmem_cache_alloc 进行分配，kmem_cache_alloc 失败后，再进行 fallback 尝试从 mempool 中进行分配，而在 mempool 内部会先尝试执行 pool-&gt;alloc 回调进行分配，pool-&gt;alloc 分配失败后，内核会将进程设置为不可中断状态并放入等待队列中进行等待，当其他进程向 mempool 归还内存或定时器超时（5s） 后，进程调度器会唤醒该进程进行重试 ，这个等待时间和我们业务监控的抖动延迟相符。 但是我们在创建 Docker 容器时，并没有设置 kmem limit，为什么还会有 kmem 不足的问题呢？为了确定 kmem limit 是否被设置，我们进入 cgroup memory controller 对容器的 kmem 信息进行查看，发现 kmem 的统计信息被开启了, 但 limit 值设置的非常大。 我们已知 kmem accounting 在 RHEL 3.10 版本内核上是不稳定的，因此怀疑 SLUB 分配失败是由内核 bug 引起的，搜索 kernel patch 信息我们发现确实是内核 bug, 在社区高版本内核中已修复： slub: make dead caches discard free slabs immediately 同时还有一个 namespace 泄漏问题也和 kmem accounting 有关： mm: memcontrol: fix cgroup creation failure after many small jobs 那么是谁开启了 kmem accounting 功能呢？我们使用 bcc 中的 opensnoop 工具对 kmem 配置文件进行监控，捕获到修改者 runc 。从 K8s 代码上可以确认是 K8s 依赖的 runc 项目默认开启了 kmem accounting。 解决方案 通过上述分析，我们要么升级到高版本内核，要么在启动容器的时候禁用 kmem accounting 功能，目前 runc 已提供条件编译选项，可以通过 Build Tags 来禁用 kmem accounting，关闭后我们测试发现抖动情况消失了，namespace 泄漏问题和 SLUB 分配失败的问题也消失了。 操作步骤 我们需要在 kubelet 和 docker 上都将 kmem account 功能关闭。 kubelet 需要重新编译，不同的版本有不同的方式。 如果 kubelet 版本是 v1.14 及以上，则可以通过在编译 kubelet 的时候加上 Build Tags 来关闭 kmem account： $ git clone --branch v1.14.1 --single-branch --depth 1 [https://github.com/kubernetes/kubernetes](https://github.com/kubernetes/kubernetes) $ cd kubernetes $ KUBE_GIT_VERSION=v1.14.1 ./build/run.sh make kubelet GOFLAGS=&quot;-tags=nokmem&quot; 复制代码但如果 kubelet 版本是 v1.13 及以下，则无法通过在编译 kubelet 的时候加 Build Tags 来关闭，需要重新编译 kubelet，步骤如下。 首先下载 Kubernetes 代码： $ git clone --branch v1.12.8 --single-branch --depth 1 https://github.com/kubernetes/kubernetes $ cd kubernetes 复制代码然后手动将开启 kmem account 功能的 两个函数 替换成 下面这样： func EnableKernelMemoryAccounting(path string) error { return nil } func setKernelMemory(path string, kernelMemoryLimit int64) error { return nil } 复制代码之后重新编译 kubelet： $ KUBE_GIT_VERSION=v1.12.8 ./build/run.sh make kubelet 复制代码编译好的 kubelet 在 ./_output/dockerized/bin/$GOOS/$GOARCH/kubelet 中。 同时需要升级 docker-ce 到 18.09.1 以上，此版本 docker 已经将 runc 的 kmem account 功能关闭。 最后需要重启机器。 验证方法是查看新创建的 pod 的所有 container 已关闭 kmem，如果为下面结果则已关闭： $ cat /sys/fs/cgroup/memory/kubepods/burstable/pod&lt;pod-uid&gt;/&lt;container-id&gt;/memory.kmem.slabinfo cat: memory.kmem.slabinfo: Input/output error 复制代码 Bug #2：诊断修复网络设备引用计数泄漏问题 关键词：kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1 社区相关 Issue: github.com/kubernetes/… github.com/projectcali… github.com/moby/moby/i… 问题起源 我们的薛定谔分布式测试集群运行一段时间后，经常会持续出现“kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1” 问题，并会导致多个进程进入不可中断状态，只能通过重启服务器来解决。 问题分析 通过使用 crash 工具对 vmcore 进行分析，我们发现内核线程阻塞在 netdev_wait_allrefs 函数，无限循环等待 dev-&gt;refcnt 降为 0。由于 pod 已经释放了，因此怀疑是引用计数泄漏问题。我们查找 K8s issue 后发现问题出在内核上，但这个问题没有简单的稳定可靠复现方法，且在社区高版本内核上依然会出现这个问题。 为避免每次出现问题都需要重启服务器，我们开发一个内核模块，当发现 net_device 引用计数已泄漏时，将引用计数清 0 后移除此内核模块（避免误删除其他非引用计数泄漏的网卡）。为了避免每次手动清理，我们写了一个监控脚本，周期性自动执行这个操作。但此方案仍然存在缺陷： 引用计数的泄漏和监控发现之间存在一定的延迟，在这段延迟中 K8s 系统可能会出现其他问题； 在内核模块中很难判断是否是引用计数泄漏，netdev_wait_allrefs 会通过 Notification Chains 向所有的消息订阅者不断重试发布 NETDEV_UNREGISTER 和 NETDEV_UNREGISTER_FINAL 消息，而经过 trace 发现消息的订阅者多达 22 个，而去弄清这 22 个订阅者注册的每个回调函数的处理逻辑来判断是否有办法避免误判也不是一件简单的事。 解决方案 在我们准备深入到每个订阅者注册的回调函数逻辑的同时，我们也在持续关注 kernel patch 和 RHEL 的进展，发现 RHEL 的 solutions:3659011 有了一个更新，提到 upstream 提交的一个 patch: route: set the deleted fnhe fnhe_daddr to 0 in ip_del_fnhe to fix a race 在尝试以 hotfix 的方式为内核打上此补丁后，我们持续测试了 1 周，问题没有再复现。我们向 RHEL 反馈测试信息，得知他们已经开始对此 patch 进行 backport。 操作步骤 推荐内核版本 Centos 7.6 kernel-3.10.0-957 及以上。 安装 kpatch 及 kpatch-build 依赖： UNAME=$(uname -r) sudo yum install gcc kernel-devel-${UNAME%.*} elfutils elfutils-devel sudo yum install pesign yum-utils zlib-devel \ binutils-devel newt-devel python-devel perl-ExtUtils-Embed \ audit-libs audit-libs-devel numactl-devel pciutils-devel bison # enable CentOS 7 debug repo sudo yum-config-manager --enable debug sudo yum-builddep kernel-${UNAME%.*} sudo debuginfo-install kernel-${UNAME%.*} # optional, but highly recommended - enable EPEL 7 sudo yum install ccache ccache --max-size=5G 复制代码 安装 kpatch 及 kpatch-build： git clone https://github.com/dynup/kpatch &amp;&amp; cd kpatch make sudo make install systemctl enable kpatch 复制代码 下载并构建热补丁内核模块： curl -SOL https://raw.githubusercontent.com/pingcap/kdt/master/kpatchs/route.patch kpatch-build -t vmlinux route.patch （编译生成内核模块） mkdir -p /var/lib/kpatch/${UNAME} cp -a livepatch-route.ko /var/lib/kpatch/${UNAME} systemctl restart kpatch (Loads the kernel module) kpatch list (Checks the loaded module) 复制代码 总结 虽然我们修复了这些内核错误，但是未来应该会有更好的解决方案。对于 Bug＃1，我们希望 K8s 社区可以为 kubelet 提供一个参数，以允许用户禁用或启用 kmem account 功能。对于 Bug＃2，最佳解决方案是由 RHEL 和 CentOS 修复内核错误，希望 TiDB 用户将 CentOS 升级到新版后，不必再担心这个问题。 转载于:https://juejin.im/post/5ceb78736fb9a07ecd3d3a69" />
<meta property="og:description" content="原文链接： https://juejin.im/post/5ceb78736fb9a07ecd3d3a69 作者：张文博 Kubernetes（K8s）是一个开源容器编排系统，可自动执行应用程序部署、扩展和管理。它是云原生世界的操作系统。 K8s 或操作系统中的任何缺陷都可能使用户进程存在风险。作为 PingCAP EE（效率工程）团队，我们在 K8s 中测试 TiDB Operator（一个创建和管理 TiDB 集群的工具）时，发现了两个 Linux 内核错误。这些错误已经困扰我们很长一段时间，并没有在整个 K8s 社区中彻底修复。 经过广泛的调查和诊断，我们已经确定了处理这些问题的方法。在这篇文章中，我们将与大家分享这些解决方法。不过，尽管这些方法很有用，但我们认为这只是权宜之策，相信未来会有更优雅的解决方案，也期望 K8s 社区、RHEL 和 CentOS 可以在不久的将来彻底修复这些问题。 Bug #1: 诊断修复不稳定的 Kmem Accounting 关键词：SLUB: Unable to allocate memory on node -1 社区相关 Issue: github.com/kubernetes/… github.com/opencontain… support.mesosphere.com/s/article/C… 问题起源 薛定谔平台是我司开发的基于 K8s 建立的一套自动化测试框架，提供各种 Chaos 能力，同时也提供自动化的 Bench 测试，各类异常监控、告警以及自动输出测试报告等功能。我们发现 TiKV 在薛定谔平台上做 OLTP 测试时偶尔会发生 I/O 性能抖动，但从下面几项来看未发现异常： TiKV 和 RocksDB 的日志 CPU 使用率 内存和磁盘等负载信息 只能偶尔看到 dmesg 命令执行的结果中包含一些 “SLUB: Unable to allocate memory on node -1” 信息。 问题分析 我们使用 perf-tools 中的 funcslower trace 来执行较慢的内核函数并调整内核参数 hung_task_timeout_secs 阈值，抓取到了一些 TiKV 执行写操作时的内核路径信息： 从上图的信息中可以看到 I/O 抖动和文件系统执行 writepage 有关。同时捕获到性能抖动的前后，在 node 内存资源充足的情况下，dmesg 返回的结果也会出现大量 “SLUB: Unable to allocate memory on node -1” 的信息。 从 hung_task 输出的 call stack 信息结合内核代码发现，内核在执行 bvec_alloc 函数分配 bio_vec 对象时，会先尝试通过 kmem_cache_alloc 进行分配，kmem_cache_alloc 失败后，再进行 fallback 尝试从 mempool 中进行分配，而在 mempool 内部会先尝试执行 pool-&gt;alloc 回调进行分配，pool-&gt;alloc 分配失败后，内核会将进程设置为不可中断状态并放入等待队列中进行等待，当其他进程向 mempool 归还内存或定时器超时（5s） 后，进程调度器会唤醒该进程进行重试 ，这个等待时间和我们业务监控的抖动延迟相符。 但是我们在创建 Docker 容器时，并没有设置 kmem limit，为什么还会有 kmem 不足的问题呢？为了确定 kmem limit 是否被设置，我们进入 cgroup memory controller 对容器的 kmem 信息进行查看，发现 kmem 的统计信息被开启了, 但 limit 值设置的非常大。 我们已知 kmem accounting 在 RHEL 3.10 版本内核上是不稳定的，因此怀疑 SLUB 分配失败是由内核 bug 引起的，搜索 kernel patch 信息我们发现确实是内核 bug, 在社区高版本内核中已修复： slub: make dead caches discard free slabs immediately 同时还有一个 namespace 泄漏问题也和 kmem accounting 有关： mm: memcontrol: fix cgroup creation failure after many small jobs 那么是谁开启了 kmem accounting 功能呢？我们使用 bcc 中的 opensnoop 工具对 kmem 配置文件进行监控，捕获到修改者 runc 。从 K8s 代码上可以确认是 K8s 依赖的 runc 项目默认开启了 kmem accounting。 解决方案 通过上述分析，我们要么升级到高版本内核，要么在启动容器的时候禁用 kmem accounting 功能，目前 runc 已提供条件编译选项，可以通过 Build Tags 来禁用 kmem accounting，关闭后我们测试发现抖动情况消失了，namespace 泄漏问题和 SLUB 分配失败的问题也消失了。 操作步骤 我们需要在 kubelet 和 docker 上都将 kmem account 功能关闭。 kubelet 需要重新编译，不同的版本有不同的方式。 如果 kubelet 版本是 v1.14 及以上，则可以通过在编译 kubelet 的时候加上 Build Tags 来关闭 kmem account： $ git clone --branch v1.14.1 --single-branch --depth 1 [https://github.com/kubernetes/kubernetes](https://github.com/kubernetes/kubernetes) $ cd kubernetes $ KUBE_GIT_VERSION=v1.14.1 ./build/run.sh make kubelet GOFLAGS=&quot;-tags=nokmem&quot; 复制代码但如果 kubelet 版本是 v1.13 及以下，则无法通过在编译 kubelet 的时候加 Build Tags 来关闭，需要重新编译 kubelet，步骤如下。 首先下载 Kubernetes 代码： $ git clone --branch v1.12.8 --single-branch --depth 1 https://github.com/kubernetes/kubernetes $ cd kubernetes 复制代码然后手动将开启 kmem account 功能的 两个函数 替换成 下面这样： func EnableKernelMemoryAccounting(path string) error { return nil } func setKernelMemory(path string, kernelMemoryLimit int64) error { return nil } 复制代码之后重新编译 kubelet： $ KUBE_GIT_VERSION=v1.12.8 ./build/run.sh make kubelet 复制代码编译好的 kubelet 在 ./_output/dockerized/bin/$GOOS/$GOARCH/kubelet 中。 同时需要升级 docker-ce 到 18.09.1 以上，此版本 docker 已经将 runc 的 kmem account 功能关闭。 最后需要重启机器。 验证方法是查看新创建的 pod 的所有 container 已关闭 kmem，如果为下面结果则已关闭： $ cat /sys/fs/cgroup/memory/kubepods/burstable/pod&lt;pod-uid&gt;/&lt;container-id&gt;/memory.kmem.slabinfo cat: memory.kmem.slabinfo: Input/output error 复制代码 Bug #2：诊断修复网络设备引用计数泄漏问题 关键词：kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1 社区相关 Issue: github.com/kubernetes/… github.com/projectcali… github.com/moby/moby/i… 问题起源 我们的薛定谔分布式测试集群运行一段时间后，经常会持续出现“kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1” 问题，并会导致多个进程进入不可中断状态，只能通过重启服务器来解决。 问题分析 通过使用 crash 工具对 vmcore 进行分析，我们发现内核线程阻塞在 netdev_wait_allrefs 函数，无限循环等待 dev-&gt;refcnt 降为 0。由于 pod 已经释放了，因此怀疑是引用计数泄漏问题。我们查找 K8s issue 后发现问题出在内核上，但这个问题没有简单的稳定可靠复现方法，且在社区高版本内核上依然会出现这个问题。 为避免每次出现问题都需要重启服务器，我们开发一个内核模块，当发现 net_device 引用计数已泄漏时，将引用计数清 0 后移除此内核模块（避免误删除其他非引用计数泄漏的网卡）。为了避免每次手动清理，我们写了一个监控脚本，周期性自动执行这个操作。但此方案仍然存在缺陷： 引用计数的泄漏和监控发现之间存在一定的延迟，在这段延迟中 K8s 系统可能会出现其他问题； 在内核模块中很难判断是否是引用计数泄漏，netdev_wait_allrefs 会通过 Notification Chains 向所有的消息订阅者不断重试发布 NETDEV_UNREGISTER 和 NETDEV_UNREGISTER_FINAL 消息，而经过 trace 发现消息的订阅者多达 22 个，而去弄清这 22 个订阅者注册的每个回调函数的处理逻辑来判断是否有办法避免误判也不是一件简单的事。 解决方案 在我们准备深入到每个订阅者注册的回调函数逻辑的同时，我们也在持续关注 kernel patch 和 RHEL 的进展，发现 RHEL 的 solutions:3659011 有了一个更新，提到 upstream 提交的一个 patch: route: set the deleted fnhe fnhe_daddr to 0 in ip_del_fnhe to fix a race 在尝试以 hotfix 的方式为内核打上此补丁后，我们持续测试了 1 周，问题没有再复现。我们向 RHEL 反馈测试信息，得知他们已经开始对此 patch 进行 backport。 操作步骤 推荐内核版本 Centos 7.6 kernel-3.10.0-957 及以上。 安装 kpatch 及 kpatch-build 依赖： UNAME=$(uname -r) sudo yum install gcc kernel-devel-${UNAME%.*} elfutils elfutils-devel sudo yum install pesign yum-utils zlib-devel \ binutils-devel newt-devel python-devel perl-ExtUtils-Embed \ audit-libs audit-libs-devel numactl-devel pciutils-devel bison # enable CentOS 7 debug repo sudo yum-config-manager --enable debug sudo yum-builddep kernel-${UNAME%.*} sudo debuginfo-install kernel-${UNAME%.*} # optional, but highly recommended - enable EPEL 7 sudo yum install ccache ccache --max-size=5G 复制代码 安装 kpatch 及 kpatch-build： git clone https://github.com/dynup/kpatch &amp;&amp; cd kpatch make sudo make install systemctl enable kpatch 复制代码 下载并构建热补丁内核模块： curl -SOL https://raw.githubusercontent.com/pingcap/kdt/master/kpatchs/route.patch kpatch-build -t vmlinux route.patch （编译生成内核模块） mkdir -p /var/lib/kpatch/${UNAME} cp -a livepatch-route.ko /var/lib/kpatch/${UNAME} systemctl restart kpatch (Loads the kernel module) kpatch list (Checks the loaded module) 复制代码 总结 虽然我们修复了这些内核错误，但是未来应该会有更好的解决方案。对于 Bug＃1，我们希望 K8s 社区可以为 kubelet 提供一个参数，以允许用户禁用或启用 kmem account 功能。对于 Bug＃2，最佳解决方案是由 RHEL 和 CentOS 修复内核错误，希望 TiDB 用户将 CentOS 升级到新版后，不必再担心这个问题。 转载于:https://juejin.im/post/5ceb78736fb9a07ecd3d3a69" />
<link rel="canonical" href="https://uzzz.org/2019/05/27/790581.html" />
<meta property="og:url" content="https://uzzz.org/2019/05/27/790581.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-27T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"原文链接： https://juejin.im/post/5ceb78736fb9a07ecd3d3a69 作者：张文博 Kubernetes（K8s）是一个开源容器编排系统，可自动执行应用程序部署、扩展和管理。它是云原生世界的操作系统。 K8s 或操作系统中的任何缺陷都可能使用户进程存在风险。作为 PingCAP EE（效率工程）团队，我们在 K8s 中测试 TiDB Operator（一个创建和管理 TiDB 集群的工具）时，发现了两个 Linux 内核错误。这些错误已经困扰我们很长一段时间，并没有在整个 K8s 社区中彻底修复。 经过广泛的调查和诊断，我们已经确定了处理这些问题的方法。在这篇文章中，我们将与大家分享这些解决方法。不过，尽管这些方法很有用，但我们认为这只是权宜之策，相信未来会有更优雅的解决方案，也期望 K8s 社区、RHEL 和 CentOS 可以在不久的将来彻底修复这些问题。 Bug #1: 诊断修复不稳定的 Kmem Accounting 关键词：SLUB: Unable to allocate memory on node -1 社区相关 Issue: github.com/kubernetes/… github.com/opencontain… support.mesosphere.com/s/article/C… 问题起源 薛定谔平台是我司开发的基于 K8s 建立的一套自动化测试框架，提供各种 Chaos 能力，同时也提供自动化的 Bench 测试，各类异常监控、告警以及自动输出测试报告等功能。我们发现 TiKV 在薛定谔平台上做 OLTP 测试时偶尔会发生 I/O 性能抖动，但从下面几项来看未发现异常： TiKV 和 RocksDB 的日志 CPU 使用率 内存和磁盘等负载信息 只能偶尔看到 dmesg 命令执行的结果中包含一些 “SLUB: Unable to allocate memory on node -1” 信息。 问题分析 我们使用 perf-tools 中的 funcslower trace 来执行较慢的内核函数并调整内核参数 hung_task_timeout_secs 阈值，抓取到了一些 TiKV 执行写操作时的内核路径信息： 从上图的信息中可以看到 I/O 抖动和文件系统执行 writepage 有关。同时捕获到性能抖动的前后，在 node 内存资源充足的情况下，dmesg 返回的结果也会出现大量 “SLUB: Unable to allocate memory on node -1” 的信息。 从 hung_task 输出的 call stack 信息结合内核代码发现，内核在执行 bvec_alloc 函数分配 bio_vec 对象时，会先尝试通过 kmem_cache_alloc 进行分配，kmem_cache_alloc 失败后，再进行 fallback 尝试从 mempool 中进行分配，而在 mempool 内部会先尝试执行 pool-&gt;alloc 回调进行分配，pool-&gt;alloc 分配失败后，内核会将进程设置为不可中断状态并放入等待队列中进行等待，当其他进程向 mempool 归还内存或定时器超时（5s） 后，进程调度器会唤醒该进程进行重试 ，这个等待时间和我们业务监控的抖动延迟相符。 但是我们在创建 Docker 容器时，并没有设置 kmem limit，为什么还会有 kmem 不足的问题呢？为了确定 kmem limit 是否被设置，我们进入 cgroup memory controller 对容器的 kmem 信息进行查看，发现 kmem 的统计信息被开启了, 但 limit 值设置的非常大。 我们已知 kmem accounting 在 RHEL 3.10 版本内核上是不稳定的，因此怀疑 SLUB 分配失败是由内核 bug 引起的，搜索 kernel patch 信息我们发现确实是内核 bug, 在社区高版本内核中已修复： slub: make dead caches discard free slabs immediately 同时还有一个 namespace 泄漏问题也和 kmem accounting 有关： mm: memcontrol: fix cgroup creation failure after many small jobs 那么是谁开启了 kmem accounting 功能呢？我们使用 bcc 中的 opensnoop 工具对 kmem 配置文件进行监控，捕获到修改者 runc 。从 K8s 代码上可以确认是 K8s 依赖的 runc 项目默认开启了 kmem accounting。 解决方案 通过上述分析，我们要么升级到高版本内核，要么在启动容器的时候禁用 kmem accounting 功能，目前 runc 已提供条件编译选项，可以通过 Build Tags 来禁用 kmem accounting，关闭后我们测试发现抖动情况消失了，namespace 泄漏问题和 SLUB 分配失败的问题也消失了。 操作步骤 我们需要在 kubelet 和 docker 上都将 kmem account 功能关闭。 kubelet 需要重新编译，不同的版本有不同的方式。 如果 kubelet 版本是 v1.14 及以上，则可以通过在编译 kubelet 的时候加上 Build Tags 来关闭 kmem account： $ git clone --branch v1.14.1 --single-branch --depth 1 [https://github.com/kubernetes/kubernetes](https://github.com/kubernetes/kubernetes) $ cd kubernetes $ KUBE_GIT_VERSION=v1.14.1 ./build/run.sh make kubelet GOFLAGS=&quot;-tags=nokmem&quot; 复制代码但如果 kubelet 版本是 v1.13 及以下，则无法通过在编译 kubelet 的时候加 Build Tags 来关闭，需要重新编译 kubelet，步骤如下。 首先下载 Kubernetes 代码： $ git clone --branch v1.12.8 --single-branch --depth 1 https://github.com/kubernetes/kubernetes $ cd kubernetes 复制代码然后手动将开启 kmem account 功能的 两个函数 替换成 下面这样： func EnableKernelMemoryAccounting(path string) error { return nil } func setKernelMemory(path string, kernelMemoryLimit int64) error { return nil } 复制代码之后重新编译 kubelet： $ KUBE_GIT_VERSION=v1.12.8 ./build/run.sh make kubelet 复制代码编译好的 kubelet 在 ./_output/dockerized/bin/$GOOS/$GOARCH/kubelet 中。 同时需要升级 docker-ce 到 18.09.1 以上，此版本 docker 已经将 runc 的 kmem account 功能关闭。 最后需要重启机器。 验证方法是查看新创建的 pod 的所有 container 已关闭 kmem，如果为下面结果则已关闭： $ cat /sys/fs/cgroup/memory/kubepods/burstable/pod&lt;pod-uid&gt;/&lt;container-id&gt;/memory.kmem.slabinfo cat: memory.kmem.slabinfo: Input/output error 复制代码 Bug #2：诊断修复网络设备引用计数泄漏问题 关键词：kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1 社区相关 Issue: github.com/kubernetes/… github.com/projectcali… github.com/moby/moby/i… 问题起源 我们的薛定谔分布式测试集群运行一段时间后，经常会持续出现“kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1” 问题，并会导致多个进程进入不可中断状态，只能通过重启服务器来解决。 问题分析 通过使用 crash 工具对 vmcore 进行分析，我们发现内核线程阻塞在 netdev_wait_allrefs 函数，无限循环等待 dev-&gt;refcnt 降为 0。由于 pod 已经释放了，因此怀疑是引用计数泄漏问题。我们查找 K8s issue 后发现问题出在内核上，但这个问题没有简单的稳定可靠复现方法，且在社区高版本内核上依然会出现这个问题。 为避免每次出现问题都需要重启服务器，我们开发一个内核模块，当发现 net_device 引用计数已泄漏时，将引用计数清 0 后移除此内核模块（避免误删除其他非引用计数泄漏的网卡）。为了避免每次手动清理，我们写了一个监控脚本，周期性自动执行这个操作。但此方案仍然存在缺陷： 引用计数的泄漏和监控发现之间存在一定的延迟，在这段延迟中 K8s 系统可能会出现其他问题； 在内核模块中很难判断是否是引用计数泄漏，netdev_wait_allrefs 会通过 Notification Chains 向所有的消息订阅者不断重试发布 NETDEV_UNREGISTER 和 NETDEV_UNREGISTER_FINAL 消息，而经过 trace 发现消息的订阅者多达 22 个，而去弄清这 22 个订阅者注册的每个回调函数的处理逻辑来判断是否有办法避免误判也不是一件简单的事。 解决方案 在我们准备深入到每个订阅者注册的回调函数逻辑的同时，我们也在持续关注 kernel patch 和 RHEL 的进展，发现 RHEL 的 solutions:3659011 有了一个更新，提到 upstream 提交的一个 patch: route: set the deleted fnhe fnhe_daddr to 0 in ip_del_fnhe to fix a race 在尝试以 hotfix 的方式为内核打上此补丁后，我们持续测试了 1 周，问题没有再复现。我们向 RHEL 反馈测试信息，得知他们已经开始对此 patch 进行 backport。 操作步骤 推荐内核版本 Centos 7.6 kernel-3.10.0-957 及以上。 安装 kpatch 及 kpatch-build 依赖： UNAME=$(uname -r) sudo yum install gcc kernel-devel-${UNAME%.*} elfutils elfutils-devel sudo yum install pesign yum-utils zlib-devel \\ binutils-devel newt-devel python-devel perl-ExtUtils-Embed \\ audit-libs audit-libs-devel numactl-devel pciutils-devel bison # enable CentOS 7 debug repo sudo yum-config-manager --enable debug sudo yum-builddep kernel-${UNAME%.*} sudo debuginfo-install kernel-${UNAME%.*} # optional, but highly recommended - enable EPEL 7 sudo yum install ccache ccache --max-size=5G 复制代码 安装 kpatch 及 kpatch-build： git clone https://github.com/dynup/kpatch &amp;&amp; cd kpatch make sudo make install systemctl enable kpatch 复制代码 下载并构建热补丁内核模块： curl -SOL https://raw.githubusercontent.com/pingcap/kdt/master/kpatchs/route.patch kpatch-build -t vmlinux route.patch （编译生成内核模块） mkdir -p /var/lib/kpatch/${UNAME} cp -a livepatch-route.ko /var/lib/kpatch/${UNAME} systemctl restart kpatch (Loads the kernel module) kpatch list (Checks the loaded module) 复制代码 总结 虽然我们修复了这些内核错误，但是未来应该会有更好的解决方案。对于 Bug＃1，我们希望 K8s 社区可以为 kubelet 提供一个参数，以允许用户禁用或启用 kmem account 功能。对于 Bug＃2，最佳解决方案是由 RHEL 和 CentOS 修复内核错误，希望 TiDB 用户将 CentOS 升级到新版后，不必再担心这个问题。 转载于:https://juejin.im/post/5ceb78736fb9a07ecd3d3a69","@type":"BlogPosting","url":"https://uzzz.org/2019/05/27/790581.html","headline":"诊断修复 TiDB Operator 在 K8s 测试中遇到的 Linux 内核问题","dateModified":"2019-05-27T00:00:00+08:00","datePublished":"2019-05-27T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/05/27/790581.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>诊断修复 TiDB Operator 在 K8s 测试中遇到的 Linux 内核问题</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix" data-track-view="{&quot;mod&quot;:&quot;popu_307&quot;,&quot;con&quot;:&quot;,https://blog.csdn.net/weixin_33795093/article/details/91460953,&quot;}" data-track-click="{&quot;mod&quot;:&quot;popu_307&quot;,&quot;con&quot;:&quot;,https://blog.csdn.net/weixin_33795093/article/details/91460953&quot;}"> 
 <div class="article-source-link">
   原文链接：
  <a href="https://juejin.im/post/5ceb78736fb9a07ecd3d3a69" target="_blank">https://juejin.im/post/5ceb78736fb9a07ecd3d3a69</a> 
 </div> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-cd6c485e8b.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-cd6c485e8b.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="article-content">
   <p>作者：张文博</p> 
   <p><a href="https://link.juejin.im?target=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FKubernetes" rel="nofollow">Kubernetes</a>（K8s）是一个开源容器编排系统，可自动执行应用程序部署、扩展和管理。它是云原生世界的操作系统。 K8s 或操作系统中的任何缺陷都可能使用户进程存在风险。作为 PingCAP EE（效率工程）团队，我们在 K8s 中测试 <a href="https://link.juejin.im?target=https%3A%2F%2Fpingcap.com%2Fblog-cn%2Ftidb-operator-introduction%2F" rel="nofollow">TiDB Operator</a>（一个创建和管理 TiDB 集群的工具）时，发现了两个 Linux 内核错误。这些错误已经困扰我们很长一段时间，并没有在整个 K8s 社区中彻底修复。</p> 
   <p>经过广泛的调查和诊断，我们已经确定了处理这些问题的方法。在这篇文章中，我们将与大家分享这些解决方法。不过，尽管这些方法很有用，但我们认为这只是权宜之策，相信未来会有更优雅的解决方案，也期望 K8s 社区、RHEL 和 CentOS 可以在不久的将来彻底修复这些问题。</p> 
   <h2 class="heading">Bug #1: 诊断修复不稳定的 Kmem Accounting</h2> 
   <p>关键词：SLUB: Unable to allocate memory on node -1</p> 
   <p>社区相关 Issue:</p> 
   <ul>
    <li><a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fkubernetes%2Fissues%2F61937" rel="nofollow">github.com/kubernetes/…</a></li> 
    <li><a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fopencontainers%2Frunc%2Fissues%2F1725" rel="nofollow">github.com/opencontain…</a></li> 
    <li><a href="https://link.juejin.im?target=https%3A%2F%2Fsupport.mesosphere.com%2Fs%2Farticle%2FCritical-Issue-KMEM-MSPH-2018-0006" rel="nofollow">support.mesosphere.com/s/article/C…</a></li> 
   </ul>
   <h3 class="heading">问题起源</h3> 
   <p>薛定谔平台是我司开发的基于 K8s 建立的一套自动化测试框架，提供各种 Chaos 能力，同时也提供自动化的 Bench 测试，各类异常监控、告警以及自动输出测试报告等功能。我们发现 <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Ftikv%2Ftikv" rel="nofollow">TiKV</a> 在薛定谔平台上做 <a href="https://link.juejin.im?target=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FOnline_transaction_processing" rel="nofollow">OLTP</a> 测试时偶尔会发生 I/O 性能抖动，但从下面几项来看未发现异常：</p> 
   <ul>
    <li>TiKV 和 RocksDB 的日志</li> 
    <li>CPU 使用率</li> 
    <li>内存和磁盘等负载信息</li> 
   </ul>
   <p>只能偶尔看到 dmesg 命令执行的结果中包含一些 “SLUB: Unable to allocate memory on node -1” 信息。</p> 
   <h3 class="heading">问题分析</h3> 
   <p>我们使用 <a href="https://link.juejin.im?target=http%3A%2F%2Fgithub.com%2Fbrendangregg%2Fperf-tools" rel="nofollow">perf-tools</a> 中的 <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fbrendangregg%2Fperf-tools%2Fblob%2Fmaster%2Fbin%2Ffuncslower" rel="nofollow">funcslower</a> trace 来执行较慢的内核函数并调整内核参数 <code>hung_task_timeout_secs</code> 阈值，抓取到了一些 TiKV 执行写操作时的内核路径信息：</p> 
   <p></p>
   <figure>
    <figcaption></figcaption>
   </figure>
   <p></p> 
   <p></p>
   <figure>
    <figcaption></figcaption>
   </figure>
   <p></p> 
   <p>从上图的信息中可以看到 I/O 抖动和文件系统执行 writepage 有关。同时捕获到性能抖动的前后，在 node 内存资源充足的情况下，<code>dmesg</code> 返回的结果也会出现大量 “SLUB: Unable to allocate memory on node -1” 的信息。</p> 
   <p>从 <code>hung_task</code> 输出的 call stack 信息结合内核代码发现，内核在执行 <code>bvec_alloc</code> 函数分配 <code>bio_vec</code> 对象时，会先尝试通过 <code>kmem_cache_alloc</code> 进行分配，<code>kmem_cache_alloc</code> 失败后，再进行 fallback 尝试从 mempool 中进行分配，而在 mempool 内部会先尝试执行 <code>pool-&gt;alloc</code> 回调进行分配，<code>pool-&gt;alloc</code> 分配失败后，内核会将进程设置为不可中断状态并放入等待队列中进行等待，当其他进程向 mempool 归还内存或定时器超时（5s） 后，进程调度器会唤醒该进程进行重试 ，这个等待时间和我们业务监控的抖动延迟相符。</p> 
   <p>但是我们在创建 Docker 容器时，并没有设置 kmem limit，为什么还会有 kmem 不足的问题呢？为了确定 kmem limit 是否被设置，我们进入 cgroup memory controller 对容器的 kmem 信息进行查看，发现 kmem 的统计信息被开启了, 但 limit 值设置的非常大。</p> 
   <p>我们已知 kmem accounting 在 RHEL 3.10 版本内核上是不稳定的，因此怀疑 SLUB 分配失败是由内核 bug 引起的，搜索 kernel patch 信息我们发现确实是内核 bug, 在社区高版本内核中已修复：</p> 
   <p><a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Ftorvalds%2Flinux%2Fcommit%2Fd6e0b7fa11862433773d986b5f995ffdf47ce672" rel="nofollow">slub: make dead caches discard free slabs immediately</a></p> 
   <p>同时还有一个 namespace 泄漏问题也和 kmem accounting 有关：</p> 
   <p><a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Ftorvalds%2Flinux%2Fcommit%2F73f576c04b9410ed19660f74f97521bee6e1c546" rel="nofollow">mm: memcontrol: fix cgroup creation failure after many small jobs</a></p> 
   <p>那么是谁开启了 kmem accounting 功能呢？我们使用 <a href="https://link.juejin.im?target=http%3A%2F%2Fgithub.com%2Fiovisor%2Fbcc" rel="nofollow">bcc</a> 中的 <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fiovisor%2Fbcc%2Fblob%2Fmaster%2Ftools%2Fopensnoop.py" rel="nofollow">opensnoop</a> 工具对 kmem 配置文件进行监控，捕获到修改者 runc 。从 K8s 代码上可以确认是 K8s 依赖的 runc 项目默认开启了 kmem accounting。</p> 
   <h3 class="heading">解决方案</h3> 
   <p>通过上述分析，我们要么升级到高版本内核，要么在启动容器的时候禁用 kmem accounting 功能，目前 runc 已提供条件编译选项，可以通过 <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fopencontainers%2Frunc%23build-tags" rel="nofollow">Build Tags</a> 来禁用 kmem accounting，关闭后我们测试发现抖动情况消失了，namespace 泄漏问题和 SLUB 分配失败的问题也消失了。</p> 
   <h3 class="heading">操作步骤</h3> 
   <p>我们需要在 kubelet 和 docker 上都将 kmem account 功能关闭。</p> 
   <ol>
    <li> <p>kubelet 需要重新编译，不同的版本有不同的方式。</p> <p>如果 kubelet 版本是 v1.14 及以上，则可以通过在编译 kubelet 的时候加上 <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fkubernetes%2Fblob%2Frelease-1.14%2Fvendor%2Fgithub.com%2Fopencontainers%2Frunc%2Flibcontainer%2Fcgroups%2Ffs%2Fkmem_disabled.go%23L1" rel="nofollow">Build Tags</a> 来关闭 kmem account：</p> <pre><code class="hljs bash copyable">$ git <span class="hljs-built_in">clone</span> --branch v1.14.1 --single-branch --depth 1 [https://github.com/kubernetes/kubernetes](https://github.com/kubernetes/kubernetes) 
$ <span class="hljs-built_in">cd</span> kubernetes

$ KUBE_GIT_VERSION=v1.14.1 ./build/run.sh make kubelet GOFLAGS=<span class="hljs-string">"-tags=nokmem"</span>
<span class="copy-code-btn">复制代码</span></code></pre><p>但如果 kubelet 版本是 v1.13 及以下，则无法通过在编译 kubelet 的时候加 Build Tags 来关闭，需要重新编译 kubelet，步骤如下。</p> <p>首先下载 Kubernetes 代码：</p> <pre><code class="hljs bash copyable">$ git <span class="hljs-built_in">clone</span> --branch v1.12.8 --single-branch --depth 1 https://github.com/kubernetes/kubernetes
$ <span class="hljs-built_in">cd</span> kubernetes
<span class="copy-code-btn">复制代码</span></code></pre><p>然后手动将开启 kmem account 功能的 <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fkubernetes%2Fblob%2Frelease-1.12%2Fvendor%2Fgithub.com%2Fopencontainers%2Frunc%2Flibcontainer%2Fcgroups%2Ffs%2Fmemory.go%23L70-L106" rel="nofollow">两个函数</a> 替换成 <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fkubernetes%2Fblob%2Frelease-1.14%2Fvendor%2Fgithub.com%2Fopencontainers%2Frunc%2Flibcontainer%2Fcgroups%2Ffs%2Fkmem_disabled.go%23L5-L11" rel="nofollow">下面这样</a>：</p> <pre><code class="hljs bash copyable">func EnableKernelMemoryAccounting(path string) error {
	<span class="hljs-built_in">return</span> nil
}

func <span class="hljs-built_in">set</span>KernelMemory(path string, kernelMemoryLimit int64) error {
	<span class="hljs-built_in">return</span> nil
}

<span class="copy-code-btn">复制代码</span></code></pre><p>之后重新编译 kubelet：</p> <pre><code class="hljs bash copyable">$ KUBE_GIT_VERSION=v1.12.8 ./build/run.sh make kubelet
<span class="copy-code-btn">复制代码</span></code></pre><p>编译好的 kubelet 在 <code>./_output/dockerized/bin/$GOOS/$GOARCH/kubelet</code> 中。</p> </li> 
    <li> <p>同时需要升级 docker-ce 到 18.09.1 以上，此版本 docker 已经将 runc 的 kmem account 功能关闭。</p> </li> 
    <li> <p>最后需要重启机器。</p> </li> 
   </ol>
   <p>验证方法是查看新创建的 pod 的所有 container 已关闭 kmem，如果为下面结果则已关闭：</p> 
   <pre><code class="hljs bash copyable">$ cat /sys/fs/cgroup/memory/kubepods/burstable/pod&lt;pod-uid&gt;/&lt;container-id&gt;/memory.kmem.slabinfo
cat: memory.kmem.slabinfo: Input/output error

<span class="copy-code-btn">复制代码</span></code></pre>
   <h2 class="heading">Bug #2：诊断修复网络设备引用计数泄漏问题</h2> 
   <p>关键词：kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1</p> 
   <p>社区相关 Issue:</p> 
   <ul>
    <li><a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fkubernetes%2Fissues%2F64743" rel="nofollow">github.com/kubernetes/…</a></li> 
    <li><a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fprojectcalico%2Fcalico%2Fissues%2F1109" rel="nofollow">github.com/projectcali…</a></li> 
    <li><a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fmoby%2Fmoby%2Fissues%2F5618" rel="nofollow">github.com/moby/moby/i…</a></li> 
   </ul>
   <h3 class="heading">问题起源</h3> 
   <p>我们的薛定谔分布式测试集群运行一段时间后，经常会持续出现“kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1” 问题，并会导致多个进程进入不可中断状态，只能通过重启服务器来解决。</p> 
   <h3 class="heading">问题分析</h3> 
   <p>通过使用 crash 工具对 vmcore 进行分析，我们发现内核线程阻塞在 <code>netdev_wait_allrefs</code> 函数，无限循环等待 <code>dev-&gt;refcnt</code> 降为 0。由于 pod 已经释放了，因此怀疑是引用计数泄漏问题。我们查找 K8s issue 后发现问题出在内核上，但这个问题没有简单的稳定可靠复现方法，且在社区高版本内核上依然会出现这个问题。</p> 
   <p>为避免每次出现问题都需要重启服务器，我们开发一个内核模块，当发现 <code>net_device</code> 引用计数已泄漏时，将引用计数清 0 后移除此内核模块（避免误删除其他非引用计数泄漏的网卡）。为了避免每次手动清理，我们写了一个监控脚本，周期性自动执行这个操作。但此方案仍然存在缺陷：</p> 
   <ul>
    <li>引用计数的泄漏和监控发现之间存在一定的延迟，在这段延迟中 K8s 系统可能会出现其他问题；</li> 
    <li>在内核模块中很难判断是否是引用计数泄漏，<code>netdev_wait_allrefs</code> 会通过 Notification Chains 向所有的消息订阅者不断重试发布 <code>NETDEV_UNREGISTER</code> 和 <code>NETDEV_UNREGISTER_FINAL</code> 消息，而经过 trace 发现消息的订阅者多达 22 个，而去弄清这 22 个订阅者注册的每个回调函数的处理逻辑来判断是否有办法避免误判也不是一件简单的事。</li> 
   </ul>
   <h3 class="heading">解决方案</h3> 
   <p>在我们准备深入到每个订阅者注册的回调函数逻辑的同时，我们也在持续关注 kernel patch 和 RHEL 的进展，发现 RHEL 的 <a href="https://link.juejin.im?target=https%3A%2F%2Faccess.redhat.com%2Fsolutions%2F3659011" rel="nofollow">solutions:3659011</a> 有了一个更新，提到 upstream 提交的一个 patch:</p> 
   <p><a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Ftorvalds%2Flinux%2Fcommit%2Fee60ad219f5c7c4fb2f047f88037770063ef785f" rel="nofollow"><code>route: set the deleted fnhe fnhe_daddr to 0 in ip_del_fnhe to fix a race</code></a></p> 
   <p>在尝试以 hotfix 的方式为内核打上此补丁后，我们持续测试了 1 周，问题没有再复现。我们向 RHEL 反馈测试信息，得知他们已经开始对此 patch 进行 backport。</p> 
   <h3 class="heading">操作步骤</h3> 
   <p>推荐内核版本 Centos 7.6 kernel-3.10.0-957 及以上。</p> 
   <ol>
    <li> <p>安装 <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fdynup%2Fkpatch" rel="nofollow">kpatch</a> 及 kpatch-build 依赖：</p> <pre><code class="hljs bash copyable">UNAME=$(uname -r)
sudo yum install gcc kernel-devel-<span class="hljs-variable">${UNAME%.*}</span> elfutils elfutils-devel
sudo yum install pesign yum-utils zlib-devel \
  binutils-devel newt-devel python-devel perl-ExtUtils-Embed \
  audit-libs audit-libs-devel numactl-devel pciutils-devel bison

<span class="hljs-comment"># enable CentOS 7 debug repo</span>
sudo yum-config-manager --enable debug

sudo yum-builddep kernel-<span class="hljs-variable">${UNAME%.*}</span>
sudo debuginfo-install kernel-<span class="hljs-variable">${UNAME%.*}</span>

<span class="hljs-comment"># optional, but highly recommended - enable EPEL 7</span>
sudo yum install ccache
ccache --max-size=5G

<span class="copy-code-btn">复制代码</span></code></pre></li> 
    <li> <p>安装 <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fdynup%2Fkpatch" rel="nofollow">kpatch</a> 及 kpatch-build：</p> <pre><code class="hljs bash copyable">git <span class="hljs-built_in">clone</span> https://github.com/dynup/kpatch &amp;&amp; <span class="hljs-built_in">cd</span> kpatch
make 
sudo make install
systemctl <span class="hljs-built_in">enable</span> kpatch
<span class="copy-code-btn">复制代码</span></code></pre></li> 
    <li> <p>下载并构建热补丁内核模块：</p> <pre><code class="hljs bash copyable">curl -SOL  https://raw.githubusercontent.com/pingcap/kdt/master/kpatchs/route.patch
kpatch-build -t vmlinux route.patch （编译生成内核模块）
mkdir -p /var/lib/kpatch/<span class="hljs-variable">${UNAME}</span> 
cp <span class="hljs-_">-a</span> livepatch-route.ko /var/lib/kpatch/<span class="hljs-variable">${UNAME}</span>
systemctl restart kpatch (Loads the kernel module)
kpatch list (Checks the loaded module)
<span class="copy-code-btn">复制代码</span></code></pre></li> 
   </ol>
   <h2 class="heading">总结</h2> 
   <p>虽然我们修复了这些内核错误，但是未来应该会有更好的解决方案。对于 Bug＃1，我们希望 K8s 社区可以为 kubelet 提供一个参数，以允许用户禁用或启用 kmem account 功能。对于 Bug＃2，最佳解决方案是由 RHEL 和 CentOS 修复内核错误，希望 TiDB 用户将 CentOS 升级到新版后，不必再担心这个问题。</p> 
   <p></p>
   <figure>
    <figcaption></figcaption>
   </figure>
   <p></p> 
  </div> 
  <p>转载于:https://juejin.im/post/5ceb78736fb9a07ecd3d3a69</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
