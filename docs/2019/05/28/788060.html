<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>hadoop单机模式、伪分布式和分布式 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="hadoop单机模式、伪分布式和分布式" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="hadoop Hadoop是一个由Apache基金会所开发的分布式系统基础架构。 Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。 Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。 HDFS架构 （1）NameNode （2）DataNode （3）Secondary NameNode NameNode （1）是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。 （2）文件包括： fsimage:元数据镜像文件。存储某一时段NameNode内存元数据信息。 edits:操作日志文件。 fstime:保存最近一次checkpoint的时间 （3）以上这些文件是保存在linux的文件系统中。 SecondaryNameNode （1）HA的一个解决方案。但不支持热备。配置即可。 （2）执行过程：从NameNode上下载元数据信息（fsimage,edits），然后把二者合并，生成新的fsimage，在本地保存，并将其推送到NameNode，替换旧的fsimage. （3）默认在安装在NameNode节点上，但这样不安全！ Datanode （1）提供真实文件数据的存储服务。 （2）文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block. dfs.block.size （3）不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间 （4）Replication。多复本。默认是三个。hdfs-site.xml的dfs.replication属性 一.单机模式 建立用户，设置密码(密码此次设置为(redhat) [root@server1 ~]# useradd -u 1000 hadoop [root@server1 ~]# passwd hadoop 2.hadoop的安装配置 [root@server1 ~]# mv hadoop-3.0.3.tar.gz jdk-8u181-linux-x64.tar.gz /home/hadoop [root@server1 ~]# su - hadoop [hadoop@server1 ~]$ ls hadoop-3.0.3.tar.gz jdk-8u181-linux-x64.tar.gz [hadoop@server1 ~]$ tar zxf jdk-8u181-linux-x64.tar.gz [hadoop@server1 ~]$ tar zxf hadoop-3.0.3.tar.gz [hadoop@server1 ~]$ ln -s jdk1.8.0_181/ java [hadoop@server1 ~]$ ln -s hadoop-3.0.3 hadoop [hadoop@server1 ~]$ ls hadoop hadoop-3.0.3.tar.gz jdk1.8.0_181 hadoop-3.0.3 java jdk-8u181-linux-x64.tar.gz 3.配置环境变量 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server1 hadoop]$ vim hadoop-env.sh 54 export JAVA_HOME=/home/hadoop/java [hadoop@server1 ~]$ vim .bash_profile PATH=$PATH:$HOME/.local/bin:$HOME/bin:$HOME/java/bin [hadoop@server1 ~]$ source .bash_profile [hadoop@server1 ~]$ jps 2133 Jps 4.测试 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ mkdir input [hadoop@server1 hadoop]$ cp etc/hadoop/*.xml input/ [hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output &#39;dfs[a-z.]+&#39; [hadoop@server1 hadoop]$ cd output/ [hadoop@server1 output]$ ls part-r-00000 _SUCCESS [hadoop@server1 output]$ cat * 1 dfsadmin 二.伪分布式 namenode和datanode都在自己这台主机上 1.编辑文件 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server1 hadoop]$ vim core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hadoop@server1 hadoop]$ vim hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 2.生成密钥做免密连接 [hadoop@server1 hadoop]$ ssh-keygen [hadoop@server1 hadoop]$ ssh-copy-id localhost 3.格式化，并开启服务 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ bin/hdfs namenode -format [hadoop@server1 hadoop]$ cd sbin/ [hadoop@server1 sbin]$ ./start-dfs.sh [hadoop@server1 sbin]$ jps 2458 NameNode 2906 Jps 2765 SecondaryNameNode 2575 DataNode 4.浏览器查看http://172.25.68.1:9870 5.测试，创建目录，并上传 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir -p /user/hadoop [hadoop@server1 hadoop]$ bin/hdfs dfs -ls [hadoop@server1 hadoop]$ bin/hdfs dfs -put input [hadoop@server1 hadoop]$ bin/hdfs dfs -ls Found 1 items drwxr-xr-x - hadoop supergroup 0 2019-05-28 10:20 input 删除input和output文件，重新执行命令(测试从分布式上拉取文件) [hadoop@server1 hadoop]$ rm -fr input/ output/ [hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output &#39;dfs[a-z.]+&#39; [hadoop@server1 hadoop]$ ls bin etc include lib libexec LICENSE.txt logs NOTICE.txt README.txt sbin share **此时input和output不会出现在当前目录下，而是上传到了分布式文件系统中，网页上可以看到** [hadoop@server1 hadoop]$ bin/hdfs dfs -cat output/* 1 dfsadmin [hadoop@server1 hadoop]$ bin/hdfs dfs -get output ##从分布式系统中get下来output目录 [hadoop@server1 hadoop]$ cd output/ [hadoop@server1 output]$ ls part-r-00000 _SUCCESS [hadoop@server1 output]$ cat * 1 dfsadmin 分布式 主机是namenode，其它从机是datanode 1.先停掉服务，清除原来的数据 [hadoop@server1 hadoop]$ sbin/stop-dfs.sh Stopping namenodes on [localhost] Stopping datanodes Stopping secondary namenodes [server1] [hadoop@server1 hadoop]$ jps 3927 Jps [hadoop@server1 hadoop]$ cd /tmp/ [hadoop@server1 tmp]$ ls hadoop hadoop-hadoop hsperfdata_hadoop [hadoop@server1 tmp]$ rm -fr * 2.新开两个虚拟机，当做节点 [root@server2 ~]# useradd -u 1000 hadoop [root@server3 ~]# useradd -u 1000 hadoop [root@server1 ~]# yum install -y nfs-utils [root@server2 ~]# yum install -y nfs-utils [root@server3 ~]# yum install -y nfs-utils [root@server1 ~]# systemctl start rpcbind [root@server2 ~]# systemctl start rpcbind [root@server3 ~]# systemctl start rpcbind 3.server1开启服务，配置 [root@server1 ~]# systemctl start nfs-server [root@server1 ~]# vim /etc/exports /home/hadoop *(rw,anonuid=1000,anongid=1000) [root@server1 ~]# exportfs -rv exporting *:/home/hadoop [root@server1 ~]# showmount -e Export list for server1: /home/hadoop * 4.server2,3挂载 [root@server2 ~]# mount 172.25.68.1:/home/hadoop/ /home/hadoop/ [root@server2 ~]# df Filesystem 1K-blocks Used Available Use% Mounted on /dev/mapper/rhel-root 10258432 1097104 9161328 11% / devtmpfs 497292 0 497292 0% /dev tmpfs 508264 0 508264 0% /dev/shm tmpfs 508264 13072 495192 3% /run tmpfs 508264 0 508264 0% /sys/fs/cgroup /dev/sda1 1038336 141516 896820 14% /boot tmpfs 101656 0 101656 0% /run/user/0 172.25.68.1:/home/hadoop 10258432 2796544 7461888 28% /home/hadoop [root@server3 ~]# mount 172.25.68.1:/home/hadoop/ /home/hadoop/ 5.server1免密登陆server2和server3 [root@server1 tmp]# su - hadoop Last login: Tue May 28 10:17:23 CST 2019 on pts/0 [hadoop@server1 ~]$ ssh 172.25.68.2 [hadoop@server2 ~]$ logout Connection to 172.25.68.2 closed. [hadoop@server1 ~]$ ssh 172.25.68.3 [hadoop@server3 ~]$ logout Connection to 172.25.68.3 closed. 6.编辑文件(server1做namenode,server2和server3做datanode) [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server1 hadoop]$ vim core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://172.25.68.1:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hadoop@server1 hadoop]$ vim hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hadoop@server1 hadoop]$ vim workers [hadoop@server1 hadoop]$ cat workers 172.25.68.2 172.25.68.3 7.格式化，并启动服务 [hadoop@server1 hadoop]$ bin/hdfs namenode -format [hadoop@server1 hadoop]$ sbin/start-dfs.sh Starting namenodes on [server1] Starting datanodes Starting secondary namenodes [server1] [hadoop@server1 hadoop]$ jps #出现SecondaryNameNode 4673 SecondaryNameNode 4451 NameNode 4787 Jps 从节点可以是datanode [hadoop@server2 hadoop]$ jps 2384 DataNode 2447 Jps [hadoop@server3 hadoop]$ jps 2386 DataNode 2447 Jps 8.测试 [hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir -p /user/hadoop [hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir input [hadoop@server1 hadoop]$ bin/hdfs dfs -put etc/hadoop/*.xml input [hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output &#39;dfs[a-z.]+&#39; 网页上查看，有两个节点，且数据已经上传 9.添加节点server4 [root@server4 ~]# useradd -u 1000 hadoop [root@server4 ~]# yum install -y nfs-utils [root@server4 ~]# systemctl start rpcbind [root@server4 ~]# mount 172.25.68.1:/home/hadoop /home/hadoop [root@server4 ~]# su - hadoop [hadoop@server4 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server4 hadoop]$ vim workers 172.25.68.2 172.25.68.3 172.25.68.4 [hadoop@server4 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ sbin/start-dfs.sh [hadoop@server4 hadoop]$ jps 3029 DataNode 3081 Jps 浏览器查看，节点添加成功 server4上传文件 [hadoop@server4 hadoop]$ dd if=/dev/zero of=bigfile bs=1M count=500 500+0 records in 500+0 records out 524288000 bytes (524 MB) copied, 15.8634 s, 33.1 MB/s [hadoop@server4 hadoop]$ bin/hdfs dfs -put bigfile" />
<meta property="og:description" content="hadoop Hadoop是一个由Apache基金会所开发的分布式系统基础架构。 Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。 Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。 HDFS架构 （1）NameNode （2）DataNode （3）Secondary NameNode NameNode （1）是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。 （2）文件包括： fsimage:元数据镜像文件。存储某一时段NameNode内存元数据信息。 edits:操作日志文件。 fstime:保存最近一次checkpoint的时间 （3）以上这些文件是保存在linux的文件系统中。 SecondaryNameNode （1）HA的一个解决方案。但不支持热备。配置即可。 （2）执行过程：从NameNode上下载元数据信息（fsimage,edits），然后把二者合并，生成新的fsimage，在本地保存，并将其推送到NameNode，替换旧的fsimage. （3）默认在安装在NameNode节点上，但这样不安全！ Datanode （1）提供真实文件数据的存储服务。 （2）文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block. dfs.block.size （3）不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间 （4）Replication。多复本。默认是三个。hdfs-site.xml的dfs.replication属性 一.单机模式 建立用户，设置密码(密码此次设置为(redhat) [root@server1 ~]# useradd -u 1000 hadoop [root@server1 ~]# passwd hadoop 2.hadoop的安装配置 [root@server1 ~]# mv hadoop-3.0.3.tar.gz jdk-8u181-linux-x64.tar.gz /home/hadoop [root@server1 ~]# su - hadoop [hadoop@server1 ~]$ ls hadoop-3.0.3.tar.gz jdk-8u181-linux-x64.tar.gz [hadoop@server1 ~]$ tar zxf jdk-8u181-linux-x64.tar.gz [hadoop@server1 ~]$ tar zxf hadoop-3.0.3.tar.gz [hadoop@server1 ~]$ ln -s jdk1.8.0_181/ java [hadoop@server1 ~]$ ln -s hadoop-3.0.3 hadoop [hadoop@server1 ~]$ ls hadoop hadoop-3.0.3.tar.gz jdk1.8.0_181 hadoop-3.0.3 java jdk-8u181-linux-x64.tar.gz 3.配置环境变量 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server1 hadoop]$ vim hadoop-env.sh 54 export JAVA_HOME=/home/hadoop/java [hadoop@server1 ~]$ vim .bash_profile PATH=$PATH:$HOME/.local/bin:$HOME/bin:$HOME/java/bin [hadoop@server1 ~]$ source .bash_profile [hadoop@server1 ~]$ jps 2133 Jps 4.测试 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ mkdir input [hadoop@server1 hadoop]$ cp etc/hadoop/*.xml input/ [hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output &#39;dfs[a-z.]+&#39; [hadoop@server1 hadoop]$ cd output/ [hadoop@server1 output]$ ls part-r-00000 _SUCCESS [hadoop@server1 output]$ cat * 1 dfsadmin 二.伪分布式 namenode和datanode都在自己这台主机上 1.编辑文件 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server1 hadoop]$ vim core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hadoop@server1 hadoop]$ vim hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 2.生成密钥做免密连接 [hadoop@server1 hadoop]$ ssh-keygen [hadoop@server1 hadoop]$ ssh-copy-id localhost 3.格式化，并开启服务 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ bin/hdfs namenode -format [hadoop@server1 hadoop]$ cd sbin/ [hadoop@server1 sbin]$ ./start-dfs.sh [hadoop@server1 sbin]$ jps 2458 NameNode 2906 Jps 2765 SecondaryNameNode 2575 DataNode 4.浏览器查看http://172.25.68.1:9870 5.测试，创建目录，并上传 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir -p /user/hadoop [hadoop@server1 hadoop]$ bin/hdfs dfs -ls [hadoop@server1 hadoop]$ bin/hdfs dfs -put input [hadoop@server1 hadoop]$ bin/hdfs dfs -ls Found 1 items drwxr-xr-x - hadoop supergroup 0 2019-05-28 10:20 input 删除input和output文件，重新执行命令(测试从分布式上拉取文件) [hadoop@server1 hadoop]$ rm -fr input/ output/ [hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output &#39;dfs[a-z.]+&#39; [hadoop@server1 hadoop]$ ls bin etc include lib libexec LICENSE.txt logs NOTICE.txt README.txt sbin share **此时input和output不会出现在当前目录下，而是上传到了分布式文件系统中，网页上可以看到** [hadoop@server1 hadoop]$ bin/hdfs dfs -cat output/* 1 dfsadmin [hadoop@server1 hadoop]$ bin/hdfs dfs -get output ##从分布式系统中get下来output目录 [hadoop@server1 hadoop]$ cd output/ [hadoop@server1 output]$ ls part-r-00000 _SUCCESS [hadoop@server1 output]$ cat * 1 dfsadmin 分布式 主机是namenode，其它从机是datanode 1.先停掉服务，清除原来的数据 [hadoop@server1 hadoop]$ sbin/stop-dfs.sh Stopping namenodes on [localhost] Stopping datanodes Stopping secondary namenodes [server1] [hadoop@server1 hadoop]$ jps 3927 Jps [hadoop@server1 hadoop]$ cd /tmp/ [hadoop@server1 tmp]$ ls hadoop hadoop-hadoop hsperfdata_hadoop [hadoop@server1 tmp]$ rm -fr * 2.新开两个虚拟机，当做节点 [root@server2 ~]# useradd -u 1000 hadoop [root@server3 ~]# useradd -u 1000 hadoop [root@server1 ~]# yum install -y nfs-utils [root@server2 ~]# yum install -y nfs-utils [root@server3 ~]# yum install -y nfs-utils [root@server1 ~]# systemctl start rpcbind [root@server2 ~]# systemctl start rpcbind [root@server3 ~]# systemctl start rpcbind 3.server1开启服务，配置 [root@server1 ~]# systemctl start nfs-server [root@server1 ~]# vim /etc/exports /home/hadoop *(rw,anonuid=1000,anongid=1000) [root@server1 ~]# exportfs -rv exporting *:/home/hadoop [root@server1 ~]# showmount -e Export list for server1: /home/hadoop * 4.server2,3挂载 [root@server2 ~]# mount 172.25.68.1:/home/hadoop/ /home/hadoop/ [root@server2 ~]# df Filesystem 1K-blocks Used Available Use% Mounted on /dev/mapper/rhel-root 10258432 1097104 9161328 11% / devtmpfs 497292 0 497292 0% /dev tmpfs 508264 0 508264 0% /dev/shm tmpfs 508264 13072 495192 3% /run tmpfs 508264 0 508264 0% /sys/fs/cgroup /dev/sda1 1038336 141516 896820 14% /boot tmpfs 101656 0 101656 0% /run/user/0 172.25.68.1:/home/hadoop 10258432 2796544 7461888 28% /home/hadoop [root@server3 ~]# mount 172.25.68.1:/home/hadoop/ /home/hadoop/ 5.server1免密登陆server2和server3 [root@server1 tmp]# su - hadoop Last login: Tue May 28 10:17:23 CST 2019 on pts/0 [hadoop@server1 ~]$ ssh 172.25.68.2 [hadoop@server2 ~]$ logout Connection to 172.25.68.2 closed. [hadoop@server1 ~]$ ssh 172.25.68.3 [hadoop@server3 ~]$ logout Connection to 172.25.68.3 closed. 6.编辑文件(server1做namenode,server2和server3做datanode) [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server1 hadoop]$ vim core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://172.25.68.1:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hadoop@server1 hadoop]$ vim hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hadoop@server1 hadoop]$ vim workers [hadoop@server1 hadoop]$ cat workers 172.25.68.2 172.25.68.3 7.格式化，并启动服务 [hadoop@server1 hadoop]$ bin/hdfs namenode -format [hadoop@server1 hadoop]$ sbin/start-dfs.sh Starting namenodes on [server1] Starting datanodes Starting secondary namenodes [server1] [hadoop@server1 hadoop]$ jps #出现SecondaryNameNode 4673 SecondaryNameNode 4451 NameNode 4787 Jps 从节点可以是datanode [hadoop@server2 hadoop]$ jps 2384 DataNode 2447 Jps [hadoop@server3 hadoop]$ jps 2386 DataNode 2447 Jps 8.测试 [hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir -p /user/hadoop [hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir input [hadoop@server1 hadoop]$ bin/hdfs dfs -put etc/hadoop/*.xml input [hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output &#39;dfs[a-z.]+&#39; 网页上查看，有两个节点，且数据已经上传 9.添加节点server4 [root@server4 ~]# useradd -u 1000 hadoop [root@server4 ~]# yum install -y nfs-utils [root@server4 ~]# systemctl start rpcbind [root@server4 ~]# mount 172.25.68.1:/home/hadoop /home/hadoop [root@server4 ~]# su - hadoop [hadoop@server4 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server4 hadoop]$ vim workers 172.25.68.2 172.25.68.3 172.25.68.4 [hadoop@server4 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ sbin/start-dfs.sh [hadoop@server4 hadoop]$ jps 3029 DataNode 3081 Jps 浏览器查看，节点添加成功 server4上传文件 [hadoop@server4 hadoop]$ dd if=/dev/zero of=bigfile bs=1M count=500 500+0 records in 500+0 records out 524288000 bytes (524 MB) copied, 15.8634 s, 33.1 MB/s [hadoop@server4 hadoop]$ bin/hdfs dfs -put bigfile" />
<link rel="canonical" href="https://uzzz.org/2019/05/28/788060.html" />
<meta property="og:url" content="https://uzzz.org/2019/05/28/788060.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-28T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"hadoop Hadoop是一个由Apache基金会所开发的分布式系统基础架构。 Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。 Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。 HDFS架构 （1）NameNode （2）DataNode （3）Secondary NameNode NameNode （1）是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。 （2）文件包括： fsimage:元数据镜像文件。存储某一时段NameNode内存元数据信息。 edits:操作日志文件。 fstime:保存最近一次checkpoint的时间 （3）以上这些文件是保存在linux的文件系统中。 SecondaryNameNode （1）HA的一个解决方案。但不支持热备。配置即可。 （2）执行过程：从NameNode上下载元数据信息（fsimage,edits），然后把二者合并，生成新的fsimage，在本地保存，并将其推送到NameNode，替换旧的fsimage. （3）默认在安装在NameNode节点上，但这样不安全！ Datanode （1）提供真实文件数据的存储服务。 （2）文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block. dfs.block.size （3）不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间 （4）Replication。多复本。默认是三个。hdfs-site.xml的dfs.replication属性 一.单机模式 建立用户，设置密码(密码此次设置为(redhat) [root@server1 ~]# useradd -u 1000 hadoop [root@server1 ~]# passwd hadoop 2.hadoop的安装配置 [root@server1 ~]# mv hadoop-3.0.3.tar.gz jdk-8u181-linux-x64.tar.gz /home/hadoop [root@server1 ~]# su - hadoop [hadoop@server1 ~]$ ls hadoop-3.0.3.tar.gz jdk-8u181-linux-x64.tar.gz [hadoop@server1 ~]$ tar zxf jdk-8u181-linux-x64.tar.gz [hadoop@server1 ~]$ tar zxf hadoop-3.0.3.tar.gz [hadoop@server1 ~]$ ln -s jdk1.8.0_181/ java [hadoop@server1 ~]$ ln -s hadoop-3.0.3 hadoop [hadoop@server1 ~]$ ls hadoop hadoop-3.0.3.tar.gz jdk1.8.0_181 hadoop-3.0.3 java jdk-8u181-linux-x64.tar.gz 3.配置环境变量 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server1 hadoop]$ vim hadoop-env.sh 54 export JAVA_HOME=/home/hadoop/java [hadoop@server1 ~]$ vim .bash_profile PATH=$PATH:$HOME/.local/bin:$HOME/bin:$HOME/java/bin [hadoop@server1 ~]$ source .bash_profile [hadoop@server1 ~]$ jps 2133 Jps 4.测试 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ mkdir input [hadoop@server1 hadoop]$ cp etc/hadoop/*.xml input/ [hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output &#39;dfs[a-z.]+&#39; [hadoop@server1 hadoop]$ cd output/ [hadoop@server1 output]$ ls part-r-00000 _SUCCESS [hadoop@server1 output]$ cat * 1 dfsadmin 二.伪分布式 namenode和datanode都在自己这台主机上 1.编辑文件 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server1 hadoop]$ vim core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hadoop@server1 hadoop]$ vim hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 2.生成密钥做免密连接 [hadoop@server1 hadoop]$ ssh-keygen [hadoop@server1 hadoop]$ ssh-copy-id localhost 3.格式化，并开启服务 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ bin/hdfs namenode -format [hadoop@server1 hadoop]$ cd sbin/ [hadoop@server1 sbin]$ ./start-dfs.sh [hadoop@server1 sbin]$ jps 2458 NameNode 2906 Jps 2765 SecondaryNameNode 2575 DataNode 4.浏览器查看http://172.25.68.1:9870 5.测试，创建目录，并上传 [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir -p /user/hadoop [hadoop@server1 hadoop]$ bin/hdfs dfs -ls [hadoop@server1 hadoop]$ bin/hdfs dfs -put input [hadoop@server1 hadoop]$ bin/hdfs dfs -ls Found 1 items drwxr-xr-x - hadoop supergroup 0 2019-05-28 10:20 input 删除input和output文件，重新执行命令(测试从分布式上拉取文件) [hadoop@server1 hadoop]$ rm -fr input/ output/ [hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output &#39;dfs[a-z.]+&#39; [hadoop@server1 hadoop]$ ls bin etc include lib libexec LICENSE.txt logs NOTICE.txt README.txt sbin share **此时input和output不会出现在当前目录下，而是上传到了分布式文件系统中，网页上可以看到** [hadoop@server1 hadoop]$ bin/hdfs dfs -cat output/* 1 dfsadmin [hadoop@server1 hadoop]$ bin/hdfs dfs -get output ##从分布式系统中get下来output目录 [hadoop@server1 hadoop]$ cd output/ [hadoop@server1 output]$ ls part-r-00000 _SUCCESS [hadoop@server1 output]$ cat * 1 dfsadmin 分布式 主机是namenode，其它从机是datanode 1.先停掉服务，清除原来的数据 [hadoop@server1 hadoop]$ sbin/stop-dfs.sh Stopping namenodes on [localhost] Stopping datanodes Stopping secondary namenodes [server1] [hadoop@server1 hadoop]$ jps 3927 Jps [hadoop@server1 hadoop]$ cd /tmp/ [hadoop@server1 tmp]$ ls hadoop hadoop-hadoop hsperfdata_hadoop [hadoop@server1 tmp]$ rm -fr * 2.新开两个虚拟机，当做节点 [root@server2 ~]# useradd -u 1000 hadoop [root@server3 ~]# useradd -u 1000 hadoop [root@server1 ~]# yum install -y nfs-utils [root@server2 ~]# yum install -y nfs-utils [root@server3 ~]# yum install -y nfs-utils [root@server1 ~]# systemctl start rpcbind [root@server2 ~]# systemctl start rpcbind [root@server3 ~]# systemctl start rpcbind 3.server1开启服务，配置 [root@server1 ~]# systemctl start nfs-server [root@server1 ~]# vim /etc/exports /home/hadoop *(rw,anonuid=1000,anongid=1000) [root@server1 ~]# exportfs -rv exporting *:/home/hadoop [root@server1 ~]# showmount -e Export list for server1: /home/hadoop * 4.server2,3挂载 [root@server2 ~]# mount 172.25.68.1:/home/hadoop/ /home/hadoop/ [root@server2 ~]# df Filesystem 1K-blocks Used Available Use% Mounted on /dev/mapper/rhel-root 10258432 1097104 9161328 11% / devtmpfs 497292 0 497292 0% /dev tmpfs 508264 0 508264 0% /dev/shm tmpfs 508264 13072 495192 3% /run tmpfs 508264 0 508264 0% /sys/fs/cgroup /dev/sda1 1038336 141516 896820 14% /boot tmpfs 101656 0 101656 0% /run/user/0 172.25.68.1:/home/hadoop 10258432 2796544 7461888 28% /home/hadoop [root@server3 ~]# mount 172.25.68.1:/home/hadoop/ /home/hadoop/ 5.server1免密登陆server2和server3 [root@server1 tmp]# su - hadoop Last login: Tue May 28 10:17:23 CST 2019 on pts/0 [hadoop@server1 ~]$ ssh 172.25.68.2 [hadoop@server2 ~]$ logout Connection to 172.25.68.2 closed. [hadoop@server1 ~]$ ssh 172.25.68.3 [hadoop@server3 ~]$ logout Connection to 172.25.68.3 closed. 6.编辑文件(server1做namenode,server2和server3做datanode) [hadoop@server1 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server1 hadoop]$ vim core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://172.25.68.1:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hadoop@server1 hadoop]$ vim hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hadoop@server1 hadoop]$ vim workers [hadoop@server1 hadoop]$ cat workers 172.25.68.2 172.25.68.3 7.格式化，并启动服务 [hadoop@server1 hadoop]$ bin/hdfs namenode -format [hadoop@server1 hadoop]$ sbin/start-dfs.sh Starting namenodes on [server1] Starting datanodes Starting secondary namenodes [server1] [hadoop@server1 hadoop]$ jps #出现SecondaryNameNode 4673 SecondaryNameNode 4451 NameNode 4787 Jps 从节点可以是datanode [hadoop@server2 hadoop]$ jps 2384 DataNode 2447 Jps [hadoop@server3 hadoop]$ jps 2386 DataNode 2447 Jps 8.测试 [hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir -p /user/hadoop [hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir input [hadoop@server1 hadoop]$ bin/hdfs dfs -put etc/hadoop/*.xml input [hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output &#39;dfs[a-z.]+&#39; 网页上查看，有两个节点，且数据已经上传 9.添加节点server4 [root@server4 ~]# useradd -u 1000 hadoop [root@server4 ~]# yum install -y nfs-utils [root@server4 ~]# systemctl start rpcbind [root@server4 ~]# mount 172.25.68.1:/home/hadoop /home/hadoop [root@server4 ~]# su - hadoop [hadoop@server4 hadoop]$ pwd /home/hadoop/hadoop/etc/hadoop [hadoop@server4 hadoop]$ vim workers 172.25.68.2 172.25.68.3 172.25.68.4 [hadoop@server4 hadoop]$ pwd /home/hadoop/hadoop [hadoop@server1 hadoop]$ sbin/start-dfs.sh [hadoop@server4 hadoop]$ jps 3029 DataNode 3081 Jps 浏览器查看，节点添加成功 server4上传文件 [hadoop@server4 hadoop]$ dd if=/dev/zero of=bigfile bs=1M count=500 500+0 records in 500+0 records out 524288000 bytes (524 MB) copied, 15.8634 s, 33.1 MB/s [hadoop@server4 hadoop]$ bin/hdfs dfs -put bigfile","@type":"BlogPosting","url":"https://uzzz.org/2019/05/28/788060.html","headline":"hadoop单机模式、伪分布式和分布式","dateModified":"2019-05-28T00:00:00+08:00","datePublished":"2019-05-28T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/05/28/788060.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>hadoop单机模式、伪分布式和分布式</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-github-gist"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h1><a id="hadoop_0"></a>hadoop</h1> 
  <p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。<br> Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。<br> Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。</p> 
  <h2><a id="HDFS_5"></a>HDFS架构</h2> 
  <p>（1）NameNode<br> （2）DataNode<br> （3）Secondary NameNode</p> 
  <h5><a id="NameNode_10"></a>NameNode</h5> 
  <p>（1）是整个文件系统的管理节点。它维护着整个文件系统的文件目录树，文件/目录的元信息和每个文件对应的数据块列表。接收用户的操作请求。<br> （2）文件包括：<br> fsimage:元数据镜像文件。存储某一时段NameNode内存元数据信息。<br> edits:操作日志文件。<br> fstime:保存最近一次checkpoint的时间<br> （3）以上这些文件是保存在linux的文件系统中。</p> 
  <h5><a id="SecondaryNameNode_18"></a>SecondaryNameNode</h5> 
  <p>（1）HA的一个解决方案。但不支持热备。配置即可。<br> （2）执行过程：从NameNode上下载元数据信息（fsimage,edits），然后把二者合并，生成新的fsimage，在本地保存，并将其推送到NameNode，替换旧的fsimage.<br> （3）默认在安装在NameNode节点上，但这样不安全！</p> 
  <h5><a id="Datanode_23"></a>Datanode</h5> 
  <p>（1）提供真实文件数据的存储服务。<br> （2）文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block.<br> dfs.block.size<br> （3）不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间<br> （4）Replication。多复本。默认是三个。hdfs-site.xml的dfs.replication属性</p> 
  <h3><a id="_31"></a>一.单机模式</h3> 
  <ol> 
   <li>建立用户，设置密码(密码此次设置为(redhat)</li> 
  </ol> 
  <pre><code>[root@server1 ~]# useradd -u 1000 hadoop
[root@server1 ~]# passwd hadoop
</code></pre> 
  <p>2.hadoop的安装配置</p> 
  <pre><code>[root@server1 ~]# mv hadoop-3.0.3.tar.gz jdk-8u181-linux-x64.tar.gz /home/hadoop
[root@server1 ~]# su - hadoop
[hadoop@server1 ~]$ ls
hadoop-3.0.3.tar.gz  jdk-8u181-linux-x64.tar.gz
[hadoop@server1 ~]$ tar zxf jdk-8u181-linux-x64.tar.gz 
[hadoop@server1 ~]$ tar zxf hadoop-3.0.3.tar.gz
[hadoop@server1 ~]$ ln -s jdk1.8.0_181/ java
[hadoop@server1 ~]$ ln -s hadoop-3.0.3 hadoop
[hadoop@server1 ~]$ ls
hadoop        hadoop-3.0.3.tar.gz  jdk1.8.0_181
hadoop-3.0.3  java                 jdk-8u181-linux-x64.tar.gz
</code></pre> 
  <p>3.配置环境变量</p> 
  <pre><code>[hadoop@server1 hadoop]$ pwd
/home/hadoop/hadoop/etc/hadoop
[hadoop@server1 hadoop]$ vim hadoop-env.sh 
 54 export JAVA_HOME=/home/hadoop/java

[hadoop@server1 ~]$ vim .bash_profile 
PATH=$PATH:$HOME/.local/bin:$HOME/bin:$HOME/java/bin
[hadoop@server1 ~]$ source .bash_profile 
[hadoop@server1 ~]$ jps
2133 Jps
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190527224645621.png" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190527224641326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2R6aDExMjU2NDEyMzk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 4.测试</p> 
  <pre><code>[hadoop@server1 hadoop]$ pwd
/home/hadoop/hadoop
[hadoop@server1 hadoop]$ mkdir input
[hadoop@server1 hadoop]$ cp etc/hadoop/*.xml input/
[hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output 'dfs[a-z.]+'

[hadoop@server1 hadoop]$ cd output/
[hadoop@server1 output]$ ls
part-r-00000  _SUCCESS
[hadoop@server1 output]$ cat *
1	dfsadmin
</code></pre> 
  <h3><a id="_89"></a>二.伪分布式</h3> 
  <p>namenode和datanode都在自己这台主机上<br> 1.编辑文件</p> 
  <pre><code>[hadoop@server1 hadoop]$ pwd
/home/hadoop/hadoop/etc/hadoop
[hadoop@server1 hadoop]$ vim core-site.xml
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;

[hadoop@server1 hadoop]$ vim hdfs-site.xml
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt; 
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre> 
  <p>2.生成密钥做免密连接</p> 
  <pre><code>[hadoop@server1 hadoop]$ ssh-keygen 
[hadoop@server1 hadoop]$ ssh-copy-id localhost
</code></pre> 
  <p>3.格式化，并开启服务</p> 
  <pre><code>[hadoop@server1 hadoop]$ pwd
/home/hadoop/hadoop
[hadoop@server1 hadoop]$ bin/hdfs namenode -format
[hadoop@server1 hadoop]$ cd sbin/
[hadoop@server1 sbin]$ ./start-dfs.sh 
[hadoop@server1 sbin]$ jps
2458 NameNode
2906 Jps
2765 SecondaryNameNode
2575 DataNode
</code></pre> 
  <p>4.浏览器查看http://172.25.68.1:9870<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528103147862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2R6aDExMjU2NDEyMzk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>5.测试，创建目录，并上传</p> 
  <pre><code>[hadoop@server1 hadoop]$ pwd
/home/hadoop/hadoop
[hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir -p /user/hadoop
[hadoop@server1 hadoop]$ bin/hdfs dfs -ls
[hadoop@server1 hadoop]$ bin/hdfs dfs -put input
[hadoop@server1 hadoop]$ bin/hdfs dfs -ls
Found 1 items
drwxr-xr-x   - hadoop supergroup          0 2019-05-28 10:20 input
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528103106135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2R6aDExMjU2NDEyMzk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528103112966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2R6aDExMjU2NDEyMzk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <ol start="6"> 
   <li>删除input和output文件，重新执行命令(测试从分布式上拉取文件)</li> 
  </ol> 
  <pre><code>[hadoop@server1 hadoop]$ rm -fr input/ output/
[hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output 'dfs[a-z.]+'
[hadoop@server1 hadoop]$ ls
bin  etc  include  lib  libexec  LICENSE.txt  logs  NOTICE.txt  README.txt  sbin  share
**此时input和output不会出现在当前目录下，而是上传到了分布式文件系统中，网页上可以看到**

[hadoop@server1 hadoop]$ bin/hdfs dfs -cat output/*
1	dfsadmin
[hadoop@server1 hadoop]$ bin/hdfs dfs -get output      ##从分布式系统中get下来output目录
[hadoop@server1 hadoop]$ cd output/
[hadoop@server1 output]$ ls
part-r-00000  _SUCCESS
[hadoop@server1 output]$ cat *
1	dfsadmin
</code></pre> 
  <h3><a id="_174"></a>分布式</h3> 
  <p>主机是namenode，其它从机是datanode<br> 1.先停掉服务，清除原来的数据</p> 
  <pre><code>[hadoop@server1 hadoop]$ sbin/stop-dfs.sh 
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [server1]
[hadoop@server1 hadoop]$ jps
3927 Jps
[hadoop@server1 hadoop]$ cd /tmp/
[hadoop@server1 tmp]$ ls
hadoop  hadoop-hadoop  hsperfdata_hadoop
[hadoop@server1 tmp]$ rm -fr *
</code></pre> 
  <p>2.新开两个虚拟机，当做节点</p> 
  <pre><code>[root@server2 ~]# useradd -u 1000 hadoop
[root@server3 ~]# useradd -u 1000 hadoop

[root@server1 ~]# yum install -y nfs-utils
[root@server2 ~]# yum install -y nfs-utils
[root@server3 ~]# yum install -y nfs-utils

[root@server1 ~]# systemctl start rpcbind
[root@server2 ~]# systemctl start rpcbind
[root@server3 ~]# systemctl start rpcbind
</code></pre> 
  <p>3.server1开启服务，配置</p> 
  <pre><code>[root@server1 ~]# systemctl start nfs-server
[root@server1 ~]# vim /etc/exports
/home/hadoop   *(rw,anonuid=1000,anongid=1000)
[root@server1 ~]# exportfs -rv
exporting *:/home/hadoop
[root@server1 ~]# showmount -e
Export list for server1:
/home/hadoop *
</code></pre> 
  <p>4.server2,3挂载</p> 
  <pre><code>[root@server2 ~]# mount 172.25.68.1:/home/hadoop/ /home/hadoop/
[root@server2 ~]# df
Filesystem               1K-blocks    Used Available Use% Mounted on
/dev/mapper/rhel-root     10258432 1097104   9161328  11% /
devtmpfs                    497292       0    497292   0% /dev
tmpfs                       508264       0    508264   0% /dev/shm
tmpfs                       508264   13072    495192   3% /run
tmpfs                       508264       0    508264   0% /sys/fs/cgroup
/dev/sda1                  1038336  141516    896820  14% /boot
tmpfs                       101656       0    101656   0% /run/user/0
172.25.68.1:/home/hadoop  10258432 2796544   7461888  28% /home/hadoop
[root@server3 ~]# mount 172.25.68.1:/home/hadoop/ /home/hadoop/
</code></pre> 
  <p>5.server1免密登陆server2和server3</p> 
  <pre><code>[root@server1 tmp]# su - hadoop
Last login: Tue May 28 10:17:23 CST 2019 on pts/0
[hadoop@server1 ~]$ ssh 172.25.68.2
[hadoop@server2 ~]$ logout
Connection to 172.25.68.2 closed.
[hadoop@server1 ~]$ ssh 172.25.68.3
[hadoop@server3 ~]$ logout
Connection to 172.25.68.3 closed.
</code></pre> 
  <p>6.编辑文件(server1做namenode,server2和server3做datanode)</p> 
  <pre><code>[hadoop@server1 hadoop]$ pwd
/home/hadoop/hadoop/etc/hadoop
[hadoop@server1 hadoop]$ vim core-site.xml 
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://172.25.68.1:9000&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;


[hadoop@server1 hadoop]$ vim hdfs-site.xml 
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;2&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;

[hadoop@server1 hadoop]$ vim workers 
[hadoop@server1 hadoop]$ cat workers 
172.25.68.2
172.25.68.3
</code></pre> 
  <p>7.格式化，并启动服务</p> 
  <pre><code>[hadoop@server1 hadoop]$ bin/hdfs namenode -format
[hadoop@server1 hadoop]$ sbin/start-dfs.sh 
Starting namenodes on [server1]
Starting datanodes
Starting secondary namenodes [server1]
[hadoop@server1 hadoop]$ jps			#出现SecondaryNameNode
4673 SecondaryNameNode
4451 NameNode
4787 Jps

从节点可以是datanode
[hadoop@server2 hadoop]$ jps
2384 DataNode
2447 Jps
[hadoop@server3 hadoop]$ jps
2386 DataNode
2447 Jps
</code></pre> 
  <p>8.测试</p> 
  <pre><code>[hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir -p /user/hadoop
[hadoop@server1 hadoop]$ bin/hdfs dfs -mkdir input
[hadoop@server1 hadoop]$ bin/hdfs dfs -put etc/hadoop/*.xml input
[hadoop@server1 hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.3.jar grep input output 'dfs[a-z.]+'
</code></pre> 
  <p>网页上查看，有两个节点，且数据已经上传<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528105840558.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2R6aDExMjU2NDEyMzk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528105847952.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2R6aDExMjU2NDEyMzk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>9.添加节点server4</p> 
  <pre><code>[root@server4 ~]# useradd -u  1000 hadoop
[root@server4 ~]# yum  install -y  nfs-utils
[root@server4 ~]# systemctl start rpcbind
[root@server4 ~]# mount 172.25.68.1:/home/hadoop /home/hadoop
[root@server4 ~]# su - hadoop
[hadoop@server4 hadoop]$ pwd
/home/hadoop/hadoop/etc/hadoop
[hadoop@server4 hadoop]$ vim workers 
172.25.68.2
172.25.68.3
172.25.68.4


[hadoop@server4 hadoop]$ pwd
/home/hadoop/hadoop
[hadoop@server1 hadoop]$ sbin/start-dfs.sh
[hadoop@server4 hadoop]$ jps
3029 DataNode
3081 Jps
</code></pre> 
  <p>浏览器查看，节点添加成功<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019052811065479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2R6aDExMjU2NDEyMzk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> server4上传文件</p> 
  <pre><code>[hadoop@server4 hadoop]$ dd if=/dev/zero of=bigfile bs=1M count=500
500+0 records in
500+0 records out
524288000 bytes (524 MB) copied, 15.8634 s, 33.1 MB/s
[hadoop@server4 hadoop]$ bin/hdfs dfs -put bigfile
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/201905281107589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2R6aDExMjU2NDEyMzk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
