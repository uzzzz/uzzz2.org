<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Hadoop新一代MapReduce框架更快、更强 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Hadoop新一代MapReduce框架更快、更强" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="对于业界的大数据存储及分布式处理系统来说，Hadoop 是耳熟能详的卓越开源分布式文件存储及处理框架，对于 Hadoop 框架的介绍在此不再累述，随着需求的发展，Yarn 框架浮出水面， @依然光荣复兴的 博客给我们做了很详细的介绍，读者通过本文中新旧 Hadoop MapReduce 框架的对比，更能深刻理解新的 yarn 框架的技术原理和设计思想。 背景 Yarn是一个分布式的资源管理系统，用以提高分布式的集群环境下的资源利用率，这些资源包括内存、IO、网络、磁盘等。其产生的原因是为了解决原MapReduce框架的不足。最初MapReduce的committer们还可以周期性的在已有的代码上进行修改，可是随着代码的增加以及原MapReduce框架设计的不足，在原MapReduce框架上进行修改变得越来越困难，所以MapReduce的committer们决定从架构上重新设计MapReduce,使下一代的MapReduce(MRv2/Yarn)框架具有更好的扩展性、可用性、可靠性、向后兼容性和更高的资源利用率以及能支持除了MapReduce计算框架外的更多的计算框架。 原MapReduce框架的不足 推荐一个大数据学习群 606859705每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享&nbsp; JobTracker是集群事务的集中处理点，存在单点故障 JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗 在taskTracker端，用map/reduce task作为资源的表示过于简单，没有考虑到CPU、内存等资源情况，当把两个需要消耗大内存的task调度到一起，很容易出现OOM 把资源强制划分为map/reduce slot,当只有map task时，reduce slot不能用；当只有reduce task时，map slot不能用，容易造成资源利用不足。 Yarn架构 Yarn/MRv2最基本的想法是将原JobTracker主要的资源管理和job调度/监视功能分开作为两个单独的守护进程。有一个全局的ResourceManager(RM)和每个Application有一个ApplicationMaster(AM)，Application相当于map-reduce job或者DAG jobs。ResourceManager和NodeManager(NM)组成了基本的数据计算框架。ResourceManager协调集群的资源利用，任何client或者运行着的applicatitonMaster想要运行job或者task都得向RM申请一定的资源。ApplicatonMaster是一个框架特殊的库，对于MapReduce框架而言有它自己的AM实现，用户也可以实现自己的AM，在运行的时候，AM会与NM一起来启动和监视tasks。 ResourceManager ResourceManager作为资源的协调者有两个主要的组件：Scheduler和ApplicationsManager(AsM)。 Scheduler负责分配最少但满足application运行所需的资源量给Application。Scheduler只是基于资源的使用情况进行调度，并不负责监视/跟踪application的状态，当然也不会处理失败的task。RM使用resource container概念来管理集群的资源，resource container是资源的抽象，每个container包括一定的内存、IO、网络等资源，不过目前的实现只包括内存一种资源。 ApplicationsManager负责处理client提交的job以及协商第一个container以供applicationMaster运行，并且在applicationMaster失败的时候会重新启动applicationMaster。下面阐述RM具体完成的一些功能。 1.资源调度：Scheduler从所有运行着的application收到资源请求后构建一个全局的资源分配计划，然后根据application特殊的限制以及全局的一些限制条件分配资源。 2.资源监视：Scheduler会周期性的接收来自NM的资源使用率的监控信息，另外applicationMaster可以从Scheduler得到属于它的已完成的container的状态信息。 3.Application提交： client向AsM获得一个applicationIDclient将application定义以及需要的jar包 client将application定义以及需要的jar包文件等上传到hdfs的指定目录，由yarn-site.xml的yarn.app.mapreduce.am.staging-dir指定 client构造资源请求的对象以及application的提交上下文发送给AsM AsM接收application的提交上下文 AsM根据application的信息向Scheduler协商一个Container供applicationMaster运行，然后启动applicationMaster 向该container所属的NM发送launchContainer信息启动该container,也即启动applicationMaster、AsM向client提供运行着的AM的状态信息。 4.AM的生命周期：AsM负责系统中所有AM的生命周期的管理。AsM负责AM的启动，当AM启动后，AM会周期性的向AsM发送heartbeat，默认是1s，AsM据此了解AM的存活情况，并且在AM失败时负责重启AM，若是一定时间过后(默认10分钟)没有收到AM的heartbeat，AsM就认为该AM失败了。 关于ResourceManager的可用性目前还没有很好的实现，不过Cloudera公司的CDH4.4以后的版本实现了一个简单的高可用性，使用了Hadoop-common项目中HA部分的代码，采用了类似hdfs namenode高可用性的设计，给RM引入了active和standby状态，不过没有与journalnode相对应的角色，只是由zookeeper来负责维护RM的状态，这样的设计只是一个最简单的方案，避免了手动重启RM，离真正的生产可用还有一段距离。 NodeManager NM主要负责启动RM分配给AM的container以及代表AM的container，并且会监视container的运行情况。在启动container的时候，NM会设置一些必要的环境变量以及将container运行所需的jar包、文件等从hdfs下载到本地，也就是所谓的资源本地化；当所有准备工作做好后，才会启动代表该container的脚本将程序启动起来。启动起来后，NM会周期性的监视该container运行占用的资源情况，若是超过了该container所声明的资源量，则会kill掉该container所代表的进程。 另外，NM还提供了一个简单的服务以管理它所在机器的本地目录。Applications可以继续访问本地目录即使那台机器上已经没有了属于它的container在运行。例如，Map-Reduce应用程序使用这个服务存储map output并且shuffle它们给相应的reduce task。 在NM上还可以扩展自己的服务，yarn提供了一个yarn.nodemanager.aux-services的配置项，通过该配置，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的。 NM在本地为每个运行着的application生成如下的目录结构：&nbsp; Container目录下的目录结构如下：&nbsp; 在启动一个container的时候，NM就执行该container的default_container___executor.sh，该脚本内部会执行launch_container.sh。launch_container.sh会先设置一些环境变量，最后启动执行程序的命令。对于MapReduce而言，启动AM就执行org.apache.hadoop.mapreduce.v2.app.MRAppMaster；启动map/reduce task就执行org.apache.hadoop.mapred.YarnChild。 ApplicationMaster ApplicationMaster是一个框架特殊的库，对于Map-Reduce计算模型而言有它自己的ApplicationMaster实现，对于其他的想要运行在yarn上的计算模型而言，必须得实现针对该计算模型的ApplicationMaster用以向RM申请资源运行task，比如运行在yarn上的spark框架也有对应的ApplicationMaster实现，归根结底，yarn是一个资源管理的框架，并不是一个计算框架，要想在yarn上运行应用程序，还得有特定的计算框架的实现。由于yarn是伴随着MRv2一起出现的，所以下面简要概述MRv2在yarn上的运行流程。 MRv2运行流程： MR JobClient向resourceManager(AsM)提交一个job AsM向Scheduler请求一个供MR AM运行的container，然后启动它 MR AM启动起来后向AsM注册 MR JobClient向AsM获取到MR AM相关的信息，然后直接与MR AM进行通信 MR AM计算splits并为所有的map构造资源请求 MR AM做一些必要的MR OutputCommitter的准备工作 MR AM向RM(Scheduler)发起资源请求，得到一组供map/reduce task运行的container，然后与NM一起对每一个container执行一些必要的任务，包括资源本地化等 MR AM 监视运行着的task 直到完成，当task失败时，申请新的container运行失败的task 当每个map/reduce task完成后，MR AM运行MR OutputCommitter的cleanup 代码，也就是进行一些收尾工作 当所有的map/reduce完成后，MR AM运行OutputCommitter的必要的job commit或者abort APIs MR AM退出。 在Yarn上写应用程序 在yarn上写应用程序并不同于我们熟知的MapReduce应用程序，必须牢记yarn只是一个资源管理的框架，并不是一个计算框架，计算框架可以运行在yarn上。我们所能做的就是向RM申请container,然后配合NM一起来启动container。就像MRv2一样，jobclient请求用于MR AM运行的container，设置环境变量和启动命令，然后交由NM去启动MR AM，随后map/reduce task就由MR AM全权负责，当然task的启动也是由MR AM向RM申请container，然后配合NM一起来启动的。所以要想在yarn上运行非特定计算框架的程序，我们就得实现自己的client和applicationMaster。另外我们自定义的AM需要放在各个NM的classpath下，因为AM可能运行在任何NM所在的机器上。 推荐一个大数据学习群 606859705每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享" />
<meta property="og:description" content="对于业界的大数据存储及分布式处理系统来说，Hadoop 是耳熟能详的卓越开源分布式文件存储及处理框架，对于 Hadoop 框架的介绍在此不再累述，随着需求的发展，Yarn 框架浮出水面， @依然光荣复兴的 博客给我们做了很详细的介绍，读者通过本文中新旧 Hadoop MapReduce 框架的对比，更能深刻理解新的 yarn 框架的技术原理和设计思想。 背景 Yarn是一个分布式的资源管理系统，用以提高分布式的集群环境下的资源利用率，这些资源包括内存、IO、网络、磁盘等。其产生的原因是为了解决原MapReduce框架的不足。最初MapReduce的committer们还可以周期性的在已有的代码上进行修改，可是随着代码的增加以及原MapReduce框架设计的不足，在原MapReduce框架上进行修改变得越来越困难，所以MapReduce的committer们决定从架构上重新设计MapReduce,使下一代的MapReduce(MRv2/Yarn)框架具有更好的扩展性、可用性、可靠性、向后兼容性和更高的资源利用率以及能支持除了MapReduce计算框架外的更多的计算框架。 原MapReduce框架的不足 推荐一个大数据学习群 606859705每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享&nbsp; JobTracker是集群事务的集中处理点，存在单点故障 JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗 在taskTracker端，用map/reduce task作为资源的表示过于简单，没有考虑到CPU、内存等资源情况，当把两个需要消耗大内存的task调度到一起，很容易出现OOM 把资源强制划分为map/reduce slot,当只有map task时，reduce slot不能用；当只有reduce task时，map slot不能用，容易造成资源利用不足。 Yarn架构 Yarn/MRv2最基本的想法是将原JobTracker主要的资源管理和job调度/监视功能分开作为两个单独的守护进程。有一个全局的ResourceManager(RM)和每个Application有一个ApplicationMaster(AM)，Application相当于map-reduce job或者DAG jobs。ResourceManager和NodeManager(NM)组成了基本的数据计算框架。ResourceManager协调集群的资源利用，任何client或者运行着的applicatitonMaster想要运行job或者task都得向RM申请一定的资源。ApplicatonMaster是一个框架特殊的库，对于MapReduce框架而言有它自己的AM实现，用户也可以实现自己的AM，在运行的时候，AM会与NM一起来启动和监视tasks。 ResourceManager ResourceManager作为资源的协调者有两个主要的组件：Scheduler和ApplicationsManager(AsM)。 Scheduler负责分配最少但满足application运行所需的资源量给Application。Scheduler只是基于资源的使用情况进行调度，并不负责监视/跟踪application的状态，当然也不会处理失败的task。RM使用resource container概念来管理集群的资源，resource container是资源的抽象，每个container包括一定的内存、IO、网络等资源，不过目前的实现只包括内存一种资源。 ApplicationsManager负责处理client提交的job以及协商第一个container以供applicationMaster运行，并且在applicationMaster失败的时候会重新启动applicationMaster。下面阐述RM具体完成的一些功能。 1.资源调度：Scheduler从所有运行着的application收到资源请求后构建一个全局的资源分配计划，然后根据application特殊的限制以及全局的一些限制条件分配资源。 2.资源监视：Scheduler会周期性的接收来自NM的资源使用率的监控信息，另外applicationMaster可以从Scheduler得到属于它的已完成的container的状态信息。 3.Application提交： client向AsM获得一个applicationIDclient将application定义以及需要的jar包 client将application定义以及需要的jar包文件等上传到hdfs的指定目录，由yarn-site.xml的yarn.app.mapreduce.am.staging-dir指定 client构造资源请求的对象以及application的提交上下文发送给AsM AsM接收application的提交上下文 AsM根据application的信息向Scheduler协商一个Container供applicationMaster运行，然后启动applicationMaster 向该container所属的NM发送launchContainer信息启动该container,也即启动applicationMaster、AsM向client提供运行着的AM的状态信息。 4.AM的生命周期：AsM负责系统中所有AM的生命周期的管理。AsM负责AM的启动，当AM启动后，AM会周期性的向AsM发送heartbeat，默认是1s，AsM据此了解AM的存活情况，并且在AM失败时负责重启AM，若是一定时间过后(默认10分钟)没有收到AM的heartbeat，AsM就认为该AM失败了。 关于ResourceManager的可用性目前还没有很好的实现，不过Cloudera公司的CDH4.4以后的版本实现了一个简单的高可用性，使用了Hadoop-common项目中HA部分的代码，采用了类似hdfs namenode高可用性的设计，给RM引入了active和standby状态，不过没有与journalnode相对应的角色，只是由zookeeper来负责维护RM的状态，这样的设计只是一个最简单的方案，避免了手动重启RM，离真正的生产可用还有一段距离。 NodeManager NM主要负责启动RM分配给AM的container以及代表AM的container，并且会监视container的运行情况。在启动container的时候，NM会设置一些必要的环境变量以及将container运行所需的jar包、文件等从hdfs下载到本地，也就是所谓的资源本地化；当所有准备工作做好后，才会启动代表该container的脚本将程序启动起来。启动起来后，NM会周期性的监视该container运行占用的资源情况，若是超过了该container所声明的资源量，则会kill掉该container所代表的进程。 另外，NM还提供了一个简单的服务以管理它所在机器的本地目录。Applications可以继续访问本地目录即使那台机器上已经没有了属于它的container在运行。例如，Map-Reduce应用程序使用这个服务存储map output并且shuffle它们给相应的reduce task。 在NM上还可以扩展自己的服务，yarn提供了一个yarn.nodemanager.aux-services的配置项，通过该配置，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的。 NM在本地为每个运行着的application生成如下的目录结构：&nbsp; Container目录下的目录结构如下：&nbsp; 在启动一个container的时候，NM就执行该container的default_container___executor.sh，该脚本内部会执行launch_container.sh。launch_container.sh会先设置一些环境变量，最后启动执行程序的命令。对于MapReduce而言，启动AM就执行org.apache.hadoop.mapreduce.v2.app.MRAppMaster；启动map/reduce task就执行org.apache.hadoop.mapred.YarnChild。 ApplicationMaster ApplicationMaster是一个框架特殊的库，对于Map-Reduce计算模型而言有它自己的ApplicationMaster实现，对于其他的想要运行在yarn上的计算模型而言，必须得实现针对该计算模型的ApplicationMaster用以向RM申请资源运行task，比如运行在yarn上的spark框架也有对应的ApplicationMaster实现，归根结底，yarn是一个资源管理的框架，并不是一个计算框架，要想在yarn上运行应用程序，还得有特定的计算框架的实现。由于yarn是伴随着MRv2一起出现的，所以下面简要概述MRv2在yarn上的运行流程。 MRv2运行流程： MR JobClient向resourceManager(AsM)提交一个job AsM向Scheduler请求一个供MR AM运行的container，然后启动它 MR AM启动起来后向AsM注册 MR JobClient向AsM获取到MR AM相关的信息，然后直接与MR AM进行通信 MR AM计算splits并为所有的map构造资源请求 MR AM做一些必要的MR OutputCommitter的准备工作 MR AM向RM(Scheduler)发起资源请求，得到一组供map/reduce task运行的container，然后与NM一起对每一个container执行一些必要的任务，包括资源本地化等 MR AM 监视运行着的task 直到完成，当task失败时，申请新的container运行失败的task 当每个map/reduce task完成后，MR AM运行MR OutputCommitter的cleanup 代码，也就是进行一些收尾工作 当所有的map/reduce完成后，MR AM运行OutputCommitter的必要的job commit或者abort APIs MR AM退出。 在Yarn上写应用程序 在yarn上写应用程序并不同于我们熟知的MapReduce应用程序，必须牢记yarn只是一个资源管理的框架，并不是一个计算框架，计算框架可以运行在yarn上。我们所能做的就是向RM申请container,然后配合NM一起来启动container。就像MRv2一样，jobclient请求用于MR AM运行的container，设置环境变量和启动命令，然后交由NM去启动MR AM，随后map/reduce task就由MR AM全权负责，当然task的启动也是由MR AM向RM申请container，然后配合NM一起来启动的。所以要想在yarn上运行非特定计算框架的程序，我们就得实现自己的client和applicationMaster。另外我们自定义的AM需要放在各个NM的classpath下，因为AM可能运行在任何NM所在的机器上。 推荐一个大数据学习群 606859705每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享" />
<link rel="canonical" href="https://uzzz.org/2019/05/28/788075.html" />
<meta property="og:url" content="https://uzzz.org/2019/05/28/788075.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-28T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"对于业界的大数据存储及分布式处理系统来说，Hadoop 是耳熟能详的卓越开源分布式文件存储及处理框架，对于 Hadoop 框架的介绍在此不再累述，随着需求的发展，Yarn 框架浮出水面， @依然光荣复兴的 博客给我们做了很详细的介绍，读者通过本文中新旧 Hadoop MapReduce 框架的对比，更能深刻理解新的 yarn 框架的技术原理和设计思想。 背景 Yarn是一个分布式的资源管理系统，用以提高分布式的集群环境下的资源利用率，这些资源包括内存、IO、网络、磁盘等。其产生的原因是为了解决原MapReduce框架的不足。最初MapReduce的committer们还可以周期性的在已有的代码上进行修改，可是随着代码的增加以及原MapReduce框架设计的不足，在原MapReduce框架上进行修改变得越来越困难，所以MapReduce的committer们决定从架构上重新设计MapReduce,使下一代的MapReduce(MRv2/Yarn)框架具有更好的扩展性、可用性、可靠性、向后兼容性和更高的资源利用率以及能支持除了MapReduce计算框架外的更多的计算框架。 原MapReduce框架的不足 推荐一个大数据学习群 606859705每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享&nbsp; JobTracker是集群事务的集中处理点，存在单点故障 JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗 在taskTracker端，用map/reduce task作为资源的表示过于简单，没有考虑到CPU、内存等资源情况，当把两个需要消耗大内存的task调度到一起，很容易出现OOM 把资源强制划分为map/reduce slot,当只有map task时，reduce slot不能用；当只有reduce task时，map slot不能用，容易造成资源利用不足。 Yarn架构 Yarn/MRv2最基本的想法是将原JobTracker主要的资源管理和job调度/监视功能分开作为两个单独的守护进程。有一个全局的ResourceManager(RM)和每个Application有一个ApplicationMaster(AM)，Application相当于map-reduce job或者DAG jobs。ResourceManager和NodeManager(NM)组成了基本的数据计算框架。ResourceManager协调集群的资源利用，任何client或者运行着的applicatitonMaster想要运行job或者task都得向RM申请一定的资源。ApplicatonMaster是一个框架特殊的库，对于MapReduce框架而言有它自己的AM实现，用户也可以实现自己的AM，在运行的时候，AM会与NM一起来启动和监视tasks。 ResourceManager ResourceManager作为资源的协调者有两个主要的组件：Scheduler和ApplicationsManager(AsM)。 Scheduler负责分配最少但满足application运行所需的资源量给Application。Scheduler只是基于资源的使用情况进行调度，并不负责监视/跟踪application的状态，当然也不会处理失败的task。RM使用resource container概念来管理集群的资源，resource container是资源的抽象，每个container包括一定的内存、IO、网络等资源，不过目前的实现只包括内存一种资源。 ApplicationsManager负责处理client提交的job以及协商第一个container以供applicationMaster运行，并且在applicationMaster失败的时候会重新启动applicationMaster。下面阐述RM具体完成的一些功能。 1.资源调度：Scheduler从所有运行着的application收到资源请求后构建一个全局的资源分配计划，然后根据application特殊的限制以及全局的一些限制条件分配资源。 2.资源监视：Scheduler会周期性的接收来自NM的资源使用率的监控信息，另外applicationMaster可以从Scheduler得到属于它的已完成的container的状态信息。 3.Application提交： client向AsM获得一个applicationIDclient将application定义以及需要的jar包 client将application定义以及需要的jar包文件等上传到hdfs的指定目录，由yarn-site.xml的yarn.app.mapreduce.am.staging-dir指定 client构造资源请求的对象以及application的提交上下文发送给AsM AsM接收application的提交上下文 AsM根据application的信息向Scheduler协商一个Container供applicationMaster运行，然后启动applicationMaster 向该container所属的NM发送launchContainer信息启动该container,也即启动applicationMaster、AsM向client提供运行着的AM的状态信息。 4.AM的生命周期：AsM负责系统中所有AM的生命周期的管理。AsM负责AM的启动，当AM启动后，AM会周期性的向AsM发送heartbeat，默认是1s，AsM据此了解AM的存活情况，并且在AM失败时负责重启AM，若是一定时间过后(默认10分钟)没有收到AM的heartbeat，AsM就认为该AM失败了。 关于ResourceManager的可用性目前还没有很好的实现，不过Cloudera公司的CDH4.4以后的版本实现了一个简单的高可用性，使用了Hadoop-common项目中HA部分的代码，采用了类似hdfs namenode高可用性的设计，给RM引入了active和standby状态，不过没有与journalnode相对应的角色，只是由zookeeper来负责维护RM的状态，这样的设计只是一个最简单的方案，避免了手动重启RM，离真正的生产可用还有一段距离。 NodeManager NM主要负责启动RM分配给AM的container以及代表AM的container，并且会监视container的运行情况。在启动container的时候，NM会设置一些必要的环境变量以及将container运行所需的jar包、文件等从hdfs下载到本地，也就是所谓的资源本地化；当所有准备工作做好后，才会启动代表该container的脚本将程序启动起来。启动起来后，NM会周期性的监视该container运行占用的资源情况，若是超过了该container所声明的资源量，则会kill掉该container所代表的进程。 另外，NM还提供了一个简单的服务以管理它所在机器的本地目录。Applications可以继续访问本地目录即使那台机器上已经没有了属于它的container在运行。例如，Map-Reduce应用程序使用这个服务存储map output并且shuffle它们给相应的reduce task。 在NM上还可以扩展自己的服务，yarn提供了一个yarn.nodemanager.aux-services的配置项，通过该配置，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的。 NM在本地为每个运行着的application生成如下的目录结构：&nbsp; Container目录下的目录结构如下：&nbsp; 在启动一个container的时候，NM就执行该container的default_container___executor.sh，该脚本内部会执行launch_container.sh。launch_container.sh会先设置一些环境变量，最后启动执行程序的命令。对于MapReduce而言，启动AM就执行org.apache.hadoop.mapreduce.v2.app.MRAppMaster；启动map/reduce task就执行org.apache.hadoop.mapred.YarnChild。 ApplicationMaster ApplicationMaster是一个框架特殊的库，对于Map-Reduce计算模型而言有它自己的ApplicationMaster实现，对于其他的想要运行在yarn上的计算模型而言，必须得实现针对该计算模型的ApplicationMaster用以向RM申请资源运行task，比如运行在yarn上的spark框架也有对应的ApplicationMaster实现，归根结底，yarn是一个资源管理的框架，并不是一个计算框架，要想在yarn上运行应用程序，还得有特定的计算框架的实现。由于yarn是伴随着MRv2一起出现的，所以下面简要概述MRv2在yarn上的运行流程。 MRv2运行流程： MR JobClient向resourceManager(AsM)提交一个job AsM向Scheduler请求一个供MR AM运行的container，然后启动它 MR AM启动起来后向AsM注册 MR JobClient向AsM获取到MR AM相关的信息，然后直接与MR AM进行通信 MR AM计算splits并为所有的map构造资源请求 MR AM做一些必要的MR OutputCommitter的准备工作 MR AM向RM(Scheduler)发起资源请求，得到一组供map/reduce task运行的container，然后与NM一起对每一个container执行一些必要的任务，包括资源本地化等 MR AM 监视运行着的task 直到完成，当task失败时，申请新的container运行失败的task 当每个map/reduce task完成后，MR AM运行MR OutputCommitter的cleanup 代码，也就是进行一些收尾工作 当所有的map/reduce完成后，MR AM运行OutputCommitter的必要的job commit或者abort APIs MR AM退出。 在Yarn上写应用程序 在yarn上写应用程序并不同于我们熟知的MapReduce应用程序，必须牢记yarn只是一个资源管理的框架，并不是一个计算框架，计算框架可以运行在yarn上。我们所能做的就是向RM申请container,然后配合NM一起来启动container。就像MRv2一样，jobclient请求用于MR AM运行的container，设置环境变量和启动命令，然后交由NM去启动MR AM，随后map/reduce task就由MR AM全权负责，当然task的启动也是由MR AM向RM申请container，然后配合NM一起来启动的。所以要想在yarn上运行非特定计算框架的程序，我们就得实现自己的client和applicationMaster。另外我们自定义的AM需要放在各个NM的classpath下，因为AM可能运行在任何NM所在的机器上。 推荐一个大数据学习群 606859705每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享","@type":"BlogPosting","url":"https://uzzz.org/2019/05/28/788075.html","headline":"Hadoop新一代MapReduce框架更快、更强","dateModified":"2019-05-28T00:00:00+08:00","datePublished":"2019-05-28T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/05/28/788075.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Hadoop新一代MapReduce框架更快、更强</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>对于业界的大数据存储及分布式处理系统来说，Hadoop 是耳熟能详的卓越开源分布式文件存储及处理框架，对于 Hadoop 框架的介绍在此不再累述，随着需求的发展，Yarn 框架浮出水面， @依然光荣复兴的 博客给我们做了很详细的介绍，读者通过本文中新旧 Hadoop MapReduce 框架的对比，更能深刻理解新的 yarn 框架的技术原理和设计思想。<br><br> 背景<br><br> Yarn是一个分布式的资源管理系统，用以提高分布式的集群环境下的资源利用率，这些资源包括内存、IO、网络、磁盘等。其产生的原因是为了解决原MapReduce框架的不足。最初MapReduce的committer们还可以周期性的在已有的代码上进行修改，可是随着代码的增加以及原MapReduce框架设计的不足，在原MapReduce框架上进行修改变得越来越困难，所以MapReduce的committer们决定从架构上重新设计MapReduce,使下一代的MapReduce(MRv2/Yarn)框架具有更好的扩展性、可用性、可靠性、向后兼容性和更高的资源利用率以及能支持除了MapReduce计算框架外的更多的计算框架。<br><br> 原MapReduce框架的不足<br><br> 推荐一个大数据学习群 606859705每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享<br><br><img alt="" class="has" height="274" id="aimg_3920" src="https://bbs.bjsxt.com/data/attachment/forum/201701/04/142632ctmrmb7b5bn5s7rm.jpg" width="490">&nbsp;<br><br><br><br> JobTracker是集群事务的集中处理点，存在单点故障<br><br> JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗<br><br> 在taskTracker端，用map/reduce task作为资源的表示过于简单，没有考虑到CPU、内存等资源情况，当把两个需要消耗大内存的task调度到一起，很容易出现OOM<br><br> 把资源强制划分为map/reduce slot,当只有map task时，reduce slot不能用；当只有reduce task时，map slot不能用，容易造成资源利用不足。<br> Yarn架构<br><br> Yarn/MRv2最基本的想法是将原JobTracker主要的资源管理和job调度/监视功能分开作为两个单独的守护进程。有一个全局的ResourceManager(RM)和每个Application有一个ApplicationMaster(AM)，Application相当于map-reduce job或者DAG jobs。ResourceManager和NodeManager(NM)组成了基本的数据计算框架。ResourceManager协调集群的资源利用，任何client或者运行着的applicatitonMaster想要运行job或者task都得向RM申请一定的资源。ApplicatonMaster是一个框架特殊的库，对于MapReduce框架而言有它自己的AM实现，用户也可以实现自己的AM，在运行的时候，AM会与NM一起来启动和监视tasks。<br><br> ResourceManager<br><br> ResourceManager作为资源的协调者有两个主要的组件：Scheduler和ApplicationsManager(AsM)。<br><br> Scheduler负责分配最少但满足application运行所需的资源量给Application。Scheduler只是基于资源的使用情况进行调度，并不负责监视/跟踪application的状态，当然也不会处理失败的task。RM使用resource container概念来管理集群的资源，resource container是资源的抽象，每个container包括一定的内存、IO、网络等资源，不过目前的实现只包括内存一种资源。<br><br> ApplicationsManager负责处理client提交的job以及协商第一个container以供applicationMaster运行，并且在applicationMaster失败的时候会重新启动applicationMaster。下面阐述RM具体完成的一些功能。<br><br> 1.资源调度：Scheduler从所有运行着的application收到资源请求后构建一个全局的资源分配计划，然后根据application特殊的限制以及全局的一些限制条件分配资源。<br><br> 2.资源监视：Scheduler会周期性的接收来自NM的资源使用率的监控信息，另外applicationMaster可以从Scheduler得到属于它的已完成的container的状态信息。<br><br> 3.Application提交：<br><br> client向AsM获得一个applicationIDclient将application定义以及需要的jar包<br><br> client将application定义以及需要的jar包文件等上传到hdfs的指定目录，由yarn-site.xml的yarn.app.mapreduce.am.staging-dir指定<br><br> client构造资源请求的对象以及application的提交上下文发送给AsM<br><br> AsM接收application的提交上下文<br><br> AsM根据application的信息向Scheduler协商一个Container供applicationMaster运行，然后启动applicationMaster<br><br> 向该container所属的NM发送launchContainer信息启动该container,也即启动applicationMaster、AsM向client提供运行着的AM的状态信息。<br><br> 4.AM的生命周期：AsM负责系统中所有AM的生命周期的管理。AsM负责AM的启动，当AM启动后，AM会周期性的向AsM发送heartbeat，默认是1s，AsM据此了解AM的存活情况，并且在AM失败时负责重启AM，若是一定时间过后(默认10分钟)没有收到AM的heartbeat，AsM就认为该AM失败了。<br><br> 关于ResourceManager的可用性目前还没有很好的实现，不过Cloudera公司的CDH4.4以后的版本实现了一个简单的高可用性，使用了Hadoop-common项目中HA部分的代码，采用了类似hdfs namenode高可用性的设计，给RM引入了active和standby状态，不过没有与journalnode相对应的角色，只是由zookeeper来负责维护RM的状态，这样的设计只是一个最简单的方案，避免了手动重启RM，离真正的生产可用还有一段距离。<br><br> NodeManager<br><br> NM主要负责启动RM分配给AM的container以及代表AM的container，并且会监视container的运行情况。在启动container的时候，NM会设置一些必要的环境变量以及将container运行所需的jar包、文件等从hdfs下载到本地，也就是所谓的资源本地化；当所有准备工作做好后，才会启动代表该container的脚本将程序启动起来。启动起来后，NM会周期性的监视该container运行占用的资源情况，若是超过了该container所声明的资源量，则会kill掉该container所代表的进程。<br><br> 另外，NM还提供了一个简单的服务以管理它所在机器的本地目录。Applications可以继续访问本地目录即使那台机器上已经没有了属于它的container在运行。例如，Map-Reduce应用程序使用这个服务存储map output并且shuffle它们给相应的reduce task。<br><br> 在NM上还可以扩展自己的服务，yarn提供了一个yarn.nodemanager.aux-services的配置项，通过该配置，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的。<br><br> NM在本地为每个运行着的application生成如下的目录结构：<br><br><img alt="" class="has" id="aimg_3921" src="https://bbs.bjsxt.com/data/attachment/forum/201701/04/142632fzo1drt9696td1r8.jpg" width="318">&nbsp;<br><br><br> Container目录下的目录结构如下：<br><br><img alt="" class="has" id="aimg_3922" src="https://bbs.bjsxt.com/data/attachment/forum/201701/04/142634wnh6tfhcatwmwnt3.jpg" width="238">&nbsp;<br><br><br> 在启动一个container的时候，NM就执行该container的default_container___executor.sh，该脚本内部会执行launch_container.sh。launch_container.sh会先设置一些环境变量，最后启动执行程序的命令。对于MapReduce而言，启动AM就执行org.apache.hadoop.mapreduce.v2.app.MRAppMaster；启动map/reduce task就执行org.apache.hadoop.mapred.YarnChild。<br> ApplicationMaster<br><br> ApplicationMaster是一个框架特殊的库，对于Map-Reduce计算模型而言有它自己的ApplicationMaster实现，对于其他的想要运行在yarn上的计算模型而言，必须得实现针对该计算模型的ApplicationMaster用以向RM申请资源运行task，比如运行在yarn上的spark框架也有对应的ApplicationMaster实现，归根结底，yarn是一个资源管理的框架，并不是一个计算框架，要想在yarn上运行应用程序，还得有特定的计算框架的实现。由于yarn是伴随着MRv2一起出现的，所以下面简要概述MRv2在yarn上的运行流程。<br><br> MRv2运行流程：<br><br> MR JobClient向resourceManager(AsM)提交一个job<br><br> AsM向Scheduler请求一个供MR AM运行的container，然后启动它<br><br> MR AM启动起来后向AsM注册<br><br> MR JobClient向AsM获取到MR AM相关的信息，然后直接与MR AM进行通信<br><br> MR AM计算splits并为所有的map构造资源请求<br><br> MR AM做一些必要的MR OutputCommitter的准备工作<br><br> MR AM向RM(Scheduler)发起资源请求，得到一组供map/reduce task运行的container，然后与NM一起对每一个container执行一些必要的任务，包括资源本地化等<br><br> MR AM 监视运行着的task 直到完成，当task失败时，申请新的container运行失败的task<br><br> 当每个map/reduce task完成后，MR AM运行MR OutputCommitter的cleanup 代码，也就是进行一些收尾工作<br><br> 当所有的map/reduce完成后，MR AM运行OutputCommitter的必要的job commit或者abort APIs<br><br> MR AM退出。<br><br> 在Yarn上写应用程序<br><br> 在yarn上写应用程序并不同于我们熟知的MapReduce应用程序，必须牢记yarn只是一个资源管理的框架，并不是一个计算框架，计算框架可以运行在yarn上。我们所能做的就是向RM申请container,然后配合NM一起来启动container。就像MRv2一样，jobclient请求用于MR AM运行的container，设置环境变量和启动命令，然后交由NM去启动MR AM，随后map/reduce task就由MR AM全权负责，当然task的启动也是由MR AM向RM申请container，然后配合NM一起来启动的。所以要想在yarn上运行非特定计算框架的程序，我们就得实现自己的client和applicationMaster。另外我们自定义的AM需要放在各个NM的classpath下，因为AM可能运行在任何NM所在的机器上。<br> 推荐一个大数据学习群 606859705每天晚上20:10都有一节【免费的】大数据直播课程，专注大数据分析方法,大数据编程，大数据仓库，大数据案例，人工智能,数据挖掘都是纯干货分享</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
