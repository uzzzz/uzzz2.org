<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>GAT集群及服务器实验个人查阅汇总 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="GAT集群及服务器实验个人查阅汇总" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="背景：汇总GPU集群及服务器实验，方便个人查阅。 集群搭建及使用方法参见：客户端配置并运用SLURM GPU集群 目录 一、集群前操作 1.1&nbsp; 文件拷贝 1.2 集群batch大小 二、重要指令汇总 2.1 ls指令 2.2 服务器上进程查询 二、集群上传实验 2.1 se_clsgat 2.2 group_clsgat_parallel 2.3 clsgat_conv_without_gat 2.4 group_clsgat_seq_train 三、服务器实验 3.1 liuzhenwei clsgat_conv_without_gat 3.2 yourenchun bld233 group_clsgat_seq_train 一、集群前操作 集群命令使用手册：http://wiki.baidu.com/pages/viewpage.action?pageId=684122549 1.1&nbsp; 文件拷贝 上传集群之前，需要将文件拷贝一份，方便更改，因为上传集群及压缩时不能进行相应操作。 进入ML-GAT文件夹之中， cp -r * ../submit_ML_GAT/ 1.2 集群batch大小 之前4张卡，batchsize设为32,总占用内存7611MiB 8张卡，每张内存32G=32768MB， batchsize可以设为&nbsp; (2*32768/7611)*32=275.5 可以将batchsize近似设置为256。即8卡256（最好不要这样，最好只用一个GPU节点） 1.3 硬盘空间删除 上传集群的时候，需要tar文件，文件若tar过多则显示内存空间不够。 gzip: stdout: No space left on device tar: /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/data-7a62c986-81d9-11e9-bdee-ac1f6b89f2df.tar.gz: cd /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/ cd /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/&nbsp; 1.4 集群注意事项 集群注意事项： submit.sh改对地址和实验名称。 job.sh中的mpirun用相对地址。 程序中，的Device ID需要与调用的GPU数对应。且算好显存不能溢出也不能过低。 压缩过程中不能有文件夹移动，home盘的存储需要够压缩缓存占用。每个压缩文件为46G 1.5 运行集群流程 保证本地运行没有问题，显卡用四张 命令行更改submit.sh其中的文件夹，实验名称，显卡node数量 更改job.sh中mpirun为相对地址不是绝对地址 run.sh中的python路径与命令行为对应的命令行 更改train代码中的显卡[0],比如如果只用一张显卡运算的时候 source submit.sh之后不要动文件夹内任何东西 二、重要指令查阅汇总 数据服务器使用手册：http://wiki.baidu.com/pages/viewpage.action?pageId=324529445 数据服务器端 ls，cp，mkdir等等 2.1 ls指令 ~/.hgcp/software-install/HGCP_client/tools/hadoop-v2/hadoop/bin/hadoop fs -D fs.default.name=afs://xingtian.afs.baidu.com:9902 -D hadoop.job.ugi=SYS_KM_Data,SYS_km_2018 -D dfs.replication=1 -D fs.afs.impl=org.apache.hadoop.fs.DFileSystem -ls /user/SYS_KM_Data/yourenchun/xingxiangrui/ copy之后在命令行对ls后面的文件夹进行更改。 ~/.hgcp/software-install/HGCP_client/tools/hadoop-v2/hadoop/bin/hadoop fs -D fs.default.name=afs://xingtian.afs.baidu.com:9902 -D hadoop.job.ugi=SYS_KM_Data,SYS_km_2018 -D dfs.replication=1 -D fs.afs.impl=org.apache.hadoop.fs.DFileSystem -rmr /user/SYS_KM_Data/yourenchun/xingxiangrui/ 2.2 服务器上进程查询 showjob -p yq01-p40-box-1-8 showjob -p yq01-p40-box-1-8&nbsp; [xingxiangrui@gzbh-mms-gpu55.gzbh.baidu.com tmp]$ showjob -p yq01-p40-box-1-8 QueueName: yq01-p40-box-1-8 ======================================================================================================================== JobId JobName Submitter RunningTime TotalGpus Status GpuUtil ======================================================================================================================== 7d4e18edc26000 xxr_resnet_gat yourenchun 01:49:59 1 RUNNING 50.00 7cedacb4c03000 xxr_clsgat_conv_without_gat yourenchun 01:15:29 1 RUNNING 23.12 7c5922bf406000 xxr_group_clsgat_seq_train yourenchun 12:04:27 1 RUNNING 80.61 7c590af340d000 xxr_se_clsgat yourenchun 12:04:55 1 RUNNING 82.90 7c57c1fac26000 xxr_group_clsgat_parallel yourenchun 12:10:29 1 RUNNING 71.43 二、集群上传实验 cp -r submit_ML_GAT/ /home/ssd1/ 先拷贝两份到ssd1里面，并且重命名，分别为1submit_ML_GAT，与 2.1 se_clsgat 描述：backbone换为SENet，服务器已经运行了36个epoch，继续在集群上的实验进行。 用1submit_ML_GAT submit.sh HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \ --hdfs afs://xingtian.afs.baidu.com:9902 \ --hdfs-user SYS_KM_Data \ --hdfs-passwd SYS_km_2018 \ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/se_clsgat \ --file-dir ./ \ --job-name xxr_se_clsgat \ --queue-name yq01-p40-box-1-8 \ --num-nodes 1 \ --num-task-pernode 1 \ --gpu-pnode 1 \ --submitter yourenchun \ --time-limit 0 \ --job-script ./job.sh run.sh # 3. Run training PYTHON=./torch/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./torch/bin/python train_se_clsgat.py 运行成功： showjob -j 7c590af340d000 &nbsp; 2.2 group_clsgat_parallel 描述：继续之前的并行的cls_gat实验，BN有不一样的地方在于这里是全局BN 在2submit_ML_GAT之中， HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \ --hdfs afs://xingtian.afs.baidu.com:9902 \ --hdfs-user SYS_KM_Data \ --hdfs-passwd SYS_km_2018 \ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/group_clsgat_parallel \ --file-dir ./ \ --job-name xxr_group_clsgat_parallel \ --queue-name yq01-p40-box-1-8 \ --num-nodes 1 \ --num-task-pernode 1 \ --gpu-pnode 1 \ --submitter yourenchun \ --time-limit 0 \ --job-script ./job.sh run.sh PYTHON=./torch/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./torch/bin/python train_group_clsgat_parallel.py 提交后： showjob -j 7c16a81341e000 变为1个GPU之后如果用 DEVICE_IDS = [0, 1, 2, 3] 就会有报错： /home/slurm/job/tmp/job-25726/models/group_clsgat_seq_train.py:40: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_. nn.init.xavier_uniform(self.W.data, gain=1.414) # fixme /home/slurm/job/tmp/job-25726/models/group_clsgat_seq_train.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_. nn.init.xavier_uniform(self.a.data, gain=1.414) # fixme Traceback (most recent call last): File &quot;train_group_clsgat_seq_train.py&quot;, line 144, in &lt;module&gt; main() File &quot;train_group_clsgat_seq_train.py&quot;, line 132, in main model = torch.nn.DataParallel(model, device_ids=args.DEVICE_IDS).cuda() File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 111, in __init__ _check_balance(self.device_ids) File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 17, in _check_balance dev_props = [torch.cuda.get_device_properties(i) for i in device_ids] File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 17, in &lt;listcomp&gt; dev_props = [torch.cuda.get_device_properties(i) for i in device_ids] File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/cuda/__init__.py&quot;, line 292, in get_device_properties raise AssertionError(&quot;Invalid device id&quot;) AssertionError: Invalid device id 因此我们将device id改为0再次尝试。 运行成功 showjob -j 7c57c1fac26000 2.3 clsgat_conv_without_gat 将服务器上的实验再次在集群上运行一次，同步运行。2.3 对应3.1， 2.4 对应3.2 服务器liuzhenwei 3chun_ML_GCN之中， env/bin/python train_cls_gat_conv_without_gat.py HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \ --hdfs afs://xingtian.afs.baidu.com:9902 \ --hdfs-user SYS_KM_Data \ --hdfs-passwd SYS_km_2018 \ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/clsgat_conv_without_gat \ --file-dir ./ \ --job-name xxr_clsgat_conv_without_gat \ --queue-name yq01-p40-box-1-8 \ --num-nodes 1 \ --num-task-pernode 1 \ --gpu-pnode 1 \ --submitter yourenchun \ --time-limit 0 \ --job-script ./job.sh run.sh PYTHON=./env/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./env/bin/python train_cls_gat_conv_without_gat.py showjob -j 7cedacb4c03000 &nbsp; 2.4 group_clsgat_seq_train 4submit_ML_GAT之中，对应服务器yourenchun bld233 描述：顺序的训练先训练GAT之前的层，再训练GAT与之后的层，再联合训练。 每个部分训练80个epoch，并且从30个之后进行0.9的decay，共240个epoch HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \ --hdfs afs://xingtian.afs.baidu.com:9902 \ --hdfs-user SYS_KM_Data \ --hdfs-passwd SYS_km_2018 \ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/group_clsgat_seq_train \ --file-dir ./ \ --job-name xxr_group_clsgat_seq_train \ --queue-name yq01-p40-box-1-8 \ --num-nodes 1 \ --num-task-pernode 1 \ --gpu-pnode 1 \ --submitter yourenchun \ --time-limit 0 \ --job-script ./job.sh run.sh PYTHON=./torch/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./torch/bin/python train_group_clsgat_seq_train.py showjob -j 7c1ab3e940d000 运行成功： showjob -j 7c5922bf406000 三、服务器实验 3.1 liuzhenwei ssh liuzhenwei@gzbh-mms-gpu56.gzbh.baidu.com /home/ssd1/xxr 压缩文件夹，并上传zip /home/ssd1/chun_ML_GCN.zip -r chun-ML_GCN/ scp chun_ML_GCN.zip -r liuzhenwei@gzbh-mms-gpu56.gzbh.baidu.com:/home/ssd1/xxr/ clsgat_conv_without_gat 与之前的最佳的mAP实验进行对比，即weight decay的clsgat进行对比。 &nbsp; 3.2 yourenchun bld233 ssh yourenchun@yq01-bdl-bdl233.yq01.baidu.com /data/yourenchun/share/xxr group_clsgat_seq_train 描述：顺序的训练先训练GAT之前的层，再训练GAT与之后的层，再联合训练。 每个部分训练80个epoch，并且从30个之后进行0.9的decay，共240个epoch 四、第二批集群实验 4.5 resnet_gat 位置 5submit_ML_GAT 描述：resnet101最后加上GAT，并且此GAT需要选用masked gat showjob -j 7d4e18edc26000 4.6 resnet_gcn 位置 6submit_ML_GAT 描述：resnet101最后加上GCN，GCN用的unnormalized adj ./torch/bin/python train_resnet_gcn.py &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="背景：汇总GPU集群及服务器实验，方便个人查阅。 集群搭建及使用方法参见：客户端配置并运用SLURM GPU集群 目录 一、集群前操作 1.1&nbsp; 文件拷贝 1.2 集群batch大小 二、重要指令汇总 2.1 ls指令 2.2 服务器上进程查询 二、集群上传实验 2.1 se_clsgat 2.2 group_clsgat_parallel 2.3 clsgat_conv_without_gat 2.4 group_clsgat_seq_train 三、服务器实验 3.1 liuzhenwei clsgat_conv_without_gat 3.2 yourenchun bld233 group_clsgat_seq_train 一、集群前操作 集群命令使用手册：http://wiki.baidu.com/pages/viewpage.action?pageId=684122549 1.1&nbsp; 文件拷贝 上传集群之前，需要将文件拷贝一份，方便更改，因为上传集群及压缩时不能进行相应操作。 进入ML-GAT文件夹之中， cp -r * ../submit_ML_GAT/ 1.2 集群batch大小 之前4张卡，batchsize设为32,总占用内存7611MiB 8张卡，每张内存32G=32768MB， batchsize可以设为&nbsp; (2*32768/7611)*32=275.5 可以将batchsize近似设置为256。即8卡256（最好不要这样，最好只用一个GPU节点） 1.3 硬盘空间删除 上传集群的时候，需要tar文件，文件若tar过多则显示内存空间不够。 gzip: stdout: No space left on device tar: /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/data-7a62c986-81d9-11e9-bdee-ac1f6b89f2df.tar.gz: cd /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/ cd /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/&nbsp; 1.4 集群注意事项 集群注意事项： submit.sh改对地址和实验名称。 job.sh中的mpirun用相对地址。 程序中，的Device ID需要与调用的GPU数对应。且算好显存不能溢出也不能过低。 压缩过程中不能有文件夹移动，home盘的存储需要够压缩缓存占用。每个压缩文件为46G 1.5 运行集群流程 保证本地运行没有问题，显卡用四张 命令行更改submit.sh其中的文件夹，实验名称，显卡node数量 更改job.sh中mpirun为相对地址不是绝对地址 run.sh中的python路径与命令行为对应的命令行 更改train代码中的显卡[0],比如如果只用一张显卡运算的时候 source submit.sh之后不要动文件夹内任何东西 二、重要指令查阅汇总 数据服务器使用手册：http://wiki.baidu.com/pages/viewpage.action?pageId=324529445 数据服务器端 ls，cp，mkdir等等 2.1 ls指令 ~/.hgcp/software-install/HGCP_client/tools/hadoop-v2/hadoop/bin/hadoop fs -D fs.default.name=afs://xingtian.afs.baidu.com:9902 -D hadoop.job.ugi=SYS_KM_Data,SYS_km_2018 -D dfs.replication=1 -D fs.afs.impl=org.apache.hadoop.fs.DFileSystem -ls /user/SYS_KM_Data/yourenchun/xingxiangrui/ copy之后在命令行对ls后面的文件夹进行更改。 ~/.hgcp/software-install/HGCP_client/tools/hadoop-v2/hadoop/bin/hadoop fs -D fs.default.name=afs://xingtian.afs.baidu.com:9902 -D hadoop.job.ugi=SYS_KM_Data,SYS_km_2018 -D dfs.replication=1 -D fs.afs.impl=org.apache.hadoop.fs.DFileSystem -rmr /user/SYS_KM_Data/yourenchun/xingxiangrui/ 2.2 服务器上进程查询 showjob -p yq01-p40-box-1-8 showjob -p yq01-p40-box-1-8&nbsp; [xingxiangrui@gzbh-mms-gpu55.gzbh.baidu.com tmp]$ showjob -p yq01-p40-box-1-8 QueueName: yq01-p40-box-1-8 ======================================================================================================================== JobId JobName Submitter RunningTime TotalGpus Status GpuUtil ======================================================================================================================== 7d4e18edc26000 xxr_resnet_gat yourenchun 01:49:59 1 RUNNING 50.00 7cedacb4c03000 xxr_clsgat_conv_without_gat yourenchun 01:15:29 1 RUNNING 23.12 7c5922bf406000 xxr_group_clsgat_seq_train yourenchun 12:04:27 1 RUNNING 80.61 7c590af340d000 xxr_se_clsgat yourenchun 12:04:55 1 RUNNING 82.90 7c57c1fac26000 xxr_group_clsgat_parallel yourenchun 12:10:29 1 RUNNING 71.43 二、集群上传实验 cp -r submit_ML_GAT/ /home/ssd1/ 先拷贝两份到ssd1里面，并且重命名，分别为1submit_ML_GAT，与 2.1 se_clsgat 描述：backbone换为SENet，服务器已经运行了36个epoch，继续在集群上的实验进行。 用1submit_ML_GAT submit.sh HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \ --hdfs afs://xingtian.afs.baidu.com:9902 \ --hdfs-user SYS_KM_Data \ --hdfs-passwd SYS_km_2018 \ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/se_clsgat \ --file-dir ./ \ --job-name xxr_se_clsgat \ --queue-name yq01-p40-box-1-8 \ --num-nodes 1 \ --num-task-pernode 1 \ --gpu-pnode 1 \ --submitter yourenchun \ --time-limit 0 \ --job-script ./job.sh run.sh # 3. Run training PYTHON=./torch/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./torch/bin/python train_se_clsgat.py 运行成功： showjob -j 7c590af340d000 &nbsp; 2.2 group_clsgat_parallel 描述：继续之前的并行的cls_gat实验，BN有不一样的地方在于这里是全局BN 在2submit_ML_GAT之中， HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \ --hdfs afs://xingtian.afs.baidu.com:9902 \ --hdfs-user SYS_KM_Data \ --hdfs-passwd SYS_km_2018 \ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/group_clsgat_parallel \ --file-dir ./ \ --job-name xxr_group_clsgat_parallel \ --queue-name yq01-p40-box-1-8 \ --num-nodes 1 \ --num-task-pernode 1 \ --gpu-pnode 1 \ --submitter yourenchun \ --time-limit 0 \ --job-script ./job.sh run.sh PYTHON=./torch/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./torch/bin/python train_group_clsgat_parallel.py 提交后： showjob -j 7c16a81341e000 变为1个GPU之后如果用 DEVICE_IDS = [0, 1, 2, 3] 就会有报错： /home/slurm/job/tmp/job-25726/models/group_clsgat_seq_train.py:40: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_. nn.init.xavier_uniform(self.W.data, gain=1.414) # fixme /home/slurm/job/tmp/job-25726/models/group_clsgat_seq_train.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_. nn.init.xavier_uniform(self.a.data, gain=1.414) # fixme Traceback (most recent call last): File &quot;train_group_clsgat_seq_train.py&quot;, line 144, in &lt;module&gt; main() File &quot;train_group_clsgat_seq_train.py&quot;, line 132, in main model = torch.nn.DataParallel(model, device_ids=args.DEVICE_IDS).cuda() File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 111, in __init__ _check_balance(self.device_ids) File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 17, in _check_balance dev_props = [torch.cuda.get_device_properties(i) for i in device_ids] File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 17, in &lt;listcomp&gt; dev_props = [torch.cuda.get_device_properties(i) for i in device_ids] File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/cuda/__init__.py&quot;, line 292, in get_device_properties raise AssertionError(&quot;Invalid device id&quot;) AssertionError: Invalid device id 因此我们将device id改为0再次尝试。 运行成功 showjob -j 7c57c1fac26000 2.3 clsgat_conv_without_gat 将服务器上的实验再次在集群上运行一次，同步运行。2.3 对应3.1， 2.4 对应3.2 服务器liuzhenwei 3chun_ML_GCN之中， env/bin/python train_cls_gat_conv_without_gat.py HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \ --hdfs afs://xingtian.afs.baidu.com:9902 \ --hdfs-user SYS_KM_Data \ --hdfs-passwd SYS_km_2018 \ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/clsgat_conv_without_gat \ --file-dir ./ \ --job-name xxr_clsgat_conv_without_gat \ --queue-name yq01-p40-box-1-8 \ --num-nodes 1 \ --num-task-pernode 1 \ --gpu-pnode 1 \ --submitter yourenchun \ --time-limit 0 \ --job-script ./job.sh run.sh PYTHON=./env/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./env/bin/python train_cls_gat_conv_without_gat.py showjob -j 7cedacb4c03000 &nbsp; 2.4 group_clsgat_seq_train 4submit_ML_GAT之中，对应服务器yourenchun bld233 描述：顺序的训练先训练GAT之前的层，再训练GAT与之后的层，再联合训练。 每个部分训练80个epoch，并且从30个之后进行0.9的decay，共240个epoch HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \ --hdfs afs://xingtian.afs.baidu.com:9902 \ --hdfs-user SYS_KM_Data \ --hdfs-passwd SYS_km_2018 \ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/group_clsgat_seq_train \ --file-dir ./ \ --job-name xxr_group_clsgat_seq_train \ --queue-name yq01-p40-box-1-8 \ --num-nodes 1 \ --num-task-pernode 1 \ --gpu-pnode 1 \ --submitter yourenchun \ --time-limit 0 \ --job-script ./job.sh run.sh PYTHON=./torch/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./torch/bin/python train_group_clsgat_seq_train.py showjob -j 7c1ab3e940d000 运行成功： showjob -j 7c5922bf406000 三、服务器实验 3.1 liuzhenwei ssh liuzhenwei@gzbh-mms-gpu56.gzbh.baidu.com /home/ssd1/xxr 压缩文件夹，并上传zip /home/ssd1/chun_ML_GCN.zip -r chun-ML_GCN/ scp chun_ML_GCN.zip -r liuzhenwei@gzbh-mms-gpu56.gzbh.baidu.com:/home/ssd1/xxr/ clsgat_conv_without_gat 与之前的最佳的mAP实验进行对比，即weight decay的clsgat进行对比。 &nbsp; 3.2 yourenchun bld233 ssh yourenchun@yq01-bdl-bdl233.yq01.baidu.com /data/yourenchun/share/xxr group_clsgat_seq_train 描述：顺序的训练先训练GAT之前的层，再训练GAT与之后的层，再联合训练。 每个部分训练80个epoch，并且从30个之后进行0.9的decay，共240个epoch 四、第二批集群实验 4.5 resnet_gat 位置 5submit_ML_GAT 描述：resnet101最后加上GAT，并且此GAT需要选用masked gat showjob -j 7d4e18edc26000 4.6 resnet_gcn 位置 6submit_ML_GAT 描述：resnet101最后加上GCN，GCN用的unnormalized adj ./torch/bin/python train_resnet_gcn.py &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/05/29/788061.html" />
<meta property="og:url" content="https://uzzz.org/2019/05/29/788061.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-29T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"背景：汇总GPU集群及服务器实验，方便个人查阅。 集群搭建及使用方法参见：客户端配置并运用SLURM GPU集群 目录 一、集群前操作 1.1&nbsp; 文件拷贝 1.2 集群batch大小 二、重要指令汇总 2.1 ls指令 2.2 服务器上进程查询 二、集群上传实验 2.1 se_clsgat 2.2 group_clsgat_parallel 2.3 clsgat_conv_without_gat 2.4 group_clsgat_seq_train 三、服务器实验 3.1 liuzhenwei clsgat_conv_without_gat 3.2 yourenchun bld233 group_clsgat_seq_train 一、集群前操作 集群命令使用手册：http://wiki.baidu.com/pages/viewpage.action?pageId=684122549 1.1&nbsp; 文件拷贝 上传集群之前，需要将文件拷贝一份，方便更改，因为上传集群及压缩时不能进行相应操作。 进入ML-GAT文件夹之中， cp -r * ../submit_ML_GAT/ 1.2 集群batch大小 之前4张卡，batchsize设为32,总占用内存7611MiB 8张卡，每张内存32G=32768MB， batchsize可以设为&nbsp; (2*32768/7611)*32=275.5 可以将batchsize近似设置为256。即8卡256（最好不要这样，最好只用一个GPU节点） 1.3 硬盘空间删除 上传集群的时候，需要tar文件，文件若tar过多则显示内存空间不够。 gzip: stdout: No space left on device tar: /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/data-7a62c986-81d9-11e9-bdee-ac1f6b89f2df.tar.gz: cd /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/ cd /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/&nbsp; 1.4 集群注意事项 集群注意事项： submit.sh改对地址和实验名称。 job.sh中的mpirun用相对地址。 程序中，的Device ID需要与调用的GPU数对应。且算好显存不能溢出也不能过低。 压缩过程中不能有文件夹移动，home盘的存储需要够压缩缓存占用。每个压缩文件为46G 1.5 运行集群流程 保证本地运行没有问题，显卡用四张 命令行更改submit.sh其中的文件夹，实验名称，显卡node数量 更改job.sh中mpirun为相对地址不是绝对地址 run.sh中的python路径与命令行为对应的命令行 更改train代码中的显卡[0],比如如果只用一张显卡运算的时候 source submit.sh之后不要动文件夹内任何东西 二、重要指令查阅汇总 数据服务器使用手册：http://wiki.baidu.com/pages/viewpage.action?pageId=324529445 数据服务器端 ls，cp，mkdir等等 2.1 ls指令 ~/.hgcp/software-install/HGCP_client/tools/hadoop-v2/hadoop/bin/hadoop fs -D fs.default.name=afs://xingtian.afs.baidu.com:9902 -D hadoop.job.ugi=SYS_KM_Data,SYS_km_2018 -D dfs.replication=1 -D fs.afs.impl=org.apache.hadoop.fs.DFileSystem -ls /user/SYS_KM_Data/yourenchun/xingxiangrui/ copy之后在命令行对ls后面的文件夹进行更改。 ~/.hgcp/software-install/HGCP_client/tools/hadoop-v2/hadoop/bin/hadoop fs -D fs.default.name=afs://xingtian.afs.baidu.com:9902 -D hadoop.job.ugi=SYS_KM_Data,SYS_km_2018 -D dfs.replication=1 -D fs.afs.impl=org.apache.hadoop.fs.DFileSystem -rmr /user/SYS_KM_Data/yourenchun/xingxiangrui/ 2.2 服务器上进程查询 showjob -p yq01-p40-box-1-8 showjob -p yq01-p40-box-1-8&nbsp; [xingxiangrui@gzbh-mms-gpu55.gzbh.baidu.com tmp]$ showjob -p yq01-p40-box-1-8 QueueName: yq01-p40-box-1-8 ======================================================================================================================== JobId JobName Submitter RunningTime TotalGpus Status GpuUtil ======================================================================================================================== 7d4e18edc26000 xxr_resnet_gat yourenchun 01:49:59 1 RUNNING 50.00 7cedacb4c03000 xxr_clsgat_conv_without_gat yourenchun 01:15:29 1 RUNNING 23.12 7c5922bf406000 xxr_group_clsgat_seq_train yourenchun 12:04:27 1 RUNNING 80.61 7c590af340d000 xxr_se_clsgat yourenchun 12:04:55 1 RUNNING 82.90 7c57c1fac26000 xxr_group_clsgat_parallel yourenchun 12:10:29 1 RUNNING 71.43 二、集群上传实验 cp -r submit_ML_GAT/ /home/ssd1/ 先拷贝两份到ssd1里面，并且重命名，分别为1submit_ML_GAT，与 2.1 se_clsgat 描述：backbone换为SENet，服务器已经运行了36个epoch，继续在集群上的实验进行。 用1submit_ML_GAT submit.sh HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \\ --hdfs afs://xingtian.afs.baidu.com:9902 \\ --hdfs-user SYS_KM_Data \\ --hdfs-passwd SYS_km_2018 \\ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/se_clsgat \\ --file-dir ./ \\ --job-name xxr_se_clsgat \\ --queue-name yq01-p40-box-1-8 \\ --num-nodes 1 \\ --num-task-pernode 1 \\ --gpu-pnode 1 \\ --submitter yourenchun \\ --time-limit 0 \\ --job-script ./job.sh run.sh # 3. Run training PYTHON=./torch/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./torch/bin/python train_se_clsgat.py 运行成功： showjob -j 7c590af340d000 &nbsp; 2.2 group_clsgat_parallel 描述：继续之前的并行的cls_gat实验，BN有不一样的地方在于这里是全局BN 在2submit_ML_GAT之中， HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \\ --hdfs afs://xingtian.afs.baidu.com:9902 \\ --hdfs-user SYS_KM_Data \\ --hdfs-passwd SYS_km_2018 \\ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/group_clsgat_parallel \\ --file-dir ./ \\ --job-name xxr_group_clsgat_parallel \\ --queue-name yq01-p40-box-1-8 \\ --num-nodes 1 \\ --num-task-pernode 1 \\ --gpu-pnode 1 \\ --submitter yourenchun \\ --time-limit 0 \\ --job-script ./job.sh run.sh PYTHON=./torch/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./torch/bin/python train_group_clsgat_parallel.py 提交后： showjob -j 7c16a81341e000 变为1个GPU之后如果用 DEVICE_IDS = [0, 1, 2, 3] 就会有报错： /home/slurm/job/tmp/job-25726/models/group_clsgat_seq_train.py:40: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_. nn.init.xavier_uniform(self.W.data, gain=1.414) # fixme /home/slurm/job/tmp/job-25726/models/group_clsgat_seq_train.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_. nn.init.xavier_uniform(self.a.data, gain=1.414) # fixme Traceback (most recent call last): File &quot;train_group_clsgat_seq_train.py&quot;, line 144, in &lt;module&gt; main() File &quot;train_group_clsgat_seq_train.py&quot;, line 132, in main model = torch.nn.DataParallel(model, device_ids=args.DEVICE_IDS).cuda() File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 111, in __init__ _check_balance(self.device_ids) File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 17, in _check_balance dev_props = [torch.cuda.get_device_properties(i) for i in device_ids] File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 17, in &lt;listcomp&gt; dev_props = [torch.cuda.get_device_properties(i) for i in device_ids] File &quot;/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/cuda/__init__.py&quot;, line 292, in get_device_properties raise AssertionError(&quot;Invalid device id&quot;) AssertionError: Invalid device id 因此我们将device id改为0再次尝试。 运行成功 showjob -j 7c57c1fac26000 2.3 clsgat_conv_without_gat 将服务器上的实验再次在集群上运行一次，同步运行。2.3 对应3.1， 2.4 对应3.2 服务器liuzhenwei 3chun_ML_GCN之中， env/bin/python train_cls_gat_conv_without_gat.py HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \\ --hdfs afs://xingtian.afs.baidu.com:9902 \\ --hdfs-user SYS_KM_Data \\ --hdfs-passwd SYS_km_2018 \\ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/clsgat_conv_without_gat \\ --file-dir ./ \\ --job-name xxr_clsgat_conv_without_gat \\ --queue-name yq01-p40-box-1-8 \\ --num-nodes 1 \\ --num-task-pernode 1 \\ --gpu-pnode 1 \\ --submitter yourenchun \\ --time-limit 0 \\ --job-script ./job.sh run.sh PYTHON=./env/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./env/bin/python train_cls_gat_conv_without_gat.py showjob -j 7cedacb4c03000 &nbsp; 2.4 group_clsgat_seq_train 4submit_ML_GAT之中，对应服务器yourenchun bld233 描述：顺序的训练先训练GAT之前的层，再训练GAT与之后的层，再联合训练。 每个部分训练80个epoch，并且从30个之后进行0.9的decay，共240个epoch HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/ ${HGCP_CLIENT_BIN}/submit \\ --hdfs afs://xingtian.afs.baidu.com:9902 \\ --hdfs-user SYS_KM_Data \\ --hdfs-passwd SYS_km_2018 \\ --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/group_clsgat_seq_train \\ --file-dir ./ \\ --job-name xxr_group_clsgat_seq_train \\ --queue-name yq01-p40-box-1-8 \\ --num-nodes 1 \\ --num-task-pernode 1 \\ --gpu-pnode 1 \\ --submitter yourenchun \\ --time-limit 0 \\ --job-script ./job.sh run.sh PYTHON=./torch/bin/python export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH #sh train_baiduyun_nls.sh #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth #../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4 #../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate #../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4 ./torch/bin/python train_group_clsgat_seq_train.py showjob -j 7c1ab3e940d000 运行成功： showjob -j 7c5922bf406000 三、服务器实验 3.1 liuzhenwei ssh liuzhenwei@gzbh-mms-gpu56.gzbh.baidu.com /home/ssd1/xxr 压缩文件夹，并上传zip /home/ssd1/chun_ML_GCN.zip -r chun-ML_GCN/ scp chun_ML_GCN.zip -r liuzhenwei@gzbh-mms-gpu56.gzbh.baidu.com:/home/ssd1/xxr/ clsgat_conv_without_gat 与之前的最佳的mAP实验进行对比，即weight decay的clsgat进行对比。 &nbsp; 3.2 yourenchun bld233 ssh yourenchun@yq01-bdl-bdl233.yq01.baidu.com /data/yourenchun/share/xxr group_clsgat_seq_train 描述：顺序的训练先训练GAT之前的层，再训练GAT与之后的层，再联合训练。 每个部分训练80个epoch，并且从30个之后进行0.9的decay，共240个epoch 四、第二批集群实验 4.5 resnet_gat 位置 5submit_ML_GAT 描述：resnet101最后加上GAT，并且此GAT需要选用masked gat showjob -j 7d4e18edc26000 4.6 resnet_gcn 位置 6submit_ML_GAT 描述：resnet101最后加上GCN，GCN用的unnormalized adj ./torch/bin/python train_resnet_gcn.py &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/05/29/788061.html","headline":"GAT集群及服务器实验个人查阅汇总","dateModified":"2019-05-29T00:00:00+08:00","datePublished":"2019-05-29T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/05/29/788061.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>GAT集群及服务器实验个人查阅汇总</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p><strong>背景：</strong>汇总GPU集群及服务器实验，方便个人查阅。</p> 
  <p>集群搭建及使用方法参见：<a href="https://blog.csdn.net/weixin_36474809/article/details/89467933" rel="nofollow">客户端配置并运用SLURM GPU集群 </a></p> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="%E4%B8%80%E3%80%81%E9%9B%86%E7%BE%A4%E5%89%8D%E6%93%8D%E4%BD%9C-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E9%9B%86%E7%BE%A4%E5%89%8D%E6%93%8D%E4%BD%9C" rel="nofollow">一、集群前操作</a></p> 
  <p id="1.1%C2%A0%20%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D-toc" style="margin-left:40px;"><a href="#1.1%C2%A0%20%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D" rel="nofollow">1.1&nbsp; 文件拷贝</a></p> 
  <p id="1.2%20%E9%9B%86%E7%BE%A4batch%E5%A4%A7%E5%B0%8F-toc" style="margin-left:40px;"><a href="#1.2%20%E9%9B%86%E7%BE%A4batch%E5%A4%A7%E5%B0%8F" rel="nofollow">1.2 集群batch大小</a></p> 
  <p id="%E4%BA%8C%E3%80%81%E9%87%8D%E8%A6%81%E6%8C%87%E4%BB%A4%E6%B1%87%E6%80%BB-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E9%87%8D%E8%A6%81%E6%8C%87%E4%BB%A4%E6%B1%87%E6%80%BB" rel="nofollow">二、重要指令汇总</a></p> 
  <p id="2.1%20ls%E6%8C%87%E4%BB%A4-toc" style="margin-left:40px;"><a href="#2.1%20ls%E6%8C%87%E4%BB%A4" rel="nofollow">2.1 ls指令</a></p> 
  <p id="2.2%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E8%BF%9B%E7%A8%8B%E6%9F%A5%E8%AF%A2-toc" style="margin-left:40px;"><a href="#2.2%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E8%BF%9B%E7%A8%8B%E6%9F%A5%E8%AF%A2" rel="nofollow">2.2 服务器上进程查询</a></p> 
  <p id="%E4%BA%8C%E3%80%81%E9%9B%86%E7%BE%A4%E4%B8%8A%E4%BC%A0%E5%AE%9E%E9%AA%8C-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E9%9B%86%E7%BE%A4%E4%B8%8A%E4%BC%A0%E5%AE%9E%E9%AA%8C" rel="nofollow">二、集群上传实验</a></p> 
  <p id="2.1%20se_clsgat-toc" style="margin-left:40px;"><a href="#2.1%20se_clsgat" rel="nofollow">2.1 se_clsgat</a></p> 
  <p id="2.2%20group_clsgat_parallel-toc" style="margin-left:40px;"><a href="#2.2%20group_clsgat_parallel" rel="nofollow">2.2 group_clsgat_parallel</a></p> 
  <p id="2.3%20clsgat_conv_without_gat-toc" style="margin-left:40px;"><a href="#2.3%20clsgat_conv_without_gat" rel="nofollow">2.3 clsgat_conv_without_gat</a></p> 
  <p id="2.4%20group_clsgat_seq_train-toc" style="margin-left:40px;"><a href="#2.4%20group_clsgat_seq_train" rel="nofollow">2.4 group_clsgat_seq_train</a></p> 
  <p id="%E4%B8%89%E3%80%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%9E%E9%AA%8C-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%9E%E9%AA%8C" rel="nofollow">三、服务器实验</a></p> 
  <p id="3.1%20liuzhenwei-toc" style="margin-left:40px;"><a href="#3.1%20liuzhenwei" rel="nofollow">3.1 liuzhenwei</a></p> 
  <p id="clsgat_conv_without_gat-toc" style="margin-left:80px;"><a href="#clsgat_conv_without_gat" rel="nofollow">clsgat_conv_without_gat</a></p> 
  <p id="3.2%20yourenchun%20bld233-toc" style="margin-left:40px;"><a href="#3.2%20yourenchun%20bld233" rel="nofollow">3.2 yourenchun bld233</a></p> 
  <p id="group_clsgat_seq_train-toc" style="margin-left:80px;"><a href="#group_clsgat_seq_train" rel="nofollow">group_clsgat_seq_train</a></p> 
  <hr id="hr-toc">
  <h1 id="%E4%B8%80%E3%80%81%E9%9B%86%E7%BE%A4%E5%89%8D%E6%93%8D%E4%BD%9C">一、集群前操作</h1> 
  <p>集群命令使用手册：<a href="http://wiki.baidu.com/pages/viewpage.action?pageId=684122549" rel="nofollow">http://wiki.baidu.com/pages/viewpage.action?pageId=684122549</a></p> 
  <h2 id="1.1%C2%A0%20%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D">1.1&nbsp; 文件拷贝</h2> 
  <p>上传集群之前，需要将文件拷贝一份，方便更改，因为上传集群及压缩时不能进行相应操作。</p> 
  <p>进入ML-GAT文件夹之中，</p> 
  <p>cp -r * ../submit_ML_GAT/</p> 
  <h2 id="1.2%20%E9%9B%86%E7%BE%A4batch%E5%A4%A7%E5%B0%8F">1.2 集群batch大小</h2> 
  <p>之前4张卡，batchsize设为32,总占用内存7611MiB</p> 
  <p>8张卡，每张内存32G=32768MB， batchsize可以设为&nbsp; (2*32768/7611)*32=275.5</p> 
  <p>可以将batchsize近似设置为256。即8卡256（最好不要这样，最好只用一个GPU节点）</p> 
  <h2>1.3 硬盘空间删除</h2> 
  <p>上传集群的时候，需要tar文件，文件若tar过多则显示内存空间不够。</p> 
  <pre class="has">
<code>gzip: stdout: No space left on device
tar: /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/data-7a62c986-81d9-11e9-bdee-ac1f6b89f2df.tar.gz:</code></pre> 
  <pre class="has">
<code>cd /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/</code></pre> 
  <p>cd /home/xingxiangrui/.hgcp/software-install/HGCP_client/tmp/&nbsp;</p> 
  <h2>1.4 集群注意事项</h2> 
  <p>集群注意事项：</p> 
  <ol>
   <li> <p>submit.sh改对地址和实验名称。</p> </li> 
   <li> <p>job.sh中的mpirun用相对地址。</p> </li> 
   <li> <p>程序中，的Device ID需要与调用的GPU数对应。且算好显存不能溢出也不能过低。</p> </li> 
   <li> <p>压缩过程中不能有文件夹移动，home盘的存储需要够压缩缓存占用。每个压缩文件为46G</p> </li> 
  </ol>
  <h2>1.5 运行集群流程</h2> 
  <ul>
   <li>保证本地运行没有问题，显卡用四张</li> 
   <li>命令行更改submit.sh其中的文件夹，实验名称，显卡node数量</li> 
   <li>更改job.sh中mpirun为相对地址不是绝对地址</li> 
   <li>run.sh中的python路径与命令行为对应的命令行</li> 
   <li>更改train代码中的显卡[0],比如如果只用一张显卡运算的时候</li> 
   <li>source submit.sh之后不要动文件夹内任何东西</li> 
  </ul>
  <h1 id="%E4%BA%8C%E3%80%81%E9%87%8D%E8%A6%81%E6%8C%87%E4%BB%A4%E6%B1%87%E6%80%BB">二、重要指令查阅汇总</h1> 
  <p>数据服务器使用手册：<a href="http://wiki.baidu.com/pages/viewpage.action?pageId=324529445" rel="nofollow">http://wiki.baidu.com/pages/viewpage.action?pageId=324529445</a></p> 
  <p>数据服务器端 ls，cp，mkdir等等</p> 
  <h2 id="2.1%20ls%E6%8C%87%E4%BB%A4">2.1 ls指令</h2> 
  <pre class="has">
<code class="language-bash">~/.hgcp/software-install/HGCP_client/tools/hadoop-v2/hadoop/bin/hadoop fs -D fs.default.name=afs://xingtian.afs.baidu.com:9902 -D hadoop.job.ugi=SYS_KM_Data,SYS_km_2018 -D dfs.replication=1 -D fs.afs.impl=org.apache.hadoop.fs.DFileSystem -ls  /user/SYS_KM_Data/yourenchun/xingxiangrui/</code></pre> 
  <p>copy之后在命令行对ls后面的文件夹进行更改。</p> 
  <pre class="has">
<code class="language-bash">~/.hgcp/software-install/HGCP_client/tools/hadoop-v2/hadoop/bin/hadoop fs -D fs.default.name=afs://xingtian.afs.baidu.com:9902 -D hadoop.job.ugi=SYS_KM_Data,SYS_km_2018 -D dfs.replication=1 -D fs.afs.impl=org.apache.hadoop.fs.DFileSystem -rmr  /user/SYS_KM_Data/yourenchun/xingxiangrui/</code></pre> 
  <h2 id="2.2%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E8%BF%9B%E7%A8%8B%E6%9F%A5%E8%AF%A2">2.2 服务器上进程查询</h2> 
  <pre class="has">
<code>showjob -p yq01-p40-box-1-8</code></pre> 
  <p>showjob -p yq01-p40-box-1-8&nbsp;</p> 
  <pre class="has">
<code class="language-bash">[xingxiangrui@gzbh-mms-gpu55.gzbh.baidu.com tmp]$ showjob -p yq01-p40-box-1-8
QueueName: yq01-p40-box-1-8
========================================================================================================================
  JobId           JobName                        Submitter       RunningTime     TotalGpus  Status     GpuUtil
========================================================================================================================
7d4e18edc26000  xxr_resnet_gat                 yourenchun      01:49:59        1          RUNNING    50.00
  7cedacb4c03000  xxr_clsgat_conv_without_gat    yourenchun      01:15:29        1          RUNNING    23.12
  7c5922bf406000  xxr_group_clsgat_seq_train     yourenchun      12:04:27        1          RUNNING    80.61
  7c590af340d000  xxr_se_clsgat                  yourenchun      12:04:55        1          RUNNING    82.90
  7c57c1fac26000  xxr_group_clsgat_parallel      yourenchun      12:10:29        1          RUNNING    71.43</code></pre> 
  <h1 id="%E4%BA%8C%E3%80%81%E9%9B%86%E7%BE%A4%E4%B8%8A%E4%BC%A0%E5%AE%9E%E9%AA%8C">二、集群上传实验</h1> 
  <p>cp -r submit_ML_GAT/ /home/ssd1/</p> 
  <p>先拷贝两份到ssd1里面，并且重命名，分别为1submit_ML_GAT，与</p> 
  <h2 id="2.1%20se_clsgat">2.1 se_clsgat</h2> 
  <p>描述：backbone换为SENet，服务器已经运行了36个epoch，继续在集群上的实验进行。</p> 
  <p>用1submit_ML_GAT</p> 
  <p>submit.sh</p> 
  <pre class="has">
<code class="language-bash">HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/

${HGCP_CLIENT_BIN}/submit \
    --hdfs afs://xingtian.afs.baidu.com:9902 \
    --hdfs-user SYS_KM_Data \
    --hdfs-passwd SYS_km_2018 \
    --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/se_clsgat \
    --file-dir ./ \
    --job-name xxr_se_clsgat \
    --queue-name yq01-p40-box-1-8 \
    --num-nodes 1 \
    --num-task-pernode 1 \
    --gpu-pnode 1 \
    --submitter yourenchun \
    --time-limit 0 \
    --job-script ./job.sh</code></pre> 
  <p>run.sh</p> 
  <pre class="has">
<code class="language-python"># 3. Run training
PYTHON=./torch/bin/python
export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH

#sh train_baiduyun_nls.sh
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth
#../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate
#../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4

./torch/bin/python train_se_clsgat.py</code></pre> 
  <p>运行成功：</p> 
  <p>showjob -j 7c590af340d000</p> 
  <p>&nbsp;</p> 
  <h2 id="2.2%20group_clsgat_parallel">2.2 group_clsgat_parallel</h2> 
  <p>描述：继续之前的并行的cls_gat实验，BN有不一样的地方在于这里是全局BN</p> 
  <p>在2submit_ML_GAT之中，</p> 
  <pre class="has">
<code class="language-bash">HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/

${HGCP_CLIENT_BIN}/submit \
    --hdfs afs://xingtian.afs.baidu.com:9902 \
    --hdfs-user SYS_KM_Data \
    --hdfs-passwd SYS_km_2018 \
    --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/group_clsgat_parallel \
    --file-dir ./ \
    --job-name xxr_group_clsgat_parallel \
    --queue-name yq01-p40-box-1-8 \
    --num-nodes 1 \
    --num-task-pernode 1 \
    --gpu-pnode 1 \
    --submitter yourenchun \
    --time-limit 0 \
    --job-script ./job.sh</code></pre> 
  <p>run.sh</p> 
  <pre class="has">
<code class="language-bash">PYTHON=./torch/bin/python
export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH

#sh train_baiduyun_nls.sh
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth
#../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate
#../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4

./torch/bin/python train_group_clsgat_parallel.py</code></pre> 
  <p>提交后：</p> 
  <p>showjob -j 7c16a81341e000</p> 
  <p>变为1个GPU之后如果用</p> 
  <pre class="has">
<code class="language-html">DEVICE_IDS = [0, 1, 2, 3]</code></pre> 
  <p>就会有报错：</p> 
  <pre class="has">
<code class="language-bash">/home/slurm/job/tmp/job-25726/models/group_clsgat_seq_train.py:40: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  nn.init.xavier_uniform(self.W.data, gain=1.414)  # fixme
/home/slurm/job/tmp/job-25726/models/group_clsgat_seq_train.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  nn.init.xavier_uniform(self.a.data, gain=1.414)  # fixme
Traceback (most recent call last):
  File "train_group_clsgat_seq_train.py", line 144, in &lt;module&gt;
    main()
  File "train_group_clsgat_seq_train.py", line 132, in main
    model = torch.nn.DataParallel(model, device_ids=args.DEVICE_IDS).cuda()
  File "/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py", line 111, in __init__
    _check_balance(self.device_ids)
  File "/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py", line 17, in _check_balance
    dev_props = [torch.cuda.get_device_properties(i) for i in device_ids]
  File "/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py", line 17, in &lt;listcomp&gt;
    dev_props = [torch.cuda.get_device_properties(i) for i in device_ids]
  File "/home/slurm/job/tmp/job-25726/torch/lib/python3.5/site-packages/torch/cuda/__init__.py", line 292, in get_device_properties
    raise AssertionError("Invalid device id")
AssertionError: Invalid device id</code></pre> 
  <p>因此我们将device id改为0再次尝试。</p> 
  <p>运行成功</p> 
  <p>showjob -j 7c57c1fac26000</p> 
  <h2 id="2.3%20clsgat_conv_without_gat">2.3 clsgat_conv_without_gat</h2> 
  <p>将服务器上的实验再次在集群上运行一次，同步运行。2.3 对应3.1， 2.4 对应3.2</p> 
  <p>服务器liuzhenwei</p> 
  <p>3chun_ML_GCN之中，</p> 
  <p>env/bin/python train_cls_gat_conv_without_gat.py</p> 
  <pre class="has">
<code class="language-bash">HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/

${HGCP_CLIENT_BIN}/submit \
    --hdfs afs://xingtian.afs.baidu.com:9902 \
    --hdfs-user SYS_KM_Data \
    --hdfs-passwd SYS_km_2018 \
    --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/clsgat_conv_without_gat \
    --file-dir ./ \
    --job-name xxr_clsgat_conv_without_gat \
    --queue-name yq01-p40-box-1-8 \
    --num-nodes 1 \
    --num-task-pernode 1 \
    --gpu-pnode 1 \
    --submitter yourenchun \
    --time-limit 0 \
    --job-script ./job.sh</code></pre> 
  <p>run.sh</p> 
  <pre class="has">
<code class="language-bash">PYTHON=./env/bin/python
export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH

#sh train_baiduyun_nls.sh
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth
#../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate
#../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4

./env/bin/python train_cls_gat_conv_without_gat.py</code></pre> 
  <p>showjob -j 7cedacb4c03000</p> 
  <p>&nbsp;</p> 
  <h2 id="2.4%20group_clsgat_seq_train">2.4 group_clsgat_seq_train</h2> 
  <p>4submit_ML_GAT之中，对应服务器yourenchun bld233</p> 
  <p>描述：顺序的训练先训练GAT之前的层，再训练GAT与之后的层，再联合训练。</p> 
  <p>每个部分训练80个epoch，并且从30个之后进行0.9的decay，共240个epoch</p> 
  <pre class="has">
<code class="language-bash">HGCP_CLIENT_BIN=/home/xingxiangrui/.hgcp/software-install/HGCP_client/bin/

${HGCP_CLIENT_BIN}/submit \
    --hdfs afs://xingtian.afs.baidu.com:9902 \
    --hdfs-user SYS_KM_Data \
    --hdfs-passwd SYS_km_2018 \
    --hdfs-path /user/SYS_KM_Data/yourenchun/xingxiangrui/group_clsgat_seq_train \
    --file-dir ./ \
    --job-name xxr_group_clsgat_seq_train \
    --queue-name yq01-p40-box-1-8 \
    --num-nodes 1 \
    --num-task-pernode 1 \
    --gpu-pnode 1 \
    --submitter yourenchun \
    --time-limit 0 \
    --job-script ./job.sh</code></pre> 
  <p>run.sh</p> 
  <pre class="has">
<code class="language-bash">PYTHON=./torch/bin/python
export LD_LIBRARY_PATH=/home/work/cuda-8.0/lib64:$LD_LIBRARY_PATH

#sh train_baiduyun_nls.sh
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_baiduyunnls.py --gpus 1 --resume_from ./work_dirs/cascade_rcnn_senet50_baiduyun_nls/epoch_288_93.4.pth
#../env/bin/python ../tools/train.py ../configs/retinanet_seresnet101_fpn_1x_baiduyunnls.py --gpus 4
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_mobilenetv2_clstest_nls.py --gpus 4
#../env/bin/python ../tools/train.py ../configs/cascade_rcnn_seresnet_50_slim_clstest_nls.py --gpus 4 --validate
#../tools/dist_train.sh ../configs/dcn/cascade_rcnn_seresnet_50_mdpool_baiduyunnls.py 4

./torch/bin/python train_group_clsgat_seq_train.py</code></pre> 
  <p>showjob -j 7c1ab3e940d000</p> 
  <p>运行成功：</p> 
  <p>showjob -j 7c5922bf406000</p> 
  <h1 id="%E4%B8%89%E3%80%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%9E%E9%AA%8C">三、服务器实验</h1> 
  <h2 id="3.1%20liuzhenwei">3.1 liuzhenwei</h2> 
  <p>ssh liuzhenwei@gzbh-mms-gpu56.gzbh.baidu.com</p> 
  <p>/home/ssd1/xxr</p> 
  <p>压缩文件夹，并上传zip /home/ssd1/chun_ML_GCN.zip -r chun-ML_GCN/</p> 
  <p>scp chun_ML_GCN.zip -r liuzhenwei@gzbh-mms-gpu56.gzbh.baidu.com:/home/ssd1/xxr/</p> 
  <h3 id="clsgat_conv_without_gat">clsgat_conv_without_gat</h3> 
  <p>与之前的最佳的mAP实验进行对比，即weight decay的clsgat进行对比。</p> 
  <p>&nbsp;</p> 
  <h2 id="3.2%20yourenchun%20bld233">3.2 yourenchun bld233</h2> 
  <p>ssh yourenchun@yq01-bdl-bdl233.yq01.baidu.com</p> 
  <p>/data/yourenchun/share/xxr</p> 
  <h3 id="group_clsgat_seq_train">group_clsgat_seq_train</h3> 
  <p>描述：顺序的训练先训练GAT之前的层，再训练GAT与之后的层，再联合训练。</p> 
  <p>每个部分训练80个epoch，并且从30个之后进行0.9的decay，共240个epoch</p> 
  <h1>四、第二批集群实验</h1> 
  <h2>4.5 resnet_gat</h2> 
  <p>位置 5submit_ML_GAT</p> 
  <p>描述：resnet101最后加上GAT，并且此GAT需要选用masked gat</p> 
  <p>showjob -j 7d4e18edc26000</p> 
  <h2>4.6 resnet_gcn</h2> 
  <p>位置 6submit_ML_GAT</p> 
  <p>描述：resnet101最后加上GCN，GCN用的unnormalized adj</p> 
  <p>./torch/bin/python train_resnet_gcn.py</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
