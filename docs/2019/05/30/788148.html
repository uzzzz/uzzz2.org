<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>014 Shuffle的概念介绍 Shuffle的细节图描述 分区案例 倒排索引案例 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="014 Shuffle的概念介绍 Shuffle的细节图描述 分区案例 倒排索引案例" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MapReduce的shuffle过程 map输出 reduce输入 的过程 好理解吧 输入 三行 做分片 一个分片一行 Mapping统计好 Shuffling这边排序 做一些分区的工作 Reducing接收 统计 输出结果 这些图讲的都是shuffle 不太清楚 可自行到官网上寻找 然后这里一张长图 拆开了 一块是128M 这里我们默认一块对应一个分片 但是有10%的slop冗余 比如说有140M的文件 也会分成一个分片 一个分片对应一个MapTask 这个圆形缓冲区默认大小100M 80M是阈值 当达到80M时 就将数据写入到临时文件取 剩下的20M继续接受map函数发送的数据 然后可以看一下FileInputFormat和 FileOutputFormat以及SpillRecord的源码 自己分析一下 shuffle的相关操作 看下配置名 cd /usr/local/hadoop-2.7.1 vi ./etc/hadoop/core-site.xml 遇到错误 中间我隔了一天继续这个 再次启动start-all.sh 这个命令后 出现错误 使用hdfs dfs的相关命令出错 表示就是两个namenode全是standby状态 没有一个active的 难受 网上查阅资料 说是可以手动开启 或者启动zkfc 说是这个没开 开了就好了 所以我就使用了 hadoop-damons.sh start zkfc 这个命令 jps后 Zk这个服务出现了 没使用上述命令前是没有的 onMygod 难受 我需要上传par文件 就是数据 现在home下建一个par然后写入 复制上就行 然后传到hdfs 然后将jar包传到home下 （导出的jar包可以是整个项目 使用时写明白类名就行） 具体的把 看截图吧 hdfs dfs -put /home/par /par 然后记得看下输出目录（我们自己定义） 我们经常使用hdfs下的out目录 注意看看out下的名字 别重复了 我刚刚删了 重新建的 所以我就别看了 总之就是输出别重复了名字 先进入hadoop目录 cd /usr/local/hadoop-2.7.1 然后 yarn jar /home/wc.jar qf.com.mr.PartitionDemo /par /out/19 这个又出错了 上次的错误 连接不上02 然后 我又 stop-all.sh 然后等着关了后 又 start-all.sh 就是又重启了服务就好了 我猜 是不是 先将zkfc启动了 再去启动其他的服务 是不是这样？ 谁能告诉我 找到答案了 这样再运行就对了 因为数据小 一个分片 一块 就是一个进程 然后四个结果文件 如下图 然后查看一下 19文件夹下确实有4个结果文件（除了代表成功的那个） 查看一下其中内容 非常正确 至此 分区案例结束 下面贴上代码 写的时候注意导入的包 PartitionDemo package qf.com.mr; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; /* *@author Shishuai E-mail：1198319583@qq.com *@version Create time ： 2019年5月28日下午5:42:34 *类说明：分区 输入数据 Hello HI HI qianfeng hi hi qianfeng qianfeng 163.com qq.com 189.com @163.com @qq.com *123 输出： 将首写字母为A-Z的放到一个文件中 将首写字母为a-z的放到一个文件中 将首写字母为0-9的放到一个文件中 其他的放到一个文件中 结果文件：part-r-00000 Hello 1 Hi 2 结果文件：part-r-00001 hi 2 qianfeng 2 结果文件：part-r-00002 .... 结果文件：part-r-00003 .... */ public class PartitionDemo implements Tool { /** * map阶段 * * @author HP * */ public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String words[] = line.split(&quot; &quot;); for (String s : words) { context.write(new Text(s), new Text(1 + &quot;&quot;)); } } } /** * reduce阶段 */ public static class MyReducer extends Reducer&lt;Text, Text, Text, Text&gt; { @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException { int counter = 0; for (Text t : values) { counter += Integer.parseInt(t.toString()); } context.write(key, new Text(counter + &quot;&quot;)); } } public void setConf(Configuration conf) { // 对conf的属性设置 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://qf&quot;); conf.set(&quot;dfs.nameservices&quot;, &quot;qf&quot;); conf.set(&quot;dfs.ha.namenodes.qf&quot;, &quot;nn1, nn2&quot;); conf.set(&quot;dfs.namenode.rpc-address.qf.nn1&quot;, &quot;hadoop01:9000&quot;); conf.set(&quot;dfs.namenode.rpc-address.qf.nn2&quot;, &quot;hadoop02:9000&quot;); conf.set(&quot;dfs.client.failover.proxy.provider.qf&quot;, &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot;); } public Configuration getConf() { return new Configuration(); } /** * 驱动方法 */ public int run(String[] args) throws Exception { // 1.获取配置对象信息 Configuration conf = getConf(); // 2.对conf进行设置（没有就不用） // 3.获取job对象 （注意导入的包） Job job = Job.getInstance(conf, &quot;job&quot;); // 4.设置job的运行主类 job.setJarByClass(PartitionDemo.class); //set inputpath and outputpath setInputAndOutput(job, conf, args); // System.out.println(&quot;jiazai finished&quot;); // 5.对map阶段进行设置 job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); //添加分区信息 job.setPartitionerClass(MyPatitioner.class); job.setNumReduceTasks(4);//上面有4种情况 分为4个文件存放 // System.out.println(&quot;map finished&quot;); // 6.对reduce阶段进行设置 job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //提交 return job.waitForCompletion(true) ? 0 : 1; } //主方法 public static void main(String[] args) throws Exception { int isok = ToolRunner.run(new Configuration(), new PartitionDemo(), args); System.out.println(isok); } /** * 处理参数的方法 * @param job * @param conf * @param args */ private void setInputAndOutput(Job job, Configuration conf, String[] args) { if(args.length != 2) { System.out.println(&quot;usage:yarn jar /*.jar package.classname /* /*&quot;); return ; } //正常处理输入输出参数 try { FileInputFormat.addInputPath(job, new Path(args[0])); FileSystem fs = FileSystem.get(conf); Path outputpath = new Path(args[1]); if(fs.exists(outputpath)) { fs.delete(outputpath, true); } FileOutputFormat.setOutputPath(job, outputpath); } catch (Exception e) { e.printStackTrace(); } } } MyPatitioner package qf.com.mr; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Partitioner; /* *@author Shishuai E-mail：1198319583@qq.com *@version Create time ： 2019年5月28日下午5:39:18 *类说明：自定义的Partitioner *自定义分区需要注意的 *1.分区需要继承Partitioner&lt;key, value&gt;, 其中的key-value需要和map阶段的输出相同 *2.实现getPartition(key, value, numPartitions)方法， 该方法只能返回int类型 *3.分区数和reduce个数相同 *4.默认使用HashPartitioner */ public class MyPatitioner extends Partitioner&lt;Text, Text&gt;{ @Override public int getPartition(Text key, Text value, int numPartitions) { String firstChar = key.toString().substring(0, 1);//把第一个字母给截下来 if(firstChar.matches(&quot;^[A-Z]&quot;)) { return 0%numPartitions; }else if(firstChar.matches(&quot;^[a-z]&quot;)) { return 1%numPartitions; }else if(firstChar.matches(&quot;^[0-9]&quot;)) { return 2%numPartitions; }else { return 3%numPartitions; } } } 倒排索引案例 在home下创建三个文件1.html 2.html 3.html 写入数据 数据在代码注释上 然后上传到hdfs 然后（先在hdfs建一个文件夹di 一遍存放这三个文件） yarn jar /home/wc.jar qf.com.mr.DescIndexDemo /di /out/20 运行完之后查看结果 然后就不对 跟上次分区的输出差不多啊 咋回事 这个地方忘记改了 卧槽 还有这个地方 我去 视频上都不说 找了半天 出现错误 注意导入的包 解决 终于看到了想要的结果" />
<meta property="og:description" content="MapReduce的shuffle过程 map输出 reduce输入 的过程 好理解吧 输入 三行 做分片 一个分片一行 Mapping统计好 Shuffling这边排序 做一些分区的工作 Reducing接收 统计 输出结果 这些图讲的都是shuffle 不太清楚 可自行到官网上寻找 然后这里一张长图 拆开了 一块是128M 这里我们默认一块对应一个分片 但是有10%的slop冗余 比如说有140M的文件 也会分成一个分片 一个分片对应一个MapTask 这个圆形缓冲区默认大小100M 80M是阈值 当达到80M时 就将数据写入到临时文件取 剩下的20M继续接受map函数发送的数据 然后可以看一下FileInputFormat和 FileOutputFormat以及SpillRecord的源码 自己分析一下 shuffle的相关操作 看下配置名 cd /usr/local/hadoop-2.7.1 vi ./etc/hadoop/core-site.xml 遇到错误 中间我隔了一天继续这个 再次启动start-all.sh 这个命令后 出现错误 使用hdfs dfs的相关命令出错 表示就是两个namenode全是standby状态 没有一个active的 难受 网上查阅资料 说是可以手动开启 或者启动zkfc 说是这个没开 开了就好了 所以我就使用了 hadoop-damons.sh start zkfc 这个命令 jps后 Zk这个服务出现了 没使用上述命令前是没有的 onMygod 难受 我需要上传par文件 就是数据 现在home下建一个par然后写入 复制上就行 然后传到hdfs 然后将jar包传到home下 （导出的jar包可以是整个项目 使用时写明白类名就行） 具体的把 看截图吧 hdfs dfs -put /home/par /par 然后记得看下输出目录（我们自己定义） 我们经常使用hdfs下的out目录 注意看看out下的名字 别重复了 我刚刚删了 重新建的 所以我就别看了 总之就是输出别重复了名字 先进入hadoop目录 cd /usr/local/hadoop-2.7.1 然后 yarn jar /home/wc.jar qf.com.mr.PartitionDemo /par /out/19 这个又出错了 上次的错误 连接不上02 然后 我又 stop-all.sh 然后等着关了后 又 start-all.sh 就是又重启了服务就好了 我猜 是不是 先将zkfc启动了 再去启动其他的服务 是不是这样？ 谁能告诉我 找到答案了 这样再运行就对了 因为数据小 一个分片 一块 就是一个进程 然后四个结果文件 如下图 然后查看一下 19文件夹下确实有4个结果文件（除了代表成功的那个） 查看一下其中内容 非常正确 至此 分区案例结束 下面贴上代码 写的时候注意导入的包 PartitionDemo package qf.com.mr; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; /* *@author Shishuai E-mail：1198319583@qq.com *@version Create time ： 2019年5月28日下午5:42:34 *类说明：分区 输入数据 Hello HI HI qianfeng hi hi qianfeng qianfeng 163.com qq.com 189.com @163.com @qq.com *123 输出： 将首写字母为A-Z的放到一个文件中 将首写字母为a-z的放到一个文件中 将首写字母为0-9的放到一个文件中 其他的放到一个文件中 结果文件：part-r-00000 Hello 1 Hi 2 结果文件：part-r-00001 hi 2 qianfeng 2 结果文件：part-r-00002 .... 结果文件：part-r-00003 .... */ public class PartitionDemo implements Tool { /** * map阶段 * * @author HP * */ public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String words[] = line.split(&quot; &quot;); for (String s : words) { context.write(new Text(s), new Text(1 + &quot;&quot;)); } } } /** * reduce阶段 */ public static class MyReducer extends Reducer&lt;Text, Text, Text, Text&gt; { @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException { int counter = 0; for (Text t : values) { counter += Integer.parseInt(t.toString()); } context.write(key, new Text(counter + &quot;&quot;)); } } public void setConf(Configuration conf) { // 对conf的属性设置 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://qf&quot;); conf.set(&quot;dfs.nameservices&quot;, &quot;qf&quot;); conf.set(&quot;dfs.ha.namenodes.qf&quot;, &quot;nn1, nn2&quot;); conf.set(&quot;dfs.namenode.rpc-address.qf.nn1&quot;, &quot;hadoop01:9000&quot;); conf.set(&quot;dfs.namenode.rpc-address.qf.nn2&quot;, &quot;hadoop02:9000&quot;); conf.set(&quot;dfs.client.failover.proxy.provider.qf&quot;, &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot;); } public Configuration getConf() { return new Configuration(); } /** * 驱动方法 */ public int run(String[] args) throws Exception { // 1.获取配置对象信息 Configuration conf = getConf(); // 2.对conf进行设置（没有就不用） // 3.获取job对象 （注意导入的包） Job job = Job.getInstance(conf, &quot;job&quot;); // 4.设置job的运行主类 job.setJarByClass(PartitionDemo.class); //set inputpath and outputpath setInputAndOutput(job, conf, args); // System.out.println(&quot;jiazai finished&quot;); // 5.对map阶段进行设置 job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); //添加分区信息 job.setPartitionerClass(MyPatitioner.class); job.setNumReduceTasks(4);//上面有4种情况 分为4个文件存放 // System.out.println(&quot;map finished&quot;); // 6.对reduce阶段进行设置 job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //提交 return job.waitForCompletion(true) ? 0 : 1; } //主方法 public static void main(String[] args) throws Exception { int isok = ToolRunner.run(new Configuration(), new PartitionDemo(), args); System.out.println(isok); } /** * 处理参数的方法 * @param job * @param conf * @param args */ private void setInputAndOutput(Job job, Configuration conf, String[] args) { if(args.length != 2) { System.out.println(&quot;usage:yarn jar /*.jar package.classname /* /*&quot;); return ; } //正常处理输入输出参数 try { FileInputFormat.addInputPath(job, new Path(args[0])); FileSystem fs = FileSystem.get(conf); Path outputpath = new Path(args[1]); if(fs.exists(outputpath)) { fs.delete(outputpath, true); } FileOutputFormat.setOutputPath(job, outputpath); } catch (Exception e) { e.printStackTrace(); } } } MyPatitioner package qf.com.mr; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Partitioner; /* *@author Shishuai E-mail：1198319583@qq.com *@version Create time ： 2019年5月28日下午5:39:18 *类说明：自定义的Partitioner *自定义分区需要注意的 *1.分区需要继承Partitioner&lt;key, value&gt;, 其中的key-value需要和map阶段的输出相同 *2.实现getPartition(key, value, numPartitions)方法， 该方法只能返回int类型 *3.分区数和reduce个数相同 *4.默认使用HashPartitioner */ public class MyPatitioner extends Partitioner&lt;Text, Text&gt;{ @Override public int getPartition(Text key, Text value, int numPartitions) { String firstChar = key.toString().substring(0, 1);//把第一个字母给截下来 if(firstChar.matches(&quot;^[A-Z]&quot;)) { return 0%numPartitions; }else if(firstChar.matches(&quot;^[a-z]&quot;)) { return 1%numPartitions; }else if(firstChar.matches(&quot;^[0-9]&quot;)) { return 2%numPartitions; }else { return 3%numPartitions; } } } 倒排索引案例 在home下创建三个文件1.html 2.html 3.html 写入数据 数据在代码注释上 然后上传到hdfs 然后（先在hdfs建一个文件夹di 一遍存放这三个文件） yarn jar /home/wc.jar qf.com.mr.DescIndexDemo /di /out/20 运行完之后查看结果 然后就不对 跟上次分区的输出差不多啊 咋回事 这个地方忘记改了 卧槽 还有这个地方 我去 视频上都不说 找了半天 出现错误 注意导入的包 解决 终于看到了想要的结果" />
<link rel="canonical" href="https://uzzz.org/2019/05/30/788148.html" />
<meta property="og:url" content="https://uzzz.org/2019/05/30/788148.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-30T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"MapReduce的shuffle过程 map输出 reduce输入 的过程 好理解吧 输入 三行 做分片 一个分片一行 Mapping统计好 Shuffling这边排序 做一些分区的工作 Reducing接收 统计 输出结果 这些图讲的都是shuffle 不太清楚 可自行到官网上寻找 然后这里一张长图 拆开了 一块是128M 这里我们默认一块对应一个分片 但是有10%的slop冗余 比如说有140M的文件 也会分成一个分片 一个分片对应一个MapTask 这个圆形缓冲区默认大小100M 80M是阈值 当达到80M时 就将数据写入到临时文件取 剩下的20M继续接受map函数发送的数据 然后可以看一下FileInputFormat和 FileOutputFormat以及SpillRecord的源码 自己分析一下 shuffle的相关操作 看下配置名 cd /usr/local/hadoop-2.7.1 vi ./etc/hadoop/core-site.xml 遇到错误 中间我隔了一天继续这个 再次启动start-all.sh 这个命令后 出现错误 使用hdfs dfs的相关命令出错 表示就是两个namenode全是standby状态 没有一个active的 难受 网上查阅资料 说是可以手动开启 或者启动zkfc 说是这个没开 开了就好了 所以我就使用了 hadoop-damons.sh start zkfc 这个命令 jps后 Zk这个服务出现了 没使用上述命令前是没有的 onMygod 难受 我需要上传par文件 就是数据 现在home下建一个par然后写入 复制上就行 然后传到hdfs 然后将jar包传到home下 （导出的jar包可以是整个项目 使用时写明白类名就行） 具体的把 看截图吧 hdfs dfs -put /home/par /par 然后记得看下输出目录（我们自己定义） 我们经常使用hdfs下的out目录 注意看看out下的名字 别重复了 我刚刚删了 重新建的 所以我就别看了 总之就是输出别重复了名字 先进入hadoop目录 cd /usr/local/hadoop-2.7.1 然后 yarn jar /home/wc.jar qf.com.mr.PartitionDemo /par /out/19 这个又出错了 上次的错误 连接不上02 然后 我又 stop-all.sh 然后等着关了后 又 start-all.sh 就是又重启了服务就好了 我猜 是不是 先将zkfc启动了 再去启动其他的服务 是不是这样？ 谁能告诉我 找到答案了 这样再运行就对了 因为数据小 一个分片 一块 就是一个进程 然后四个结果文件 如下图 然后查看一下 19文件夹下确实有4个结果文件（除了代表成功的那个） 查看一下其中内容 非常正确 至此 分区案例结束 下面贴上代码 写的时候注意导入的包 PartitionDemo package qf.com.mr; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; /* *@author Shishuai E-mail：1198319583@qq.com *@version Create time ： 2019年5月28日下午5:42:34 *类说明：分区 输入数据 Hello HI HI qianfeng hi hi qianfeng qianfeng 163.com qq.com 189.com @163.com @qq.com *123 输出： 将首写字母为A-Z的放到一个文件中 将首写字母为a-z的放到一个文件中 将首写字母为0-9的放到一个文件中 其他的放到一个文件中 结果文件：part-r-00000 Hello 1 Hi 2 结果文件：part-r-00001 hi 2 qianfeng 2 结果文件：part-r-00002 .... 结果文件：part-r-00003 .... */ public class PartitionDemo implements Tool { /** * map阶段 * * @author HP * */ public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String words[] = line.split(&quot; &quot;); for (String s : words) { context.write(new Text(s), new Text(1 + &quot;&quot;)); } } } /** * reduce阶段 */ public static class MyReducer extends Reducer&lt;Text, Text, Text, Text&gt; { @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException { int counter = 0; for (Text t : values) { counter += Integer.parseInt(t.toString()); } context.write(key, new Text(counter + &quot;&quot;)); } } public void setConf(Configuration conf) { // 对conf的属性设置 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://qf&quot;); conf.set(&quot;dfs.nameservices&quot;, &quot;qf&quot;); conf.set(&quot;dfs.ha.namenodes.qf&quot;, &quot;nn1, nn2&quot;); conf.set(&quot;dfs.namenode.rpc-address.qf.nn1&quot;, &quot;hadoop01:9000&quot;); conf.set(&quot;dfs.namenode.rpc-address.qf.nn2&quot;, &quot;hadoop02:9000&quot;); conf.set(&quot;dfs.client.failover.proxy.provider.qf&quot;, &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot;); } public Configuration getConf() { return new Configuration(); } /** * 驱动方法 */ public int run(String[] args) throws Exception { // 1.获取配置对象信息 Configuration conf = getConf(); // 2.对conf进行设置（没有就不用） // 3.获取job对象 （注意导入的包） Job job = Job.getInstance(conf, &quot;job&quot;); // 4.设置job的运行主类 job.setJarByClass(PartitionDemo.class); //set inputpath and outputpath setInputAndOutput(job, conf, args); // System.out.println(&quot;jiazai finished&quot;); // 5.对map阶段进行设置 job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); //添加分区信息 job.setPartitionerClass(MyPatitioner.class); job.setNumReduceTasks(4);//上面有4种情况 分为4个文件存放 // System.out.println(&quot;map finished&quot;); // 6.对reduce阶段进行设置 job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //提交 return job.waitForCompletion(true) ? 0 : 1; } //主方法 public static void main(String[] args) throws Exception { int isok = ToolRunner.run(new Configuration(), new PartitionDemo(), args); System.out.println(isok); } /** * 处理参数的方法 * @param job * @param conf * @param args */ private void setInputAndOutput(Job job, Configuration conf, String[] args) { if(args.length != 2) { System.out.println(&quot;usage:yarn jar /*.jar package.classname /* /*&quot;); return ; } //正常处理输入输出参数 try { FileInputFormat.addInputPath(job, new Path(args[0])); FileSystem fs = FileSystem.get(conf); Path outputpath = new Path(args[1]); if(fs.exists(outputpath)) { fs.delete(outputpath, true); } FileOutputFormat.setOutputPath(job, outputpath); } catch (Exception e) { e.printStackTrace(); } } } MyPatitioner package qf.com.mr; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Partitioner; /* *@author Shishuai E-mail：1198319583@qq.com *@version Create time ： 2019年5月28日下午5:39:18 *类说明：自定义的Partitioner *自定义分区需要注意的 *1.分区需要继承Partitioner&lt;key, value&gt;, 其中的key-value需要和map阶段的输出相同 *2.实现getPartition(key, value, numPartitions)方法， 该方法只能返回int类型 *3.分区数和reduce个数相同 *4.默认使用HashPartitioner */ public class MyPatitioner extends Partitioner&lt;Text, Text&gt;{ @Override public int getPartition(Text key, Text value, int numPartitions) { String firstChar = key.toString().substring(0, 1);//把第一个字母给截下来 if(firstChar.matches(&quot;^[A-Z]&quot;)) { return 0%numPartitions; }else if(firstChar.matches(&quot;^[a-z]&quot;)) { return 1%numPartitions; }else if(firstChar.matches(&quot;^[0-9]&quot;)) { return 2%numPartitions; }else { return 3%numPartitions; } } } 倒排索引案例 在home下创建三个文件1.html 2.html 3.html 写入数据 数据在代码注释上 然后上传到hdfs 然后（先在hdfs建一个文件夹di 一遍存放这三个文件） yarn jar /home/wc.jar qf.com.mr.DescIndexDemo /di /out/20 运行完之后查看结果 然后就不对 跟上次分区的输出差不多啊 咋回事 这个地方忘记改了 卧槽 还有这个地方 我去 视频上都不说 找了半天 出现错误 注意导入的包 解决 终于看到了想要的结果","@type":"BlogPosting","url":"https://uzzz.org/2019/05/30/788148.html","headline":"014 Shuffle的概念介绍 Shuffle的细节图描述 分区案例 倒排索引案例","dateModified":"2019-05-30T00:00:00+08:00","datePublished":"2019-05-30T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/05/30/788148.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>014 Shuffle的概念介绍 Shuffle的细节图描述 分区案例 倒排索引案例</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h2><a id="MapReduceshuffle_1"></a>MapReduce的shuffle过程</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528164601917.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> map输出 reduce输入 的过程</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528164948664.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528165024437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 好理解吧<br> 输入 三行<br> 做分片 一个分片一行<br> Mapping统计好<br> Shuffling这边排序 做一些分区的工作<br> Reducing接收 统计 输出结果<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528165126944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528165316453.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528165424466.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528165455245.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 这些图讲的都是shuffle<br> 不太清楚<br> 可自行到官网上寻找</p> 
  <p>然后这里一张长图 拆开了<br> 一块是128M<br> 这里我们默认一块对应一个分片<br> 但是有10%的slop冗余<br> 比如说有140M的文件 也会分成一个分片<br> 一个分片对应一个MapTask<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528165721340.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 这个圆形缓冲区默认大小100M<br> 80M是阈值<br> 当达到80M时 就将数据写入到临时文件取<br> 剩下的20M继续接受map函数发送的数据<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528170033587.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528170432598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528170538436.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528170625214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528170725296.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528170812435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019052817084615.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 然后可以看一下FileInputFormat和 FileOutputFormat以及SpillRecord的源码 自己分析一下</p> 
  <hr> 
  <h2><a id="shuffle_44"></a>shuffle的相关操作</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528173110615.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>看下配置名</p> 
  <pre><code>cd /usr/local/hadoop-2.7.1
vi ./etc/hadoop/core-site.xml
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019052820082230.png" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190528200810532.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <hr> 
  <h2><a id="_59"></a>遇到错误</h2> 
  <p>中间我隔了一天继续这个<br> 再次启动<code>start-all.sh</code><br> 这个命令后<br> 出现错误<br> 使用hdfs dfs的相关命令出错<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190529231014682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 表示就是两个namenode全是standby状态</p> 
  <p>没有一个active的<br> 难受<br> 网上查阅资料<br> 说是可以手动开启<br> 或者启动zkfc 说是这个没开 开了就好了<br> 所以我就使用了</p> 
  <pre><code>hadoop-damons.sh start zkfc
</code></pre> 
  <p>这个命令<br> jps后<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190529231217541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> Zk这个服务出现了<br> 没使用上述命令前是没有的 onMygod 难受</p> 
  <hr> 
  <p>我需要上传par文件 就是数据<br> 现在home下建一个par然后写入 复制上就行<br> 然后传到hdfs<br> 然后将jar包传到home下<br> （导出的jar包可以是整个项目 使用时写明白类名就行）<br> 具体的把 看截图吧</p> 
  <pre><code>hdfs dfs -put /home/par /par
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190529231613156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190529231558656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 然后记得看下输出目录（我们自己定义） 我们经常使用hdfs下的out目录 注意看看out下的名字 别重复了<br> 我刚刚删了 重新建的 所以我就别看了<br> 总之就是输出别重复了名字</p> 
  <p>先进入hadoop目录</p> 
  <pre><code>cd /usr/local/hadoop-2.7.1
</code></pre> 
  <p>然后</p> 
  <pre><code>yarn jar /home/wc.jar qf.com.mr.PartitionDemo /par /out/19
</code></pre> 
  <h2><a id="_117"></a>这个又出错了</h2> 
  <p>上次的错误<br> 连接不上02<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190529232627454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 然后<br> 我又</p> 
  <pre><code>stop-all.sh
然后等着关了后
又
start-all.sh
</code></pre> 
  <p>就是又重启了服务就好了</p> 
  <h2><a id="

zkfc

_133"></a>我猜<br> 是不是<br> 先将zkfc启动了<br> 再去启动其他的服务<br> 是不是这样？</h2> 
  <h2><a id="
_139"></a>谁能告诉我<br> 找到答案了</h2> 
  <h2><a id="httpsimgblogcsdnimgcn20190530161821341pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQsize_16color_FFFFFFt_70_142"></a><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190530161821341.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></h2> 
  <p>这样再运行就对了<br> 因为数据小<br> 一个分片 一块 就是一个进程<br> 然后四个结果文件<br> 如下图</p> 
  <h1><a id="httpsimgblogcsdnimgcn20190529232941411pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQsize_16color_FFFFFFt_70
_194
_httpsimgblogcsdnimgcn20190529233036231pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQsize_16color_FFFFFFt_70
_
_
_httpsimgblogcsdnimgcn20190529233232509pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQsize_16color_FFFFFFt_70

_152"></a><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190529232941411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 然后查看一下 19文件夹下确实有4个结果文件（除了代表成功的那个）<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190529233036231.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 查看一下其中内容<br> 非常正确<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190529233232509.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 至此<br> 分区案例结束</h1> 
  <p>下面贴上代码<br> 写的时候注意导入的包</p> 
  <h2><a id="PartitionDemo_165"></a>PartitionDemo</h2> 
  <pre><code class="prism language-java"><span class="token keyword">package</span> qf<span class="token punctuation">.</span>com<span class="token punctuation">.</span>mr<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>


<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span>FileSystem<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span>Path<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>LongWritable<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Job<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Mapper<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Reducer<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span>FileInputFormat<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>output<span class="token punctuation">.</span>FileOutputFormat<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Tool<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>util<span class="token punctuation">.</span>ToolRunner<span class="token punctuation">;</span>


<span class="token comment">/* *@author Shishuai E-mail：1198319583@qq.com *@version Create time ： 2019年5月28日下午5:42:34 *类说明：分区 输入数据 Hello HI HI qianfeng hi hi qianfeng qianfeng 163.com qq.com 189.com @163.com @qq.com *123 输出： 将首写字母为A-Z的放到一个文件中 将首写字母为a-z的放到一个文件中 将首写字母为0-9的放到一个文件中 其他的放到一个文件中 结果文件：part-r-00000 Hello 1 Hi 2 结果文件：part-r-00001 hi 2 qianfeng 2 结果文件：part-r-00002 .... 结果文件：part-r-00003 .... */</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">PartitionDemo</span> <span class="token keyword">implements</span> <span class="token class-name">Tool</span> <span class="token punctuation">{</span>

	<span class="token comment">/** * map阶段 * * @author HP * */</span>
	<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">MyMapper</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token generics function"><span class="token punctuation">&lt;</span>LongWritable<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> Text<span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>

		<span class="token annotation punctuation">@Override</span>
		<span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>LongWritable key<span class="token punctuation">,</span> Text value<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>
			String line <span class="token operator">=</span> value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			String words<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> line<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token keyword">for</span> <span class="token punctuation">(</span>String s <span class="token operator">:</span> words<span class="token punctuation">)</span> <span class="token punctuation">{</span>
				context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>

	<span class="token punctuation">}</span>

	<span class="token comment">/** * reduce阶段 */</span>
	<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">MyReducer</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token generics function"><span class="token punctuation">&lt;</span>Text<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> Text<span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>

		<span class="token annotation punctuation">@Override</span>
		<span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span>Text key<span class="token punctuation">,</span> Iterable<span class="token generics function"><span class="token punctuation">&lt;</span>Text<span class="token punctuation">&gt;</span></span> values<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span>
				<span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>
			<span class="token keyword">int</span> counter <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
			<span class="token keyword">for</span> <span class="token punctuation">(</span>Text t <span class="token operator">:</span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>
				counter <span class="token operator">+=</span> Integer<span class="token punctuation">.</span><span class="token function">parseInt</span><span class="token punctuation">(</span>t<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span>
			context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span>counter <span class="token operator">+</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span>

	<span class="token punctuation">}</span>

	<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">setConf</span><span class="token punctuation">(</span>Configuration conf<span class="token punctuation">)</span> <span class="token punctuation">{</span>
		<span class="token comment">// 对conf的属性设置</span>
		conf<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"fs.defaultFS"</span><span class="token punctuation">,</span> <span class="token string">"hdfs://qf"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		conf<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"dfs.nameservices"</span><span class="token punctuation">,</span> <span class="token string">"qf"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		conf<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"dfs.ha.namenodes.qf"</span><span class="token punctuation">,</span> <span class="token string">"nn1, nn2"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		conf<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"dfs.namenode.rpc-address.qf.nn1"</span><span class="token punctuation">,</span> <span class="token string">"hadoop01:9000"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		conf<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"dfs.namenode.rpc-address.qf.nn2"</span><span class="token punctuation">,</span> <span class="token string">"hadoop02:9000"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		conf<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"dfs.client.failover.proxy.provider.qf"</span><span class="token punctuation">,</span> <span class="token string">"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>

	<span class="token keyword">public</span> Configuration <span class="token function">getConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
		<span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>

	<span class="token comment">/** * 驱动方法 */</span>
	<span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">run</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
		<span class="token comment">// 1.获取配置对象信息</span>
		Configuration conf <span class="token operator">=</span> <span class="token function">getConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 2.对conf进行设置（没有就不用）</span>
		<span class="token comment">// 3.获取job对象 （注意导入的包）</span>
		Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>conf<span class="token punctuation">,</span> <span class="token string">"job"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 4.设置job的运行主类</span>
		job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>PartitionDemo<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token comment">//set inputpath and outputpath</span>
		<span class="token function">setInputAndOutput</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> conf<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		
		
		<span class="token comment">// System.out.println("jiazai finished");</span>
		<span class="token comment">// 5.对map阶段进行设置</span>
		job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>MyMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token comment">//添加分区信息</span>
		job<span class="token punctuation">.</span><span class="token function">setPartitionerClass</span><span class="token punctuation">(</span>MyPatitioner<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		job<span class="token punctuation">.</span><span class="token function">setNumReduceTasks</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//上面有4种情况 分为4个文件存放</span>
		
		
		<span class="token comment">// System.out.println("map finished");</span>
		<span class="token comment">// 6.对reduce阶段进行设置</span>
		job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>MyReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token comment">//提交</span>
		<span class="token keyword">return</span> job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">;</span>
		

	<span class="token punctuation">}</span>
	
	
	<span class="token comment">//主方法</span>
	<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
		<span class="token keyword">int</span> isok <span class="token operator">=</span> ToolRunner<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">PartitionDemo</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">;</span>
		System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>isok<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>

	<span class="token comment">/** * 处理参数的方法 * @param job * @param conf * @param args */</span>
	
	<span class="token keyword">private</span> <span class="token keyword">void</span> <span class="token function">setInputAndOutput</span><span class="token punctuation">(</span>Job job<span class="token punctuation">,</span> Configuration conf<span class="token punctuation">,</span> String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>
		<span class="token keyword">if</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>length <span class="token operator">!=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
			System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"usage:yarn jar /*.jar package.classname /* /*"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token keyword">return</span> <span class="token punctuation">;</span>
		<span class="token punctuation">}</span>
		<span class="token comment">//正常处理输入输出参数</span>
		<span class="token keyword">try</span> <span class="token punctuation">{</span>
			FileInputFormat<span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			
			FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
			Path outputpath <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token keyword">if</span><span class="token punctuation">(</span>fs<span class="token punctuation">.</span><span class="token function">exists</span><span class="token punctuation">(</span>outputpath<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
				fs<span class="token punctuation">.</span><span class="token function">delete</span><span class="token punctuation">(</span>outputpath<span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span>
			FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> outputpath<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">Exception</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>
			e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span>
		
	<span class="token punctuation">}</span>

<span class="token punctuation">}</span>

</code></pre> 
  <h2><a id="MyPatitioner_352"></a>MyPatitioner</h2> 
  <pre><code class="prism language-java"><span class="token keyword">package</span> qf<span class="token punctuation">.</span>com<span class="token punctuation">.</span>mr<span class="token punctuation">;</span>

<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Partitioner<span class="token punctuation">;</span>

<span class="token comment">/* *@author Shishuai E-mail：1198319583@qq.com *@version Create time ： 2019年5月28日下午5:39:18 *类说明：自定义的Partitioner *自定义分区需要注意的 *1.分区需要继承Partitioner&lt;key, value&gt;, 其中的key-value需要和map阶段的输出相同 *2.实现getPartition(key, value, numPartitions)方法， 该方法只能返回int类型 *3.分区数和reduce个数相同 *4.默认使用HashPartitioner */</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MyPatitioner</span> <span class="token keyword">extends</span> <span class="token class-name">Partitioner</span><span class="token generics function"><span class="token punctuation">&lt;</span>Text<span class="token punctuation">,</span> Text<span class="token punctuation">&gt;</span></span><span class="token punctuation">{</span>

	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">getPartition</span><span class="token punctuation">(</span>Text key<span class="token punctuation">,</span> Text value<span class="token punctuation">,</span> <span class="token keyword">int</span> numPartitions<span class="token punctuation">)</span> <span class="token punctuation">{</span>
		String firstChar <span class="token operator">=</span> key<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//把第一个字母给截下来</span>
		<span class="token keyword">if</span><span class="token punctuation">(</span>firstChar<span class="token punctuation">.</span><span class="token function">matches</span><span class="token punctuation">(</span><span class="token string">"^[A-Z]"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
			<span class="token keyword">return</span> <span class="token number">0</span><span class="token operator">%</span>numPartitions<span class="token punctuation">;</span>
		<span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span>firstChar<span class="token punctuation">.</span><span class="token function">matches</span><span class="token punctuation">(</span><span class="token string">"^[a-z]"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
			<span class="token keyword">return</span> <span class="token number">1</span><span class="token operator">%</span>numPartitions<span class="token punctuation">;</span>
		<span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span>firstChar<span class="token punctuation">.</span><span class="token function">matches</span><span class="token punctuation">(</span><span class="token string">"^[0-9]"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
			<span class="token keyword">return</span> <span class="token number">2</span><span class="token operator">%</span>numPartitions<span class="token punctuation">;</span>
		<span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span>
			<span class="token keyword">return</span> <span class="token number">3</span><span class="token operator">%</span>numPartitions<span class="token punctuation">;</span>
		<span class="token punctuation">}</span>
		
	<span class="token punctuation">}</span>
	
<span class="token punctuation">}</span>

</code></pre> 
  <hr> 
  <p>倒排索引案例<br> 在home下创建三个文件1.html 2.html 3.html<br> 写入数据 数据在代码注释上<br> 然后上传到hdfs<br> 然后（先在hdfs建一个文件夹di 一遍存放这三个文件）</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190530162355777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <pre><code>yarn jar /home/wc.jar qf.com.mr.DescIndexDemo /di /out/20
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190530162635685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190530162803447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 运行完之后查看结果</p> 
  <p>然后就不对 跟上次分区的输出差不多啊 咋回事<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019053016572028.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 这个地方忘记改了<br> 卧槽<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190530165805310.png" alt="在这里插入图片描述"><br> 还有这个地方<br> 我去<br> 视频上都不说<br> 找了半天</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190530165043961.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>出现错误<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190530171113140.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 注意导入的包<br> 解决<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190530171033390.png" alt="在这里插入图片描述"></p> 
  <hr> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190530171925437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NfdGltZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 终于看到了想要的结果</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
