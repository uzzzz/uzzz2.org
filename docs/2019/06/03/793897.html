<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>spark-面试题（含答案） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="spark-面试题（含答案）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1 var, val和def三个关键字之间的区别? var immutable variable val mutable variable def function defined keyword 2.object 和 class 的区别? object 单例 无构造器 成员变量和method都是static 可以直接访问main方法 可以与class同名 构成伴生对象 class 有构造器 需要创建对象 才能在main方法中执行 3.case class (样本类）是什么？ 经过scala编译器优化 适合于对不可变数据建模 可用于模式匹配 Case class的每个参数默认以val(不变形式)存在，除非显式的声明为var 自动产生伴生对象，、且半生对象中自动产生appay方法来构建对象 半生对象自动产生unapply方法，提取主构造器的参数进行模式匹配 产生copy方法，来构建一个与现有值相同的新对象 自动产生hashcode，toString，equals方法 默认是可以序列化的，也就是实现了Serializable ； 4 Option , Try和Either三者的区別? 三者内部都包含两个case class 区别是： Option 代表可选值，一个是 Some（代表有值），一个是 None （值为空）；常用于结果可能为 null 的情况； Try 运算的结果有两种情况，一个是运行正常，即 Success ，一个是运行出错，抛出异常 ，即 Failure ，其中 Failure 里面包含的是异常的信息； Either 代表结果有两种可能 Right 或者 left 5.什么是函数柯里化 函数可以定义多个参数列表。当使用较少的参数列表调用方法时，这将生成一个将缺少的参数列表作为参数的函数。如果固定某些参数，将得到接受余下参数的一个函数 作用： 1.提高适用性，参数复用 2.延迟计算 trait (特质)和abstract class (抽象类)的区別？ 一个class可以实现多个trait 只能继承一个abstract class trait 用于在类之间共享接口和字段。它们类似于Java 8的接口。不能实例化，因此没有参数。 abstract class 可以定义有参数构造器 二、编程题(每题10分，共30分} 99乘法表。 def main(args: Array[String]): Unit = { for ( i &lt;- 1 to 9; j &lt;- 1 to i; res = s&quot;$j*$i=${i * j}\t&quot; ) yield { if (j == i) s&quot;$res\n&quot; else res }.foreach(print) } 2.编写一个BankAccount类，加入deposit和withdraw方法，和一个只读的 balance属性。 class BankAcount(val balance : Int = 0) { def deposit() {} def withdraw() {} } Spark wordcount 计算。 object WordCount { def main(args: Array[String]) { if (args.length &lt; 1) { System.err.println(&quot;Usage: WordCount &lt;directory&gt;&quot;) System.exit(1) } val sparkConf = new SparkConf().setAppName(&quot;WordCount&quot; val ssc = new StreamingContext(sparkConf, Seconds(2)) val lines = ssc.textFileStream(args(0)) val words = lines.flatMap(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _) wordCounts.print() ssc.start() ssc.awaitTermination() } } 三、简答题（每题8分 共40分） 1、driver的功能是什么？ 一个程序，它声明数据的RDD上的转换和操作，并将这些请求提交给master 功能 1）、创建SparkContext的程序，连接到给定的SparkMaster 2）、划分rdd stage 并生成 DAGScheduler 3）、通过taskScheduler生成并发送task到executor 它的位置独立于master、slave。您在master节点，也可以从另一个节点运行它。唯一的要求是它必须在一个可以从Spark Workers访问的网络中 2、RDD宽依赖和窄依赖？ wide dependency or shuffle dependency 多个子RDD的Partition会依赖同一个父RDD的Partition narrow dependency 每一个父RDD的Partition最多被子RDD的一个Partition使用 3、map 和 flatMap、mapPartition 的区别？ map 通过向这个RDD的所有元素应用一个函数来返回一个新的RDD。 flatMap 返回一个新的RDD 首将一个函数应用于该RDD的所有元素，然后将结果展平。 mapPartitions 通过向这个RDD的每个分区应用一个函数来返回一个新的RDD preservespartitioning 指示输入函数是否保留分区器， 默认为false，除非这是pairRDD，并且输入函数不修改keys 4、spark中的RDD是什么？有哪些特性? RDD是弹性分布式数据集 不可变的可并行操作的元素的分区集合 每个RDD具有五个主要特性： 1）、有分区列表 2）、可用于计算每个分区的函数 3）、依赖于其他RDD的列表 4）、可选pairRDD的分区器（例如说RDD是哈希分区的） 5）、优先选择本地最优的位置去计算每个分片（例如，HDFS文件块位置）即数据的本地性 5、spark中如何划分 stage stage是一组并行任务，作为job的部分 计算相同的函数 stage通过是否存在Shuffle或者宽依赖为边界来划分的 如遇到reduceByKey,groupByKey的算子等算子 从当前job的最后一个算子往前推，遇到宽依赖，那么当前在这个批次中的所有算子操作都划分成一个stage,然后继续按照这种方式在继续往前推，如在遇到宽依赖，又划分成一个stage,一直到最前面的一个算子。最后整个job会被划分成多个stage,而stage之间又存在依赖关系，后面的stage依赖于前面的stage" />
<meta property="og:description" content="1 var, val和def三个关键字之间的区别? var immutable variable val mutable variable def function defined keyword 2.object 和 class 的区别? object 单例 无构造器 成员变量和method都是static 可以直接访问main方法 可以与class同名 构成伴生对象 class 有构造器 需要创建对象 才能在main方法中执行 3.case class (样本类）是什么？ 经过scala编译器优化 适合于对不可变数据建模 可用于模式匹配 Case class的每个参数默认以val(不变形式)存在，除非显式的声明为var 自动产生伴生对象，、且半生对象中自动产生appay方法来构建对象 半生对象自动产生unapply方法，提取主构造器的参数进行模式匹配 产生copy方法，来构建一个与现有值相同的新对象 自动产生hashcode，toString，equals方法 默认是可以序列化的，也就是实现了Serializable ； 4 Option , Try和Either三者的区別? 三者内部都包含两个case class 区别是： Option 代表可选值，一个是 Some（代表有值），一个是 None （值为空）；常用于结果可能为 null 的情况； Try 运算的结果有两种情况，一个是运行正常，即 Success ，一个是运行出错，抛出异常 ，即 Failure ，其中 Failure 里面包含的是异常的信息； Either 代表结果有两种可能 Right 或者 left 5.什么是函数柯里化 函数可以定义多个参数列表。当使用较少的参数列表调用方法时，这将生成一个将缺少的参数列表作为参数的函数。如果固定某些参数，将得到接受余下参数的一个函数 作用： 1.提高适用性，参数复用 2.延迟计算 trait (特质)和abstract class (抽象类)的区別？ 一个class可以实现多个trait 只能继承一个abstract class trait 用于在类之间共享接口和字段。它们类似于Java 8的接口。不能实例化，因此没有参数。 abstract class 可以定义有参数构造器 二、编程题(每题10分，共30分} 99乘法表。 def main(args: Array[String]): Unit = { for ( i &lt;- 1 to 9; j &lt;- 1 to i; res = s&quot;$j*$i=${i * j}\t&quot; ) yield { if (j == i) s&quot;$res\n&quot; else res }.foreach(print) } 2.编写一个BankAccount类，加入deposit和withdraw方法，和一个只读的 balance属性。 class BankAcount(val balance : Int = 0) { def deposit() {} def withdraw() {} } Spark wordcount 计算。 object WordCount { def main(args: Array[String]) { if (args.length &lt; 1) { System.err.println(&quot;Usage: WordCount &lt;directory&gt;&quot;) System.exit(1) } val sparkConf = new SparkConf().setAppName(&quot;WordCount&quot; val ssc = new StreamingContext(sparkConf, Seconds(2)) val lines = ssc.textFileStream(args(0)) val words = lines.flatMap(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _) wordCounts.print() ssc.start() ssc.awaitTermination() } } 三、简答题（每题8分 共40分） 1、driver的功能是什么？ 一个程序，它声明数据的RDD上的转换和操作，并将这些请求提交给master 功能 1）、创建SparkContext的程序，连接到给定的SparkMaster 2）、划分rdd stage 并生成 DAGScheduler 3）、通过taskScheduler生成并发送task到executor 它的位置独立于master、slave。您在master节点，也可以从另一个节点运行它。唯一的要求是它必须在一个可以从Spark Workers访问的网络中 2、RDD宽依赖和窄依赖？ wide dependency or shuffle dependency 多个子RDD的Partition会依赖同一个父RDD的Partition narrow dependency 每一个父RDD的Partition最多被子RDD的一个Partition使用 3、map 和 flatMap、mapPartition 的区别？ map 通过向这个RDD的所有元素应用一个函数来返回一个新的RDD。 flatMap 返回一个新的RDD 首将一个函数应用于该RDD的所有元素，然后将结果展平。 mapPartitions 通过向这个RDD的每个分区应用一个函数来返回一个新的RDD preservespartitioning 指示输入函数是否保留分区器， 默认为false，除非这是pairRDD，并且输入函数不修改keys 4、spark中的RDD是什么？有哪些特性? RDD是弹性分布式数据集 不可变的可并行操作的元素的分区集合 每个RDD具有五个主要特性： 1）、有分区列表 2）、可用于计算每个分区的函数 3）、依赖于其他RDD的列表 4）、可选pairRDD的分区器（例如说RDD是哈希分区的） 5）、优先选择本地最优的位置去计算每个分片（例如，HDFS文件块位置）即数据的本地性 5、spark中如何划分 stage stage是一组并行任务，作为job的部分 计算相同的函数 stage通过是否存在Shuffle或者宽依赖为边界来划分的 如遇到reduceByKey,groupByKey的算子等算子 从当前job的最后一个算子往前推，遇到宽依赖，那么当前在这个批次中的所有算子操作都划分成一个stage,然后继续按照这种方式在继续往前推，如在遇到宽依赖，又划分成一个stage,一直到最前面的一个算子。最后整个job会被划分成多个stage,而stage之间又存在依赖关系，后面的stage依赖于前面的stage" />
<link rel="canonical" href="https://uzzz.org/2019/06/03/793897.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/03/793897.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-03T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"1 var, val和def三个关键字之间的区别? var immutable variable val mutable variable def function defined keyword 2.object 和 class 的区别? object 单例 无构造器 成员变量和method都是static 可以直接访问main方法 可以与class同名 构成伴生对象 class 有构造器 需要创建对象 才能在main方法中执行 3.case class (样本类）是什么？ 经过scala编译器优化 适合于对不可变数据建模 可用于模式匹配 Case class的每个参数默认以val(不变形式)存在，除非显式的声明为var 自动产生伴生对象，、且半生对象中自动产生appay方法来构建对象 半生对象自动产生unapply方法，提取主构造器的参数进行模式匹配 产生copy方法，来构建一个与现有值相同的新对象 自动产生hashcode，toString，equals方法 默认是可以序列化的，也就是实现了Serializable ； 4 Option , Try和Either三者的区別? 三者内部都包含两个case class 区别是： Option 代表可选值，一个是 Some（代表有值），一个是 None （值为空）；常用于结果可能为 null 的情况； Try 运算的结果有两种情况，一个是运行正常，即 Success ，一个是运行出错，抛出异常 ，即 Failure ，其中 Failure 里面包含的是异常的信息； Either 代表结果有两种可能 Right 或者 left 5.什么是函数柯里化 函数可以定义多个参数列表。当使用较少的参数列表调用方法时，这将生成一个将缺少的参数列表作为参数的函数。如果固定某些参数，将得到接受余下参数的一个函数 作用： 1.提高适用性，参数复用 2.延迟计算 trait (特质)和abstract class (抽象类)的区別？ 一个class可以实现多个trait 只能继承一个abstract class trait 用于在类之间共享接口和字段。它们类似于Java 8的接口。不能实例化，因此没有参数。 abstract class 可以定义有参数构造器 二、编程题(每题10分，共30分} 99乘法表。 def main(args: Array[String]): Unit = { for ( i &lt;- 1 to 9; j &lt;- 1 to i; res = s&quot;$j*$i=${i * j}\\t&quot; ) yield { if (j == i) s&quot;$res\\n&quot; else res }.foreach(print) } 2.编写一个BankAccount类，加入deposit和withdraw方法，和一个只读的 balance属性。 class BankAcount(val balance : Int = 0) { def deposit() {} def withdraw() {} } Spark wordcount 计算。 object WordCount { def main(args: Array[String]) { if (args.length &lt; 1) { System.err.println(&quot;Usage: WordCount &lt;directory&gt;&quot;) System.exit(1) } val sparkConf = new SparkConf().setAppName(&quot;WordCount&quot; val ssc = new StreamingContext(sparkConf, Seconds(2)) val lines = ssc.textFileStream(args(0)) val words = lines.flatMap(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _) wordCounts.print() ssc.start() ssc.awaitTermination() } } 三、简答题（每题8分 共40分） 1、driver的功能是什么？ 一个程序，它声明数据的RDD上的转换和操作，并将这些请求提交给master 功能 1）、创建SparkContext的程序，连接到给定的SparkMaster 2）、划分rdd stage 并生成 DAGScheduler 3）、通过taskScheduler生成并发送task到executor 它的位置独立于master、slave。您在master节点，也可以从另一个节点运行它。唯一的要求是它必须在一个可以从Spark Workers访问的网络中 2、RDD宽依赖和窄依赖？ wide dependency or shuffle dependency 多个子RDD的Partition会依赖同一个父RDD的Partition narrow dependency 每一个父RDD的Partition最多被子RDD的一个Partition使用 3、map 和 flatMap、mapPartition 的区别？ map 通过向这个RDD的所有元素应用一个函数来返回一个新的RDD。 flatMap 返回一个新的RDD 首将一个函数应用于该RDD的所有元素，然后将结果展平。 mapPartitions 通过向这个RDD的每个分区应用一个函数来返回一个新的RDD preservespartitioning 指示输入函数是否保留分区器， 默认为false，除非这是pairRDD，并且输入函数不修改keys 4、spark中的RDD是什么？有哪些特性? RDD是弹性分布式数据集 不可变的可并行操作的元素的分区集合 每个RDD具有五个主要特性： 1）、有分区列表 2）、可用于计算每个分区的函数 3）、依赖于其他RDD的列表 4）、可选pairRDD的分区器（例如说RDD是哈希分区的） 5）、优先选择本地最优的位置去计算每个分片（例如，HDFS文件块位置）即数据的本地性 5、spark中如何划分 stage stage是一组并行任务，作为job的部分 计算相同的函数 stage通过是否存在Shuffle或者宽依赖为边界来划分的 如遇到reduceByKey,groupByKey的算子等算子 从当前job的最后一个算子往前推，遇到宽依赖，那么当前在这个批次中的所有算子操作都划分成一个stage,然后继续按照这种方式在继续往前推，如在遇到宽依赖，又划分成一个stage,一直到最前面的一个算子。最后整个job会被划分成多个stage,而stage之间又存在依赖关系，后面的stage依赖于前面的stage","@type":"BlogPosting","url":"https://uzzz.org/2019/06/03/793897.html","headline":"spark-面试题（含答案）","dateModified":"2019-06-03T00:00:00+08:00","datePublished":"2019-06-03T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/03/793897.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>spark-面试题（含答案）</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>1 var, val和def三个关键字之间的区别?<br> var immutable variable<br> val mutable variable<br> def function defined keyword</p> 
  <p>2.object 和 class 的区别?</p> 
  <p>object 单例 无构造器 成员变量和method都是static 可以直接访问main方法 可以与class同名 构成伴生对象</p> 
  <p>class 有构造器 需要创建对象 才能在main方法中执行</p> 
  <p>3.case class (样本类）是什么？<br> 经过scala编译器优化 适合于对不可变数据建模 可用于模式匹配<br> Case class的每个参数默认以val(不变形式)存在，除非显式的声明为var<br> 自动产生伴生对象，、且半生对象中自动产生appay方法来构建对象<br> 半生对象自动产生unapply方法，提取主构造器的参数进行模式匹配<br> 产生copy方法，来构建一个与现有值相同的新对象<br> 自动产生hashcode，toString，equals方法<br> 默认是可以序列化的，也就是实现了Serializable ；</p> 
  <p>4 Option , Try和Either三者的区別?</p> 
  <p>三者内部都包含两个case class</p> 
  <p>区别是：</p> 
  <p>Option 代表可选值，一个是 Some（代表有值），一个是 None （值为空）；常用于结果可能为 null 的情况；</p> 
  <p>Try<br> 运算的结果有两种情况，一个是运行正常，即 Success ，一个是运行出错，抛出异常 ，即 Failure ，其中 Failure 里面包含的是异常的信息；</p> 
  <p>Either 代表结果有两种可能 Right 或者 left</p> 
  <p>5.什么是函数柯里化</p> 
  <p>函数可以定义多个参数列表。当使用较少的参数列表调用方法时，这将生成一个将缺少的参数列表作为参数的函数。如果固定某些参数，将得到接受余下参数的一个函数</p> 
  <p>作用：<br> 1.提高适用性，参数复用</p> 
  <p>2.延迟计算</p> 
  <ol start="6"> 
   <li>trait (特质)和abstract class (抽象类)的区別？</li> 
  </ol> 
  <p>一个class可以实现多个trait 只能继承一个abstract class<br> trait<br> 用于在类之间共享接口和字段。它们类似于Java 8的接口。不能实例化，因此没有参数。</p> 
  <p>abstract class 可以定义有参数构造器</p> 
  <p>二、编程题(每题10分，共30分}</p> 
  <ol> 
   <li>99乘法表。</li> 
  </ol> 
  <pre><code> def main(args: Array[String]): Unit = {
    for (
      i &lt;- 1 to 9;
      j &lt;- 1 to i;
      res = s"$j*$i=${i * j}\t"
    ) yield {
      if (j == i) s"$res\n" else res
    }.foreach(print)

  }
</code></pre> 
  <p>2.编写一个BankAccount类，加入deposit和withdraw方法，和一个只读的<br> balance属性。</p> 
  <pre><code>class BankAcount(val balance : Int = 0) {
  def deposit() {}
  def withdraw() {}
}

</code></pre> 
  <ol start="3"> 
   <li>Spark wordcount 计算。</li> 
  </ol> 
  <pre><code>object WordCount {
  def main(args: Array[String]) {
    if (args.length &lt; 1) {
      System.err.println("Usage: WordCount &lt;directory&gt;")
      System.exit(1)
    }
    val sparkConf = new SparkConf().setAppName("WordCount" 
    val ssc = new StreamingContext(sparkConf, Seconds(2))
    val lines = ssc.textFileStream(args(0))
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
</code></pre> 
  <p>三、简答题（每题8分 共40分）</p> 
  <p>1、driver的功能是什么？<br> 一个程序，它声明数据的RDD上的转换和操作，并将这些请求提交给master<br> 功能<br> 1）、创建SparkContext的程序，连接到给定的SparkMaster</p> 
  <p>2）、划分rdd stage 并生成 DAGScheduler</p> 
  <p>3）、通过taskScheduler生成并发送task到executor</p> 
  <p>它的位置独立于master、slave。您在master节点，也可以从另一个节点运行它。唯一的要求是它必须在一个可以从Spark Workers访问的网络中</p> 
  <p>2、RDD宽依赖和窄依赖？</p> 
  <p>wide dependency or shuffle dependency<br> 多个子RDD的Partition会依赖同一个父RDD的Partition</p> 
  <p>narrow dependency<br> 每一个父RDD的Partition最多被子RDD的一个Partition使用</p> 
  <p>3、map 和 flatMap、mapPartition 的区别？<br> map<br> 通过向这个RDD的所有元素应用一个函数来返回一个新的RDD。</p> 
  <p>flatMap<br> 返回一个新的RDD 首将一个函数应用于该RDD的所有元素，然后将结果展平。</p> 
  <p>mapPartitions<br> 通过向这个RDD的每个分区应用一个函数来返回一个新的RDD<br> preservespartitioning 指示输入函数是否保留分区器，<br> 默认为false，除非这是pairRDD，并且输入函数不修改keys</p> 
  <p>4、spark中的RDD是什么？有哪些特性?</p> 
  <p>RDD是弹性分布式数据集 不可变的可并行操作的元素的分区集合<br> 每个RDD具有五个主要特性：</p> 
  <p>1）、有分区列表</p> 
  <p>2）、可用于计算每个分区的函数</p> 
  <p>3）、依赖于其他RDD的列表</p> 
  <p>4）、可选pairRDD的分区器（例如说RDD是哈希分区的）</p> 
  <p>5）、优先选择本地最优的位置去计算每个分片（例如，HDFS文件块位置）即数据的本地性</p> 
  <p>5、spark中如何划分 stage<br> stage是一组并行任务，作为job的部分 计算相同的函数<br> stage通过是否存在Shuffle或者宽依赖为边界来划分的 如遇到reduceByKey,groupByKey的算子等算子<br> 从当前job的最后一个算子往前推，遇到宽依赖，那么当前在这个批次中的所有算子操作都划分成一个stage,然后继续按照这种方式在继续往前推，如在遇到宽依赖，又划分成一个stage,一直到最前面的一个算子。最后整个job会被划分成多个stage,而stage之间又存在依赖关系，后面的stage依赖于前面的stage</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
