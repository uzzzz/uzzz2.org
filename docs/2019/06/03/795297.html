<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>PySpark-RDD Basics | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="PySpark-RDD Basics" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="导包 import numpy as np import pandas as ps from pyspark import SparkContext,SparkConf %config ZMQInteractiveShell.ast_node_interactivity=&#39;all&#39; 初始化sparkcontent #方法一： sc = SparkContext(master=&#39;local[2]&#39;)#只能初始化一次 #方法二：通过设置的方式先设置sparkconf，然后在初始化sparkcontent conf = (SparkConf() .setMaster(&#39;local&#39;) .setAppName(&#39;My app&#39;) .set(&#39;spark.executor.memory&#39;,&#39;1g&#39;)) sc = SparkContext(conf=conf) spark参数信息 sc.version sc.pythonVer sc.master str(sc.sparkHome) str(sc.sparkUser()) sc.appName sc.applicationId sc.defaultParallelism sc.defaultMinPartitions sc.stop() Rdd初始化 #rdd 初始化 rdd = sc.parallelize([(&#39;a&#39;,7),(&#39;a&#39;,2),(&#39;b&#39;,2)]) rdd2 = sc.parallelize([(&#39;a&#39;,2),(&#39;d&#39;,1),(&#39;b&#39;,1)]) rdd3 = sc.parallelize(range(100)) rdd4 = sc.parallelize([ (&#39;a&#39;,[&#39;x&#39;,&#39;y&#39;,&#39;z&#39;]),(&#39;b&#39;,[&#39;p&#39;,&#39;r&#39; ]) ]) textFile = sc.textFile(&#39;/my/directory/*.txt&#39;) #遍历文件每一行 返回一个array textFile2 = sc.wholeTextFiles(&#39;/my/directory&#39;) #把这个目录下的所有文件遍历 返回的是 (文件路径,内容)的一个元组 Rdd函数 #检索rdd信息 rdd.getNumPartitions() #1 rdd.count() #3 rdd.countByKey() #defaultdict(int, {&#39;a&#39;: 2, &#39;b&#39;: 1}) rdd.collectAsMap() #{&#39;a&#39;: 2, &#39;b&#39;: 2} rdd3.sum() #4950 sc.parallelize([]).isEmpty()#判断这个rdd是否为空 #检索rdd信息（count） rdd3.max() #99 rdd3.min() #0 rdd3.mean() #平均值49.5 rdd3.stdev() #标准值28.86607 rdd3.variance() #方差 rdd3.histogram(3) #柱状图 rdd3.stats() #(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0) #applying function rdd.map(lambda x : x+(x[1],x[0])).collect() rdd5 =rdd.flatMap(lambda x:x+(x[1],x[0])) rdd5.collect() rdd4.flatMapValues(lambda x : x).collect() #打平k 与每一个value #查询数据 #Getting rdd.collect() rdd.take(2) rdd.first() rdd.top(2) #Sampleing rdd3.sample(False,0.15,81).collect() #[3, 4, 27, 28, 35, 41, 43, 49, 53, 58, 85, 93] #Filtering rdd.filter(lambda x : &quot;a&quot; in x).collect() #[(&#39;a&#39;, 7), (&#39;a&#39;, 2)] rdd5.distinct().collect() #[&#39;a&#39;, 7, 2, &#39;b&#39;] rdd.keys().collect() #Iterating def g(x):print(x) rdd.foreach(g) #reshaping data #reducint rdd.reduceByKey(lambda x,y:x+y).collect() rdd.reduce(lambda a,b:a+b) #grouping by rdd3.groupBy(lambda x : x%2).mapValues(list).collect() rdd.groupByKey().mapValues(list).collect() #aggregating nums = sc.parallelize([1,2,3,4,5]) seqOp=(lambda x,y:(x[0]+y,x[1]+1)) #计算每个分区的总和与总个数 combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1])) #计算所有分区的数 nums.aggregate((0,0),seqOp,combOp) #总数等于15 一共5个数 x[0]表示元组的第一个数也就是总和 x[1]表示总个数 rdd.aggregateByKey((0,0),seqOp,combOp).collect() #[(&#39;a&#39;, (9, 2)), (&#39;b&#39;, (2, 1))] 统计所有的values遍历所有rdd的key def add(x,y): return x+y rdd3.fold(0,add) #聚合rdd的每个分区 第一个参数为初始值，op为方法 rdd3.keyBy(lambda x:x+x).collect() #通过f方法为rdd所有元素创建元组，返回值作为key #Mathematical Operations rdd.subtract(rdd2).collect() #rdd-rdd2相同的元组[(&#39;a&#39;, 7), (&#39;b&#39;, 2)] rdd2.subtractByKey(rdd).collect() #rdd2-rdd相同的key [(&#39;d&#39;, 1)] rdd.cartesian(rdd2).collect() #rdd1对rdd2笛卡尔积 #sort rdd2.sortByKey(lambda x:x[1]).collect() rdd2.sortByKey().collect() #按key去排序 #Repartitioning rdd.repartition(4)#reparation是coalesce(numPartitions, shuffle = true)，repartition不仅会调整Partition数，也会将Partitioner修改为hashPartitioner，产生shuffle操作。 rdd.coalesce(1)#coalesce函数可以控制是否shuffle，但当shuffle为false时，只能减小Partition数，无法增大 #Saving rdd.saveAsTextFile(&#39;rdd.txt&#39;) rdd.saveAsHadoopFile(&#39;hdfs://namenodehost/parent/child&#39;,&#39;org.apache.hadoop.mapred.TextOutputFormat&#39;) stopping SparkContext sc.stop()" />
<meta property="og:description" content="导包 import numpy as np import pandas as ps from pyspark import SparkContext,SparkConf %config ZMQInteractiveShell.ast_node_interactivity=&#39;all&#39; 初始化sparkcontent #方法一： sc = SparkContext(master=&#39;local[2]&#39;)#只能初始化一次 #方法二：通过设置的方式先设置sparkconf，然后在初始化sparkcontent conf = (SparkConf() .setMaster(&#39;local&#39;) .setAppName(&#39;My app&#39;) .set(&#39;spark.executor.memory&#39;,&#39;1g&#39;)) sc = SparkContext(conf=conf) spark参数信息 sc.version sc.pythonVer sc.master str(sc.sparkHome) str(sc.sparkUser()) sc.appName sc.applicationId sc.defaultParallelism sc.defaultMinPartitions sc.stop() Rdd初始化 #rdd 初始化 rdd = sc.parallelize([(&#39;a&#39;,7),(&#39;a&#39;,2),(&#39;b&#39;,2)]) rdd2 = sc.parallelize([(&#39;a&#39;,2),(&#39;d&#39;,1),(&#39;b&#39;,1)]) rdd3 = sc.parallelize(range(100)) rdd4 = sc.parallelize([ (&#39;a&#39;,[&#39;x&#39;,&#39;y&#39;,&#39;z&#39;]),(&#39;b&#39;,[&#39;p&#39;,&#39;r&#39; ]) ]) textFile = sc.textFile(&#39;/my/directory/*.txt&#39;) #遍历文件每一行 返回一个array textFile2 = sc.wholeTextFiles(&#39;/my/directory&#39;) #把这个目录下的所有文件遍历 返回的是 (文件路径,内容)的一个元组 Rdd函数 #检索rdd信息 rdd.getNumPartitions() #1 rdd.count() #3 rdd.countByKey() #defaultdict(int, {&#39;a&#39;: 2, &#39;b&#39;: 1}) rdd.collectAsMap() #{&#39;a&#39;: 2, &#39;b&#39;: 2} rdd3.sum() #4950 sc.parallelize([]).isEmpty()#判断这个rdd是否为空 #检索rdd信息（count） rdd3.max() #99 rdd3.min() #0 rdd3.mean() #平均值49.5 rdd3.stdev() #标准值28.86607 rdd3.variance() #方差 rdd3.histogram(3) #柱状图 rdd3.stats() #(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0) #applying function rdd.map(lambda x : x+(x[1],x[0])).collect() rdd5 =rdd.flatMap(lambda x:x+(x[1],x[0])) rdd5.collect() rdd4.flatMapValues(lambda x : x).collect() #打平k 与每一个value #查询数据 #Getting rdd.collect() rdd.take(2) rdd.first() rdd.top(2) #Sampleing rdd3.sample(False,0.15,81).collect() #[3, 4, 27, 28, 35, 41, 43, 49, 53, 58, 85, 93] #Filtering rdd.filter(lambda x : &quot;a&quot; in x).collect() #[(&#39;a&#39;, 7), (&#39;a&#39;, 2)] rdd5.distinct().collect() #[&#39;a&#39;, 7, 2, &#39;b&#39;] rdd.keys().collect() #Iterating def g(x):print(x) rdd.foreach(g) #reshaping data #reducint rdd.reduceByKey(lambda x,y:x+y).collect() rdd.reduce(lambda a,b:a+b) #grouping by rdd3.groupBy(lambda x : x%2).mapValues(list).collect() rdd.groupByKey().mapValues(list).collect() #aggregating nums = sc.parallelize([1,2,3,4,5]) seqOp=(lambda x,y:(x[0]+y,x[1]+1)) #计算每个分区的总和与总个数 combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1])) #计算所有分区的数 nums.aggregate((0,0),seqOp,combOp) #总数等于15 一共5个数 x[0]表示元组的第一个数也就是总和 x[1]表示总个数 rdd.aggregateByKey((0,0),seqOp,combOp).collect() #[(&#39;a&#39;, (9, 2)), (&#39;b&#39;, (2, 1))] 统计所有的values遍历所有rdd的key def add(x,y): return x+y rdd3.fold(0,add) #聚合rdd的每个分区 第一个参数为初始值，op为方法 rdd3.keyBy(lambda x:x+x).collect() #通过f方法为rdd所有元素创建元组，返回值作为key #Mathematical Operations rdd.subtract(rdd2).collect() #rdd-rdd2相同的元组[(&#39;a&#39;, 7), (&#39;b&#39;, 2)] rdd2.subtractByKey(rdd).collect() #rdd2-rdd相同的key [(&#39;d&#39;, 1)] rdd.cartesian(rdd2).collect() #rdd1对rdd2笛卡尔积 #sort rdd2.sortByKey(lambda x:x[1]).collect() rdd2.sortByKey().collect() #按key去排序 #Repartitioning rdd.repartition(4)#reparation是coalesce(numPartitions, shuffle = true)，repartition不仅会调整Partition数，也会将Partitioner修改为hashPartitioner，产生shuffle操作。 rdd.coalesce(1)#coalesce函数可以控制是否shuffle，但当shuffle为false时，只能减小Partition数，无法增大 #Saving rdd.saveAsTextFile(&#39;rdd.txt&#39;) rdd.saveAsHadoopFile(&#39;hdfs://namenodehost/parent/child&#39;,&#39;org.apache.hadoop.mapred.TextOutputFormat&#39;) stopping SparkContext sc.stop()" />
<link rel="canonical" href="https://uzzz.org/2019/06/03/795297.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/03/795297.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-03T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"导包 import numpy as np import pandas as ps from pyspark import SparkContext,SparkConf %config ZMQInteractiveShell.ast_node_interactivity=&#39;all&#39; 初始化sparkcontent #方法一： sc = SparkContext(master=&#39;local[2]&#39;)#只能初始化一次 #方法二：通过设置的方式先设置sparkconf，然后在初始化sparkcontent conf = (SparkConf() .setMaster(&#39;local&#39;) .setAppName(&#39;My app&#39;) .set(&#39;spark.executor.memory&#39;,&#39;1g&#39;)) sc = SparkContext(conf=conf) spark参数信息 sc.version sc.pythonVer sc.master str(sc.sparkHome) str(sc.sparkUser()) sc.appName sc.applicationId sc.defaultParallelism sc.defaultMinPartitions sc.stop() Rdd初始化 #rdd 初始化 rdd = sc.parallelize([(&#39;a&#39;,7),(&#39;a&#39;,2),(&#39;b&#39;,2)]) rdd2 = sc.parallelize([(&#39;a&#39;,2),(&#39;d&#39;,1),(&#39;b&#39;,1)]) rdd3 = sc.parallelize(range(100)) rdd4 = sc.parallelize([ (&#39;a&#39;,[&#39;x&#39;,&#39;y&#39;,&#39;z&#39;]),(&#39;b&#39;,[&#39;p&#39;,&#39;r&#39; ]) ]) textFile = sc.textFile(&#39;/my/directory/*.txt&#39;) #遍历文件每一行 返回一个array textFile2 = sc.wholeTextFiles(&#39;/my/directory&#39;) #把这个目录下的所有文件遍历 返回的是 (文件路径,内容)的一个元组 Rdd函数 #检索rdd信息 rdd.getNumPartitions() #1 rdd.count() #3 rdd.countByKey() #defaultdict(int, {&#39;a&#39;: 2, &#39;b&#39;: 1}) rdd.collectAsMap() #{&#39;a&#39;: 2, &#39;b&#39;: 2} rdd3.sum() #4950 sc.parallelize([]).isEmpty()#判断这个rdd是否为空 #检索rdd信息（count） rdd3.max() #99 rdd3.min() #0 rdd3.mean() #平均值49.5 rdd3.stdev() #标准值28.86607 rdd3.variance() #方差 rdd3.histogram(3) #柱状图 rdd3.stats() #(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0) #applying function rdd.map(lambda x : x+(x[1],x[0])).collect() rdd5 =rdd.flatMap(lambda x:x+(x[1],x[0])) rdd5.collect() rdd4.flatMapValues(lambda x : x).collect() #打平k 与每一个value #查询数据 #Getting rdd.collect() rdd.take(2) rdd.first() rdd.top(2) #Sampleing rdd3.sample(False,0.15,81).collect() #[3, 4, 27, 28, 35, 41, 43, 49, 53, 58, 85, 93] #Filtering rdd.filter(lambda x : &quot;a&quot; in x).collect() #[(&#39;a&#39;, 7), (&#39;a&#39;, 2)] rdd5.distinct().collect() #[&#39;a&#39;, 7, 2, &#39;b&#39;] rdd.keys().collect() #Iterating def g(x):print(x) rdd.foreach(g) #reshaping data #reducint rdd.reduceByKey(lambda x,y:x+y).collect() rdd.reduce(lambda a,b:a+b) #grouping by rdd3.groupBy(lambda x : x%2).mapValues(list).collect() rdd.groupByKey().mapValues(list).collect() #aggregating nums = sc.parallelize([1,2,3,4,5]) seqOp=(lambda x,y:(x[0]+y,x[1]+1)) #计算每个分区的总和与总个数 combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1])) #计算所有分区的数 nums.aggregate((0,0),seqOp,combOp) #总数等于15 一共5个数 x[0]表示元组的第一个数也就是总和 x[1]表示总个数 rdd.aggregateByKey((0,0),seqOp,combOp).collect() #[(&#39;a&#39;, (9, 2)), (&#39;b&#39;, (2, 1))] 统计所有的values遍历所有rdd的key def add(x,y): return x+y rdd3.fold(0,add) #聚合rdd的每个分区 第一个参数为初始值，op为方法 rdd3.keyBy(lambda x:x+x).collect() #通过f方法为rdd所有元素创建元组，返回值作为key #Mathematical Operations rdd.subtract(rdd2).collect() #rdd-rdd2相同的元组[(&#39;a&#39;, 7), (&#39;b&#39;, 2)] rdd2.subtractByKey(rdd).collect() #rdd2-rdd相同的key [(&#39;d&#39;, 1)] rdd.cartesian(rdd2).collect() #rdd1对rdd2笛卡尔积 #sort rdd2.sortByKey(lambda x:x[1]).collect() rdd2.sortByKey().collect() #按key去排序 #Repartitioning rdd.repartition(4)#reparation是coalesce(numPartitions, shuffle = true)，repartition不仅会调整Partition数，也会将Partitioner修改为hashPartitioner，产生shuffle操作。 rdd.coalesce(1)#coalesce函数可以控制是否shuffle，但当shuffle为false时，只能减小Partition数，无法增大 #Saving rdd.saveAsTextFile(&#39;rdd.txt&#39;) rdd.saveAsHadoopFile(&#39;hdfs://namenodehost/parent/child&#39;,&#39;org.apache.hadoop.mapred.TextOutputFormat&#39;) stopping SparkContext sc.stop()","@type":"BlogPosting","url":"https://uzzz.org/2019/06/03/795297.html","headline":"PySpark-RDD Basics","dateModified":"2019-06-03T00:00:00+08:00","datePublished":"2019-06-03T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/03/795297.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>PySpark-RDD Basics</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h3><a id="_0"></a>导包</h3> 
  <pre><code>import numpy as np
import pandas as ps
from pyspark import SparkContext,SparkConf
%config ZMQInteractiveShell.ast_node_interactivity='all'
</code></pre> 
  <h3><a id="sparkcontent_7"></a>初始化sparkcontent</h3> 
  <pre><code>#方法一：
sc = SparkContext(master='local[2]')#只能初始化一次
#方法二：通过设置的方式先设置sparkconf，然后在初始化sparkcontent
conf = (SparkConf()
        .setMaster('local')
        .setAppName('My app')
        .set('spark.executor.memory','1g'))
sc = SparkContext(conf=conf)
</code></pre> 
  <h3><a id="spark_18"></a>spark参数信息</h3> 
  <pre><code>sc.version
sc.pythonVer
sc.master
str(sc.sparkHome)
str(sc.sparkUser())
sc.appName
sc.applicationId
sc.defaultParallelism
sc.defaultMinPartitions
sc.stop()
</code></pre> 
  <h3><a id="Rdd_31"></a>Rdd初始化</h3> 
  <pre><code>#rdd 初始化
rdd = sc.parallelize([('a',7),('a',2),('b',2)])
rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])
rdd3 = sc.parallelize(range(100))
rdd4 = sc.parallelize([ ('a',['x','y','z']),('b',['p','r' ]) ])

textFile = sc.textFile('/my/directory/*.txt') #遍历文件每一行 返回一个array
textFile2 = sc.wholeTextFiles('/my/directory') #把这个目录下的所有文件遍历 返回的是 (文件路径,内容)的一个元组
</code></pre> 
  <h3><a id="Rdd_42"></a>Rdd函数</h3> 
  <pre><code>#检索rdd信息
rdd.getNumPartitions() #1
rdd.count() #3
rdd.countByKey() #defaultdict(int, {'a': 2, 'b': 1})
rdd.collectAsMap() #{'a': 2, 'b': 2}
rdd3.sum() #4950
sc.parallelize([]).isEmpty()#判断这个rdd是否为空

#检索rdd信息（count）
rdd3.max() #99
rdd3.min() #0
rdd3.mean() #平均值49.5
rdd3.stdev() #标准值28.86607
rdd3.variance() #方差
rdd3.histogram(3) #柱状图
rdd3.stats() #(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0)

#applying function
rdd.map(lambda x : x+(x[1],x[0])).collect()
rdd5 =rdd.flatMap(lambda x:x+(x[1],x[0]))
rdd5.collect()
rdd4.flatMapValues(lambda x : x).collect() #打平k 与每一个value

#查询数据
#Getting
rdd.collect()
rdd.take(2)
rdd.first()
rdd.top(2)

#Sampleing
rdd3.sample(False,0.15,81).collect() #[3, 4, 27, 28, 35, 41, 43, 49, 53, 58, 85, 93]

#Filtering
rdd.filter(lambda x : "a" in x).collect() #[('a', 7), ('a', 2)]
rdd5.distinct().collect() #['a', 7, 2, 'b']
rdd.keys().collect()

#Iterating
def g(x):print(x)
rdd.foreach(g)

#reshaping data
#reducint
rdd.reduceByKey(lambda x,y:x+y).collect()
rdd.reduce(lambda a,b:a+b)

#grouping by
rdd3.groupBy(lambda x : x%2).mapValues(list).collect()
rdd.groupByKey().mapValues(list).collect()

#aggregating
nums = sc.parallelize([1,2,3,4,5])
seqOp=(lambda x,y:(x[0]+y,x[1]+1)) #计算每个分区的总和与总个数
combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1])) #计算所有分区的数
nums.aggregate((0,0),seqOp,combOp) #总数等于15  一共5个数 x[0]表示元组的第一个数也就是总和 x[1]表示总个数

rdd.aggregateByKey((0,0),seqOp,combOp).collect() #[('a', (9, 2)), ('b', (2, 1))] 统计所有的values遍历所有rdd的key

def add(x,y): return x+y
rdd3.fold(0,add) #聚合rdd的每个分区 第一个参数为初始值，op为方法

rdd3.keyBy(lambda x:x+x).collect() #通过f方法为rdd所有元素创建元组，返回值作为key

#Mathematical Operations
rdd.subtract(rdd2).collect() #rdd-rdd2相同的元组[('a', 7), ('b', 2)]
rdd2.subtractByKey(rdd).collect() #rdd2-rdd相同的key [('d', 1)]
rdd.cartesian(rdd2).collect() #rdd1对rdd2笛卡尔积

#sort
rdd2.sortByKey(lambda x:x[1]).collect()
rdd2.sortByKey().collect()  #按key去排序

#Repartitioning
rdd.repartition(4)#reparation是coalesce(numPartitions, shuffle = true)，repartition不仅会调整Partition数，也会将Partitioner修改为hashPartitioner，产生shuffle操作。
rdd.coalesce(1)#coalesce函数可以控制是否shuffle，但当shuffle为false时，只能减小Partition数，无法增大

#Saving
rdd.saveAsTextFile('rdd.txt')
rdd.saveAsHadoopFile('hdfs://namenodehost/parent/child','org.apache.hadoop.mapred.TextOutputFormat')

</code></pre> 
  <h3><a id="stopping_SparkContext_126"></a>stopping SparkContext</h3> 
  <p>sc.stop()</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
