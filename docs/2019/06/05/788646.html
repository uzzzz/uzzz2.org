<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>原理篇 推荐系统之矩阵分解模型 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="原理篇 推荐系统之矩阵分解模型" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="导语：本系列文章一共有三篇，分别是 《科普篇 |&nbsp;推荐系统之矩阵分解模型》 《原理篇 |&nbsp;推荐系统之矩阵分解模型》 《实践篇 |&nbsp;推荐系统之矩阵分解模型》 第一篇用一个具体的例子介绍了MF是如何做推荐的。第二篇讲的是MF的数学原理，包括MF模型的目标函数和求解公式的推导等。第三篇回归现实，讲述MF算法在图文推荐中的应用实践（将于后续发布）。下文是第二篇——《原理篇 | 推荐系统之矩阵分解模型》，敬请阅读。 上一篇我们用一个简单的例子讲述了矩阵分解(Matrix Factorization, MF)是如何做推荐的，但没有深入到算法的细节。如果想编写自己的代码实现MF，那么就需要了解其中的细节了。本文是MF系列的第二篇文章，主要介绍了显式矩阵分解和隐式矩阵分解的数学原理，包括模型思想、目标函数、优化求解的公式推导等，旨在为需要了解算法细节的同学提供参考。 1.显式数据和隐式数据 MF用到的用户行为数据分为显式数据和隐式数据两种。显式数据是指用户对item的显式打分，比如用户对电影、商品的评分，通常有5分制和10分制。隐式数据是指用户对item的浏览、点击、购买、收藏、点赞、评论、分享等数据，其特点是用户没有显式地给item打分，用户对item的感兴趣程度都体现在他对item的浏览、点击、购买、收藏、点赞、评论、分享等行为的强度上。 显式数据的优点是行为的置信度高，因为是用户明确给出的打分，所以真实反映了用户对item的喜欢程度。缺点是这种数据的量太小，因为绝大部分用户都不会去给item评分，这就导致数据非常稀疏，同时这部分评分也仅代表了小部分用户的兴趣，可能会导致数据有偏。隐式数据的优点是容易获取，数据量很大。因为几乎所有用户都会有浏览、点击等行为，所以数据量大，几乎覆盖所有用户，不会导致数据有偏。其缺点是置信度不如显式数据的高，比如浏览不一定代表感兴趣，还要看强度，经常浏览同一类东西才能以较高置信度认为用户感兴趣。 根据所使用的数据是显式数据还是隐式数据，矩阵分解算法又分为两种[4,6]。使用显式数据的矩阵分解算法称为显式矩阵分解算法，使用隐式数据的矩阵分解算法称为隐式矩阵分解算法。由于矩阵分解算法有众多的改进版本和各种变体[4,5,6,7,8,9,10,11]，本文不打算一一列举，因此下文将以实践中用得最多的矩阵分解算法为例，介绍其具体的数据原理，这也是spark机器学习库mllib中实现的矩阵分解算法[4,6]。从实际应用的效果来看，隐式矩阵分解的效果一般会更好。 2.显式矩阵分解 在本系列第一篇文章中，我们提到，矩阵分解算法的输入是user对item的评分矩阵(图1等号左边的矩阵)，输出是User矩阵和Item矩阵(图1等号右边的矩阵)，其中User矩阵的每一行代表一个用户向量，Item矩阵的每一列代表一个item的向量。User对item的预测评分用它们的向量内积来表示，通过最小化预测评分和实际评分的差异来学习User矩阵和Item矩阵。 图1 2.1 目标函数 为了用数学的语言定量表示上述思想，我们先引入一些符号。设rui&nbsp;表示用户u&nbsp;对item i&nbsp;的显式评分，当rui&nbsp;&gt;0时，表示用户u&nbsp;对item i&nbsp;有评分，当rui&nbsp;=0时，表示用户u&nbsp;对item i&nbsp;没有评分，xu&nbsp;表示用户u&nbsp;的向量，yi&nbsp;表示item i&nbsp;的向量，则显式矩阵分解的目标函数为： 其中xu&nbsp;和yi&nbsp;都是k&nbsp;维的列向量，k&nbsp;为隐变量的个数， 是所有xu&nbsp;构成的矩阵， 为所有yi&nbsp;构成的矩阵，N&nbsp;为用户数，M&nbsp;为item数，λ为正则化参数。 在上述公式中， 为用户向量与物品向量的内积，表示用户u&nbsp;对物品i&nbsp;的预测评分，目标函数通过最小化预测评分和实际评分rui&nbsp;之间的残差平方和，来学习所有用户向量和物品向量。这里的残差项只包含了有评分的数据，不包括没有评分的数据。目标函数中第二项是L2正则项，用于保证数值计算稳定性和防止过拟合。 2.2 求解方法： 求解X&nbsp;和Y&nbsp;采用的是交替最小二乘法(alternative least square, ALS)，也就是先固定X&nbsp;优化Y&nbsp;，然后固定Y&nbsp;优化X&nbsp;，这个过程不断重复，直到X&nbsp;和Y&nbsp;收敛为止。每次固定其中一个优化另一个都需要解一个最小二乘问题，所以这个算法叫做交替最小二乘方法。 (1)Y&nbsp;固定为上一步迭代值或初始化值，优化X&nbsp;： 此时，Y&nbsp;被当做常数处理，目标函数被分解为多个独立的子目标函数，每个子目标函数对应一个用户。对于用户u&nbsp;，目标函数为： 这里面残差项求和的个数等于用于u&nbsp;评过分的物品的个数，记为m&nbsp;个。把这个目标函数化为矩阵形式，得&nbsp; 其中， 表示用户u&nbsp;对这m&nbsp;个物品的评分构成的向量， 表示这m&nbsp;个物品的向量构成的矩阵，顺序跟Ru&nbsp;中物品的顺序一致。 对目标函数J关于xu&nbsp;求梯度，并令梯度为零，得： 解这个线性方程组，可得到xu&nbsp;的解析解为： (2) X&nbsp;固定为上一步迭代值或初始化值，优化Y： 此时，X&nbsp;被当做常数处理，目标函数也被分解为多个独立的子目标函数，每个子目标函数对应一个物品。类似上面的推导，我们可以得到yi&nbsp;的解析解为： 其中， 表示n&nbsp;个用户对物品i&nbsp;的评分构成的向量， 表示这n&nbsp;个用户的向量构成的矩阵，顺序跟Ri&nbsp;中用户的顺序一致。 2.3 工程实现 当固定Y&nbsp;时，各个xu&nbsp;的计算是独立的，因此可以对xu&nbsp;进行分布式并行计算。同理，当固定X 时，各个yi&nbsp;的计算也是独立的，因此也可以对yi&nbsp;做分布式并行计算。因为Xi&nbsp;和Yu&nbsp;中只包含了有评分的用户或物品，而非全部用户或物品，因此xu&nbsp;和yi&nbsp;的计算时间复杂度为O(k2nu+k3)其中nu&nbsp;是有评分的用户数或物品数，k&nbsp;为隐变量个数。 3.隐式矩阵分解 隐式矩阵分解与显式矩阵分解的一个比较大的区别，就是它会去拟合评分矩阵中的零，即没有评分的地方也要拟合。 3.1 目标函数 我们仍然用rui&nbsp;表示用户u&nbsp;对物品i&nbsp;的评分，但这里的评分表示的是行为的强度，比如浏览次数、阅读时长、播放完整度等。当rui&nbsp;&gt;0时，表示用户u&nbsp;对物品i有过行为，当rui&nbsp;=0时，表示用户u&nbsp;对物品i没有过行为。首先，我们定义一个二值变量pui&nbsp;如下： 这个pui&nbsp;是一个依赖于rui&nbsp;的量，用于表示用户u&nbsp;对物品i&nbsp;是否感兴趣，也称为用户偏好。当用户u&nbsp;对物品i&nbsp;有过行为时，我们认为用户u&nbsp;对物品i感兴趣，此时pui&nbsp;=1；当用户u&nbsp;对物品i&nbsp;没有过行为时，我们认为用户u&nbsp;对物品i&nbsp;不感兴趣，此时pui&nbsp;=0。 模型除了要刻画用户对物品是否感兴趣外，而且还要刻画感兴趣或不感兴趣的程度，所以这里的隐式矩阵分解还引入了置信度的概念。从直观上来说，当rui&nbsp;&gt;0时，rui&nbsp;越大，我们越确信用户u&nbsp;喜欢物品i&nbsp;，而当rui&nbsp;=0时，我们不能确定用户u&nbsp;是否喜欢物品i&nbsp;，没有行为可能只是因为用户u&nbsp;并不知道物品i&nbsp;的存在。 因此，置信度是rui&nbsp;的函数，并且当rui&nbsp;&gt;0时，置信度是rui&nbsp;的增函数；当rui&nbsp;=0时，置信度取值要小。论文中给出的置信度cui&nbsp;的表达式为： 当rui&nbsp;&gt;0时，cui&nbsp;关于rui&nbsp;线性递增，表示对于有评分的物品，行为强度越大，我们越相信用户u&nbsp;对物品i&nbsp;感兴趣；当rui&nbsp;=0时，置信度恒等于1，表示对所有没有评分的物品，用户不感兴趣的置信度都一样，并且比有评分物品的置信度低。用xu&nbsp;表示用户u&nbsp;的向量，yi&nbsp;表示item i&nbsp;的向量，引入置信度以后，隐式矩阵分解[6]的目标函数为： 其中xu&nbsp;和yi&nbsp;都是k&nbsp;维的列向量，k&nbsp;为隐变量的个数， 是所有xu&nbsp;构成的矩阵， 为所有yi&nbsp;构成的矩阵，N&nbsp;为用户数，M&nbsp;为item数，λ为正则化参数。目标函数里的内积用于表示用户对物品的预测偏好，拟合实际偏好pui，拟合强度由cui&nbsp;&nbsp;控制。并且对于pui&nbsp;=0的项也要拟合。目标函数中的第二项是正则项，用于保证数值计算稳定性以及防止过拟合。 3.2 求解方法 目标函数的求解仍然可以采用交替最小二乘法。具体如下： (1)Y&nbsp;固定为上一步迭代值或初始化值，优化X&nbsp;： 此时，Y&nbsp;被当做常数处理，目标函数被分解为多个独立的子目标函数，每个子目标函数都是某个xu&nbsp;的函数。对于用户u&nbsp;，目标函数为： 把这个目标函数化为矩阵形式，得 其中， 为用户u&nbsp;对每个物品的偏好构成的列向量， 表示所有物品向量构成的矩阵，Λu&nbsp;为用户u&nbsp;对所有物品的置信度cui&nbsp;构成的对角阵，即： 对目标函数J&nbsp;关于xu&nbsp;求梯度，并令梯度为零，得： 解这个线性方程组，可得到xu&nbsp;的解析解为： (2) X&nbsp;固定为上一步迭代值或初始化值，优化Y： 此时，X&nbsp;被当做常数处理，目标函数也被分解为多个独立的子目标函数，每个子目标函数都是关于某个yi&nbsp;的函数。通过同样的推导方法，可以得到yi&nbsp;的解析解为： 其中， 为所有用户对物品i&nbsp;的偏好构成的向量， 表示所有用户的向量构成的矩阵，Λi&nbsp;为所有用户对物品i&nbsp;的偏好的置信度构成的对角矩阵，即 3.3 工程实现 由于固定Y&nbsp;时，各个xu&nbsp;的求解都是独立的，所以在固定Y&nbsp;时可以并行计算各个xu，同理，在固定X时可以并行计算各个yi&nbsp;。 在计算xu&nbsp;和yi&nbsp;时，如果直接用上述解析解的表达式来计算，复杂度将会很高。以xu&nbsp;的表达式来说，Y&nbsp;Λu&nbsp;YT&nbsp;这一项就涉及到所有物品的向量，少则几十万，大则上千万，而且每个用户的都不一样，每个用户都算一遍时间上不可行。所以，这里要先对xu&nbsp;的表达式化简，降低复杂度。 注意到Λi&nbsp;的特殊性，它是由置信度构成的对角阵，对于一个用户来说，由于大部分物品都没有评分，以此Λi&nbsp;对角线中大部分元素都是1，利用这个特点，我们可以把Λi&nbsp;拆成两部分的和，即 其中I为单位阵，Λu&nbsp;- I&nbsp;为对角阵，并且对角线上大部分元素为0，于是，可以重写为如下形式： 分解成这两项之后，第一项Y YT&nbsp;对每个用户都是一样的，只需要计算一次，存起来，后面可以重复利用，对于第二项，由于Λu&nbsp;- I&nbsp;为对角线大部分是0的对角阵，所以计算Y&nbsp;(Λu&nbsp;- I&nbsp;)YT&nbsp;的复杂度是O(k2nu)。其中nu&nbsp;是Λu&nbsp;- I&nbsp;中非零元的个数，也就是用户u&nbsp;评过分的物品数，通常不会很多，所以整个Y&nbsp;ΛuYT的计算复杂度由O(k2M)&nbsp;降为O(k2nu)。由于M&gt;&gt;nu，所以计算速度大大加快。对于xu&nbsp;表达式的Y&nbsp;Λu&nbsp;Pu这一项，则应Y&nbsp;(&nbsp;Λu&nbsp;Pu)&nbsp;这样计算，利用Pu&nbsp;中大部分元素是0的特点，将计算复杂度由O(kM&nbsp;)&nbsp;降低到O(knu)。通过使用上述数学技巧，整个xu的计算复杂度可以降低到O(k2nu+k3)，其中nu是有评分的用户数或物品数，k&nbsp;为隐变量个数，完全满足在线计算的需求。 4.增量矩阵分解算法 无论是显式矩阵分解，还是隐式矩阵分解，我们在模型训练完以后，就会得到训练集里每个用户的向量和每个物品的向量。假设现在有一个用户，在训练集里没出现过，但是我们有他的历史行为数据，那这个用户的向量该怎么计算呢？当然，最简单的方法就是把这个用户的行为数据合并到旧的训练集里，重新做一次矩阵分解，进而得到这个用户的向量，但是这样做计算代价太大了，在时间上不可行。 为了解决训练数据集以外的用户(我们称之为新用户)的推荐问题，我们就需要用到增量矩阵分解算法。增量矩阵分解算法能根据用户历史行为数据，在不重算所有用户向量的前提下，快速计算出新用户向量。 在交替最小二乘法里，当固定Y&nbsp;计算xu&nbsp;时，我们只需要用到用户u&nbsp;的历史行为数据rui&nbsp;以及Y&nbsp;的当前值，不同用户之间xu的计算是相互独立的。这就启发我们，对于训练集以外的用户，我们同样可以用他的历史行为数据以及训练集上收敛时学到的Y，来计算新用户的用户向量。下面的图2表示了这一过程。 增量矩阵分解 设用户历史行为数据为Pu={Pui&nbsp;}，训练集上学到的物品矩阵为Y，要求解的用户向量为xu，则增量矩阵分解算法求解的目标为： 这个目标函数跟第3节中固定Y&nbsp;时求解xu&nbsp;的目标函数是一样的，但有两个不同点： (1)这里的Y&nbsp;是不需要迭代的，它是MF在训练集上收敛时得到的Y； (2)用户的历史行为数据Pu&nbsp;要过滤掉在Y中没出现过的物品。由于Y&nbsp;是固定的，我们不需要迭代，直接通过xu&nbsp;的解析表达式求解xu，即： 式中的所有符号和上一节相同。 事实上，增量矩阵分解的目标函数中的Y&nbsp;也不一定要是MF在训练集上学出来的，只要Y&nbsp;中的每个向量都能表示对应物品的特征就行，也就是说，Y&nbsp;可以是由其他数据和其他算法事先学出来的。矩阵分解的增量算法在图文推荐系统中有着广泛应用，具体的应用将在下一篇文章中介绍。 5.推荐结果的可解释性 好的推荐算法不仅要推得准确，而且还要有良好的可解释性，也就是根据什么给用户推荐了这个物品。传统的ItemCF算法就有很好的可解释性，因为在ItemCF中，用户u&nbsp;对物品i&nbsp;的预测评分R&nbsp;(u, i&nbsp;)&nbsp;的计算公式为 其中N(u&nbsp;)&nbsp;表示用户u&nbsp;有过行为的物品集合，ruj&nbsp;表示用户u&nbsp;对物品j&nbsp;的历史评分，sji&nbsp;表示物品j&nbsp;和物品i&nbsp;的相似度。在这个公式中，N(u&nbsp;)&nbsp;中的物品j&nbsp;对R(u, i )&nbsp;的贡献为ruj&nbsp;sji，因此可以很好地解释物品i&nbsp;具体是由N(u)&nbsp;中哪个物品推荐而来。那对于矩阵分解算法来说，是否也能给出类似的可解释性呢？答案是肯定的。 以隐式矩阵分解为例，我们已经推导出，已知物品的矩阵Y&nbsp;时，用户u&nbsp;的向量的计算表达式为： 假设物品i&nbsp;的向量为yi，那么用户u&nbsp;对物品i&nbsp;的预测评分为： 令 并把Y&nbsp;Λu&nbsp;Pu&nbsp;展开来写，则的表达式可以写成 其中， 可以看成是物品j&nbsp;和物品i&nbsp;之间的相似度， 可以看成是用户u&nbsp;对用户j的评分，这样就能像ItemCF那样去解释N(u&nbsp;)中每一项对推荐物品i&nbsp;的贡献了。从sji&nbsp;的计算表达式中，我们还可以看到，物品j&nbsp;和物品i&nbsp;之间的相似度sji&nbsp;是跟用户u&nbsp;有关系的，也就是说，即使是相同的两个物品，在不同用户看来，它们的相似度是不一样的，这跟ItemCF的固定相似度有着本质上的区别，MF的相似度看起来更合理一些。 6.小结 (1)根据用户行为数据的特点，矩阵分解又分为显式矩阵分解和隐式矩阵分解两种； (2)在显式MF算法中，用户向量和物品向量的内积拟合的是用户对物品的实际评分，并且只拟合有评分的项； (3)在隐式MF算法中，用户向量和物品向量的内积拟合的是用户对物品的偏好(0或1)，拟合的强度由置信度控制，置信度又由行为的强度决定； (4)在隐式MF中，需要使用一些数学技巧降低计算复杂度，才能满足线上实时计算的性能要求； (5)对于有行为数据，但不在训练集里的用户，可以使用增量MF算法计算出他的用户向量，进而为他做推荐； (6)MF算法也能像ItemCF一样，能给出推荐的理由，具有良好的可解释性。 参考文献 [1] 项亮. 推荐系统实践. 北京: 人民邮电出版社. 2012. [2] Sarwar B M, Karypis G, Konstan J A, et al. Item-based collaborative filtering recommendation algorithms. WWW. 2001, 1: 285-295. [3] Linden G, Smith B, York J. Amazon. com recommendations: Item-to-item collaborative filtering. IEEE Internet computing. 2003 (1): 76-80. [4] Koren Y, Bell R, Volinsky C. Matrix factorization techniques for recommender systems. Computer. 2009 (8): 30-37. [5] Koren Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model. SIGKDD. 2008: 426-434. [6] Hu Y, Koren Y, Volinsky C. Collaborative filtering for implicit feedback datasets. ICDM. 2008: 263-272. [7] Mnih A, Salakhutdinov R R. Probabilistic matrix factorization. NIPS. 2008: 1257-1264. [8] Koren Y. Collaborative filtering with temporal dynamics. SIGKDD. 2009: 447-456. [9] Pilászy I, Zibriczky D, Tikk D. Fast als-based matrix factorization for explicit and implicit feedback datasets. RecSys. 2010: 71-78. [10] Johnson C C. Logistic matrix factorization for implicit feedback data. NIPS, 2014, 27. [11] He X, Zhang H, Kan M Y, et al. Fast matrix factorization for online recommendation with implicit feedback. SIGIR. 2016: 549-558. [12] Hofmann T. Probabilistic latent semantic analysis. UAI. 1999: 289-296. [13] Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation. JMLR. 2003, 3(Jan): 993-1022. [14] Zhang S, Yao L, Sun A, et al. Deep learning based recommender system: A survey and new perspectives. ACM Computing Surveys (CSUR). 2019, 52(1): 5. [15] Mikolov, Tomas &amp; Chen, Kai &amp; Corrado, G.s &amp; Dean, Jeffrey. Efficient Estimation of Word Representations in Vector Space. Proceedings of Workshop at ICLR. 2013. [16] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality. NIPS. 2013: 3111-3119. [17] Pennington J, Socher R, Manning C. Glove: Global vectors for word representation. EMNLP. 2014: 1532-1543. [18] Covington P, Adams J, Sargin E. Deep neural networks for youtube recommendations. RecSys. 2016: 191-198. [19] Chen T, Guestrin C. Xgboost: A scalable tree boosting system. SIGKDD. 2016: 785-794. [20] Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree. NIPS. 2017: 3146-3154. 更多精彩阅读： 科普篇 | 推荐系统之矩阵分解模型" />
<meta property="og:description" content="导语：本系列文章一共有三篇，分别是 《科普篇 |&nbsp;推荐系统之矩阵分解模型》 《原理篇 |&nbsp;推荐系统之矩阵分解模型》 《实践篇 |&nbsp;推荐系统之矩阵分解模型》 第一篇用一个具体的例子介绍了MF是如何做推荐的。第二篇讲的是MF的数学原理，包括MF模型的目标函数和求解公式的推导等。第三篇回归现实，讲述MF算法在图文推荐中的应用实践（将于后续发布）。下文是第二篇——《原理篇 | 推荐系统之矩阵分解模型》，敬请阅读。 上一篇我们用一个简单的例子讲述了矩阵分解(Matrix Factorization, MF)是如何做推荐的，但没有深入到算法的细节。如果想编写自己的代码实现MF，那么就需要了解其中的细节了。本文是MF系列的第二篇文章，主要介绍了显式矩阵分解和隐式矩阵分解的数学原理，包括模型思想、目标函数、优化求解的公式推导等，旨在为需要了解算法细节的同学提供参考。 1.显式数据和隐式数据 MF用到的用户行为数据分为显式数据和隐式数据两种。显式数据是指用户对item的显式打分，比如用户对电影、商品的评分，通常有5分制和10分制。隐式数据是指用户对item的浏览、点击、购买、收藏、点赞、评论、分享等数据，其特点是用户没有显式地给item打分，用户对item的感兴趣程度都体现在他对item的浏览、点击、购买、收藏、点赞、评论、分享等行为的强度上。 显式数据的优点是行为的置信度高，因为是用户明确给出的打分，所以真实反映了用户对item的喜欢程度。缺点是这种数据的量太小，因为绝大部分用户都不会去给item评分，这就导致数据非常稀疏，同时这部分评分也仅代表了小部分用户的兴趣，可能会导致数据有偏。隐式数据的优点是容易获取，数据量很大。因为几乎所有用户都会有浏览、点击等行为，所以数据量大，几乎覆盖所有用户，不会导致数据有偏。其缺点是置信度不如显式数据的高，比如浏览不一定代表感兴趣，还要看强度，经常浏览同一类东西才能以较高置信度认为用户感兴趣。 根据所使用的数据是显式数据还是隐式数据，矩阵分解算法又分为两种[4,6]。使用显式数据的矩阵分解算法称为显式矩阵分解算法，使用隐式数据的矩阵分解算法称为隐式矩阵分解算法。由于矩阵分解算法有众多的改进版本和各种变体[4,5,6,7,8,9,10,11]，本文不打算一一列举，因此下文将以实践中用得最多的矩阵分解算法为例，介绍其具体的数据原理，这也是spark机器学习库mllib中实现的矩阵分解算法[4,6]。从实际应用的效果来看，隐式矩阵分解的效果一般会更好。 2.显式矩阵分解 在本系列第一篇文章中，我们提到，矩阵分解算法的输入是user对item的评分矩阵(图1等号左边的矩阵)，输出是User矩阵和Item矩阵(图1等号右边的矩阵)，其中User矩阵的每一行代表一个用户向量，Item矩阵的每一列代表一个item的向量。User对item的预测评分用它们的向量内积来表示，通过最小化预测评分和实际评分的差异来学习User矩阵和Item矩阵。 图1 2.1 目标函数 为了用数学的语言定量表示上述思想，我们先引入一些符号。设rui&nbsp;表示用户u&nbsp;对item i&nbsp;的显式评分，当rui&nbsp;&gt;0时，表示用户u&nbsp;对item i&nbsp;有评分，当rui&nbsp;=0时，表示用户u&nbsp;对item i&nbsp;没有评分，xu&nbsp;表示用户u&nbsp;的向量，yi&nbsp;表示item i&nbsp;的向量，则显式矩阵分解的目标函数为： 其中xu&nbsp;和yi&nbsp;都是k&nbsp;维的列向量，k&nbsp;为隐变量的个数， 是所有xu&nbsp;构成的矩阵， 为所有yi&nbsp;构成的矩阵，N&nbsp;为用户数，M&nbsp;为item数，λ为正则化参数。 在上述公式中， 为用户向量与物品向量的内积，表示用户u&nbsp;对物品i&nbsp;的预测评分，目标函数通过最小化预测评分和实际评分rui&nbsp;之间的残差平方和，来学习所有用户向量和物品向量。这里的残差项只包含了有评分的数据，不包括没有评分的数据。目标函数中第二项是L2正则项，用于保证数值计算稳定性和防止过拟合。 2.2 求解方法： 求解X&nbsp;和Y&nbsp;采用的是交替最小二乘法(alternative least square, ALS)，也就是先固定X&nbsp;优化Y&nbsp;，然后固定Y&nbsp;优化X&nbsp;，这个过程不断重复，直到X&nbsp;和Y&nbsp;收敛为止。每次固定其中一个优化另一个都需要解一个最小二乘问题，所以这个算法叫做交替最小二乘方法。 (1)Y&nbsp;固定为上一步迭代值或初始化值，优化X&nbsp;： 此时，Y&nbsp;被当做常数处理，目标函数被分解为多个独立的子目标函数，每个子目标函数对应一个用户。对于用户u&nbsp;，目标函数为： 这里面残差项求和的个数等于用于u&nbsp;评过分的物品的个数，记为m&nbsp;个。把这个目标函数化为矩阵形式，得&nbsp; 其中， 表示用户u&nbsp;对这m&nbsp;个物品的评分构成的向量， 表示这m&nbsp;个物品的向量构成的矩阵，顺序跟Ru&nbsp;中物品的顺序一致。 对目标函数J关于xu&nbsp;求梯度，并令梯度为零，得： 解这个线性方程组，可得到xu&nbsp;的解析解为： (2) X&nbsp;固定为上一步迭代值或初始化值，优化Y： 此时，X&nbsp;被当做常数处理，目标函数也被分解为多个独立的子目标函数，每个子目标函数对应一个物品。类似上面的推导，我们可以得到yi&nbsp;的解析解为： 其中， 表示n&nbsp;个用户对物品i&nbsp;的评分构成的向量， 表示这n&nbsp;个用户的向量构成的矩阵，顺序跟Ri&nbsp;中用户的顺序一致。 2.3 工程实现 当固定Y&nbsp;时，各个xu&nbsp;的计算是独立的，因此可以对xu&nbsp;进行分布式并行计算。同理，当固定X 时，各个yi&nbsp;的计算也是独立的，因此也可以对yi&nbsp;做分布式并行计算。因为Xi&nbsp;和Yu&nbsp;中只包含了有评分的用户或物品，而非全部用户或物品，因此xu&nbsp;和yi&nbsp;的计算时间复杂度为O(k2nu+k3)其中nu&nbsp;是有评分的用户数或物品数，k&nbsp;为隐变量个数。 3.隐式矩阵分解 隐式矩阵分解与显式矩阵分解的一个比较大的区别，就是它会去拟合评分矩阵中的零，即没有评分的地方也要拟合。 3.1 目标函数 我们仍然用rui&nbsp;表示用户u&nbsp;对物品i&nbsp;的评分，但这里的评分表示的是行为的强度，比如浏览次数、阅读时长、播放完整度等。当rui&nbsp;&gt;0时，表示用户u&nbsp;对物品i有过行为，当rui&nbsp;=0时，表示用户u&nbsp;对物品i没有过行为。首先，我们定义一个二值变量pui&nbsp;如下： 这个pui&nbsp;是一个依赖于rui&nbsp;的量，用于表示用户u&nbsp;对物品i&nbsp;是否感兴趣，也称为用户偏好。当用户u&nbsp;对物品i&nbsp;有过行为时，我们认为用户u&nbsp;对物品i感兴趣，此时pui&nbsp;=1；当用户u&nbsp;对物品i&nbsp;没有过行为时，我们认为用户u&nbsp;对物品i&nbsp;不感兴趣，此时pui&nbsp;=0。 模型除了要刻画用户对物品是否感兴趣外，而且还要刻画感兴趣或不感兴趣的程度，所以这里的隐式矩阵分解还引入了置信度的概念。从直观上来说，当rui&nbsp;&gt;0时，rui&nbsp;越大，我们越确信用户u&nbsp;喜欢物品i&nbsp;，而当rui&nbsp;=0时，我们不能确定用户u&nbsp;是否喜欢物品i&nbsp;，没有行为可能只是因为用户u&nbsp;并不知道物品i&nbsp;的存在。 因此，置信度是rui&nbsp;的函数，并且当rui&nbsp;&gt;0时，置信度是rui&nbsp;的增函数；当rui&nbsp;=0时，置信度取值要小。论文中给出的置信度cui&nbsp;的表达式为： 当rui&nbsp;&gt;0时，cui&nbsp;关于rui&nbsp;线性递增，表示对于有评分的物品，行为强度越大，我们越相信用户u&nbsp;对物品i&nbsp;感兴趣；当rui&nbsp;=0时，置信度恒等于1，表示对所有没有评分的物品，用户不感兴趣的置信度都一样，并且比有评分物品的置信度低。用xu&nbsp;表示用户u&nbsp;的向量，yi&nbsp;表示item i&nbsp;的向量，引入置信度以后，隐式矩阵分解[6]的目标函数为： 其中xu&nbsp;和yi&nbsp;都是k&nbsp;维的列向量，k&nbsp;为隐变量的个数， 是所有xu&nbsp;构成的矩阵， 为所有yi&nbsp;构成的矩阵，N&nbsp;为用户数，M&nbsp;为item数，λ为正则化参数。目标函数里的内积用于表示用户对物品的预测偏好，拟合实际偏好pui，拟合强度由cui&nbsp;&nbsp;控制。并且对于pui&nbsp;=0的项也要拟合。目标函数中的第二项是正则项，用于保证数值计算稳定性以及防止过拟合。 3.2 求解方法 目标函数的求解仍然可以采用交替最小二乘法。具体如下： (1)Y&nbsp;固定为上一步迭代值或初始化值，优化X&nbsp;： 此时，Y&nbsp;被当做常数处理，目标函数被分解为多个独立的子目标函数，每个子目标函数都是某个xu&nbsp;的函数。对于用户u&nbsp;，目标函数为： 把这个目标函数化为矩阵形式，得 其中， 为用户u&nbsp;对每个物品的偏好构成的列向量， 表示所有物品向量构成的矩阵，Λu&nbsp;为用户u&nbsp;对所有物品的置信度cui&nbsp;构成的对角阵，即： 对目标函数J&nbsp;关于xu&nbsp;求梯度，并令梯度为零，得： 解这个线性方程组，可得到xu&nbsp;的解析解为： (2) X&nbsp;固定为上一步迭代值或初始化值，优化Y： 此时，X&nbsp;被当做常数处理，目标函数也被分解为多个独立的子目标函数，每个子目标函数都是关于某个yi&nbsp;的函数。通过同样的推导方法，可以得到yi&nbsp;的解析解为： 其中， 为所有用户对物品i&nbsp;的偏好构成的向量， 表示所有用户的向量构成的矩阵，Λi&nbsp;为所有用户对物品i&nbsp;的偏好的置信度构成的对角矩阵，即 3.3 工程实现 由于固定Y&nbsp;时，各个xu&nbsp;的求解都是独立的，所以在固定Y&nbsp;时可以并行计算各个xu，同理，在固定X时可以并行计算各个yi&nbsp;。 在计算xu&nbsp;和yi&nbsp;时，如果直接用上述解析解的表达式来计算，复杂度将会很高。以xu&nbsp;的表达式来说，Y&nbsp;Λu&nbsp;YT&nbsp;这一项就涉及到所有物品的向量，少则几十万，大则上千万，而且每个用户的都不一样，每个用户都算一遍时间上不可行。所以，这里要先对xu&nbsp;的表达式化简，降低复杂度。 注意到Λi&nbsp;的特殊性，它是由置信度构成的对角阵，对于一个用户来说，由于大部分物品都没有评分，以此Λi&nbsp;对角线中大部分元素都是1，利用这个特点，我们可以把Λi&nbsp;拆成两部分的和，即 其中I为单位阵，Λu&nbsp;- I&nbsp;为对角阵，并且对角线上大部分元素为0，于是，可以重写为如下形式： 分解成这两项之后，第一项Y YT&nbsp;对每个用户都是一样的，只需要计算一次，存起来，后面可以重复利用，对于第二项，由于Λu&nbsp;- I&nbsp;为对角线大部分是0的对角阵，所以计算Y&nbsp;(Λu&nbsp;- I&nbsp;)YT&nbsp;的复杂度是O(k2nu)。其中nu&nbsp;是Λu&nbsp;- I&nbsp;中非零元的个数，也就是用户u&nbsp;评过分的物品数，通常不会很多，所以整个Y&nbsp;ΛuYT的计算复杂度由O(k2M)&nbsp;降为O(k2nu)。由于M&gt;&gt;nu，所以计算速度大大加快。对于xu&nbsp;表达式的Y&nbsp;Λu&nbsp;Pu这一项，则应Y&nbsp;(&nbsp;Λu&nbsp;Pu)&nbsp;这样计算，利用Pu&nbsp;中大部分元素是0的特点，将计算复杂度由O(kM&nbsp;)&nbsp;降低到O(knu)。通过使用上述数学技巧，整个xu的计算复杂度可以降低到O(k2nu+k3)，其中nu是有评分的用户数或物品数，k&nbsp;为隐变量个数，完全满足在线计算的需求。 4.增量矩阵分解算法 无论是显式矩阵分解，还是隐式矩阵分解，我们在模型训练完以后，就会得到训练集里每个用户的向量和每个物品的向量。假设现在有一个用户，在训练集里没出现过，但是我们有他的历史行为数据，那这个用户的向量该怎么计算呢？当然，最简单的方法就是把这个用户的行为数据合并到旧的训练集里，重新做一次矩阵分解，进而得到这个用户的向量，但是这样做计算代价太大了，在时间上不可行。 为了解决训练数据集以外的用户(我们称之为新用户)的推荐问题，我们就需要用到增量矩阵分解算法。增量矩阵分解算法能根据用户历史行为数据，在不重算所有用户向量的前提下，快速计算出新用户向量。 在交替最小二乘法里，当固定Y&nbsp;计算xu&nbsp;时，我们只需要用到用户u&nbsp;的历史行为数据rui&nbsp;以及Y&nbsp;的当前值，不同用户之间xu的计算是相互独立的。这就启发我们，对于训练集以外的用户，我们同样可以用他的历史行为数据以及训练集上收敛时学到的Y，来计算新用户的用户向量。下面的图2表示了这一过程。 增量矩阵分解 设用户历史行为数据为Pu={Pui&nbsp;}，训练集上学到的物品矩阵为Y，要求解的用户向量为xu，则增量矩阵分解算法求解的目标为： 这个目标函数跟第3节中固定Y&nbsp;时求解xu&nbsp;的目标函数是一样的，但有两个不同点： (1)这里的Y&nbsp;是不需要迭代的，它是MF在训练集上收敛时得到的Y； (2)用户的历史行为数据Pu&nbsp;要过滤掉在Y中没出现过的物品。由于Y&nbsp;是固定的，我们不需要迭代，直接通过xu&nbsp;的解析表达式求解xu，即： 式中的所有符号和上一节相同。 事实上，增量矩阵分解的目标函数中的Y&nbsp;也不一定要是MF在训练集上学出来的，只要Y&nbsp;中的每个向量都能表示对应物品的特征就行，也就是说，Y&nbsp;可以是由其他数据和其他算法事先学出来的。矩阵分解的增量算法在图文推荐系统中有着广泛应用，具体的应用将在下一篇文章中介绍。 5.推荐结果的可解释性 好的推荐算法不仅要推得准确，而且还要有良好的可解释性，也就是根据什么给用户推荐了这个物品。传统的ItemCF算法就有很好的可解释性，因为在ItemCF中，用户u&nbsp;对物品i&nbsp;的预测评分R&nbsp;(u, i&nbsp;)&nbsp;的计算公式为 其中N(u&nbsp;)&nbsp;表示用户u&nbsp;有过行为的物品集合，ruj&nbsp;表示用户u&nbsp;对物品j&nbsp;的历史评分，sji&nbsp;表示物品j&nbsp;和物品i&nbsp;的相似度。在这个公式中，N(u&nbsp;)&nbsp;中的物品j&nbsp;对R(u, i )&nbsp;的贡献为ruj&nbsp;sji，因此可以很好地解释物品i&nbsp;具体是由N(u)&nbsp;中哪个物品推荐而来。那对于矩阵分解算法来说，是否也能给出类似的可解释性呢？答案是肯定的。 以隐式矩阵分解为例，我们已经推导出，已知物品的矩阵Y&nbsp;时，用户u&nbsp;的向量的计算表达式为： 假设物品i&nbsp;的向量为yi，那么用户u&nbsp;对物品i&nbsp;的预测评分为： 令 并把Y&nbsp;Λu&nbsp;Pu&nbsp;展开来写，则的表达式可以写成 其中， 可以看成是物品j&nbsp;和物品i&nbsp;之间的相似度， 可以看成是用户u&nbsp;对用户j的评分，这样就能像ItemCF那样去解释N(u&nbsp;)中每一项对推荐物品i&nbsp;的贡献了。从sji&nbsp;的计算表达式中，我们还可以看到，物品j&nbsp;和物品i&nbsp;之间的相似度sji&nbsp;是跟用户u&nbsp;有关系的，也就是说，即使是相同的两个物品，在不同用户看来，它们的相似度是不一样的，这跟ItemCF的固定相似度有着本质上的区别，MF的相似度看起来更合理一些。 6.小结 (1)根据用户行为数据的特点，矩阵分解又分为显式矩阵分解和隐式矩阵分解两种； (2)在显式MF算法中，用户向量和物品向量的内积拟合的是用户对物品的实际评分，并且只拟合有评分的项； (3)在隐式MF算法中，用户向量和物品向量的内积拟合的是用户对物品的偏好(0或1)，拟合的强度由置信度控制，置信度又由行为的强度决定； (4)在隐式MF中，需要使用一些数学技巧降低计算复杂度，才能满足线上实时计算的性能要求； (5)对于有行为数据，但不在训练集里的用户，可以使用增量MF算法计算出他的用户向量，进而为他做推荐； (6)MF算法也能像ItemCF一样，能给出推荐的理由，具有良好的可解释性。 参考文献 [1] 项亮. 推荐系统实践. 北京: 人民邮电出版社. 2012. [2] Sarwar B M, Karypis G, Konstan J A, et al. Item-based collaborative filtering recommendation algorithms. WWW. 2001, 1: 285-295. [3] Linden G, Smith B, York J. Amazon. com recommendations: Item-to-item collaborative filtering. IEEE Internet computing. 2003 (1): 76-80. [4] Koren Y, Bell R, Volinsky C. Matrix factorization techniques for recommender systems. Computer. 2009 (8): 30-37. [5] Koren Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model. SIGKDD. 2008: 426-434. [6] Hu Y, Koren Y, Volinsky C. Collaborative filtering for implicit feedback datasets. ICDM. 2008: 263-272. [7] Mnih A, Salakhutdinov R R. Probabilistic matrix factorization. NIPS. 2008: 1257-1264. [8] Koren Y. Collaborative filtering with temporal dynamics. SIGKDD. 2009: 447-456. [9] Pilászy I, Zibriczky D, Tikk D. Fast als-based matrix factorization for explicit and implicit feedback datasets. RecSys. 2010: 71-78. [10] Johnson C C. Logistic matrix factorization for implicit feedback data. NIPS, 2014, 27. [11] He X, Zhang H, Kan M Y, et al. Fast matrix factorization for online recommendation with implicit feedback. SIGIR. 2016: 549-558. [12] Hofmann T. Probabilistic latent semantic analysis. UAI. 1999: 289-296. [13] Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation. JMLR. 2003, 3(Jan): 993-1022. [14] Zhang S, Yao L, Sun A, et al. Deep learning based recommender system: A survey and new perspectives. ACM Computing Surveys (CSUR). 2019, 52(1): 5. [15] Mikolov, Tomas &amp; Chen, Kai &amp; Corrado, G.s &amp; Dean, Jeffrey. Efficient Estimation of Word Representations in Vector Space. Proceedings of Workshop at ICLR. 2013. [16] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality. NIPS. 2013: 3111-3119. [17] Pennington J, Socher R, Manning C. Glove: Global vectors for word representation. EMNLP. 2014: 1532-1543. [18] Covington P, Adams J, Sargin E. Deep neural networks for youtube recommendations. RecSys. 2016: 191-198. [19] Chen T, Guestrin C. Xgboost: A scalable tree boosting system. SIGKDD. 2016: 785-794. [20] Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree. NIPS. 2017: 3146-3154. 更多精彩阅读： 科普篇 | 推荐系统之矩阵分解模型" />
<link rel="canonical" href="https://uzzz.org/2019/06/05/788646.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/05/788646.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-05T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"导语：本系列文章一共有三篇，分别是 《科普篇 |&nbsp;推荐系统之矩阵分解模型》 《原理篇 |&nbsp;推荐系统之矩阵分解模型》 《实践篇 |&nbsp;推荐系统之矩阵分解模型》 第一篇用一个具体的例子介绍了MF是如何做推荐的。第二篇讲的是MF的数学原理，包括MF模型的目标函数和求解公式的推导等。第三篇回归现实，讲述MF算法在图文推荐中的应用实践（将于后续发布）。下文是第二篇——《原理篇 | 推荐系统之矩阵分解模型》，敬请阅读。 上一篇我们用一个简单的例子讲述了矩阵分解(Matrix Factorization, MF)是如何做推荐的，但没有深入到算法的细节。如果想编写自己的代码实现MF，那么就需要了解其中的细节了。本文是MF系列的第二篇文章，主要介绍了显式矩阵分解和隐式矩阵分解的数学原理，包括模型思想、目标函数、优化求解的公式推导等，旨在为需要了解算法细节的同学提供参考。 1.显式数据和隐式数据 MF用到的用户行为数据分为显式数据和隐式数据两种。显式数据是指用户对item的显式打分，比如用户对电影、商品的评分，通常有5分制和10分制。隐式数据是指用户对item的浏览、点击、购买、收藏、点赞、评论、分享等数据，其特点是用户没有显式地给item打分，用户对item的感兴趣程度都体现在他对item的浏览、点击、购买、收藏、点赞、评论、分享等行为的强度上。 显式数据的优点是行为的置信度高，因为是用户明确给出的打分，所以真实反映了用户对item的喜欢程度。缺点是这种数据的量太小，因为绝大部分用户都不会去给item评分，这就导致数据非常稀疏，同时这部分评分也仅代表了小部分用户的兴趣，可能会导致数据有偏。隐式数据的优点是容易获取，数据量很大。因为几乎所有用户都会有浏览、点击等行为，所以数据量大，几乎覆盖所有用户，不会导致数据有偏。其缺点是置信度不如显式数据的高，比如浏览不一定代表感兴趣，还要看强度，经常浏览同一类东西才能以较高置信度认为用户感兴趣。 根据所使用的数据是显式数据还是隐式数据，矩阵分解算法又分为两种[4,6]。使用显式数据的矩阵分解算法称为显式矩阵分解算法，使用隐式数据的矩阵分解算法称为隐式矩阵分解算法。由于矩阵分解算法有众多的改进版本和各种变体[4,5,6,7,8,9,10,11]，本文不打算一一列举，因此下文将以实践中用得最多的矩阵分解算法为例，介绍其具体的数据原理，这也是spark机器学习库mllib中实现的矩阵分解算法[4,6]。从实际应用的效果来看，隐式矩阵分解的效果一般会更好。 2.显式矩阵分解 在本系列第一篇文章中，我们提到，矩阵分解算法的输入是user对item的评分矩阵(图1等号左边的矩阵)，输出是User矩阵和Item矩阵(图1等号右边的矩阵)，其中User矩阵的每一行代表一个用户向量，Item矩阵的每一列代表一个item的向量。User对item的预测评分用它们的向量内积来表示，通过最小化预测评分和实际评分的差异来学习User矩阵和Item矩阵。 图1 2.1 目标函数 为了用数学的语言定量表示上述思想，我们先引入一些符号。设rui&nbsp;表示用户u&nbsp;对item i&nbsp;的显式评分，当rui&nbsp;&gt;0时，表示用户u&nbsp;对item i&nbsp;有评分，当rui&nbsp;=0时，表示用户u&nbsp;对item i&nbsp;没有评分，xu&nbsp;表示用户u&nbsp;的向量，yi&nbsp;表示item i&nbsp;的向量，则显式矩阵分解的目标函数为： 其中xu&nbsp;和yi&nbsp;都是k&nbsp;维的列向量，k&nbsp;为隐变量的个数， 是所有xu&nbsp;构成的矩阵， 为所有yi&nbsp;构成的矩阵，N&nbsp;为用户数，M&nbsp;为item数，λ为正则化参数。 在上述公式中， 为用户向量与物品向量的内积，表示用户u&nbsp;对物品i&nbsp;的预测评分，目标函数通过最小化预测评分和实际评分rui&nbsp;之间的残差平方和，来学习所有用户向量和物品向量。这里的残差项只包含了有评分的数据，不包括没有评分的数据。目标函数中第二项是L2正则项，用于保证数值计算稳定性和防止过拟合。 2.2 求解方法： 求解X&nbsp;和Y&nbsp;采用的是交替最小二乘法(alternative least square, ALS)，也就是先固定X&nbsp;优化Y&nbsp;，然后固定Y&nbsp;优化X&nbsp;，这个过程不断重复，直到X&nbsp;和Y&nbsp;收敛为止。每次固定其中一个优化另一个都需要解一个最小二乘问题，所以这个算法叫做交替最小二乘方法。 (1)Y&nbsp;固定为上一步迭代值或初始化值，优化X&nbsp;： 此时，Y&nbsp;被当做常数处理，目标函数被分解为多个独立的子目标函数，每个子目标函数对应一个用户。对于用户u&nbsp;，目标函数为： 这里面残差项求和的个数等于用于u&nbsp;评过分的物品的个数，记为m&nbsp;个。把这个目标函数化为矩阵形式，得&nbsp; 其中， 表示用户u&nbsp;对这m&nbsp;个物品的评分构成的向量， 表示这m&nbsp;个物品的向量构成的矩阵，顺序跟Ru&nbsp;中物品的顺序一致。 对目标函数J关于xu&nbsp;求梯度，并令梯度为零，得： 解这个线性方程组，可得到xu&nbsp;的解析解为： (2) X&nbsp;固定为上一步迭代值或初始化值，优化Y： 此时，X&nbsp;被当做常数处理，目标函数也被分解为多个独立的子目标函数，每个子目标函数对应一个物品。类似上面的推导，我们可以得到yi&nbsp;的解析解为： 其中， 表示n&nbsp;个用户对物品i&nbsp;的评分构成的向量， 表示这n&nbsp;个用户的向量构成的矩阵，顺序跟Ri&nbsp;中用户的顺序一致。 2.3 工程实现 当固定Y&nbsp;时，各个xu&nbsp;的计算是独立的，因此可以对xu&nbsp;进行分布式并行计算。同理，当固定X 时，各个yi&nbsp;的计算也是独立的，因此也可以对yi&nbsp;做分布式并行计算。因为Xi&nbsp;和Yu&nbsp;中只包含了有评分的用户或物品，而非全部用户或物品，因此xu&nbsp;和yi&nbsp;的计算时间复杂度为O(k2nu+k3)其中nu&nbsp;是有评分的用户数或物品数，k&nbsp;为隐变量个数。 3.隐式矩阵分解 隐式矩阵分解与显式矩阵分解的一个比较大的区别，就是它会去拟合评分矩阵中的零，即没有评分的地方也要拟合。 3.1 目标函数 我们仍然用rui&nbsp;表示用户u&nbsp;对物品i&nbsp;的评分，但这里的评分表示的是行为的强度，比如浏览次数、阅读时长、播放完整度等。当rui&nbsp;&gt;0时，表示用户u&nbsp;对物品i有过行为，当rui&nbsp;=0时，表示用户u&nbsp;对物品i没有过行为。首先，我们定义一个二值变量pui&nbsp;如下： 这个pui&nbsp;是一个依赖于rui&nbsp;的量，用于表示用户u&nbsp;对物品i&nbsp;是否感兴趣，也称为用户偏好。当用户u&nbsp;对物品i&nbsp;有过行为时，我们认为用户u&nbsp;对物品i感兴趣，此时pui&nbsp;=1；当用户u&nbsp;对物品i&nbsp;没有过行为时，我们认为用户u&nbsp;对物品i&nbsp;不感兴趣，此时pui&nbsp;=0。 模型除了要刻画用户对物品是否感兴趣外，而且还要刻画感兴趣或不感兴趣的程度，所以这里的隐式矩阵分解还引入了置信度的概念。从直观上来说，当rui&nbsp;&gt;0时，rui&nbsp;越大，我们越确信用户u&nbsp;喜欢物品i&nbsp;，而当rui&nbsp;=0时，我们不能确定用户u&nbsp;是否喜欢物品i&nbsp;，没有行为可能只是因为用户u&nbsp;并不知道物品i&nbsp;的存在。 因此，置信度是rui&nbsp;的函数，并且当rui&nbsp;&gt;0时，置信度是rui&nbsp;的增函数；当rui&nbsp;=0时，置信度取值要小。论文中给出的置信度cui&nbsp;的表达式为： 当rui&nbsp;&gt;0时，cui&nbsp;关于rui&nbsp;线性递增，表示对于有评分的物品，行为强度越大，我们越相信用户u&nbsp;对物品i&nbsp;感兴趣；当rui&nbsp;=0时，置信度恒等于1，表示对所有没有评分的物品，用户不感兴趣的置信度都一样，并且比有评分物品的置信度低。用xu&nbsp;表示用户u&nbsp;的向量，yi&nbsp;表示item i&nbsp;的向量，引入置信度以后，隐式矩阵分解[6]的目标函数为： 其中xu&nbsp;和yi&nbsp;都是k&nbsp;维的列向量，k&nbsp;为隐变量的个数， 是所有xu&nbsp;构成的矩阵， 为所有yi&nbsp;构成的矩阵，N&nbsp;为用户数，M&nbsp;为item数，λ为正则化参数。目标函数里的内积用于表示用户对物品的预测偏好，拟合实际偏好pui，拟合强度由cui&nbsp;&nbsp;控制。并且对于pui&nbsp;=0的项也要拟合。目标函数中的第二项是正则项，用于保证数值计算稳定性以及防止过拟合。 3.2 求解方法 目标函数的求解仍然可以采用交替最小二乘法。具体如下： (1)Y&nbsp;固定为上一步迭代值或初始化值，优化X&nbsp;： 此时，Y&nbsp;被当做常数处理，目标函数被分解为多个独立的子目标函数，每个子目标函数都是某个xu&nbsp;的函数。对于用户u&nbsp;，目标函数为： 把这个目标函数化为矩阵形式，得 其中， 为用户u&nbsp;对每个物品的偏好构成的列向量， 表示所有物品向量构成的矩阵，Λu&nbsp;为用户u&nbsp;对所有物品的置信度cui&nbsp;构成的对角阵，即： 对目标函数J&nbsp;关于xu&nbsp;求梯度，并令梯度为零，得： 解这个线性方程组，可得到xu&nbsp;的解析解为： (2) X&nbsp;固定为上一步迭代值或初始化值，优化Y： 此时，X&nbsp;被当做常数处理，目标函数也被分解为多个独立的子目标函数，每个子目标函数都是关于某个yi&nbsp;的函数。通过同样的推导方法，可以得到yi&nbsp;的解析解为： 其中， 为所有用户对物品i&nbsp;的偏好构成的向量， 表示所有用户的向量构成的矩阵，Λi&nbsp;为所有用户对物品i&nbsp;的偏好的置信度构成的对角矩阵，即 3.3 工程实现 由于固定Y&nbsp;时，各个xu&nbsp;的求解都是独立的，所以在固定Y&nbsp;时可以并行计算各个xu，同理，在固定X时可以并行计算各个yi&nbsp;。 在计算xu&nbsp;和yi&nbsp;时，如果直接用上述解析解的表达式来计算，复杂度将会很高。以xu&nbsp;的表达式来说，Y&nbsp;Λu&nbsp;YT&nbsp;这一项就涉及到所有物品的向量，少则几十万，大则上千万，而且每个用户的都不一样，每个用户都算一遍时间上不可行。所以，这里要先对xu&nbsp;的表达式化简，降低复杂度。 注意到Λi&nbsp;的特殊性，它是由置信度构成的对角阵，对于一个用户来说，由于大部分物品都没有评分，以此Λi&nbsp;对角线中大部分元素都是1，利用这个特点，我们可以把Λi&nbsp;拆成两部分的和，即 其中I为单位阵，Λu&nbsp;- I&nbsp;为对角阵，并且对角线上大部分元素为0，于是，可以重写为如下形式： 分解成这两项之后，第一项Y YT&nbsp;对每个用户都是一样的，只需要计算一次，存起来，后面可以重复利用，对于第二项，由于Λu&nbsp;- I&nbsp;为对角线大部分是0的对角阵，所以计算Y&nbsp;(Λu&nbsp;- I&nbsp;)YT&nbsp;的复杂度是O(k2nu)。其中nu&nbsp;是Λu&nbsp;- I&nbsp;中非零元的个数，也就是用户u&nbsp;评过分的物品数，通常不会很多，所以整个Y&nbsp;ΛuYT的计算复杂度由O(k2M)&nbsp;降为O(k2nu)。由于M&gt;&gt;nu，所以计算速度大大加快。对于xu&nbsp;表达式的Y&nbsp;Λu&nbsp;Pu这一项，则应Y&nbsp;(&nbsp;Λu&nbsp;Pu)&nbsp;这样计算，利用Pu&nbsp;中大部分元素是0的特点，将计算复杂度由O(kM&nbsp;)&nbsp;降低到O(knu)。通过使用上述数学技巧，整个xu的计算复杂度可以降低到O(k2nu+k3)，其中nu是有评分的用户数或物品数，k&nbsp;为隐变量个数，完全满足在线计算的需求。 4.增量矩阵分解算法 无论是显式矩阵分解，还是隐式矩阵分解，我们在模型训练完以后，就会得到训练集里每个用户的向量和每个物品的向量。假设现在有一个用户，在训练集里没出现过，但是我们有他的历史行为数据，那这个用户的向量该怎么计算呢？当然，最简单的方法就是把这个用户的行为数据合并到旧的训练集里，重新做一次矩阵分解，进而得到这个用户的向量，但是这样做计算代价太大了，在时间上不可行。 为了解决训练数据集以外的用户(我们称之为新用户)的推荐问题，我们就需要用到增量矩阵分解算法。增量矩阵分解算法能根据用户历史行为数据，在不重算所有用户向量的前提下，快速计算出新用户向量。 在交替最小二乘法里，当固定Y&nbsp;计算xu&nbsp;时，我们只需要用到用户u&nbsp;的历史行为数据rui&nbsp;以及Y&nbsp;的当前值，不同用户之间xu的计算是相互独立的。这就启发我们，对于训练集以外的用户，我们同样可以用他的历史行为数据以及训练集上收敛时学到的Y，来计算新用户的用户向量。下面的图2表示了这一过程。 增量矩阵分解 设用户历史行为数据为Pu={Pui&nbsp;}，训练集上学到的物品矩阵为Y，要求解的用户向量为xu，则增量矩阵分解算法求解的目标为： 这个目标函数跟第3节中固定Y&nbsp;时求解xu&nbsp;的目标函数是一样的，但有两个不同点： (1)这里的Y&nbsp;是不需要迭代的，它是MF在训练集上收敛时得到的Y； (2)用户的历史行为数据Pu&nbsp;要过滤掉在Y中没出现过的物品。由于Y&nbsp;是固定的，我们不需要迭代，直接通过xu&nbsp;的解析表达式求解xu，即： 式中的所有符号和上一节相同。 事实上，增量矩阵分解的目标函数中的Y&nbsp;也不一定要是MF在训练集上学出来的，只要Y&nbsp;中的每个向量都能表示对应物品的特征就行，也就是说，Y&nbsp;可以是由其他数据和其他算法事先学出来的。矩阵分解的增量算法在图文推荐系统中有着广泛应用，具体的应用将在下一篇文章中介绍。 5.推荐结果的可解释性 好的推荐算法不仅要推得准确，而且还要有良好的可解释性，也就是根据什么给用户推荐了这个物品。传统的ItemCF算法就有很好的可解释性，因为在ItemCF中，用户u&nbsp;对物品i&nbsp;的预测评分R&nbsp;(u, i&nbsp;)&nbsp;的计算公式为 其中N(u&nbsp;)&nbsp;表示用户u&nbsp;有过行为的物品集合，ruj&nbsp;表示用户u&nbsp;对物品j&nbsp;的历史评分，sji&nbsp;表示物品j&nbsp;和物品i&nbsp;的相似度。在这个公式中，N(u&nbsp;)&nbsp;中的物品j&nbsp;对R(u, i )&nbsp;的贡献为ruj&nbsp;sji，因此可以很好地解释物品i&nbsp;具体是由N(u)&nbsp;中哪个物品推荐而来。那对于矩阵分解算法来说，是否也能给出类似的可解释性呢？答案是肯定的。 以隐式矩阵分解为例，我们已经推导出，已知物品的矩阵Y&nbsp;时，用户u&nbsp;的向量的计算表达式为： 假设物品i&nbsp;的向量为yi，那么用户u&nbsp;对物品i&nbsp;的预测评分为： 令 并把Y&nbsp;Λu&nbsp;Pu&nbsp;展开来写，则的表达式可以写成 其中， 可以看成是物品j&nbsp;和物品i&nbsp;之间的相似度， 可以看成是用户u&nbsp;对用户j的评分，这样就能像ItemCF那样去解释N(u&nbsp;)中每一项对推荐物品i&nbsp;的贡献了。从sji&nbsp;的计算表达式中，我们还可以看到，物品j&nbsp;和物品i&nbsp;之间的相似度sji&nbsp;是跟用户u&nbsp;有关系的，也就是说，即使是相同的两个物品，在不同用户看来，它们的相似度是不一样的，这跟ItemCF的固定相似度有着本质上的区别，MF的相似度看起来更合理一些。 6.小结 (1)根据用户行为数据的特点，矩阵分解又分为显式矩阵分解和隐式矩阵分解两种； (2)在显式MF算法中，用户向量和物品向量的内积拟合的是用户对物品的实际评分，并且只拟合有评分的项； (3)在隐式MF算法中，用户向量和物品向量的内积拟合的是用户对物品的偏好(0或1)，拟合的强度由置信度控制，置信度又由行为的强度决定； (4)在隐式MF中，需要使用一些数学技巧降低计算复杂度，才能满足线上实时计算的性能要求； (5)对于有行为数据，但不在训练集里的用户，可以使用增量MF算法计算出他的用户向量，进而为他做推荐； (6)MF算法也能像ItemCF一样，能给出推荐的理由，具有良好的可解释性。 参考文献 [1] 项亮. 推荐系统实践. 北京: 人民邮电出版社. 2012. [2] Sarwar B M, Karypis G, Konstan J A, et al. Item-based collaborative filtering recommendation algorithms. WWW. 2001, 1: 285-295. [3] Linden G, Smith B, York J. Amazon. com recommendations: Item-to-item collaborative filtering. IEEE Internet computing. 2003 (1): 76-80. [4] Koren Y, Bell R, Volinsky C. Matrix factorization techniques for recommender systems. Computer. 2009 (8): 30-37. [5] Koren Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model. SIGKDD. 2008: 426-434. [6] Hu Y, Koren Y, Volinsky C. Collaborative filtering for implicit feedback datasets. ICDM. 2008: 263-272. [7] Mnih A, Salakhutdinov R R. Probabilistic matrix factorization. NIPS. 2008: 1257-1264. [8] Koren Y. Collaborative filtering with temporal dynamics. SIGKDD. 2009: 447-456. [9] Pilászy I, Zibriczky D, Tikk D. Fast als-based matrix factorization for explicit and implicit feedback datasets. RecSys. 2010: 71-78. [10] Johnson C C. Logistic matrix factorization for implicit feedback data. NIPS, 2014, 27. [11] He X, Zhang H, Kan M Y, et al. Fast matrix factorization for online recommendation with implicit feedback. SIGIR. 2016: 549-558. [12] Hofmann T. Probabilistic latent semantic analysis. UAI. 1999: 289-296. [13] Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation. JMLR. 2003, 3(Jan): 993-1022. [14] Zhang S, Yao L, Sun A, et al. Deep learning based recommender system: A survey and new perspectives. ACM Computing Surveys (CSUR). 2019, 52(1): 5. [15] Mikolov, Tomas &amp; Chen, Kai &amp; Corrado, G.s &amp; Dean, Jeffrey. Efficient Estimation of Word Representations in Vector Space. Proceedings of Workshop at ICLR. 2013. [16] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality. NIPS. 2013: 3111-3119. [17] Pennington J, Socher R, Manning C. Glove: Global vectors for word representation. EMNLP. 2014: 1532-1543. [18] Covington P, Adams J, Sargin E. Deep neural networks for youtube recommendations. RecSys. 2016: 191-198. [19] Chen T, Guestrin C. Xgboost: A scalable tree boosting system. SIGKDD. 2016: 785-794. [20] Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree. NIPS. 2017: 3146-3154. 更多精彩阅读： 科普篇 | 推荐系统之矩阵分解模型","@type":"BlogPosting","url":"https://uzzz.org/2019/06/05/788646.html","headline":"原理篇 推荐系统之矩阵分解模型","dateModified":"2019-06-05T00:00:00+08:00","datePublished":"2019-06-05T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/05/788646.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>原理篇 | 推荐系统之矩阵分解模型</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="rich_media_content" id="js_content"> 
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><span style="text-decoration:none;"></span><br></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/WNTtYW98ZSFicUFwibpI7bCG5SYxEVBnicjzNN1Xjzo8T7UkT87ibyPQ3krcpzica4ibsVERYibrFrraSJd0Rl7pBMb0g/640?wx_fmt=gif" alt="640?wx_fmt=gif"></p>
   <p><br></p>
   <p><br></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><strong><span style="color:rgb(62,71,83);font-size:14px;">导语</span></strong><span style="color:rgb(62,71,83);font-size:14px;">：本系列文章一共有三篇，分别是</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&amp;mid=2649742233&amp;idx=1&amp;sn=f97cb1fcb3c4182168bdc5cef33bc9d3&amp;chksm=bed348e289a4c1f40de9869c720cc9be2a027ca6919234fdfccec43d78c4b0e289296b630ca5&amp;token=77606591&amp;lang=zh_CN&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;color:rgb(0,122,170);"><span style="font-size:14px;color:rgb(0,122,170);">《科普篇 |&nbsp;推荐系统之矩阵分解模型》</span></a></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(62,71,83);font-size:14px;">《原理篇 |&nbsp;推荐系统之矩阵分解模型》</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(62,71,83);font-size:14px;">《实践篇 |&nbsp;推荐系统之矩阵分解模型》</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(62,71,83);font-size:14px;">第一篇用一个具体的例子介绍了MF是如何做推荐的。第二篇讲的是MF的数学原理，包括MF模型的目标函数和求解公式的推导等。第三篇回归现实，讲述MF算法在图文推荐中的应用实践（<span style="color:rgb(62,71,83);font-size:14px;">将于后续发布</span>）。下文是第二篇——</span><span style="font-size:14px;color:rgb(0,0,0);">《原理篇 | 推荐系统之矩阵分解模型》</span><span style="color:rgb(62,71,83);font-size:14px;"><span style="color:rgb(62,71,83);font-size:14px;">，</span>敬请阅读。</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">上一篇我们用一个简单的例子讲述了矩阵分解(Matrix Factorization, MF)是如何做推荐的，但没有深入到算法的细节。如果想编写自己的代码实现MF，那么就需要了解其中的细节了。本文是MF系列的第二篇文章，主要介绍了显式矩阵分解和隐式矩阵分解的数学原理，包括模型思想、目标函数、优化求解的公式推导等，旨在为需要了解算法细节的同学提供参考。</span></p>
   <p><br></p>
   <strong>1.显式数据和隐式数据</strong>
   <br>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">MF用到的用户行为数据分为显式数据和隐式数据两种。显式数据是指用户对item的显式打分，比如用户对电影、商品的评分，通常有5分制和10分制。隐式数据是指用户对item的浏览、点击、购买、收藏、点赞、评论、分享等数据，其特点是用户没有显式地给item打分，用户对item的感兴趣程度都体现在他对item的浏览、点击、购买、收藏、点赞、评论、分享等行为的强度上。</span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">显式数据的优点是行为的置信度高，因为是用户明确给出的打分，所以真实反映了用户对item的喜欢程度。缺点是这种数据的量太小，因为绝大部分用户都不会去给item评分，这就导致数据非常稀疏，同时这部分评分也仅代表了小部分用户的兴趣，可能会导致数据有偏。隐式数据的优点是容易获取，数据量很大。因为几乎所有用户都会有浏览、点击等行为，所以数据量大，几乎覆盖所有用户，不会导致数据有偏。其缺点是置信度不如显式数据的高，比如浏览不一定代表感兴趣，还要看强度，经常浏览同一类东西才能以较高置信度认为用户感兴趣。</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">根据所使用的数据是显式数据还是隐式数据，矩阵分解算法又分为两种[4,6]。使用显式数据的矩阵分解算法称为显式矩阵分解算法，使用隐式数据的矩阵分解算法称为隐式矩阵分解算法。由于矩阵分解算法有众多的改进版本和各种变体[4,5,6,7,8,9,10,11]，本文不打算一一列举，因此下文将以实践中用得最多的矩阵分解算法为例，介绍其具体的数据原理，这也是spark机器学习库mllib中实现的矩阵分解算法[4,6]。从实际应用的效果来看，隐式矩阵分解的效果一般会更好。</span></p>
   <p><br></p>
   <strong>2.显式矩阵分解</strong>
   <br>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">在本系列第一篇文章中，我们提到，矩阵分解算法的输入是user对item的评分矩阵(图1等号左边的矩阵)，输出是User矩阵和Item矩阵(图1等号右边的矩阵)，其中User矩阵的每一行代表一个用户向量，Item矩阵的每一列代表一个item的向量。User对item的预测评分用它们的向量内积来表示，通过最小化预测评分和实际评分的差异来学习User矩阵和Item矩阵。</span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/b96CibCt70iab2ZoTpy1PORwmsB4kEyhNupA73OvQyEEicTbl9DRTD6qibY4iaZtaViaxnmT8gAIHib5NqFovFlByXfqg/640" alt="640"></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><span style="color:rgb(62,71,83);font-size:14px;">图1</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(0,0,0);"><strong><span style="font-size:16px;">2.1 目标函数</span></strong></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">为了用数学的语言定量表示上述思想，我们先引入一些符号。设</span><span style="color:rgb(62,71,83);font-size:16px;"><em>r<sub>ui&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">表示用户<em><span style="font-size:14px;">u&nbsp;</span></em>对item <em><span style="font-size:14px;">i&nbsp;</span></em>的显式评分，当<em><span style="font-size:14px;"><em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em></span></em>&gt;0时，表示用户<em><span style="font-size:14px;">u&nbsp;</span></em>对item <em><span style="font-size:14px;">i&nbsp;</span></em>有评分，当<em><span style="font-size:14px;"><em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em></span></em>=0时，表示用户<em><span style="font-size:14px;">u&nbsp;</span></em>对item <em><span style="font-size:14px;">i&nbsp;</span></em>没有评分，<em>x<sub>u&nbsp;</sub></em></span><span style="color:rgb(62,71,83);font-size:16px;">表示用户<em><span style="font-size:14px;">u&nbsp;</span></em>的向量，<em>y<sub>i&nbsp;</sub></em>表示item <em><span style="font-size:14px;">i&nbsp;</span></em>的向量，则显式矩阵分解的目标函数为：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3Ugt5GqibjhZ3ib3OChLAvcKp7WL2PtLkxD6WqYtIDHI2xxVZR2MicWyvg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">其中<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>和<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>都是<em><span style="font-size:14px;">k&nbsp;</span></em>维的列向量，<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">k&nbsp;</span></em>为隐变量的个数，</span><img src="" alt=""></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdrVCxlrTfOlhOm2zf32l97Sq8pjj7gHHJqqaYyRINmgndr1qiavO4OTA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">是所有<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>构成的矩阵，</span><img src="" alt=""><br></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdQoWFqXmpu3WYBR3KfHVu8HTPgdElBpqrodPicicmn25XibwRmX0AI8dGA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">为所有<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>构成的矩阵，</span><em><span style="font-size:16px;color:rgb(62,71,83);">N&nbsp;</span></em><span style="font-size:16px;color:rgb(62,71,83);">为用户数，</span><em><span style="font-size:16px;color:rgb(62,71,83);">M&nbsp;</span></em><span style="font-size:16px;color:rgb(62,71,83);">为item数，λ为正则化参数。</span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">在上述公式中，</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img src="" alt=""><img class="rich_pages" style="text-align:center;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdpr1bBmGeJlMZgMuY6MNJ9z8zeJm0TMQAP6D8kIHYCEay3PyPDAKHNA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(62,71,83);font-size:16px;">为用户向量与物品向量的内积，表示用户<em><span style="font-size:14px;">u&nbsp;</span></em>对物品<em><span style="font-size:14px;">i&nbsp;</span></em>的预测评分，目标函数通过最小化预测评分和实际评分</span><em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;"><em style="font-size:16px;">r<sub>ui&nbsp;</sub></em></span></em><span style="color:rgb(62,71,83);font-size:16px;">之间的残差平方和，来学习所有用户向量和物品向量。这里的残差项只包含了有评分的数据，不包括没有评分的数据。目标函数中第二项是L2正则项，用于保证数值计算稳定性和防止过拟合。</span><br></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(0,0,0);"><strong><span style="color:rgb(0,0,0);font-size:16px;">2.2 求解方法：</span></strong></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">求解<em><span style="font-size:14px;">X&nbsp;</span></em>和<em>Y&nbsp;</em>采用的是交替最小二乘法(alternative least square, ALS)，也就是先固定<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">X&nbsp;</span></em>优化<em>Y&nbsp;</em>，然后固定<em>Y&nbsp;</em>优化<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">X&nbsp;</span></em>，这个过程不断重复，直到<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">X&nbsp;</span></em>和<em>Y&nbsp;</em>收敛为止。每次固定其中一个优化另一个都需要解一个最小二乘问题，所以这个算法叫做交替最小二乘方法。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(0,0,0);">(1)<em>Y&nbsp;</em>固定为上一步迭代值或初始化值，优化<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">X&nbsp;</span></em>：</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">此时，<em>Y&nbsp;</em>被当做常数处理，目标函数被分解为多个独立的子目标函数，每个子目标函数对应一个用户。对于用户<em><span style="font-size:14px;">u&nbsp;</span></em>，目标函数为：</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img src="" alt=""><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3yZHkAFuD6Bt4YT8Dpd4pxojUQsLbvssURXgZia2zHVfMezgpjfGnPHg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">这里面残差项求和的个数等于用于<em><span style="font-size:14px;">u&nbsp;</span></em>评过分的物品的个数，记为<em><span style="font-size:14px;">m&nbsp;</span></em>个。把这个目标函数化为矩阵形式，得&nbsp;</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img src="" alt=""><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3zHao7RdGA7r8Ap9icM3K63H3c5J1t2PZlKAnx1vefJibibNGFqOSdIjHQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">其中，</span><img src="" alt=""></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdBqBFO8ia9Nt2u689K4Aj8ZFNybT7DEDLrtlVMEfvFSr9oJY2SwkB9jg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">表示用户<em><span style="font-size:14px;">u&nbsp;</span></em>对这<em><span style="font-size:14px;">m&nbsp;</span></em>个物品的评分构成的向量，</span><img src="" alt=""><br></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdHeYo44hf0ZqopIxaibic6Scwwg8X1icadGTYP48cLJm4K7hUwAGia0APww/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(62,71,83);font-size:16px;">表示这<em><span style="font-size:14px;">m&nbsp;</span></em>个物品的向量构成的矩阵，顺序跟<em>R<sub>u&nbsp;</sub></em>中物品的顺序一致。</span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">对目标函数J关于<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>求梯度，并令梯度为零，得：</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img src="" alt=""><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3kblT7eB1IG4ia1aZWUL0GZGtK7OFmgLLKnySJQdZFZGWDxCccgboNdw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">解这个线性方程组，可得到<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>的解析解为：</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img src="" alt=""><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3HU9oY7wmWRIp9siccNpcN0CJBoPNNiaO4OgoNmWDm5jxQfUOcTbOM9lQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(0,0,0);">(2) <em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">X&nbsp;</span></em>固定为上一步迭代值或初始化值，优化<em>Y</em>：</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">此时，<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">X&nbsp;</span></em>被当做常数处理，目标函数也被分解为多个独立的子目标函数，每个子目标函数对应一个物品。类似上面的推导，我们可以得到<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>的解析解为：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3MU40gIdZRiaM3uhayVFmOkzOJJwqQNzDVj1hjHDg5JYCW60HawIF5EQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">其中，</span></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdGLoWGlGRUnYDcShVoTlyUBo1hayubO8oJnKmupYTGbhK9B7ye0O62w/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><img src="" alt=""><span style="font-size:16px;color:rgb(62,71,83);">表示<em><span style="font-size:14px;">n&nbsp;</span></em>个用户对物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>的评分构成的向量，</span><img src="" alt=""></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdXKv6tA6TPT092TMrBdrribqmHva3X5kIHicnjD4MrofVj388S8vMs6hA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">表示这<em><span style="font-size:14px;">n&nbsp;</span></em>个用户的向量构成的矩阵，顺序跟</span><span style="color:rgb(62,71,83);font-size:16px;"><em>R<sub>i&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">中用户的顺序一致。</span><br></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(0,0,0);"><strong><span style="color:rgb(0,0,0);font-size:16px;">2.3 工程实现</span></strong></span></p>
   <p style="line-height:1.75em;margin-left:8px;"><span style="font-size:16px;color:rgb(62,71,83);">当固定<em>Y&nbsp;</em>时，各个<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>的计算是独立的，因此可以对<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>进行分布式并行计算。同理，当固定<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">X </span></em><span style="font-size:14px;">时</span>，各个<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>的计算也是独立的，因此也可以对<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>做分布式并行计算。因为</span><span style="color:rgb(62,71,83);font-size:16px;"><em>X<sub>i&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">和</span><span style="color:rgb(62,71,83);font-size:16px;"><em>Y<sub>u&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">中只包含了有评分的用户或物品，而非全部用户或物品，因此<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>和<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>的计算时间复杂度为<em>O</em>(<em>k<sup>2</sup>n<sub>u+</sub>k<sup>3</sup></em>)</span><span style="text-indent:43px;font-size:16px;color:rgb(62,71,83);">其中</span><span style="text-indent:43px;color:rgb(62,71,83);font-size:16px;"><em>n<sub>u&nbsp;</sub></em></span><span style="text-indent:43px;font-size:16px;color:rgb(62,71,83);">是</span><span style="text-indent:43px;color:rgb(62,71,83);font-size:16px;">有评分的用户数或物品数，<em><span style="font-size:14px;">k&nbsp;</span></em>为隐变量个数。</span></p>
   <p><br></p>
   <strong>3.隐式矩阵分解</strong>
   <br>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">隐式矩阵分解与显式矩阵分解的一个比较大的区别，就是它会去拟合评分矩阵中的零，即没有评分的地方也要拟合。</span><br></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(0,0,0);"><strong><span style="color:rgb(0,0,0);font-size:16px;">3.1 目标函数</span></strong></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">我们仍然用</span><span style="color:rgb(62,71,83);font-size:16px;"><em>r<sub>ui&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">表示用户<em><span style="font-size:14px;">u&nbsp;</span></em>对物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>的评分，但这里的评分表示的是行为的强度，比如浏览次数、阅读时长、播放完整度等。当</span><span style="color:rgb(62,71,83);font-size:16px;"><em>r<sub>ui&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">&gt;0时，表示用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品i有过行为，当<em><span style="font-size:14px;"><sub><em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em></sub></span></em>=0时，表示用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品i没有过行为。首先，我们定义一个二值变量</span><span style="color:rgb(62,71,83);font-size:16px;"><em>p<sub>ui&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">如下：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3lb8cEgk5uKlSl6TuyguvW0TVIliaVY9OyEEsBlicVF3iaFZqh7Pa0PBPw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">这个<em style="color:rgb(62,71,83);font-size:16px;">p<sub>ui&nbsp;</sub></em>是一个依赖于<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em>的量，用于表示用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>是否感兴趣，也称为用户偏好。当用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>有过行为时，我们认为用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品i感兴趣，此时<em style="color:rgb(62,71,83);font-size:16px;">p<sub>ui&nbsp;</sub></em>=1；当用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>没有过行为时，我们认为用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>不感兴趣，此时<em style="color:rgb(62,71,83);font-size:16px;">p<sub>ui&nbsp;</sub></em>=0。</span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">模型除了要刻画用户对物品是否感兴趣外，而且还要刻画感兴趣或不感兴趣的程度，所以这里的隐式矩阵分解还引入了置信度的概念。从直观上来说，当<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em>&gt;0时，<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em></span><span style="color:rgb(62,71,83);font-size:16px;">越大，我们越确信用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>喜欢物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>，而当</span><em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em><span style="color:rgb(62,71,83);font-size:16px;">=0时，我们不能确定用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>是否喜欢物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>，没有行为可能只是因为用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>并不知道物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>的存在。</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">因此，置信度是<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em>的函数，并且当<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em>&gt;0时，置信度是<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em>的增函数；当<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em>=0时，置信度取值要小。论文中给出的置信度</span><span style="color:rgb(62,71,83);font-size:16px;"><em>c<sub>ui&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">的表达式为：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3daY0Na2wLtjyKtCNID6BJjVp3t224msx1VUxib0b8ZywjyYAhibwb75g/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">当<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em>&gt;0时，<em style="color:rgb(62,71,83);font-size:16px;">c<sub>ui&nbsp;</sub></em>关于<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em>线性递增，表示对于有评分的物品，行为强度越大，我们越相信用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>感兴趣；当<em style="color:rgb(62,71,83);font-size:16px;">r<sub>ui&nbsp;</sub></em>=0时，置信度恒等于1，表示对所有没有评分的物品，用户不感兴趣的置信度都一样，并且比有评分物品的置信度低。用</span><span style="color:rgb(62,71,83);font-size:16px;"><em>x<sub>u&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">表示用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>的向量，</span><span style="color:rgb(62,71,83);font-size:16px;"><em>y<sub>i&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">表示item <em><span style="font-size:14px;">i&nbsp;</span></em>的向量，引入置信度以后，隐式矩阵分解[6]的目标函数为：</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img style="text-align:center;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3N32blGhaMSroAdnNjzSkBjiaMzialyUOicNjASqh3Ye3JefAcvvBibUXCQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><img src="" alt=""><span style="font-size:16px;color:rgb(62,71,83);">其中<em>x<sub>u&nbsp;</sub></em>和<em>y<sub>i&nbsp;</sub></em>都是<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">k&nbsp;</span></em>维的列向量，<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">k&nbsp;</span></em>为隐变量的个数，</span><img src="" alt=""></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdumAiaGBeYV4buMwYINGBrYpzPW1W1gFicF6SBh2Nxf0ibZvHzEicnDian6g/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">是所有<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>构成的矩阵，</span><img src="" alt=""></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdQYicBb9icr3gApJo51kXFmeWXYIP48rXibum80VVwq9hRqHzT85DpWG8Q/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">为所有<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>构成的矩阵，<em>N&nbsp;</em>为用户数，<em>M&nbsp;</em>为item数，λ为正则化参数。目标函数里的内积</span><img src="" alt=""><span style="font-size:16px;color:rgb(62,71,83);">用于表示用户对物品的预测偏好，拟合实际偏好</span><span style="color:rgb(62,71,83);font-size:16px;"><em>p<sub>ui</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">，拟合强度由</span><span style="color:rgb(62,71,83);font-size:16px;"><em>c<sub>ui&nbsp;&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">控制。并且对于<em style="color:rgb(62,71,83);font-size:16px;">p<sub>ui&nbsp;</sub></em>=0的项也要拟合。目标函数中的第二项是正则项，用于保证数值计算稳定性以及防止过拟合。</span><br></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(0,0,0);"><strong><span style="color:rgb(0,0,0);font-size:16px;">3.2 求解方法</span></strong></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">目标函数的求解仍然可以采用交替最小二乘法。具体如下：</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(0,0,0);">(1)<em>Y&nbsp;</em>固定为上一步迭代值或初始化值，优化<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">X&nbsp;</span></em>：</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">此时，<em>Y&nbsp;</em>被当做常数处理，目标函数被分解为多个独立的子目标函数，每个子目标函数都是某个<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>的函数。对于用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>，目标函数为：</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><span style="font-size:16px;color:rgb(62,71,83);"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3v0gyWxL0oabPeM8EVUEeVG4J8ZmfSo3koyiaseKg51yXTcn0yWbjibqw/640?wx_fmt=png" alt="640?wx_fmt=png"></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><img src="" alt=""><span style="color:rgb(62,71,83);font-size:16px;">把这个目标函数化为矩阵形式，得</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3Dsj0TjWVJwE9WdHeKlB91POaa2BkYnQxUjNJNqZ1MBlZRVUDyKFcvg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">其中，</span><img src="" alt=""></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdiaqezficTzyg3s1nzTcfJxOeUhmj3aTEOfa8icIWGbG2DmIVbic2icibTIoA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">为用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对每个物品的偏好构成的列向量，</span><img src="" alt=""></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdRmIQCxRQz0TPGuoSPRzPETnqY9w5N5X2icunjsswtMTkq9QMocrhoyg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">表示所有物品向量构成的矩阵，</span><img src="" alt=""><em style="text-align:center;"><span style="font-size:16px;">Λ<sub>u&nbsp;</sub></span></em><span style="color:rgb(62,71,83);font-size:16px;">为用户<em><span style="font-size:14px;">u&nbsp;</span></em>对所有物品的置信度</span><em style="color:rgb(62,71,83);font-size:16px;">c<sub>ui&nbsp;</sub></em><span style="color:rgb(62,71,83);font-size:16px;">构成的对角阵，即：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3NwtK8iaIGH9fAhN3zZ7ysGe5WiaqE4DSLy1lprsdyKrLv7QTNeH1cZSA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">对目标函数<em>J&nbsp;</em>关于<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>求梯度，并令梯度为零，得：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3UEjdeck8Mku3n2EPAO5OOOntia3R7RN7T76VtFue7lZWjKAvBr6oDNg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p>解这个线性方程组，可得到<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>的解析解为：</p>
   <p style="text-align:center;"><img style="text-align:center;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU36fnvlAoRYIXWll4Ns8Q5CBkdQXMRAW5GZiaN8UFhhLWddeJP70ibvVnQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="text-align:center;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(0,0,0);">(2) <em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">X&nbsp;</span></em>固定为上一步迭代值或初始化值，优化<em>Y</em>：</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">此时，<em>X&nbsp;</em>被当做常数处理，目标函数也被分解为多个独立的子目标函数，每个子目标函数都是关于某个<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>的函数。通过同样的推导方法，可以得到<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>的解析解为：</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img src="" alt=""><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3wZGScvtecU6Vbbq7siaGc9XJCwcEic3pLicQZqkgyuYfoia2qiaM7gjcgrg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">其中，</span><img src="" alt=""></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdC5sicpUxOfaqGAEJhN0CRtoNUw6KcPBkglRNGr6CKKG7XCpW4UQgVnA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">为所有用户对物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>的偏好构成的向量，</span><img src="" alt=""></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdjRgSuAGN0xYOdN35yCfG0GakEzdwoIGj8L9gWh0Dm7wOCib27Fc8iawg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">表示所有用户的向量构成的矩阵，</span><span style="color:rgb(62,71,83);font-size:17px;"><em><span style="color:rgb(62,71,83);font-family:Calibri, sans-serif;">Λ<sub>i</sub></span></em></span><span style="color:rgb(62,71,83);font-size:16px;"><em><span style="color:rgb(62,71,83);font-family:Calibri, sans-serif;"><sub>&nbsp;</sub></span></em>为所有用户对物品</span><em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em><span style="color:rgb(62,71,83);font-size:16px;">的偏好的置信度构成的对角矩阵，即</span></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdicicMUb8a4PPCvOFkKE1r77lLhK41DIZlSib8BRlv94JUCeo8lXKeGA3w/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:rgb(0,0,0);"><strong><span style="color:rgb(0,0,0);font-size:16px;">3.3 工程实现</span></strong></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">由于固定<em>Y&nbsp;</em>时，各个<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>的求解都是独立的，所以在固定<em>Y&nbsp;</em>时可以并行计算各个<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u</sub></em>，同理，在固定X时可以并行计算各个<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>。</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">在计算<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>和<em style="color:rgb(62,71,83);font-size:16px;">y<sub>i&nbsp;</sub></em>时，如果直接用上述解析解的表达式来计算，复杂度将会很高。以<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>的表达式来说，<em>Y&nbsp;<strong><span style="font-size:16px;font-family:'宋体';"></span></strong><em style="text-align:center;"><span style="font-size:16px;">Λ<sub>u&nbsp;</sub></span></em>Y<sup>T&nbsp;</sup></em>这一项就涉及到所有物品的向量，少则几十万，大则上千万，而且每个用户的</span><img src="" alt=""><span style="font-size:16px;color:rgb(62,71,83);">都不一样，每个用户都算一遍时间上不可行。所以，这里要先对<em>x<sub>u&nbsp;</sub></em>的表达式化简，降低复杂度。</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">注意到<em style="color:rgb(62,71,83);"><span style="font-family:Calibri, sans-serif;">Λ<sub>i&nbsp;</sub></span></em></span><span style="color:rgb(62,71,83);font-size:16px;">的特殊性，它是由置信度构成的对角阵，对于一个用户来说，由于大部分物品都没有评分，以此<em style="color:rgb(62,71,83);"><span style="font-family:Calibri, sans-serif;">Λ<sub>i&nbsp;</sub></span></em></span><span style="font-size:16px;color:rgb(62,71,83);">对角线中大部分元素都是1，利用这个特点，我们可以把</span><img src="" alt=""><em style="color:rgb(62,71,83);"><span style="font-family:Calibri, sans-serif;">Λ<sub>i&nbsp;</sub></span></em><span style="color:rgb(62,71,83);font-size:16px;">拆成两部分的和，即</span></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSd9FmYCUwwS1re7EJm9NdsXmPT8WOdHzAHWmNfBtaxbfnSrVwZib6FB1A/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">其中I为单位阵，<em style="font-size:16px;"><span style="color:rgb(62,71,83);">Λ<sub>u</sub></span></em></span><em><sub><span style="color:#3E4753;">&nbsp;</span></sub></em><em><span style="color:#3E4753;">- I&nbsp;</span></em><span style="font-size:16px;color:rgb(62,71,83);">为对角阵，并且对角线上大部分元素为0，于是，</span><img src="" alt=""><span style="font-size:16px;color:rgb(62,71,83);">可以重写为如下形式：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3VmC4lOzmBLlhxHYxmHibPicXZyHDnzFI2gPvbVVia7EZHcKeFiaNClpDkw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">分解成这两项之后，第一项</span><img src="" alt=""><span style="font-size:16px;"><em>Y Y<sup>T&nbsp;</sup></em></span><span style="font-size:16px;color:rgb(62,71,83);">对每个用户都是一样的，只需要计算一次，存起来，后面可以重复利用，对于第二项，由于<em><span style="color:rgb(62,71,83);">Λ</span></em><em><sub><span style="color:rgb(62,71,83);">u&nbsp;</span></sub></em><em><span style="color:rgb(62,71,83);">- I&nbsp;</span></em></span><span style="color:rgb(62,71,83);font-size:16px;">为对角线大部分是0的对角阵，所以计算<em>Y&nbsp;</em>(</span><em><span style="color:#3E4753;"><em style="color:rgb(62,71,83);font-size:16px;">Λ</em><em style="color:rgb(62,71,83);font-size:16px;"><sub>u&nbsp;</sub></em><em style="color:rgb(62,71,83);font-size:16px;">- I&nbsp;</em></span></em><span style="color:rgb(62,71,83);font-size:16px;">)<em>Y<sup>T&nbsp;</sup></em>的复杂度是</span><span style="font-size:16px;color:rgb(62,71,83);"><em>O</em>(<em>k<sup>2</sup>n<sub>u</sub></em>)<em>。</em>其中<em>n<sub>u&nbsp;</sub></em>是</span><img src="" alt=""><em><span style="color:#3E4753;"></span></em><em style="color:rgb(62,71,83);font-size:16px;">Λ</em><em style="color:rgb(62,71,83);font-size:16px;"><sub>u&nbsp;</sub></em><em style="color:rgb(62,71,83);font-size:16px;">- I&nbsp;</em><em><span style="color:#3E4753;"></span></em><span style="font-size:16px;color:rgb(62,71,83);">中非零元的个数，也就是用户<em><span style="font-size:14px;">u&nbsp;</span></em>评过分的物品数，通常不会很多，所以整个</span><span style="font-size:16px;"><em><span style="color:rgb(62,71,83);">Y&nbsp;Λ<sub>u</sub>Y<sup>T</sup></span></em></span><em style="color:rgb(62,71,83);font-size:16px;"></em><span style="color:rgb(62,71,83);font-size:16px;">的计算复杂度由<em>O</em>(<em>k<sup>2</sup>M</em>)&nbsp;降为<em>O</em>(<em>k<sup>2</sup>n<sub>u</sub></em>)<em>。</em>由于</span><em><span style="color:rgb(62,71,83);font-size:16px;">M&gt;&gt;</span><em style="color:rgb(62,71,83);font-size:16px;">n<sub>u</sub></em></em><span style="color:rgb(62,71,83);font-size:16px;">，所以计算速度大大加快。对于</span><em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em><span style="color:rgb(62,71,83);font-size:16px;">表达式的</span><span style="font-size:16px;"><em style="text-indent:28px;">Y&nbsp;</em></span><span style="font-size:16px;"><em style="color:rgb(62,71,83);font-size:16px;"><em style="text-align:center;">Λ<sub>u&nbsp;</sub></em></em><em style="text-indent:28px;"></em><em style="text-indent:28px;">P<sub>u</sub></em></span><img src="" alt=""><span style="font-size:16px;color:rgb(62,71,83);">这一项，则应</span><span style="color:rgb(62,71,83);font-size:16px;"><em>Y&nbsp;</em>(<em><strong><span style="font-family:'宋体';"></span></strong><em style="color:rgb(62,71,83);font-size:16px;">&nbsp;<em style="text-align:center;">Λ<sub>u&nbsp;</sub></em></em>P<sub>u</sub></em>)<em>&nbsp;</em>这样计算，利用</span><span style="font-size:16px;color:rgb(62,71,83);"><em>P<sub>u&nbsp;</sub></em></span><span style="color:rgb(62,71,83);font-size:16px;">中大部分元素是0的特点，将计算复杂度由<em>O</em>(<em>kM</em>&nbsp;)&nbsp;降低到<em>O</em>(<em>kn<sub>u</sub></em>)。通过使用上述数学技巧，整个<em>x<sub>u</sub></em>的计算复杂度可以降低到</span><span style="font-size:16px;color:rgb(62,71,83);"><em>O</em>(<em><span style="font-size:14px;">k</span><sup>2</sup>n<sub>u</sub>+<span style="font-size:14px;">k</span><sup>3</sup></em>)</span><span style="color:rgb(62,71,83);font-size:16px;">，其中<em>n<sub>u</sub></em>是有评分的用户数或物品数，<em><span style="font-size:14px;">k&nbsp;</span></em>为隐变量个数，完全满足在线计算的需求。</span></p>
   <p><br></p>
   <strong>4.增量矩阵分解算法</strong>
   <br>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">无论是显式矩阵分解，还是隐式矩阵分解，我们在模型训练完以后，就会得到训练集里每个用户的向量和每个物品的向量。假设现在有一个用户，在训练集里没出现过，但是我们有他的历史行为数据，那这个用户的向量该怎么计算呢？当然，最简单的方法就是把这个用户的行为数据合并到旧的训练集里，重新做一次矩阵分解，进而得到这个用户的向量，但是这样做计算代价太大了，在时间上不可行。</span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">为了解决训练数据集以外的用户(我们称之为新用户)的推荐问题，我们就需要用到增量矩阵分解算法。增量矩阵分解算法能根据用户历史行为数据，在不重算所有用户向量的前提下，快速计算出新用户向量。</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">在交替最小二乘法里，当固定<em>Y&nbsp;</em>计算<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>时，我们只需要用到用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>的历史行为数据<em>r<sub>ui&nbsp;</sub></em>以及<em>Y&nbsp;</em>的当前值，不同用户之间<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u</sub></em>的计算是相互独立的。这就启发我们，对于训练集以外的用户，我们同样可以用他的历史行为数据以及训练集上收敛时学到的<em>Y</em>，来计算新用户的用户向量。下面的图2表示了这一过程。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/b96CibCt70iab2ZoTpy1PORwmsB4kEyhNuibiaiaRdtoCbsGh8Xib5uOpMrm6If2n8CB7d0u2l2scW52XnF1TJ7ibX83A/640" alt="640"></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><span style="color:rgb(62,71,83);font-size:14px;">增量矩阵分解</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">设用户历史行为数据为<em>P</em><em><sub>u</sub></em>={<em>P</em><em><sub>ui&nbsp;</sub></em>}，训练集上学到的物品矩阵为<em>Y</em>，要求解的用户向量为<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u</sub></em>，则增量矩阵分解算法求解的目标为：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3oyudB2afcgsQmFEs9Ybtk7IKMVCaZtyhkh9X4yhZibdyHZrnATBxwPQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">这个目标函数跟第3节中固定<em>Y&nbsp;</em>时求解<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>的目标函数是一样的，但有两个不同点：</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">(1)这里的<em>Y&nbsp;</em>是不需要迭代的，它是MF在训练集上收敛时得到的<em>Y</em>；</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">(2)用户的历史行为数据</span><em><span style="font-size:16px;color:rgb(62,71,83);">P</span><span style="color:rgb(62,71,83);font-size:16px;"><sub>u&nbsp;</sub></span></em><span style="font-size:16px;color:rgb(62,71,83);">要过滤掉在Y中没出现过的物品。由于<em>Y&nbsp;</em>是固定的，我们不需要迭代，直接通过<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u&nbsp;</sub></em>的解析表达式求解<em style="color:rgb(62,71,83);font-size:16px;">x<sub>u</sub></em>，即：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3TM873pxQEpoPIt7ia1Pcp4ibY6QnnWvRHqvH1pPy3Bp1dN9slZXaicYEA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">式中的所有符号和上一节相同。</span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">事实上，增量矩阵分解的目标函数中的<em>Y&nbsp;</em>也不一定要是MF在训练集上学出来的，只要<em>Y&nbsp;</em>中的每个向量都能表示对应物品的特征就行，也就是说，<em>Y&nbsp;</em>可以是由其他数据和其他算法事先学出来的。矩阵分解的增量算法在图文推荐系统中有着广泛应用，具体的应用将在下一篇文章中介绍。</span></p>
   <p><br></p>
   <strong>5.推荐结果的可解释性</strong>
   <br>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">好的推荐算法不仅要推得准确，而且还要有良好的可解释性，也就是根据什么给用户推荐了这个物品。传统的ItemCF算法就有很好的可解释性，因为在ItemCF中，用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">i&nbsp;</span></em>的预测评分<em><span style="font-size:14px;">R</span></em><span style="font-size:14px;">&nbsp;(<em>u, i&nbsp;</em>)&nbsp;</span>的计算公式为</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3jqk0T2l7eHK0HwKnMRn10mVdqGGwHWSicsLRwKbH94CD8XHX9DeByuA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">其中<em>N</em>(<em>u&nbsp;</em>)<em>&nbsp;</em>表示用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>有过行为的物品集合，</span><span style="color:rgb(62,71,83);font-size:16px;"><em>r<sub>uj&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">表示用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品<em><span style="font-size:14px;">j&nbsp;</span></em>的历史评分，<em>s<sub>ji&nbsp;</sub></em>表示物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">j&nbsp;</span></em>和物品<em style="color:rgb(62,71,83);font-size:14px;">i&nbsp;</em>的相似度。在这个公式中，<em>N</em>(<em>u&nbsp;</em>)&nbsp;中的物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">j&nbsp;</span></em>对<em>R</em>(<em>u</em>, <em>i </em>)<em>&nbsp;</em>的贡献为<em>r<sub>uj&nbsp;</sub>s<sub>ji</sub></em>，因此可以很好地解释物品<em style="color:rgb(62,71,83);font-size:14px;">i&nbsp;</em>具体是由<em>N</em>(<em>u</em>)&nbsp;中哪个物品推荐而来。那对于矩阵分解算法来说，是否也能给出类似的可解释性呢？答案是肯定的。</span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">以隐式矩阵分解为例，我们已经推导出，已知物品的矩阵<em>Y&nbsp;</em>时，用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>的向量的计算表达式为：</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img src="" alt=""><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3kkv7vS1sQ2UtScBpYzgJZbFIVSdvMuaibNBFVZianz0KuznGK5ycVIIA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">假设物品<em style="color:rgb(62,71,83);font-size:14px;">i&nbsp;</em>的向量为</span><span style="color:rgb(62,71,83);font-size:16px;"><em>y<sub>i</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">，那么用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对物品<em style="color:rgb(62,71,83);font-size:14px;">i&nbsp;</em>的预测评分为：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3XcKSJG5K9X6w6I5LyFHxsrD7oDD3zkdGPLIsHTNmgROqGL7EibYMe9Q/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">令</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img class="rich_pages" style="text-align:center;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdW8DCibrdQibO863p9mQ8kMA3RaZcPuyDgPrErfmfQ208MWp57g4nErRQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">并把</span><img src="" alt=""><span style="font-size:16px;"><em>Y&nbsp;<em style="font-size:16px;"><span style="color:rgb(62,71,83);">Λ<sub>u&nbsp;</sub></span></em>P<sub>u&nbsp;</sub></em></span><span style="font-size:16px;color:rgb(62,71,83);">展开来写，则</span><img src="" alt=""><span style="font-size:16px;color:rgb(62,71,83);">的表达式可以写成</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img src="" alt=""><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/WNTtYW98ZSHyYeJ4PR4JlIZCPib6BkMU3rKiav2zsKmDeR7TcFLFbJfIdhz4LFZqLA6CQXeG8fia6swQacKqqfcUQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">其中，</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img class="rich_pages" style="text-align:center;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdtOq8lFCgys8Bs2bISjd9VBBBKF9Aw3TAicPl65ZTyMZlmMiaC28luUew/640?wx_fmt=png" alt="640?wx_fmt=png"><img src="" alt=""></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">可以看成是物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">j&nbsp;</span></em>和物品<em style="color:rgb(62,71,83);font-size:14px;">i&nbsp;</em>之间的相似度，</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><img class="rich_pages" style="text-align:center;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauEZOibGZq5otpJ41WwPKOSdhrezGhKSjJdOOuTPKhH58wqzV8xzewN89icZom2LWaeMuerVGYJqWDw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;"><img src="" alt=""><span style="font-size:16px;color:rgb(62,71,83);">可以看成是用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>对用户j的评分，这样就能像ItemCF那样去解释<em>N</em>(<em>u&nbsp;</em>)中每一项对推荐物品<em>i&nbsp;</em>的贡献了。从<em>s<sub>ji&nbsp;</sub></em>的计算表达式中，我们还可以看到，物品<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">j&nbsp;</span></em>和物品<em style="color:rgb(62,71,83);font-size:14px;">i&nbsp;</em>之间的相似度<em style="color:rgb(62,71,83);font-size:16px;">s<sub>ji&nbsp;</sub></em>是跟用户<em style="color:rgb(62,71,83);font-size:16px;"><span style="font-size:14px;">u&nbsp;</span></em>有关系的，也就是说，即使是相同的两个物品，在不同用户看来，它们的相似度是不一样的，这跟ItemCF的固定相似度有着本质上的区别，MF的相似度看起来更合理一些。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"></span></p>
   <strong>6.小结</strong>
   <br>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">(1)根据用户行为数据的特点，矩阵分解又分为显式矩阵分解和隐式矩阵分解两种；</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">(2)在显式MF算法中，用户向量和物品向量的内积拟合的是用户对物品的实际评分，并且只拟合有评分的项；</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">(3)在隐式MF算法中，用户向量和物品向量的内积拟合的是用户对物品的偏好(0或1)，拟合的强度由置信度控制，置信度又由行为的强度决定；</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">(4)在隐式MF中，需要使用一些数学技巧降低计算复杂度，才能满足线上实时计算的性能要求；</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">(5)对于有行为数据，但不在训练集里的用户，可以使用增量MF算法计算出他的用户向量，进而为他做推荐；</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">(6)MF算法也能像ItemCF一样，能给出推荐的理由，具有良好的可解释性。</span></p>
   <p><br></p>
   <strong>参考文献</strong>
   <br>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[1] 项亮. 推荐系统实践. 北京: 人民邮电出版社. 2012.</span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[2] Sarwar B M, Karypis G, Konstan J A, et al. Item-based collaborative filtering recommendation algorithms. WWW. 2001, 1: 285-295.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[3] Linden G, Smith B, York J. Amazon. com recommendations: Item-to-item collaborative filtering. IEEE Internet computing. 2003 (1): 76-80.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[4] Koren Y, Bell R, Volinsky C. Matrix factorization techniques for recommender systems. Computer. 2009 (8): 30-37.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[5] Koren Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model. SIGKDD. 2008: 426-434.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[6] Hu Y, Koren Y, Volinsky C. Collaborative filtering for implicit feedback datasets. ICDM. 2008: 263-272.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[7] Mnih A, Salakhutdinov R R. Probabilistic matrix factorization. NIPS. 2008: 1257-1264.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[8] Koren Y. Collaborative filtering with temporal dynamics. SIGKDD. 2009: 447-456.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[9] Pilászy I, Zibriczky D, Tikk D. Fast als-based matrix factorization for explicit and implicit feedback datasets. RecSys. 2010: 71-78.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[10] Johnson C C. Logistic matrix factorization for implicit feedback data. NIPS, 2014, 27.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[11] He X, Zhang H, Kan M Y, et al. Fast matrix factorization for online recommendation with implicit feedback. SIGIR. 2016: 549-558.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[12] Hofmann T. Probabilistic latent semantic analysis. UAI. 1999: 289-296.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[13] Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation. JMLR. 2003, 3(Jan): 993-1022.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[14] Zhang S, Yao L, Sun A, et al. Deep learning based recommender system: A survey and new perspectives. ACM Computing Surveys (CSUR). 2019, 52(1): 5.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[15] Mikolov, Tomas &amp; Chen, Kai &amp; Corrado, G.s &amp; Dean, Jeffrey. Efficient Estimation of Word Representations in Vector Space. Proceedings of Workshop at ICLR. 2013.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[16] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality. NIPS. 2013: 3111-3119.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[17] Pennington J, Socher R, Manning C. Glove: Global vectors for word representation. EMNLP. 2014: 1532-1543.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[18] Covington P, Adams J, Sargin E. Deep neural networks for youtube recommendations. RecSys. 2016: 191-198.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[19] Chen T, Guestrin C. Xgboost: A scalable tree boosting system. SIGKDD. 2016: 785-794.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);">[20] Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree. NIPS. 2017: 3146-3154.</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:#3e4753;"><span style="font-size:15px;">更多精彩阅读：</span></span></p>
   <p style="text-align:center;"><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&amp;mid=2649742233&amp;idx=1&amp;sn=f97cb1fcb3c4182168bdc5cef33bc9d3&amp;chksm=bed348e289a4c1f40de9869c720cc9be2a027ca6919234fdfccec43d78c4b0e289296b630ca5&amp;token=881863062&amp;lang=zh_CN&amp;scene=21#wechat_redirect" rel="nofollow"><span class="js_jump_icon h5_image_link"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatVcf3ORbLibCzk9qA9CwAe8Lvqfyoxnn851LaMMWuvNFOFyo1ktaFiamh4KLlwjVv6JXZ5mB2sNuTA/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></span></a></p>
   <h2 class="rich_media_title" style="font-size:22px;line-height:1.4;letter-spacing:.544px;text-align:center;"><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&amp;mid=2649742233&amp;idx=1&amp;sn=f97cb1fcb3c4182168bdc5cef33bc9d3&amp;chksm=bed348e289a4c1f40de9869c720cc9be2a027ca6919234fdfccec43d78c4b0e289296b630ca5&amp;token=881863062&amp;lang=zh_CN&amp;scene=21#wechat_redirect" rel="nofollow"><span style="font-size:15px;color:rgb(62,71,83);">科普篇 | 推荐系统之矩阵分解模型</span></a></h2>
   <p style="margin-left:8px;line-height:1.75em;"><span style="color:#3e4753;"><span style="font-size:16px;"></span></span><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"><br></span></p>
   <p style="text-align:center;"><img class="rich_pages" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/WNTtYW98ZSFicUFwibpI7bCG5SYxEVBnicjnPC4UunOGLic1tGMX1Wia4qJvicosicy3QBUefYQP2htTiaxOoYQH96mWRA/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:16px;color:rgb(62,71,83);"></span><br></p> 
  </div> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
