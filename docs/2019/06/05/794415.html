<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Datax 二次开发插件详细过程 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Datax 二次开发插件详细过程" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="目录 &nbsp; 1.背景 2.需求 3.开发步骤 &nbsp; 3.1&nbsp;去github上下载datax的代码 &nbsp;3.2 本地解压，并导入idea 3.3创建一个模块kafkareader 3.4将任意一个模块的以下两个文件考入到resource目录下 3.5进行修改plugin.json 3.6修改pom.xml(复制其中一个文件的依赖和插件到pom.xml) 3.7将其他模块下面的，这个文件夹复制到我们模块的对应的文件夹，并且修改package.xml ​ 3.8 在最外层的package.xml加上下面这个 4.开发代码 4.1 开发前将datax的开发插件的手册认真观看一遍，对开发有帮助的。 4.2编写代码（要继承什么类实现什么方法，4.1的开发宝典上都写了） 5.打包运行 5.1将其他模块注释只留下公共模块和自己的项目模块 5.2 进入到项目最外层的目录输入cmd（前提配置了本地maven的环境变量） 5.3 使用maven命令打包 5.4 打包后将下图目录下的包上传到集群的datax对应目录 5.5 写好配置文件就可以运行了 1.背景 &nbsp; &nbsp; 公司要求：统一入库平台，使用datax这个工具。需要采集kafka，elasticsearch，mysql，sqlserver等数据源的数据，并且只打算用datax。 2.需求 &nbsp;开发datax的kafkaReader组件，从kafka数据源读取数据，然后同步到其他的数据。 &nbsp;1.要求：可以同步json格式的数据，要求可以用正则来解析数据，可以指定数据的分隔符来解析数据。 &nbsp;2.可以同步到hive，mysql，hbase中 3.开发步骤 &nbsp; 3.1&nbsp;去github上下载datax的代码 &nbsp;3.2 本地解压，并导入idea 3.3创建一个模块kafkareader 3.4将任意一个模块的以下两个文件考入到resource目录下 3.5进行修改plugin.json 3.6修改pom.xml(复制其中一个文件的依赖和插件到pom.xml) 3.7将其他模块下面的，这个文件夹复制到我们模块的对应的文件夹，并且修改package.xml 要修改的我已经注释好了。就是把你之前复制过来的reader修改为kafkareader &lt;assembly xmlns=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd&quot;&gt; &lt;id&gt;&lt;/id&gt; &lt;formats&gt; &lt;format&gt;dir&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;plugin.json&lt;/include&gt; &lt;include&gt;plugin_job_template.json&lt;/include&gt; &lt;/includes&gt; //修改为kafkareader &lt;outputDirectory&gt;plugin/reader/kafkareader&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;fileSet&gt; &lt;directory&gt;target/&lt;/directory&gt; &lt;includes&gt; //修改为kafkareader &lt;include&gt;kafkareader-0.0.1-SNAPSHOT.jar&lt;/include&gt; &lt;/includes&gt; //修改为kafkareader &lt;outputDirectory&gt;plugin/reader/kafkareader&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;!--&lt;fileSet&gt;--&gt; &lt;!--&lt;directory&gt;src/main/cpp&lt;/directory&gt;--&gt; &lt;!--&lt;includes&gt;--&gt; &lt;!--&lt;include&gt;libhadoop.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadoop.so&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadoop.so.1.0.0&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadooppipes.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadooputils.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhdfs.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhdfs.so&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhdfs.so.0.0.0&lt;/include&gt;--&gt; &lt;!--&lt;/includes&gt;--&gt; &lt;!--&lt;outputDirectory&gt;plugin/reader/hdfsreader/libs&lt;/outputDirectory&gt;--&gt; &lt;!--&lt;/fileSet&gt;--&gt; &lt;/fileSets&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; //修改为kafkareader &lt;outputDirectory&gt;plugin/reader/kafkareader/libs&lt;/outputDirectory&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;/assembly&gt; 3.8 在最外层的package.xml加上下面这个 4.开发代码 4.1 开发前将datax的开发插件的手册认真观看一遍，对开发有帮助的。 地址：https://github.com/alibaba/DataX/blob/master/dataxPluginDev.md 4.2编写代码（要继承什么类实现什么方法，4.1的开发宝典上都写了） 懒得贴那么多代码了，就贴一个关键代码。需要整个项目的话在下方留言 package com.alibaba.datax.plugin.reader.kafkareader; import com.alibaba.datax.common.element.Record; import com.alibaba.datax.common.element.StringColumn; import com.alibaba.datax.common.exception.DataXException; import com.alibaba.datax.common.plugin.RecordSender; import com.alibaba.datax.common.spi.Reader; import com.alibaba.datax.common.util.Configuration; import com.alibaba.datax.plugin.rdbms.reader.CommonRdbmsReader; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.io.File; import java.io.FileNotFoundException; import java.io.FileOutputStream; import java.io.IOException; import java.util.*; import java.util.regex.Matcher; import java.util.regex.Pattern; public class KafkaReader extends Reader { public static class Job extends Reader.Job { private static final Logger LOG = LoggerFactory .getLogger(Job.class); private Configuration originalConfig = null; @Override public void init() { this.originalConfig = super.getPluginJobConf(); // warn: 忽略大小写 String topic = this.originalConfig .getString(Key.TOPIC); Integer partitions = this.originalConfig .getInt(Key.KAFKA_PARTITIONS); String bootstrapServers = this.originalConfig .getString(Key.BOOTSTRAP_SERVERS); String groupId = this.originalConfig .getString(Key.GROUP_ID); Integer columnCount = this.originalConfig .getInt(Key.COLUMNCOUNT); String split = this.originalConfig.getString(Key.SPLIT); String filterContaintsStr = this.originalConfig.getString(Key.CONTAINTS_STR); String filterContaintsFlag = this.originalConfig.getString(Key.CONTAINTS_STR_FLAG); String conditionAllOrOne = this.originalConfig.getString(Key.CONDITION_ALL_OR_ONE); String parsingRules = this.originalConfig.getString(Key.PARSING_RULES); String writerOrder = this.originalConfig.getString(Key.WRITER_ORDER); String kafkaReaderColumnKey = this.originalConfig.getString(Key.KAFKA_READER_COLUMN_KEY); System.out.println(topic); System.out.println(partitions); System.out.println(bootstrapServers); System.out.println(groupId); System.out.println(columnCount); System.out.println(split); System.out.println(parsingRules); if (null == topic) { throw DataXException.asDataXException(KafkaReaderErrorCode.TOPIC_ERROR, &quot;没有设置参数[topic].&quot;); } if (partitions == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR, &quot;没有设置参数[kafka.partitions].&quot;); } else if (partitions &lt; 1) { throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR, &quot;[kafka.partitions]不能小于1.&quot;); } if (null == bootstrapServers) { throw DataXException.asDataXException(KafkaReaderErrorCode.ADDRESS_ERROR, &quot;没有设置参数[bootstrap.servers].&quot;); } if (null == groupId) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置参数[groupid].&quot;); } if (columnCount == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR, &quot;没有设置参数[columnCount].&quot;); } else if (columnCount &lt; 1) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;[columnCount]不能小于1.&quot;); } if (null == split) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;[split]不能为空.&quot;); } if (filterContaintsStr != null) { if (conditionAllOrOne == null || filterContaintsFlag == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;设置了[filterContaintsStr],但是没有设置[conditionAllOrOne]或者[filterContaintsFlag]&quot;); } } if (parsingRules == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置[parsingRules]参数&quot;); } else if (!parsingRules.equals(&quot;regex&quot;) &amp;&amp; parsingRules.equals(&quot;json&quot;) &amp;&amp; parsingRules.equals(&quot;split&quot;)) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;[parsingRules]参数设置错误，不是regex，json，split其中一个&quot;); } if (writerOrder == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置[writerOrder]参数&quot;); } if (kafkaReaderColumnKey == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置[kafkaReaderColumnKey]参数&quot;); } } @Override public void preCheck() { init(); } @Override public List&lt;Configuration&gt; split(int adviceNumber) { List&lt;Configuration&gt; configurations = new ArrayList&lt;Configuration&gt;(); Integer partitions = this.originalConfig.getInt(Key.KAFKA_PARTITIONS); for (int i = 0; i &lt; partitions; i++) { configurations.add(this.originalConfig.clone()); } return configurations; } @Override public void post() { } @Override public void destroy() { } } public static class Task extends Reader.Task { private static final Logger LOG = LoggerFactory .getLogger(CommonRdbmsReader.Task.class); //配置文件 private Configuration readerSliceConfig; //kafka消息的分隔符 private String split; //解析规则 private String parsingRules; //是否停止拉去数据 private boolean flag; //kafka address private String bootstrapServers; //kafka groupid private String groupId; //kafkatopic private String kafkaTopic; //kafka中的数据一共有多少个字段 private int count; //是否需要data_from //kafka ip 端口+ topic //将包含/不包含该字符串的数据过滤掉 private String filterContaintsStr; //是包含containtsStr 还是不包含 //1 表示包含 0 表示不包含 private int filterContaintsStrFlag; //全部包含或不包含，包含其中一个或者不包含其中一个。 private int conditionAllOrOne; //writer端要求的顺序。 private String writerOrder; //kafkareader端的每个关键子的key private String kafkaReaderColumnKey; //异常文件路径 private String exceptionPath; @Override public void init() { flag = true; this.readerSliceConfig = super.getPluginJobConf(); split = this.readerSliceConfig.getString(Key.SPLIT); bootstrapServers = this.readerSliceConfig.getString(Key.BOOTSTRAP_SERVERS); groupId = this.readerSliceConfig.getString(Key.GROUP_ID); kafkaTopic = this.readerSliceConfig.getString(Key.TOPIC); count = this.readerSliceConfig.getInt(Key.COLUMNCOUNT); filterContaintsStr = this.readerSliceConfig.getString(Key.CONTAINTS_STR); filterContaintsStrFlag = this.readerSliceConfig.getInt(Key.CONTAINTS_STR_FLAG); conditionAllOrOne = this.readerSliceConfig.getInt(Key.CONTAINTS_STR_FLAG); parsingRules = this.readerSliceConfig.getString(Key.PARSING_RULES); writerOrder = this.readerSliceConfig.getString(Key.WRITER_ORDER); kafkaReaderColumnKey = this.readerSliceConfig.getString(Key.KAFKA_READER_COLUMN_KEY); exceptionPath = this.readerSliceConfig.getString(Key.EXECPTION_PATH); LOG.info(filterContaintsStr); } @Override public void startRead(RecordSender recordSender) { Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, bootstrapServers); props.put(&quot;group.id&quot;, groupId != null ? groupId : UUID.randomUUID().toString()); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); consumer.subscribe(Collections.singletonList(kafkaTopic)); Record oneRecord = null; while (flag) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { String value = record.value(); //定义过滤标志 int ifNotContinue = filterMessage(value); //如果标志修改为1了那么就过滤掉这条数据。 if (ifNotContinue == 1) { LOG.info(&quot;过滤数据： &quot; + record.value()); continue; } oneRecord = buildOneRecord(recordSender, value); //如果返回值不等于null表示不是异常消息。 if (oneRecord != null) { recordSender.sendToWriter(oneRecord); } } consumer.commitSync(); //判断当前事件是不是0点,0点的话进程他退出 Date date = new Date(); if (DateUtil.targetFormat(date).split(&quot; &quot;)[1].substring(0, 2).equals(&quot;00&quot;)) { destroy(); } } } private int filterMessage(String value) { //如果要过滤的条件配置了 int ifNotContinue = 0; if (filterContaintsStr != null) { String[] filterStrs = filterContaintsStr.split(&quot;,&quot;); //所有 if (conditionAllOrOne == 1) { //过滤掉包含filterContaintsStr的所有项的值。 if (filterContaintsStrFlag == 1) { int i = 0; for (; i &lt; filterStrs.length; i++) { if (!value.contains(filterStrs[i])) break; } if (i &gt;= filterStrs.length) ifNotContinue = 1; } else { //留下掉包含filterContaintsStr的所有项的值 int i = 0; for (; i &lt; filterStrs.length; i++) { if (!value.contains(filterStrs[i])) break; } if (i &lt; filterStrs.length) ifNotContinue = 1; } } else { //过滤掉包含其中一项的值 if (filterContaintsStrFlag == 1) { int i = 0; for (; i &lt; filterStrs.length; i++) { if (value.contains(filterStrs[i])) break; } if (i &lt; filterStrs.length) ifNotContinue = 1; } //留下包含其中一下的值 else { int i = 0; for (; i &lt; filterStrs.length; i++) { if (value.contains(filterStrs[i])) break; } if (i &gt;= filterStrs.length) ifNotContinue = 1; } } } return ifNotContinue; } private Record buildOneRecord(RecordSender recordSender, String value) { Record record = null; if (parsingRules.equals(&quot;regex&quot;)) { record = parseRegex(value, recordSender); } else if (parsingRules.equals(&quot;json&quot;)) { record = parseJson(value, recordSender); } else if (parsingRules.equals(&quot;split&quot;)) { record = parseSplit(value, recordSender); } return record; } private Record parseSplit(String value, RecordSender recordSender) { Record record = recordSender.createRecord(); String[] splits = value.split(this.split); if (splits.length != count) { writerErrorPath(value); return null; } parseOrders(Arrays.asList(splits), record); return record; } private Record parseJson(String value, RecordSender recordSender) { Record record = recordSender.createRecord(); HashMap&lt;String, Object&gt; map = JsonUtilJava.parseJsonStrToMap(value); String[] columns = kafkaReaderColumnKey.split(&quot;,&quot;); ArrayList&lt;String&gt; datas = new ArrayList&lt;String&gt;(); for (String column : columns) { datas.add(map.get(column).toString()); } if (datas.size() != count) { writerErrorPath(value); return null; } parseOrders(datas, record); return record; } private Record parseRegex(String value, RecordSender recordSender) { Record record = recordSender.createRecord(); ArrayList&lt;String&gt; datas = new ArrayList&lt;String&gt;(); Pattern r = Pattern.compile(split); Matcher m = r.matcher(value); if (m.find()) { if (m.groupCount() != count) { writerErrorPath(value); } for (int i = 1; i &lt;= count; i++) { // record.addColumn(new StringColumn(m.group(i))); datas.add(m.group(i)); return record; } } else { writerErrorPath(value); } parseOrders(datas, record); return null; } private void writerErrorPath(String value) { if (exceptionPath == null) return; FileOutputStream fileOutputStream = null; try { fileOutputStream = getFileOutputStream(); fileOutputStream.write((value + &quot;\n&quot;).getBytes()); fileOutputStream.close(); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } private FileOutputStream getFileOutputStream() throws FileNotFoundException { return new FileOutputStream(exceptionPath + &quot;/&quot; + kafkaTopic + &quot;errordata&quot; + DateUtil.targetFormat(new Date(), &quot;yyyyMMdd&quot;), true); } private void parseOrders(List&lt;String&gt; datas, Record record) { //writerOrder String[] orders = writerOrder.split(&quot;,&quot;); for (String order : orders) { if (order.equals(&quot;data_from&quot;)) { record.addColumn(new StringColumn(bootstrapServers + &quot;|&quot; + kafkaTopic)); } else if (order.equals(&quot;uuid&quot;)) { record.addColumn(new StringColumn(UUID.randomUUID().toString())); } else if (order.equals(&quot;null&quot;)) { record.addColumn(new StringColumn(&quot;null&quot;)); } else if (order.equals(&quot;datax_time&quot;)) { record.addColumn(new StringColumn(DateUtil.targetFormat(new Date()))); } else if (isNumeric(order)) { record.addColumn(new StringColumn(datas.get(new Integer(order) - 1))); } } } public static boolean isNumeric(String str) { for (int i = 0; i &lt; str.length(); i++) { if (!Character.isDigit(str.charAt(i))) { return false; } } return true; } @Override public void post() { } @Override public void destroy() { flag = false; } } 5.打包运行 5.1将其他模块注释只留下公共模块和自己的项目模块 在最为外层的pom.xml中注释 5.2 进入到项目最外层的目录输入cmd（前提配置了本地maven的环境变量） 5.3 使用maven命令打包 &nbsp;mvn -U clean package assembly:assembly -Dmaven.test.skip=true 5.4 打包后将下图目录下的包上传到集群的datax对应目录 本地地址：D:\DataX-master\kafkareader\target\datax\plugin\reader 集群地址：/opt/module/datax/plugin/reader \ 5.5 写好配置文件就可以运行了 { &quot;job&quot;: { &quot;content&quot;: [ { &quot;reader&quot;: { &quot;name&quot;: &quot;kafkareader&quot;, &quot;parameter&quot;: { &quot;topic&quot;: &quot;Event&quot;, &quot;bootstrapServers&quot;: &quot;192.168.7.128:9092&quot;, &quot;kafkaPartitions&quot;: &quot;1&quot;, &quot;columnCount&quot;:11, &quot;groupId&quot;:&quot;ast&quot;, &quot;filterContaints&quot;:&quot;5^1,6^5&quot;, &quot;filterContaintsFlag&quot;:1, &quot;conditionAllOrOne&quot;:0, &quot;parsingRules&quot;:&quot;regex&quot;, &quot;writerOrder&quot;:&quot;uuid,1,3,6,4,8,9,10,11,5,7,2,null,datax_time,data_from&quot;, &quot;kafkaReaderColumnKey&quot;:&quot;a&quot;, &quot;execptionPath&quot;:&quot;/opt/module/datax/log/errorlog&quot; } }, &quot;writer&quot;: { &quot;name&quot;: &quot;hdfswriter&quot;, &quot;parameter&quot;: { &quot;defaultFS&quot;: &quot;hdfs://master:8020&quot;, &quot;fileType&quot;: &quot;orc&quot;, &quot;path&quot;: &quot;${path}&quot;, &quot;fileName&quot;: &quot;t_rsd_amber_agent_event_log&quot;, &quot;column&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;risk_level&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;device_uuid&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_device_id&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;device_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_sub_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;repeats&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;description&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_time&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;report_device_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_report_time&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;last_update_time&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;datax_time&quot;, &quot;type&quot;: &quot;string&quot; } , { &quot;name&quot;: &quot;data_from&quot;, &quot;type&quot;: &quot;string&quot; }, ], &quot;writeMode&quot;: &quot;append&quot;, &quot;fieldDelimiter&quot;: &quot;\t&quot;, &quot;compress&quot;:&quot;NONE&quot;, &quot;scrollFileTime&quot;:300000 } } } ], &quot;setting&quot;: { &quot;speed&quot;: { &quot;channel&quot;: 3, &quot;record&quot;: 20000, &quot;byte&quot;:5000 , &quot;batchSize&quot;:2048 } } } } 运行命令： &nbsp;python /opt/module/datax/bin/datax.py -p &quot;-Dpath=/data/warehouse/rsd/t_rsd_amber_agent_event_log/2019/06/05&quot; /opt/module/datax/job/kafkatohdfs.json" />
<meta property="og:description" content="目录 &nbsp; 1.背景 2.需求 3.开发步骤 &nbsp; 3.1&nbsp;去github上下载datax的代码 &nbsp;3.2 本地解压，并导入idea 3.3创建一个模块kafkareader 3.4将任意一个模块的以下两个文件考入到resource目录下 3.5进行修改plugin.json 3.6修改pom.xml(复制其中一个文件的依赖和插件到pom.xml) 3.7将其他模块下面的，这个文件夹复制到我们模块的对应的文件夹，并且修改package.xml ​ 3.8 在最外层的package.xml加上下面这个 4.开发代码 4.1 开发前将datax的开发插件的手册认真观看一遍，对开发有帮助的。 4.2编写代码（要继承什么类实现什么方法，4.1的开发宝典上都写了） 5.打包运行 5.1将其他模块注释只留下公共模块和自己的项目模块 5.2 进入到项目最外层的目录输入cmd（前提配置了本地maven的环境变量） 5.3 使用maven命令打包 5.4 打包后将下图目录下的包上传到集群的datax对应目录 5.5 写好配置文件就可以运行了 1.背景 &nbsp; &nbsp; 公司要求：统一入库平台，使用datax这个工具。需要采集kafka，elasticsearch，mysql，sqlserver等数据源的数据，并且只打算用datax。 2.需求 &nbsp;开发datax的kafkaReader组件，从kafka数据源读取数据，然后同步到其他的数据。 &nbsp;1.要求：可以同步json格式的数据，要求可以用正则来解析数据，可以指定数据的分隔符来解析数据。 &nbsp;2.可以同步到hive，mysql，hbase中 3.开发步骤 &nbsp; 3.1&nbsp;去github上下载datax的代码 &nbsp;3.2 本地解压，并导入idea 3.3创建一个模块kafkareader 3.4将任意一个模块的以下两个文件考入到resource目录下 3.5进行修改plugin.json 3.6修改pom.xml(复制其中一个文件的依赖和插件到pom.xml) 3.7将其他模块下面的，这个文件夹复制到我们模块的对应的文件夹，并且修改package.xml 要修改的我已经注释好了。就是把你之前复制过来的reader修改为kafkareader &lt;assembly xmlns=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd&quot;&gt; &lt;id&gt;&lt;/id&gt; &lt;formats&gt; &lt;format&gt;dir&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;plugin.json&lt;/include&gt; &lt;include&gt;plugin_job_template.json&lt;/include&gt; &lt;/includes&gt; //修改为kafkareader &lt;outputDirectory&gt;plugin/reader/kafkareader&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;fileSet&gt; &lt;directory&gt;target/&lt;/directory&gt; &lt;includes&gt; //修改为kafkareader &lt;include&gt;kafkareader-0.0.1-SNAPSHOT.jar&lt;/include&gt; &lt;/includes&gt; //修改为kafkareader &lt;outputDirectory&gt;plugin/reader/kafkareader&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;!--&lt;fileSet&gt;--&gt; &lt;!--&lt;directory&gt;src/main/cpp&lt;/directory&gt;--&gt; &lt;!--&lt;includes&gt;--&gt; &lt;!--&lt;include&gt;libhadoop.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadoop.so&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadoop.so.1.0.0&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadooppipes.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadooputils.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhdfs.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhdfs.so&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhdfs.so.0.0.0&lt;/include&gt;--&gt; &lt;!--&lt;/includes&gt;--&gt; &lt;!--&lt;outputDirectory&gt;plugin/reader/hdfsreader/libs&lt;/outputDirectory&gt;--&gt; &lt;!--&lt;/fileSet&gt;--&gt; &lt;/fileSets&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; //修改为kafkareader &lt;outputDirectory&gt;plugin/reader/kafkareader/libs&lt;/outputDirectory&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;/assembly&gt; 3.8 在最外层的package.xml加上下面这个 4.开发代码 4.1 开发前将datax的开发插件的手册认真观看一遍，对开发有帮助的。 地址：https://github.com/alibaba/DataX/blob/master/dataxPluginDev.md 4.2编写代码（要继承什么类实现什么方法，4.1的开发宝典上都写了） 懒得贴那么多代码了，就贴一个关键代码。需要整个项目的话在下方留言 package com.alibaba.datax.plugin.reader.kafkareader; import com.alibaba.datax.common.element.Record; import com.alibaba.datax.common.element.StringColumn; import com.alibaba.datax.common.exception.DataXException; import com.alibaba.datax.common.plugin.RecordSender; import com.alibaba.datax.common.spi.Reader; import com.alibaba.datax.common.util.Configuration; import com.alibaba.datax.plugin.rdbms.reader.CommonRdbmsReader; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.io.File; import java.io.FileNotFoundException; import java.io.FileOutputStream; import java.io.IOException; import java.util.*; import java.util.regex.Matcher; import java.util.regex.Pattern; public class KafkaReader extends Reader { public static class Job extends Reader.Job { private static final Logger LOG = LoggerFactory .getLogger(Job.class); private Configuration originalConfig = null; @Override public void init() { this.originalConfig = super.getPluginJobConf(); // warn: 忽略大小写 String topic = this.originalConfig .getString(Key.TOPIC); Integer partitions = this.originalConfig .getInt(Key.KAFKA_PARTITIONS); String bootstrapServers = this.originalConfig .getString(Key.BOOTSTRAP_SERVERS); String groupId = this.originalConfig .getString(Key.GROUP_ID); Integer columnCount = this.originalConfig .getInt(Key.COLUMNCOUNT); String split = this.originalConfig.getString(Key.SPLIT); String filterContaintsStr = this.originalConfig.getString(Key.CONTAINTS_STR); String filterContaintsFlag = this.originalConfig.getString(Key.CONTAINTS_STR_FLAG); String conditionAllOrOne = this.originalConfig.getString(Key.CONDITION_ALL_OR_ONE); String parsingRules = this.originalConfig.getString(Key.PARSING_RULES); String writerOrder = this.originalConfig.getString(Key.WRITER_ORDER); String kafkaReaderColumnKey = this.originalConfig.getString(Key.KAFKA_READER_COLUMN_KEY); System.out.println(topic); System.out.println(partitions); System.out.println(bootstrapServers); System.out.println(groupId); System.out.println(columnCount); System.out.println(split); System.out.println(parsingRules); if (null == topic) { throw DataXException.asDataXException(KafkaReaderErrorCode.TOPIC_ERROR, &quot;没有设置参数[topic].&quot;); } if (partitions == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR, &quot;没有设置参数[kafka.partitions].&quot;); } else if (partitions &lt; 1) { throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR, &quot;[kafka.partitions]不能小于1.&quot;); } if (null == bootstrapServers) { throw DataXException.asDataXException(KafkaReaderErrorCode.ADDRESS_ERROR, &quot;没有设置参数[bootstrap.servers].&quot;); } if (null == groupId) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置参数[groupid].&quot;); } if (columnCount == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR, &quot;没有设置参数[columnCount].&quot;); } else if (columnCount &lt; 1) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;[columnCount]不能小于1.&quot;); } if (null == split) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;[split]不能为空.&quot;); } if (filterContaintsStr != null) { if (conditionAllOrOne == null || filterContaintsFlag == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;设置了[filterContaintsStr],但是没有设置[conditionAllOrOne]或者[filterContaintsFlag]&quot;); } } if (parsingRules == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置[parsingRules]参数&quot;); } else if (!parsingRules.equals(&quot;regex&quot;) &amp;&amp; parsingRules.equals(&quot;json&quot;) &amp;&amp; parsingRules.equals(&quot;split&quot;)) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;[parsingRules]参数设置错误，不是regex，json，split其中一个&quot;); } if (writerOrder == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置[writerOrder]参数&quot;); } if (kafkaReaderColumnKey == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置[kafkaReaderColumnKey]参数&quot;); } } @Override public void preCheck() { init(); } @Override public List&lt;Configuration&gt; split(int adviceNumber) { List&lt;Configuration&gt; configurations = new ArrayList&lt;Configuration&gt;(); Integer partitions = this.originalConfig.getInt(Key.KAFKA_PARTITIONS); for (int i = 0; i &lt; partitions; i++) { configurations.add(this.originalConfig.clone()); } return configurations; } @Override public void post() { } @Override public void destroy() { } } public static class Task extends Reader.Task { private static final Logger LOG = LoggerFactory .getLogger(CommonRdbmsReader.Task.class); //配置文件 private Configuration readerSliceConfig; //kafka消息的分隔符 private String split; //解析规则 private String parsingRules; //是否停止拉去数据 private boolean flag; //kafka address private String bootstrapServers; //kafka groupid private String groupId; //kafkatopic private String kafkaTopic; //kafka中的数据一共有多少个字段 private int count; //是否需要data_from //kafka ip 端口+ topic //将包含/不包含该字符串的数据过滤掉 private String filterContaintsStr; //是包含containtsStr 还是不包含 //1 表示包含 0 表示不包含 private int filterContaintsStrFlag; //全部包含或不包含，包含其中一个或者不包含其中一个。 private int conditionAllOrOne; //writer端要求的顺序。 private String writerOrder; //kafkareader端的每个关键子的key private String kafkaReaderColumnKey; //异常文件路径 private String exceptionPath; @Override public void init() { flag = true; this.readerSliceConfig = super.getPluginJobConf(); split = this.readerSliceConfig.getString(Key.SPLIT); bootstrapServers = this.readerSliceConfig.getString(Key.BOOTSTRAP_SERVERS); groupId = this.readerSliceConfig.getString(Key.GROUP_ID); kafkaTopic = this.readerSliceConfig.getString(Key.TOPIC); count = this.readerSliceConfig.getInt(Key.COLUMNCOUNT); filterContaintsStr = this.readerSliceConfig.getString(Key.CONTAINTS_STR); filterContaintsStrFlag = this.readerSliceConfig.getInt(Key.CONTAINTS_STR_FLAG); conditionAllOrOne = this.readerSliceConfig.getInt(Key.CONTAINTS_STR_FLAG); parsingRules = this.readerSliceConfig.getString(Key.PARSING_RULES); writerOrder = this.readerSliceConfig.getString(Key.WRITER_ORDER); kafkaReaderColumnKey = this.readerSliceConfig.getString(Key.KAFKA_READER_COLUMN_KEY); exceptionPath = this.readerSliceConfig.getString(Key.EXECPTION_PATH); LOG.info(filterContaintsStr); } @Override public void startRead(RecordSender recordSender) { Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, bootstrapServers); props.put(&quot;group.id&quot;, groupId != null ? groupId : UUID.randomUUID().toString()); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); consumer.subscribe(Collections.singletonList(kafkaTopic)); Record oneRecord = null; while (flag) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { String value = record.value(); //定义过滤标志 int ifNotContinue = filterMessage(value); //如果标志修改为1了那么就过滤掉这条数据。 if (ifNotContinue == 1) { LOG.info(&quot;过滤数据： &quot; + record.value()); continue; } oneRecord = buildOneRecord(recordSender, value); //如果返回值不等于null表示不是异常消息。 if (oneRecord != null) { recordSender.sendToWriter(oneRecord); } } consumer.commitSync(); //判断当前事件是不是0点,0点的话进程他退出 Date date = new Date(); if (DateUtil.targetFormat(date).split(&quot; &quot;)[1].substring(0, 2).equals(&quot;00&quot;)) { destroy(); } } } private int filterMessage(String value) { //如果要过滤的条件配置了 int ifNotContinue = 0; if (filterContaintsStr != null) { String[] filterStrs = filterContaintsStr.split(&quot;,&quot;); //所有 if (conditionAllOrOne == 1) { //过滤掉包含filterContaintsStr的所有项的值。 if (filterContaintsStrFlag == 1) { int i = 0; for (; i &lt; filterStrs.length; i++) { if (!value.contains(filterStrs[i])) break; } if (i &gt;= filterStrs.length) ifNotContinue = 1; } else { //留下掉包含filterContaintsStr的所有项的值 int i = 0; for (; i &lt; filterStrs.length; i++) { if (!value.contains(filterStrs[i])) break; } if (i &lt; filterStrs.length) ifNotContinue = 1; } } else { //过滤掉包含其中一项的值 if (filterContaintsStrFlag == 1) { int i = 0; for (; i &lt; filterStrs.length; i++) { if (value.contains(filterStrs[i])) break; } if (i &lt; filterStrs.length) ifNotContinue = 1; } //留下包含其中一下的值 else { int i = 0; for (; i &lt; filterStrs.length; i++) { if (value.contains(filterStrs[i])) break; } if (i &gt;= filterStrs.length) ifNotContinue = 1; } } } return ifNotContinue; } private Record buildOneRecord(RecordSender recordSender, String value) { Record record = null; if (parsingRules.equals(&quot;regex&quot;)) { record = parseRegex(value, recordSender); } else if (parsingRules.equals(&quot;json&quot;)) { record = parseJson(value, recordSender); } else if (parsingRules.equals(&quot;split&quot;)) { record = parseSplit(value, recordSender); } return record; } private Record parseSplit(String value, RecordSender recordSender) { Record record = recordSender.createRecord(); String[] splits = value.split(this.split); if (splits.length != count) { writerErrorPath(value); return null; } parseOrders(Arrays.asList(splits), record); return record; } private Record parseJson(String value, RecordSender recordSender) { Record record = recordSender.createRecord(); HashMap&lt;String, Object&gt; map = JsonUtilJava.parseJsonStrToMap(value); String[] columns = kafkaReaderColumnKey.split(&quot;,&quot;); ArrayList&lt;String&gt; datas = new ArrayList&lt;String&gt;(); for (String column : columns) { datas.add(map.get(column).toString()); } if (datas.size() != count) { writerErrorPath(value); return null; } parseOrders(datas, record); return record; } private Record parseRegex(String value, RecordSender recordSender) { Record record = recordSender.createRecord(); ArrayList&lt;String&gt; datas = new ArrayList&lt;String&gt;(); Pattern r = Pattern.compile(split); Matcher m = r.matcher(value); if (m.find()) { if (m.groupCount() != count) { writerErrorPath(value); } for (int i = 1; i &lt;= count; i++) { // record.addColumn(new StringColumn(m.group(i))); datas.add(m.group(i)); return record; } } else { writerErrorPath(value); } parseOrders(datas, record); return null; } private void writerErrorPath(String value) { if (exceptionPath == null) return; FileOutputStream fileOutputStream = null; try { fileOutputStream = getFileOutputStream(); fileOutputStream.write((value + &quot;\n&quot;).getBytes()); fileOutputStream.close(); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } private FileOutputStream getFileOutputStream() throws FileNotFoundException { return new FileOutputStream(exceptionPath + &quot;/&quot; + kafkaTopic + &quot;errordata&quot; + DateUtil.targetFormat(new Date(), &quot;yyyyMMdd&quot;), true); } private void parseOrders(List&lt;String&gt; datas, Record record) { //writerOrder String[] orders = writerOrder.split(&quot;,&quot;); for (String order : orders) { if (order.equals(&quot;data_from&quot;)) { record.addColumn(new StringColumn(bootstrapServers + &quot;|&quot; + kafkaTopic)); } else if (order.equals(&quot;uuid&quot;)) { record.addColumn(new StringColumn(UUID.randomUUID().toString())); } else if (order.equals(&quot;null&quot;)) { record.addColumn(new StringColumn(&quot;null&quot;)); } else if (order.equals(&quot;datax_time&quot;)) { record.addColumn(new StringColumn(DateUtil.targetFormat(new Date()))); } else if (isNumeric(order)) { record.addColumn(new StringColumn(datas.get(new Integer(order) - 1))); } } } public static boolean isNumeric(String str) { for (int i = 0; i &lt; str.length(); i++) { if (!Character.isDigit(str.charAt(i))) { return false; } } return true; } @Override public void post() { } @Override public void destroy() { flag = false; } } 5.打包运行 5.1将其他模块注释只留下公共模块和自己的项目模块 在最为外层的pom.xml中注释 5.2 进入到项目最外层的目录输入cmd（前提配置了本地maven的环境变量） 5.3 使用maven命令打包 &nbsp;mvn -U clean package assembly:assembly -Dmaven.test.skip=true 5.4 打包后将下图目录下的包上传到集群的datax对应目录 本地地址：D:\DataX-master\kafkareader\target\datax\plugin\reader 集群地址：/opt/module/datax/plugin/reader \ 5.5 写好配置文件就可以运行了 { &quot;job&quot;: { &quot;content&quot;: [ { &quot;reader&quot;: { &quot;name&quot;: &quot;kafkareader&quot;, &quot;parameter&quot;: { &quot;topic&quot;: &quot;Event&quot;, &quot;bootstrapServers&quot;: &quot;192.168.7.128:9092&quot;, &quot;kafkaPartitions&quot;: &quot;1&quot;, &quot;columnCount&quot;:11, &quot;groupId&quot;:&quot;ast&quot;, &quot;filterContaints&quot;:&quot;5^1,6^5&quot;, &quot;filterContaintsFlag&quot;:1, &quot;conditionAllOrOne&quot;:0, &quot;parsingRules&quot;:&quot;regex&quot;, &quot;writerOrder&quot;:&quot;uuid,1,3,6,4,8,9,10,11,5,7,2,null,datax_time,data_from&quot;, &quot;kafkaReaderColumnKey&quot;:&quot;a&quot;, &quot;execptionPath&quot;:&quot;/opt/module/datax/log/errorlog&quot; } }, &quot;writer&quot;: { &quot;name&quot;: &quot;hdfswriter&quot;, &quot;parameter&quot;: { &quot;defaultFS&quot;: &quot;hdfs://master:8020&quot;, &quot;fileType&quot;: &quot;orc&quot;, &quot;path&quot;: &quot;${path}&quot;, &quot;fileName&quot;: &quot;t_rsd_amber_agent_event_log&quot;, &quot;column&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;risk_level&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;device_uuid&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_device_id&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;device_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_sub_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;repeats&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;description&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_time&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;report_device_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_report_time&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;last_update_time&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;datax_time&quot;, &quot;type&quot;: &quot;string&quot; } , { &quot;name&quot;: &quot;data_from&quot;, &quot;type&quot;: &quot;string&quot; }, ], &quot;writeMode&quot;: &quot;append&quot;, &quot;fieldDelimiter&quot;: &quot;\t&quot;, &quot;compress&quot;:&quot;NONE&quot;, &quot;scrollFileTime&quot;:300000 } } } ], &quot;setting&quot;: { &quot;speed&quot;: { &quot;channel&quot;: 3, &quot;record&quot;: 20000, &quot;byte&quot;:5000 , &quot;batchSize&quot;:2048 } } } } 运行命令： &nbsp;python /opt/module/datax/bin/datax.py -p &quot;-Dpath=/data/warehouse/rsd/t_rsd_amber_agent_event_log/2019/06/05&quot; /opt/module/datax/job/kafkatohdfs.json" />
<link rel="canonical" href="https://uzzz.org/2019/06/05/794415.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/05/794415.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-05T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"目录 &nbsp; 1.背景 2.需求 3.开发步骤 &nbsp; 3.1&nbsp;去github上下载datax的代码 &nbsp;3.2 本地解压，并导入idea 3.3创建一个模块kafkareader 3.4将任意一个模块的以下两个文件考入到resource目录下 3.5进行修改plugin.json 3.6修改pom.xml(复制其中一个文件的依赖和插件到pom.xml) 3.7将其他模块下面的，这个文件夹复制到我们模块的对应的文件夹，并且修改package.xml ​ 3.8 在最外层的package.xml加上下面这个 4.开发代码 4.1 开发前将datax的开发插件的手册认真观看一遍，对开发有帮助的。 4.2编写代码（要继承什么类实现什么方法，4.1的开发宝典上都写了） 5.打包运行 5.1将其他模块注释只留下公共模块和自己的项目模块 5.2 进入到项目最外层的目录输入cmd（前提配置了本地maven的环境变量） 5.3 使用maven命令打包 5.4 打包后将下图目录下的包上传到集群的datax对应目录 5.5 写好配置文件就可以运行了 1.背景 &nbsp; &nbsp; 公司要求：统一入库平台，使用datax这个工具。需要采集kafka，elasticsearch，mysql，sqlserver等数据源的数据，并且只打算用datax。 2.需求 &nbsp;开发datax的kafkaReader组件，从kafka数据源读取数据，然后同步到其他的数据。 &nbsp;1.要求：可以同步json格式的数据，要求可以用正则来解析数据，可以指定数据的分隔符来解析数据。 &nbsp;2.可以同步到hive，mysql，hbase中 3.开发步骤 &nbsp; 3.1&nbsp;去github上下载datax的代码 &nbsp;3.2 本地解压，并导入idea 3.3创建一个模块kafkareader 3.4将任意一个模块的以下两个文件考入到resource目录下 3.5进行修改plugin.json 3.6修改pom.xml(复制其中一个文件的依赖和插件到pom.xml) 3.7将其他模块下面的，这个文件夹复制到我们模块的对应的文件夹，并且修改package.xml 要修改的我已经注释好了。就是把你之前复制过来的reader修改为kafkareader &lt;assembly xmlns=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd&quot;&gt; &lt;id&gt;&lt;/id&gt; &lt;formats&gt; &lt;format&gt;dir&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;plugin.json&lt;/include&gt; &lt;include&gt;plugin_job_template.json&lt;/include&gt; &lt;/includes&gt; //修改为kafkareader &lt;outputDirectory&gt;plugin/reader/kafkareader&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;fileSet&gt; &lt;directory&gt;target/&lt;/directory&gt; &lt;includes&gt; //修改为kafkareader &lt;include&gt;kafkareader-0.0.1-SNAPSHOT.jar&lt;/include&gt; &lt;/includes&gt; //修改为kafkareader &lt;outputDirectory&gt;plugin/reader/kafkareader&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;!--&lt;fileSet&gt;--&gt; &lt;!--&lt;directory&gt;src/main/cpp&lt;/directory&gt;--&gt; &lt;!--&lt;includes&gt;--&gt; &lt;!--&lt;include&gt;libhadoop.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadoop.so&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadoop.so.1.0.0&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadooppipes.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhadooputils.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhdfs.a&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhdfs.so&lt;/include&gt;--&gt; &lt;!--&lt;include&gt;libhdfs.so.0.0.0&lt;/include&gt;--&gt; &lt;!--&lt;/includes&gt;--&gt; &lt;!--&lt;outputDirectory&gt;plugin/reader/hdfsreader/libs&lt;/outputDirectory&gt;--&gt; &lt;!--&lt;/fileSet&gt;--&gt; &lt;/fileSets&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; //修改为kafkareader &lt;outputDirectory&gt;plugin/reader/kafkareader/libs&lt;/outputDirectory&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;/assembly&gt; 3.8 在最外层的package.xml加上下面这个 4.开发代码 4.1 开发前将datax的开发插件的手册认真观看一遍，对开发有帮助的。 地址：https://github.com/alibaba/DataX/blob/master/dataxPluginDev.md 4.2编写代码（要继承什么类实现什么方法，4.1的开发宝典上都写了） 懒得贴那么多代码了，就贴一个关键代码。需要整个项目的话在下方留言 package com.alibaba.datax.plugin.reader.kafkareader; import com.alibaba.datax.common.element.Record; import com.alibaba.datax.common.element.StringColumn; import com.alibaba.datax.common.exception.DataXException; import com.alibaba.datax.common.plugin.RecordSender; import com.alibaba.datax.common.spi.Reader; import com.alibaba.datax.common.util.Configuration; import com.alibaba.datax.plugin.rdbms.reader.CommonRdbmsReader; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.io.File; import java.io.FileNotFoundException; import java.io.FileOutputStream; import java.io.IOException; import java.util.*; import java.util.regex.Matcher; import java.util.regex.Pattern; public class KafkaReader extends Reader { public static class Job extends Reader.Job { private static final Logger LOG = LoggerFactory .getLogger(Job.class); private Configuration originalConfig = null; @Override public void init() { this.originalConfig = super.getPluginJobConf(); // warn: 忽略大小写 String topic = this.originalConfig .getString(Key.TOPIC); Integer partitions = this.originalConfig .getInt(Key.KAFKA_PARTITIONS); String bootstrapServers = this.originalConfig .getString(Key.BOOTSTRAP_SERVERS); String groupId = this.originalConfig .getString(Key.GROUP_ID); Integer columnCount = this.originalConfig .getInt(Key.COLUMNCOUNT); String split = this.originalConfig.getString(Key.SPLIT); String filterContaintsStr = this.originalConfig.getString(Key.CONTAINTS_STR); String filterContaintsFlag = this.originalConfig.getString(Key.CONTAINTS_STR_FLAG); String conditionAllOrOne = this.originalConfig.getString(Key.CONDITION_ALL_OR_ONE); String parsingRules = this.originalConfig.getString(Key.PARSING_RULES); String writerOrder = this.originalConfig.getString(Key.WRITER_ORDER); String kafkaReaderColumnKey = this.originalConfig.getString(Key.KAFKA_READER_COLUMN_KEY); System.out.println(topic); System.out.println(partitions); System.out.println(bootstrapServers); System.out.println(groupId); System.out.println(columnCount); System.out.println(split); System.out.println(parsingRules); if (null == topic) { throw DataXException.asDataXException(KafkaReaderErrorCode.TOPIC_ERROR, &quot;没有设置参数[topic].&quot;); } if (partitions == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR, &quot;没有设置参数[kafka.partitions].&quot;); } else if (partitions &lt; 1) { throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR, &quot;[kafka.partitions]不能小于1.&quot;); } if (null == bootstrapServers) { throw DataXException.asDataXException(KafkaReaderErrorCode.ADDRESS_ERROR, &quot;没有设置参数[bootstrap.servers].&quot;); } if (null == groupId) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置参数[groupid].&quot;); } if (columnCount == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR, &quot;没有设置参数[columnCount].&quot;); } else if (columnCount &lt; 1) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;[columnCount]不能小于1.&quot;); } if (null == split) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;[split]不能为空.&quot;); } if (filterContaintsStr != null) { if (conditionAllOrOne == null || filterContaintsFlag == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;设置了[filterContaintsStr],但是没有设置[conditionAllOrOne]或者[filterContaintsFlag]&quot;); } } if (parsingRules == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置[parsingRules]参数&quot;); } else if (!parsingRules.equals(&quot;regex&quot;) &amp;&amp; parsingRules.equals(&quot;json&quot;) &amp;&amp; parsingRules.equals(&quot;split&quot;)) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;[parsingRules]参数设置错误，不是regex，json，split其中一个&quot;); } if (writerOrder == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置[writerOrder]参数&quot;); } if (kafkaReaderColumnKey == null) { throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR, &quot;没有设置[kafkaReaderColumnKey]参数&quot;); } } @Override public void preCheck() { init(); } @Override public List&lt;Configuration&gt; split(int adviceNumber) { List&lt;Configuration&gt; configurations = new ArrayList&lt;Configuration&gt;(); Integer partitions = this.originalConfig.getInt(Key.KAFKA_PARTITIONS); for (int i = 0; i &lt; partitions; i++) { configurations.add(this.originalConfig.clone()); } return configurations; } @Override public void post() { } @Override public void destroy() { } } public static class Task extends Reader.Task { private static final Logger LOG = LoggerFactory .getLogger(CommonRdbmsReader.Task.class); //配置文件 private Configuration readerSliceConfig; //kafka消息的分隔符 private String split; //解析规则 private String parsingRules; //是否停止拉去数据 private boolean flag; //kafka address private String bootstrapServers; //kafka groupid private String groupId; //kafkatopic private String kafkaTopic; //kafka中的数据一共有多少个字段 private int count; //是否需要data_from //kafka ip 端口+ topic //将包含/不包含该字符串的数据过滤掉 private String filterContaintsStr; //是包含containtsStr 还是不包含 //1 表示包含 0 表示不包含 private int filterContaintsStrFlag; //全部包含或不包含，包含其中一个或者不包含其中一个。 private int conditionAllOrOne; //writer端要求的顺序。 private String writerOrder; //kafkareader端的每个关键子的key private String kafkaReaderColumnKey; //异常文件路径 private String exceptionPath; @Override public void init() { flag = true; this.readerSliceConfig = super.getPluginJobConf(); split = this.readerSliceConfig.getString(Key.SPLIT); bootstrapServers = this.readerSliceConfig.getString(Key.BOOTSTRAP_SERVERS); groupId = this.readerSliceConfig.getString(Key.GROUP_ID); kafkaTopic = this.readerSliceConfig.getString(Key.TOPIC); count = this.readerSliceConfig.getInt(Key.COLUMNCOUNT); filterContaintsStr = this.readerSliceConfig.getString(Key.CONTAINTS_STR); filterContaintsStrFlag = this.readerSliceConfig.getInt(Key.CONTAINTS_STR_FLAG); conditionAllOrOne = this.readerSliceConfig.getInt(Key.CONTAINTS_STR_FLAG); parsingRules = this.readerSliceConfig.getString(Key.PARSING_RULES); writerOrder = this.readerSliceConfig.getString(Key.WRITER_ORDER); kafkaReaderColumnKey = this.readerSliceConfig.getString(Key.KAFKA_READER_COLUMN_KEY); exceptionPath = this.readerSliceConfig.getString(Key.EXECPTION_PATH); LOG.info(filterContaintsStr); } @Override public void startRead(RecordSender recordSender) { Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, bootstrapServers); props.put(&quot;group.id&quot;, groupId != null ? groupId : UUID.randomUUID().toString()); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); consumer.subscribe(Collections.singletonList(kafkaTopic)); Record oneRecord = null; while (flag) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { String value = record.value(); //定义过滤标志 int ifNotContinue = filterMessage(value); //如果标志修改为1了那么就过滤掉这条数据。 if (ifNotContinue == 1) { LOG.info(&quot;过滤数据： &quot; + record.value()); continue; } oneRecord = buildOneRecord(recordSender, value); //如果返回值不等于null表示不是异常消息。 if (oneRecord != null) { recordSender.sendToWriter(oneRecord); } } consumer.commitSync(); //判断当前事件是不是0点,0点的话进程他退出 Date date = new Date(); if (DateUtil.targetFormat(date).split(&quot; &quot;)[1].substring(0, 2).equals(&quot;00&quot;)) { destroy(); } } } private int filterMessage(String value) { //如果要过滤的条件配置了 int ifNotContinue = 0; if (filterContaintsStr != null) { String[] filterStrs = filterContaintsStr.split(&quot;,&quot;); //所有 if (conditionAllOrOne == 1) { //过滤掉包含filterContaintsStr的所有项的值。 if (filterContaintsStrFlag == 1) { int i = 0; for (; i &lt; filterStrs.length; i++) { if (!value.contains(filterStrs[i])) break; } if (i &gt;= filterStrs.length) ifNotContinue = 1; } else { //留下掉包含filterContaintsStr的所有项的值 int i = 0; for (; i &lt; filterStrs.length; i++) { if (!value.contains(filterStrs[i])) break; } if (i &lt; filterStrs.length) ifNotContinue = 1; } } else { //过滤掉包含其中一项的值 if (filterContaintsStrFlag == 1) { int i = 0; for (; i &lt; filterStrs.length; i++) { if (value.contains(filterStrs[i])) break; } if (i &lt; filterStrs.length) ifNotContinue = 1; } //留下包含其中一下的值 else { int i = 0; for (; i &lt; filterStrs.length; i++) { if (value.contains(filterStrs[i])) break; } if (i &gt;= filterStrs.length) ifNotContinue = 1; } } } return ifNotContinue; } private Record buildOneRecord(RecordSender recordSender, String value) { Record record = null; if (parsingRules.equals(&quot;regex&quot;)) { record = parseRegex(value, recordSender); } else if (parsingRules.equals(&quot;json&quot;)) { record = parseJson(value, recordSender); } else if (parsingRules.equals(&quot;split&quot;)) { record = parseSplit(value, recordSender); } return record; } private Record parseSplit(String value, RecordSender recordSender) { Record record = recordSender.createRecord(); String[] splits = value.split(this.split); if (splits.length != count) { writerErrorPath(value); return null; } parseOrders(Arrays.asList(splits), record); return record; } private Record parseJson(String value, RecordSender recordSender) { Record record = recordSender.createRecord(); HashMap&lt;String, Object&gt; map = JsonUtilJava.parseJsonStrToMap(value); String[] columns = kafkaReaderColumnKey.split(&quot;,&quot;); ArrayList&lt;String&gt; datas = new ArrayList&lt;String&gt;(); for (String column : columns) { datas.add(map.get(column).toString()); } if (datas.size() != count) { writerErrorPath(value); return null; } parseOrders(datas, record); return record; } private Record parseRegex(String value, RecordSender recordSender) { Record record = recordSender.createRecord(); ArrayList&lt;String&gt; datas = new ArrayList&lt;String&gt;(); Pattern r = Pattern.compile(split); Matcher m = r.matcher(value); if (m.find()) { if (m.groupCount() != count) { writerErrorPath(value); } for (int i = 1; i &lt;= count; i++) { // record.addColumn(new StringColumn(m.group(i))); datas.add(m.group(i)); return record; } } else { writerErrorPath(value); } parseOrders(datas, record); return null; } private void writerErrorPath(String value) { if (exceptionPath == null) return; FileOutputStream fileOutputStream = null; try { fileOutputStream = getFileOutputStream(); fileOutputStream.write((value + &quot;\\n&quot;).getBytes()); fileOutputStream.close(); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } private FileOutputStream getFileOutputStream() throws FileNotFoundException { return new FileOutputStream(exceptionPath + &quot;/&quot; + kafkaTopic + &quot;errordata&quot; + DateUtil.targetFormat(new Date(), &quot;yyyyMMdd&quot;), true); } private void parseOrders(List&lt;String&gt; datas, Record record) { //writerOrder String[] orders = writerOrder.split(&quot;,&quot;); for (String order : orders) { if (order.equals(&quot;data_from&quot;)) { record.addColumn(new StringColumn(bootstrapServers + &quot;|&quot; + kafkaTopic)); } else if (order.equals(&quot;uuid&quot;)) { record.addColumn(new StringColumn(UUID.randomUUID().toString())); } else if (order.equals(&quot;null&quot;)) { record.addColumn(new StringColumn(&quot;null&quot;)); } else if (order.equals(&quot;datax_time&quot;)) { record.addColumn(new StringColumn(DateUtil.targetFormat(new Date()))); } else if (isNumeric(order)) { record.addColumn(new StringColumn(datas.get(new Integer(order) - 1))); } } } public static boolean isNumeric(String str) { for (int i = 0; i &lt; str.length(); i++) { if (!Character.isDigit(str.charAt(i))) { return false; } } return true; } @Override public void post() { } @Override public void destroy() { flag = false; } } 5.打包运行 5.1将其他模块注释只留下公共模块和自己的项目模块 在最为外层的pom.xml中注释 5.2 进入到项目最外层的目录输入cmd（前提配置了本地maven的环境变量） 5.3 使用maven命令打包 &nbsp;mvn -U clean package assembly:assembly -Dmaven.test.skip=true 5.4 打包后将下图目录下的包上传到集群的datax对应目录 本地地址：D:\\DataX-master\\kafkareader\\target\\datax\\plugin\\reader 集群地址：/opt/module/datax/plugin/reader \\ 5.5 写好配置文件就可以运行了 { &quot;job&quot;: { &quot;content&quot;: [ { &quot;reader&quot;: { &quot;name&quot;: &quot;kafkareader&quot;, &quot;parameter&quot;: { &quot;topic&quot;: &quot;Event&quot;, &quot;bootstrapServers&quot;: &quot;192.168.7.128:9092&quot;, &quot;kafkaPartitions&quot;: &quot;1&quot;, &quot;columnCount&quot;:11, &quot;groupId&quot;:&quot;ast&quot;, &quot;filterContaints&quot;:&quot;5^1,6^5&quot;, &quot;filterContaintsFlag&quot;:1, &quot;conditionAllOrOne&quot;:0, &quot;parsingRules&quot;:&quot;regex&quot;, &quot;writerOrder&quot;:&quot;uuid,1,3,6,4,8,9,10,11,5,7,2,null,datax_time,data_from&quot;, &quot;kafkaReaderColumnKey&quot;:&quot;a&quot;, &quot;execptionPath&quot;:&quot;/opt/module/datax/log/errorlog&quot; } }, &quot;writer&quot;: { &quot;name&quot;: &quot;hdfswriter&quot;, &quot;parameter&quot;: { &quot;defaultFS&quot;: &quot;hdfs://master:8020&quot;, &quot;fileType&quot;: &quot;orc&quot;, &quot;path&quot;: &quot;${path}&quot;, &quot;fileName&quot;: &quot;t_rsd_amber_agent_event_log&quot;, &quot;column&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;risk_level&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;device_uuid&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_device_id&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;device_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_sub_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;repeats&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;description&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_time&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;report_device_type&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;event_report_time&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;last_update_time&quot;, &quot;type&quot;: &quot;string&quot; }, { &quot;name&quot;: &quot;datax_time&quot;, &quot;type&quot;: &quot;string&quot; } , { &quot;name&quot;: &quot;data_from&quot;, &quot;type&quot;: &quot;string&quot; }, ], &quot;writeMode&quot;: &quot;append&quot;, &quot;fieldDelimiter&quot;: &quot;\\t&quot;, &quot;compress&quot;:&quot;NONE&quot;, &quot;scrollFileTime&quot;:300000 } } } ], &quot;setting&quot;: { &quot;speed&quot;: { &quot;channel&quot;: 3, &quot;record&quot;: 20000, &quot;byte&quot;:5000 , &quot;batchSize&quot;:2048 } } } } 运行命令： &nbsp;python /opt/module/datax/bin/datax.py -p &quot;-Dpath=/data/warehouse/rsd/t_rsd_amber_agent_event_log/2019/06/05&quot; /opt/module/datax/job/kafkatohdfs.json","@type":"BlogPosting","url":"https://uzzz.org/2019/06/05/794415.html","headline":"Datax 二次开发插件详细过程","dateModified":"2019-06-05T00:00:00+08:00","datePublished":"2019-06-05T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/05/794415.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Datax 二次开发插件详细过程</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="-toc" style="margin-left:0px;">&nbsp;</p> 
  <p id="1.%E8%83%8C%E6%99%AF-toc" style="margin-left:0px;"><a href="#1.%E8%83%8C%E6%99%AF" rel="nofollow" data-token="5f0ad2787acd7a730fc1266e4886baa8">1.背景</a></p> 
  <p id="2.%E9%9C%80%E6%B1%82-toc" style="margin-left:0px;"><a href="#2.%E9%9C%80%E6%B1%82" rel="nofollow" data-token="32f145d60701d249dcd14e0eb3387c45">2.需求</a></p> 
  <p id="3.%E5%BC%80%E5%8F%91%E6%AD%A5%E9%AA%A4-toc" style="margin-left:0px;"><a href="#3.%E5%BC%80%E5%8F%91%E6%AD%A5%E9%AA%A4" rel="nofollow" data-token="b86a8454ffdd7d6581aaf37c23c86bd3">3.开发步骤</a></p> 
  <p id="%C2%A0%203.1%C2%A0%E5%8E%BBgithub%E4%B8%8A%E4%B8%8B%E8%BD%BDdatax%E7%9A%84%E4%BB%A3%E7%A0%81-toc" style="margin-left:80px;"><a href="#%C2%A0%203.1%C2%A0%E5%8E%BBgithub%E4%B8%8A%E4%B8%8B%E8%BD%BDdatax%E7%9A%84%E4%BB%A3%E7%A0%81" rel="nofollow" data-token="779b90596ca4253295e812f2e4d41c26">&nbsp; 3.1&nbsp;去github上下载datax的代码</a></p> 
  <p id="%C2%A03.2%20%E6%9C%AC%E5%9C%B0%E8%A7%A3%E5%8E%8B%EF%BC%8C%E5%B9%B6%E5%AF%BC%E5%85%A5idea-toc" style="margin-left:80px;"><a href="#%C2%A03.2%20%E6%9C%AC%E5%9C%B0%E8%A7%A3%E5%8E%8B%EF%BC%8C%E5%B9%B6%E5%AF%BC%E5%85%A5idea" rel="nofollow" data-token="6fc99d608545f29ebd23404dbe900dd9">&nbsp;3.2 本地解压，并导入idea</a></p> 
  <p id="3.3%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97kafkareader-toc" style="margin-left:80px;"><a href="#3.3%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97kafkareader" rel="nofollow" data-token="84c30bc8e96b9b29e8831c1c212ab27a">3.3创建一个模块kafkareader</a></p> 
  <p id="3.4%E5%B0%86%E4%BB%BB%E6%84%8F%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BB%A5%E4%B8%8B%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E8%80%83%E5%85%A5%E5%88%B0resource%E7%9B%AE%E5%BD%95%E4%B8%8B-toc" style="margin-left:80px;"><a href="#3.4%E5%B0%86%E4%BB%BB%E6%84%8F%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BB%A5%E4%B8%8B%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E8%80%83%E5%85%A5%E5%88%B0resource%E7%9B%AE%E5%BD%95%E4%B8%8B" rel="nofollow" data-token="1938b8df5f06074eb6dc42ed7ab5d4a6">3.4将任意一个模块的以下两个文件考入到resource目录下</a></p> 
  <p id="3.5%E8%BF%9B%E8%A1%8C%E4%BF%AE%E6%94%B9plugin.json-toc" style="margin-left:80px;"><a href="#3.5%E8%BF%9B%E8%A1%8C%E4%BF%AE%E6%94%B9plugin.json" rel="nofollow" data-token="4518781ee91734922c79e81f9f0a3d31">3.5进行修改plugin.json</a></p> 
  <p id="3.6%E4%BF%AE%E6%94%B9pom.xml(%E5%A4%8D%E5%88%B6%E5%85%B6%E4%B8%AD%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E4%BE%9D%E8%B5%96%E5%92%8C%E6%8F%92%E4%BB%B6%E5%88%B0pom.xml)-toc" style="margin-left:80px;"><a href="#3.6%E4%BF%AE%E6%94%B9pom.xml(%E5%A4%8D%E5%88%B6%E5%85%B6%E4%B8%AD%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E4%BE%9D%E8%B5%96%E5%92%8C%E6%8F%92%E4%BB%B6%E5%88%B0pom.xml)" rel="nofollow" data-token="4458c2982bda829e04ba96a397d516ce">3.6修改pom.xml(复制其中一个文件的依赖和插件到pom.xml)</a></p> 
  <p id="3.7%E5%B0%86%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9D%97%E4%B8%8B%E9%9D%A2%E7%9A%84%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9%E5%A4%8D%E5%88%B6%E5%88%B0%E6%88%91%E4%BB%AC%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AF%B9%E5%BA%94%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9%EF%BC%8C%E5%B9%B6%E4%B8%94%E4%BF%AE%E6%94%B9package.xml-toc" style="margin-left:80px;"><a href="#3.7%E5%B0%86%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9D%97%E4%B8%8B%E9%9D%A2%E7%9A%84%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9%E5%A4%8D%E5%88%B6%E5%88%B0%E6%88%91%E4%BB%AC%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AF%B9%E5%BA%94%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9%EF%BC%8C%E5%B9%B6%E4%B8%94%E4%BF%AE%E6%94%B9package.xml" rel="nofollow" data-token="4c48cf4e597a5cfa4dac10255abd52dc">3.7将其他模块下面的，这个文件夹复制到我们模块的对应的文件夹，并且修改package.xml</a></p> 
  <p id="%E2%80%8B-toc" style="margin-left:80px;"><a href="#%E2%80%8B" rel="nofollow" data-token="85c7dc7a0e0e073cffc463115c3799fb">​</a></p> 
  <p id="3.8%20%E5%9C%A8%E6%9C%80%E5%A4%96%E5%B1%82%E7%9A%84package.xml%E5%8A%A0%E4%B8%8A%E4%B8%8B%E9%9D%A2%E8%BF%99%E4%B8%AA-toc" style="margin-left:80px;"><a href="#3.8%20%E5%9C%A8%E6%9C%80%E5%A4%96%E5%B1%82%E7%9A%84package.xml%E5%8A%A0%E4%B8%8A%E4%B8%8B%E9%9D%A2%E8%BF%99%E4%B8%AA" rel="nofollow" data-token="1134c45ae40d86565c76196e50d1fb81">3.8 在最外层的package.xml加上下面这个</a></p> 
  <p id="4.%E5%BC%80%E5%8F%91%E4%BB%A3%E7%A0%81-toc" style="margin-left:0px;"><a href="#4.%E5%BC%80%E5%8F%91%E4%BB%A3%E7%A0%81" rel="nofollow" data-token="a485d438ff425411ba3792a64c42e934">4.开发代码</a></p> 
  <p id="4.1%20%E5%BC%80%E5%8F%91%E5%89%8D%E5%B0%86datax%E7%9A%84%E5%BC%80%E5%8F%91%E6%8F%92%E4%BB%B6%E7%9A%84%E6%89%8B%E5%86%8C%E8%AE%A4%E7%9C%9F%E8%A7%82%E7%9C%8B%E4%B8%80%E9%81%8D%EF%BC%8C%E5%AF%B9%E5%BC%80%E5%8F%91%E6%9C%89%E5%B8%AE%E5%8A%A9%E7%9A%84%E3%80%82-toc" style="margin-left:80px;"><a href="#4.1%20%E5%BC%80%E5%8F%91%E5%89%8D%E5%B0%86datax%E7%9A%84%E5%BC%80%E5%8F%91%E6%8F%92%E4%BB%B6%E7%9A%84%E6%89%8B%E5%86%8C%E8%AE%A4%E7%9C%9F%E8%A7%82%E7%9C%8B%E4%B8%80%E9%81%8D%EF%BC%8C%E5%AF%B9%E5%BC%80%E5%8F%91%E6%9C%89%E5%B8%AE%E5%8A%A9%E7%9A%84%E3%80%82" rel="nofollow" data-token="44085fee968b6450525df8fa57c4965d">4.1 开发前将datax的开发插件的手册认真观看一遍，对开发有帮助的。</a></p> 
  <p id="4.2%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81%EF%BC%88%E8%A6%81%E7%BB%A7%E6%89%BF%E4%BB%80%E4%B9%88%E7%B1%BB%E5%AE%9E%E7%8E%B0%E4%BB%80%E4%B9%88%E6%96%B9%E6%B3%95%EF%BC%8C4.1%E7%9A%84%E5%BC%80%E5%8F%91%E5%AE%9D%E5%85%B8%E4%B8%8A%E9%83%BD%E5%86%99%E4%BA%86%EF%BC%89-toc" style="margin-left:80px;"><a href="#4.2%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81%EF%BC%88%E8%A6%81%E7%BB%A7%E6%89%BF%E4%BB%80%E4%B9%88%E7%B1%BB%E5%AE%9E%E7%8E%B0%E4%BB%80%E4%B9%88%E6%96%B9%E6%B3%95%EF%BC%8C4.1%E7%9A%84%E5%BC%80%E5%8F%91%E5%AE%9D%E5%85%B8%E4%B8%8A%E9%83%BD%E5%86%99%E4%BA%86%EF%BC%89" rel="nofollow" data-token="49ffa57340ccc99f25bca1936723fef0">4.2编写代码（要继承什么类实现什么方法，4.1的开发宝典上都写了）</a></p> 
  <p id="5.%E6%89%93%E5%8C%85%E8%BF%90%E8%A1%8C-toc" style="margin-left:0px;"><a href="#5.%E6%89%93%E5%8C%85%E8%BF%90%E8%A1%8C" rel="nofollow" data-token="3a6ff5eaeedbb3395b99664162f4d23f">5.打包运行</a></p> 
  <p id="5.1%E5%B0%86%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9D%97%E6%B3%A8%E9%87%8A%E5%8F%AA%E7%95%99%E4%B8%8B%E5%85%AC%E5%85%B1%E6%A8%A1%E5%9D%97%E5%92%8C%E8%87%AA%E5%B7%B1%E7%9A%84%E9%A1%B9%E7%9B%AE%E6%A8%A1%E5%9D%97-toc" style="margin-left:80px;"><a href="#5.1%E5%B0%86%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9D%97%E6%B3%A8%E9%87%8A%E5%8F%AA%E7%95%99%E4%B8%8B%E5%85%AC%E5%85%B1%E6%A8%A1%E5%9D%97%E5%92%8C%E8%87%AA%E5%B7%B1%E7%9A%84%E9%A1%B9%E7%9B%AE%E6%A8%A1%E5%9D%97" rel="nofollow" data-token="58ea46d5f2f00b07378028a64a04d7ea">5.1将其他模块注释只留下公共模块和自己的项目模块</a></p> 
  <p id="5.2%20%E8%BF%9B%E5%85%A5%E5%88%B0%E9%A1%B9%E7%9B%AE%E6%9C%80%E5%A4%96%E5%B1%82%E7%9A%84%E7%9B%AE%E5%BD%95%E8%BE%93%E5%85%A5cmd%EF%BC%88%E5%89%8D%E6%8F%90%E9%85%8D%E7%BD%AE%E4%BA%86%E6%9C%AC%E5%9C%B0maven%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%EF%BC%89-toc" style="margin-left:80px;"><a href="#5.2%20%E8%BF%9B%E5%85%A5%E5%88%B0%E9%A1%B9%E7%9B%AE%E6%9C%80%E5%A4%96%E5%B1%82%E7%9A%84%E7%9B%AE%E5%BD%95%E8%BE%93%E5%85%A5cmd%EF%BC%88%E5%89%8D%E6%8F%90%E9%85%8D%E7%BD%AE%E4%BA%86%E6%9C%AC%E5%9C%B0maven%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%EF%BC%89" rel="nofollow" data-token="a5c67b8f51b686a1f9e18e90e43e894f">5.2 进入到项目最外层的目录输入cmd（前提配置了本地maven的环境变量）</a></p> 
  <p id="5.3%20%E4%BD%BF%E7%94%A8maven%E5%91%BD%E4%BB%A4%E6%89%93%E5%8C%85-toc" style="margin-left:80px;"><a href="#5.3%20%E4%BD%BF%E7%94%A8maven%E5%91%BD%E4%BB%A4%E6%89%93%E5%8C%85" rel="nofollow" data-token="f5f39dc4a61fe9070eb4ffa8a94fb708">5.3 使用maven命令打包</a></p> 
  <p id="5.4%20%E6%89%93%E5%8C%85%E5%90%8E%E5%B0%86%E4%B8%8B%E5%9B%BE%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84%E5%8C%85%E4%B8%8A%E4%BC%A0%E5%88%B0%E9%9B%86%E7%BE%A4%E7%9A%84datax%E5%AF%B9%E5%BA%94%E7%9B%AE%E5%BD%95-toc" style="margin-left:80px;"><a href="#5.4%20%E6%89%93%E5%8C%85%E5%90%8E%E5%B0%86%E4%B8%8B%E5%9B%BE%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84%E5%8C%85%E4%B8%8A%E4%BC%A0%E5%88%B0%E9%9B%86%E7%BE%A4%E7%9A%84datax%E5%AF%B9%E5%BA%94%E7%9B%AE%E5%BD%95" rel="nofollow" data-token="091701ceb28c96637e1661e70dbc6ecc">5.4 打包后将下图目录下的包上传到集群的datax对应目录</a></p> 
  <p id="5.5%20%E5%86%99%E5%A5%BD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B0%B1%E5%8F%AF%E4%BB%A5%E8%BF%90%E8%A1%8C%E4%BA%86-toc" style="margin-left:80px;"><a href="#5.5%20%E5%86%99%E5%A5%BD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B0%B1%E5%8F%AF%E4%BB%A5%E8%BF%90%E8%A1%8C%E4%BA%86" rel="nofollow" data-token="13bb1909eb55205efd664df5cc657847">5.5 写好配置文件就可以运行了</a></p> 
  <hr id="hr-toc">
  <h1 id="1.%E8%83%8C%E6%99%AF">1.背景</h1> 
  <p>&nbsp; &nbsp; 公司要求：统一入库平台，使用datax这个工具。需要采集kafka，elasticsearch，mysql，sqlserver等数据源的数据，并且只打算用datax。</p> 
  <h1 id="2.%E9%9C%80%E6%B1%82"><strong>2.需求</strong></h1> 
  <p>&nbsp;开发datax的kafkaReader组件，从kafka数据源读取数据，然后同步到其他的数据。</p> 
  <p>&nbsp;1.要求：可以同步json格式的数据，要求可以用正则来解析数据，可以指定数据的分隔符来解析数据。</p> 
  <p>&nbsp;2.可以同步到hive，mysql，hbase中</p> 
  <h1 id="3.%E5%BC%80%E5%8F%91%E6%AD%A5%E9%AA%A4">3.开发步骤</h1> 
  <h3 id="%C2%A0%203.1%C2%A0%E5%8E%BBgithub%E4%B8%8A%E4%B8%8B%E8%BD%BDdatax%E7%9A%84%E4%BB%A3%E7%A0%81"><strong>&nbsp; 3.1&nbsp;去github上下载datax的代码</strong></h3> 
  <p><img alt="" class="has" height="700" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190605153759408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3Mjc3MjMyMjQw,size_16,color_FFFFFF,t_70" width="1200"></p> 
  <h3 id="%C2%A03.2%20%E6%9C%AC%E5%9C%B0%E8%A7%A3%E5%8E%8B%EF%BC%8C%E5%B9%B6%E5%AF%BC%E5%85%A5idea">&nbsp;<strong>3.</strong>2 本地解压，并导入idea</h3> 
  <p><img alt="" class="has" height="332" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190605154424233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3Mjc3MjMyMjQw,size_16,color_FFFFFF,t_70" width="413"></p> 
  <h3 id="3.3%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97kafkareader"><strong>3.</strong>3创建一个模块kafkareader</h3> 
  <p><img alt="" class="has" height="613" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019060515450613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3Mjc3MjMyMjQw,size_16,color_FFFFFF,t_70" width="770"></p> 
  <h3 id="3.4%E5%B0%86%E4%BB%BB%E6%84%8F%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BB%A5%E4%B8%8B%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E8%80%83%E5%85%A5%E5%88%B0resource%E7%9B%AE%E5%BD%95%E4%B8%8B"><strong>3.</strong>4将任意一个模块的以下两个文件考入到resource目录下</h3> 
  <p><img alt="" class="has" height="69" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190605154652547.png" width="248"></p> 
  <h3 id="3.5%E8%BF%9B%E8%A1%8C%E4%BF%AE%E6%94%B9plugin.json"><strong>3.</strong>5进行修改plugin.json</h3> 
  <p><img alt="" class="has" height="129" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190605154759934.png" width="1200"></p> 
  <h3 id="3.6%E4%BF%AE%E6%94%B9pom.xml(%E5%A4%8D%E5%88%B6%E5%85%B6%E4%B8%AD%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E4%BE%9D%E8%B5%96%E5%92%8C%E6%8F%92%E4%BB%B6%E5%88%B0pom.xml)"><strong>3.</strong>6修改pom.xml(复制其中一个文件的依赖和插件到pom.xml)</h3> 
  <h3 id="3.7%E5%B0%86%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9D%97%E4%B8%8B%E9%9D%A2%E7%9A%84%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9%E5%A4%8D%E5%88%B6%E5%88%B0%E6%88%91%E4%BB%AC%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AF%B9%E5%BA%94%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9%EF%BC%8C%E5%B9%B6%E4%B8%94%E4%BF%AE%E6%94%B9package.xml"><strong>3.</strong>7将其他模块下面的，这个文件夹复制到我们模块的对应的文件夹，并且修改package.xml</h3> 
  <h3 id="%E2%80%8B"><img alt="" class="has" height="109" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190605155005247.png" width="325"></h3> 
  <p>要修改的我已经注释好了。就是把你之前复制过来的reader修改为kafkareader</p> 
  <pre class="has">
<code>&lt;assembly
        xmlns="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.0 http://maven.apache.org/xsd/assembly-1.1.0.xsd"&gt;
    &lt;id&gt;&lt;/id&gt;
    &lt;formats&gt;
        &lt;format&gt;dir&lt;/format&gt;
    &lt;/formats&gt;
    &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt;
    &lt;fileSets&gt;
        &lt;fileSet&gt;
            &lt;directory&gt;src/main/resources&lt;/directory&gt;
            &lt;includes&gt;
                &lt;include&gt;plugin.json&lt;/include&gt;
                &lt;include&gt;plugin_job_template.json&lt;/include&gt;
            &lt;/includes&gt;
           //修改为kafkareader
            &lt;outputDirectory&gt;plugin/reader/kafkareader&lt;/outputDirectory&gt;
        &lt;/fileSet&gt;
        &lt;fileSet&gt;
            &lt;directory&gt;target/&lt;/directory&gt;
            &lt;includes&gt;
             //修改为kafkareader
                &lt;include&gt;kafkareader-0.0.1-SNAPSHOT.jar&lt;/include&gt;
            &lt;/includes&gt;
            //修改为kafkareader
            &lt;outputDirectory&gt;plugin/reader/kafkareader&lt;/outputDirectory&gt;
        &lt;/fileSet&gt;
        &lt;!--&lt;fileSet&gt;--&gt;
            &lt;!--&lt;directory&gt;src/main/cpp&lt;/directory&gt;--&gt;
            &lt;!--&lt;includes&gt;--&gt;
                &lt;!--&lt;include&gt;libhadoop.a&lt;/include&gt;--&gt;
                &lt;!--&lt;include&gt;libhadoop.so&lt;/include&gt;--&gt;
                &lt;!--&lt;include&gt;libhadoop.so.1.0.0&lt;/include&gt;--&gt;
                &lt;!--&lt;include&gt;libhadooppipes.a&lt;/include&gt;--&gt;
                &lt;!--&lt;include&gt;libhadooputils.a&lt;/include&gt;--&gt;
                &lt;!--&lt;include&gt;libhdfs.a&lt;/include&gt;--&gt;
                &lt;!--&lt;include&gt;libhdfs.so&lt;/include&gt;--&gt;
                &lt;!--&lt;include&gt;libhdfs.so.0.0.0&lt;/include&gt;--&gt;
            &lt;!--&lt;/includes&gt;--&gt;
            &lt;!--&lt;outputDirectory&gt;plugin/reader/hdfsreader/libs&lt;/outputDirectory&gt;--&gt;
        &lt;!--&lt;/fileSet&gt;--&gt;

    &lt;/fileSets&gt;

    &lt;dependencySets&gt;
        &lt;dependencySet&gt;
            &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt;
             //修改为kafkareader
            &lt;outputDirectory&gt;plugin/reader/kafkareader/libs&lt;/outputDirectory&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependencySet&gt;
    &lt;/dependencySets&gt;
&lt;/assembly&gt;</code></pre> 
  <h3 id="3.8%20%E5%9C%A8%E6%9C%80%E5%A4%96%E5%B1%82%E7%9A%84package.xml%E5%8A%A0%E4%B8%8A%E4%B8%8B%E9%9D%A2%E8%BF%99%E4%B8%AA">3.8 在最外层的package.xml加上下面这个</h3> 
  <p><img alt="" class="has" height="442" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190605155518898.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3Mjc3MjMyMjQw,size_16,color_FFFFFF,t_70" width="896"></p> 
  <h1 id="4.%E5%BC%80%E5%8F%91%E4%BB%A3%E7%A0%81">4.开发代码</h1> 
  <h3 id="4.1%20%E5%BC%80%E5%8F%91%E5%89%8D%E5%B0%86datax%E7%9A%84%E5%BC%80%E5%8F%91%E6%8F%92%E4%BB%B6%E7%9A%84%E6%89%8B%E5%86%8C%E8%AE%A4%E7%9C%9F%E8%A7%82%E7%9C%8B%E4%B8%80%E9%81%8D%EF%BC%8C%E5%AF%B9%E5%BC%80%E5%8F%91%E6%9C%89%E5%B8%AE%E5%8A%A9%E7%9A%84%E3%80%82">4.1 开发前将datax的开发插件的手册认真观看一遍，对开发有帮助的。</h3> 
  <p>地址：<a href="https://github.com/alibaba/DataX/blob/master/dataxPluginDev.md" rel="nofollow" data-token="b1fcbfad075372bb1d517041fc7a7f24">https://github.com/alibaba/DataX/blob/master/dataxPluginDev.md</a></p> 
  <h3 id="4.2%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81%EF%BC%88%E8%A6%81%E7%BB%A7%E6%89%BF%E4%BB%80%E4%B9%88%E7%B1%BB%E5%AE%9E%E7%8E%B0%E4%BB%80%E4%B9%88%E6%96%B9%E6%B3%95%EF%BC%8C4.1%E7%9A%84%E5%BC%80%E5%8F%91%E5%AE%9D%E5%85%B8%E4%B8%8A%E9%83%BD%E5%86%99%E4%BA%86%EF%BC%89">4.2编写代码（要继承什么类实现什么方法，4.1的开发宝典上都写了）</h3> 
  <p>懒得贴那么多代码了，就贴一个关键代码。需要整个项目的话在下方留言</p> 
  <pre class="has">
<code class="language-java">package com.alibaba.datax.plugin.reader.kafkareader;

import com.alibaba.datax.common.element.Record;
import com.alibaba.datax.common.element.StringColumn;
import com.alibaba.datax.common.exception.DataXException;
import com.alibaba.datax.common.plugin.RecordSender;
import com.alibaba.datax.common.spi.Reader;
import com.alibaba.datax.common.util.Configuration;
import com.alibaba.datax.plugin.rdbms.reader.CommonRdbmsReader;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class KafkaReader extends Reader {

    public static class Job extends Reader.Job {
        private static final Logger LOG = LoggerFactory
                .getLogger(Job.class);

        private Configuration originalConfig = null;


        @Override
        public void init() {
            this.originalConfig = super.getPluginJobConf();
            // warn: 忽略大小写

            String topic = this.originalConfig
                    .getString(Key.TOPIC);
            Integer partitions = this.originalConfig
                    .getInt(Key.KAFKA_PARTITIONS);
            String bootstrapServers = this.originalConfig
                    .getString(Key.BOOTSTRAP_SERVERS);

            String groupId = this.originalConfig
                    .getString(Key.GROUP_ID);
            Integer columnCount = this.originalConfig
                    .getInt(Key.COLUMNCOUNT);
            String split = this.originalConfig.getString(Key.SPLIT);
            String filterContaintsStr = this.originalConfig.getString(Key.CONTAINTS_STR);
            String filterContaintsFlag = this.originalConfig.getString(Key.CONTAINTS_STR_FLAG);
            String conditionAllOrOne = this.originalConfig.getString(Key.CONDITION_ALL_OR_ONE);
            String parsingRules = this.originalConfig.getString(Key.PARSING_RULES);
            String writerOrder = this.originalConfig.getString(Key.WRITER_ORDER);
            String kafkaReaderColumnKey = this.originalConfig.getString(Key.KAFKA_READER_COLUMN_KEY);

            System.out.println(topic);
            System.out.println(partitions);
            System.out.println(bootstrapServers);
            System.out.println(groupId);
            System.out.println(columnCount);
            System.out.println(split);
            System.out.println(parsingRules);
            if (null == topic) {

                throw DataXException.asDataXException(KafkaReaderErrorCode.TOPIC_ERROR,
                        "没有设置参数[topic].");
            }
            if (partitions == null) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR,
                        "没有设置参数[kafka.partitions].");
            } else if (partitions &lt; 1) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR,
                        "[kafka.partitions]不能小于1.");
            }
            if (null == bootstrapServers) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.ADDRESS_ERROR,
                        "没有设置参数[bootstrap.servers].");
            }
            if (null == groupId) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR,
                        "没有设置参数[groupid].");
            }

            if (columnCount == null) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.PARTITION_ERROR,
                        "没有设置参数[columnCount].");
            } else if (columnCount &lt; 1) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR,
                        "[columnCount]不能小于1.");
            }
            if (null == split) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR,
                        "[split]不能为空.");
            }
            if (filterContaintsStr != null) {
                if (conditionAllOrOne == null || filterContaintsFlag == null) {
                    throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR,
                            "设置了[filterContaintsStr],但是没有设置[conditionAllOrOne]或者[filterContaintsFlag]");
                }
            }
            if (parsingRules == null) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR,
                        "没有设置[parsingRules]参数");
            } else if (!parsingRules.equals("regex") &amp;&amp; parsingRules.equals("json") &amp;&amp; parsingRules.equals("split")) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR,
                        "[parsingRules]参数设置错误，不是regex，json，split其中一个");
            }
            if (writerOrder == null) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR,
                        "没有设置[writerOrder]参数");
            }
            if (kafkaReaderColumnKey == null) {
                throw DataXException.asDataXException(KafkaReaderErrorCode.KAFKA_READER_ERROR,
                        "没有设置[kafkaReaderColumnKey]参数");
            }
        }

        @Override
        public void preCheck() {
            init();

        }

        @Override
        public List&lt;Configuration&gt; split(int adviceNumber) {
            List&lt;Configuration&gt; configurations = new ArrayList&lt;Configuration&gt;();

            Integer partitions = this.originalConfig.getInt(Key.KAFKA_PARTITIONS);
            for (int i = 0; i &lt; partitions; i++) {
                configurations.add(this.originalConfig.clone());
            }
            return configurations;
        }

        @Override
        public void post() {
        }

        @Override
        public void destroy() {

        }

    }

    public static class Task extends Reader.Task {

        private static final Logger LOG = LoggerFactory
                .getLogger(CommonRdbmsReader.Task.class);
        //配置文件
        private Configuration readerSliceConfig;
        //kafka消息的分隔符
        private String split;
        //解析规则
        private String parsingRules;
        //是否停止拉去数据
        private boolean flag;
        //kafka address
        private String bootstrapServers;
        //kafka groupid
        private String groupId;
        //kafkatopic
        private String kafkaTopic;
        //kafka中的数据一共有多少个字段
        private int count;
        //是否需要data_from
        //kafka ip 端口+ topic
        //将包含/不包含该字符串的数据过滤掉
        private String filterContaintsStr;
        //是包含containtsStr 还是不包含
        //1 表示包含 0 表示不包含
        private int filterContaintsStrFlag;
        //全部包含或不包含，包含其中一个或者不包含其中一个。
        private int conditionAllOrOne;
        //writer端要求的顺序。
        private String writerOrder;
        //kafkareader端的每个关键子的key
        private String kafkaReaderColumnKey;
        //异常文件路径
        private String exceptionPath;

        @Override
        public void init() {
            flag = true;
            this.readerSliceConfig = super.getPluginJobConf();
            split = this.readerSliceConfig.getString(Key.SPLIT);
            bootstrapServers = this.readerSliceConfig.getString(Key.BOOTSTRAP_SERVERS);
            groupId = this.readerSliceConfig.getString(Key.GROUP_ID);
            kafkaTopic = this.readerSliceConfig.getString(Key.TOPIC);
            count = this.readerSliceConfig.getInt(Key.COLUMNCOUNT);
            filterContaintsStr = this.readerSliceConfig.getString(Key.CONTAINTS_STR);
            filterContaintsStrFlag = this.readerSliceConfig.getInt(Key.CONTAINTS_STR_FLAG);
            conditionAllOrOne = this.readerSliceConfig.getInt(Key.CONTAINTS_STR_FLAG);
            parsingRules = this.readerSliceConfig.getString(Key.PARSING_RULES);
            writerOrder = this.readerSliceConfig.getString(Key.WRITER_ORDER);
            kafkaReaderColumnKey = this.readerSliceConfig.getString(Key.KAFKA_READER_COLUMN_KEY);
            exceptionPath = this.readerSliceConfig.getString(Key.EXECPTION_PATH);
            LOG.info(filterContaintsStr);
        }

        @Override
        public void startRead(RecordSender recordSender) {

            Properties props = new Properties();
            props.put("bootstrap.servers", bootstrapServers);
            props.put("group.id", groupId != null ? groupId : UUID.randomUUID().toString());
            props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
            props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
            props.put("enable.auto.commit", "false");
            KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props);
            consumer.subscribe(Collections.singletonList(kafkaTopic));
            Record oneRecord = null;
            while (flag) {
                ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);
                for (ConsumerRecord&lt;String, String&gt; record : records) {

                    String value = record.value();
                    //定义过滤标志
                    int ifNotContinue = filterMessage(value);
                    //如果标志修改为1了那么就过滤掉这条数据。
                    if (ifNotContinue == 1) {
                        LOG.info("过滤数据： " + record.value());
                        continue;
                    }
                    oneRecord = buildOneRecord(recordSender, value);
                    //如果返回值不等于null表示不是异常消息。
                    if (oneRecord != null) {
                        recordSender.sendToWriter(oneRecord);
                    }
                }
                consumer.commitSync();
                //判断当前事件是不是0点,0点的话进程他退出
                Date date = new Date();
                if (DateUtil.targetFormat(date).split(" ")[1].substring(0, 2).equals("00")) {
                    destroy();
                }

            }
        }

        private int filterMessage(String value) {
            //如果要过滤的条件配置了
            int ifNotContinue = 0;

            if (filterContaintsStr != null) {
                String[] filterStrs = filterContaintsStr.split(",");
                //所有
                if (conditionAllOrOne == 1) {
                    //过滤掉包含filterContaintsStr的所有项的值。
                    if (filterContaintsStrFlag == 1) {
                        int i = 0;
                        for (; i &lt; filterStrs.length; i++) {
                            if (!value.contains(filterStrs[i])) break;
                        }
                        if (i &gt;= filterStrs.length) ifNotContinue = 1;
                    } else {
                        //留下掉包含filterContaintsStr的所有项的值
                        int i = 0;
                        for (; i &lt; filterStrs.length; i++) {
                            if (!value.contains(filterStrs[i])) break;
                        }
                        if (i &lt; filterStrs.length) ifNotContinue = 1;
                    }

                } else {
                    //过滤掉包含其中一项的值
                    if (filterContaintsStrFlag == 1) {
                        int i = 0;
                        for (; i &lt; filterStrs.length; i++) {
                            if (value.contains(filterStrs[i])) break;
                        }
                        if (i &lt; filterStrs.length) ifNotContinue = 1;
                    }
                    //留下包含其中一下的值
                    else {
                        int i = 0;
                        for (; i &lt; filterStrs.length; i++) {
                            if (value.contains(filterStrs[i])) break;
                        }
                        if (i &gt;= filterStrs.length) ifNotContinue = 1;
                    }
                }
            }
            return ifNotContinue;

        }

        private Record buildOneRecord(RecordSender recordSender, String value) {
            Record record = null;
            if (parsingRules.equals("regex")) {
                record = parseRegex(value, recordSender);
            } else if (parsingRules.equals("json")) {
                record = parseJson(value, recordSender);
            } else if (parsingRules.equals("split")) {
                record = parseSplit(value, recordSender);
            }
            return record;
        }

        private Record parseSplit(String value, RecordSender recordSender) {
            Record record = recordSender.createRecord();
            String[] splits = value.split(this.split);
            if (splits.length != count) {
                writerErrorPath(value);
                return null;
            }
            parseOrders(Arrays.asList(splits), record);
            return record;
        }

        private Record parseJson(String value, RecordSender recordSender) {
            Record record = recordSender.createRecord();
            HashMap&lt;String, Object&gt; map = JsonUtilJava.parseJsonStrToMap(value);
            String[] columns = kafkaReaderColumnKey.split(",");
            ArrayList&lt;String&gt; datas = new ArrayList&lt;String&gt;();
            for (String column : columns) {
                datas.add(map.get(column).toString());
            }
            if (datas.size() != count) {
                writerErrorPath(value);
                return null;
            }
            parseOrders(datas, record);
            return record;
        }

        private Record parseRegex(String value, RecordSender recordSender) {
            Record record = recordSender.createRecord();
            ArrayList&lt;String&gt; datas = new ArrayList&lt;String&gt;();
            Pattern r = Pattern.compile(split);
            Matcher m = r.matcher(value);
            if (m.find()) {
                if (m.groupCount() != count) {
                    writerErrorPath(value);
                }
                for (int i = 1; i &lt;= count; i++) {
                    //  record.addColumn(new StringColumn(m.group(i)));
                    datas.add(m.group(i));
                    return record;
                }
            } else {
                writerErrorPath(value);
            }

            parseOrders(datas, record);

            return null;
        }

        private void writerErrorPath(String value) {
            if (exceptionPath == null) return;
            FileOutputStream fileOutputStream = null;
            try {
                fileOutputStream = getFileOutputStream();
                fileOutputStream.write((value + "\n").getBytes());
                fileOutputStream.close();
            } catch (FileNotFoundException e) {
                e.printStackTrace();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }

        private FileOutputStream getFileOutputStream() throws FileNotFoundException {
            return new FileOutputStream(exceptionPath + "/" + kafkaTopic + "errordata" + DateUtil.targetFormat(new Date(), "yyyyMMdd"), true);
        }

        private void parseOrders(List&lt;String&gt; datas, Record record) {
            //writerOrder
            String[] orders = writerOrder.split(",");
            for (String order : orders) {
                if (order.equals("data_from")) {
                    record.addColumn(new StringColumn(bootstrapServers + "|" + kafkaTopic));
                } else if (order.equals("uuid")) {
                    record.addColumn(new StringColumn(UUID.randomUUID().toString()));
                } else if (order.equals("null")) {
                    record.addColumn(new StringColumn("null"));
                } else if (order.equals("datax_time")) {
                    record.addColumn(new StringColumn(DateUtil.targetFormat(new Date())));
                } else if (isNumeric(order)) {
                    record.addColumn(new StringColumn(datas.get(new Integer(order) - 1)));
                }
            }
        }

        public static boolean isNumeric(String str) {
            for (int i = 0; i &lt; str.length(); i++) {
                if (!Character.isDigit(str.charAt(i))) {
                    return false;
                }
            }
            return true;
        }

        @Override
        public void post() {
        }

        @Override
        public void destroy() {
            flag = false;
        }

       
}
</code></pre> 
  <h1 id="5.%E6%89%93%E5%8C%85%E8%BF%90%E8%A1%8C">5.打包运行</h1> 
  <h3 id="5.1%E5%B0%86%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9D%97%E6%B3%A8%E9%87%8A%E5%8F%AA%E7%95%99%E4%B8%8B%E5%85%AC%E5%85%B1%E6%A8%A1%E5%9D%97%E5%92%8C%E8%87%AA%E5%B7%B1%E7%9A%84%E9%A1%B9%E7%9B%AE%E6%A8%A1%E5%9D%97">5.1将其他模块注释只留下公共模块和自己的项目模块</h3> 
  <p>在最为外层的pom.xml中注释</p> 
  <p><img alt="" class="has" height="770" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190605160143813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3Mjc3MjMyMjQw,size_16,color_FFFFFF,t_70" width="1200"></p> 
  <h3 id="5.2%20%E8%BF%9B%E5%85%A5%E5%88%B0%E9%A1%B9%E7%9B%AE%E6%9C%80%E5%A4%96%E5%B1%82%E7%9A%84%E7%9B%AE%E5%BD%95%E8%BE%93%E5%85%A5cmd%EF%BC%88%E5%89%8D%E6%8F%90%E9%85%8D%E7%BD%AE%E4%BA%86%E6%9C%AC%E5%9C%B0maven%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%EF%BC%89">5.2 进入到项目最外层的目录输入cmd（前提配置了本地maven的环境变量）</h3> 
  <p><img alt="" class="has" height="295" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190605160248935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3Mjc3MjMyMjQw,size_16,color_FFFFFF,t_70" width="1200"></p> 
  <h3 id="5.3%20%E4%BD%BF%E7%94%A8maven%E5%91%BD%E4%BB%A4%E6%89%93%E5%8C%85">5.3 使用maven命令打包</h3> 
  <p>&nbsp;mvn -U clean package assembly:assembly -Dmaven.test.skip=true</p> 
  <h3 id="5.4%20%E6%89%93%E5%8C%85%E5%90%8E%E5%B0%86%E4%B8%8B%E5%9B%BE%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84%E5%8C%85%E4%B8%8A%E4%BC%A0%E5%88%B0%E9%9B%86%E7%BE%A4%E7%9A%84datax%E5%AF%B9%E5%BA%94%E7%9B%AE%E5%BD%95">5.4 打包后将下图目录下的包上传到集群的datax对应目录</h3> 
  <p>本地地址：D:\DataX-master\kafkareader\target\datax\plugin\reader</p> 
  <p><img alt="" class="has" height="219" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190605160502777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3Mjc3MjMyMjQw,size_16,color_FFFFFF,t_70" width="1009"></p> 
  <p>集群地址：/opt/module/datax/plugin/reader</p> 
  <p><img alt="" class="has" height="315" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/201906051606421.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3Mjc3MjMyMjQw,size_16,color_FFFFFF,t_70" width="511">\</p> 
  <h3 id="5.5%20%E5%86%99%E5%A5%BD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B0%B1%E5%8F%AF%E4%BB%A5%E8%BF%90%E8%A1%8C%E4%BA%86">5.5 写好配置文件就可以运行了</h3> 
  <pre class="has">
<code class="language-java">{
    "job": {
        "content": [
            {
               "reader": {
                    "name": "kafkareader",
                    "parameter": {
                        "topic": "Event",
                        "bootstrapServers": "192.168.7.128:9092",
                        "kafkaPartitions": "1",
                        "columnCount":11,
                        "groupId":"ast",
                        "filterContaints":"5^1,6^5",
                        "filterContaintsFlag":1,
                        "conditionAllOrOne":0,
                        "parsingRules":"regex",
                        "writerOrder":"uuid,1,3,6,4,8,9,10,11,5,7,2,null,datax_time,data_from",
                        "kafkaReaderColumnKey":"a",
                        "execptionPath":"/opt/module/datax/log/errorlog"
                         }
                },
                 "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://master:8020",
                        "fileType": "orc",
                        "path": "${path}",
                        "fileName": "t_rsd_amber_agent_event_log",
                        "column": [
                             {
                                "name": "id",
                                "type": "string"
                            },
                            {
                                "name": "risk_level",
                                "type": "string"
                            },
                            {
                                "name": "device_uuid",
                                "type": "string"
                            },
                             {
                                "name": "event_device_id",
                                "type": "string"
                            },
                            {
                                "name": "device_type",
                                "type": "string"
                            },
                            {
                                "name": "event_type",
                                "type": "string"
                            },
                            {
                                "name": "event_sub_type",
                                "type": "string"
                            },
                             {
                                "name": "repeats",
                                "type": "string"
                            },
                            {
                                "name": "description",
                                "type": "string"
                            },
                            {
                                "name": "event_time",
                                "type": "string"
                            },
                            {
                                "name": "report_device_type",
                                "type": "string"
                            },
                            {
                                "name": "event_report_time",
                                "type": "string"
                            },
                             {
                                "name": "last_update_time",
                                "type": "string"
                            },
                            {
                                "name": "datax_time",
                                "type": "string"
                            }
                             , {
                                "name": "data_from",
                                "type": "string"
                            },
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": "\t",
                        "compress":"NONE",
                        "scrollFileTime":300000
                    }
                }

            }
        ],
         "setting": {
            "speed": {
                "channel": 3,
                "record": 20000,
                 "byte":5000 ,
                 "batchSize":2048

            }
        }
    }
}
</code></pre> 
  <p>运行命令：</p> 
  <p>&nbsp;python /opt/module/datax/bin/datax.py -p "-Dpath=/data/warehouse/rsd/t_rsd_amber_agent_event_log/2019/06/05" /opt/module/datax/job/kafkatohdfs.json</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
