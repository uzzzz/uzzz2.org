<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>飞桨带你了解：基于百科类数据训练的 ELMo 中文预训练模型 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="飞桨带你了解：基于百科类数据训练的 ELMo 中文预训练模型" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="在NLP世界里，有一支很重要的家族，英文叫做LARK（LAnguage Representations Kit），翻译成中文是语言表示工具箱。目前LARK家族最新最重要的三种算法，分别是ELMo，BERT和ERNIE。 你一定不知道，这三个普通的名字，竟然包含着一个有趣的秘密。 真相，即将揭开！ 我们先从算法模型的名字寻找一些蛛丝马迹 第一位，ELMo： 来自英文Embedding from Language Models 的缩写，来自论文名为Deep contextualized word representation 第二位，BERT: 来自英文Bidirectional Encoder Representations from Transformers的缩写，来自论文名为Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding 第三位，ERNIE: 来自英文Enhanced Representation through kNowledge IntEgration) 的缩，来自论文名为Enhanced Representation through Knowledge Integration 看完了，是不是，还是一头雾水，哪里有什么秘密？ 不卖关子了，直接上图！ What？？ 再回头看看，你还记得那三个算法的名字么？ &nbsp; ELMo，BERT，ERNIE 竟然都是美国经典动画片，《Sesame Street（芝麻街）》里面的卡通人物！！！ &nbsp; 好吧，如果你说，没看过这个动画片，没感觉啊。那我举个例子，如果把《芝麻街》类比成中文《舒克和贝塔》。那么，第一篇论文把模型取做“舒克”，第二篇很有爱的就叫做“贝塔”，第三篇就硬把模型叫做“皮皮鲁”，也许不久的下一个模型就命名为“鲁西西”啦。 &nbsp; 谁说科学家们很无聊，是不是也很童趣？ 好了，扯远了，今天我们先给大家介绍LARK家族的ELMo！ 提出它的论文获得2018年NAACL最佳paper，它在NLP领域可是有着响当当的名头，让我们来认识它！ ELMo模型简介 ELMo(Embeddings from Language Models) 是重要的通用语义表示模型之一，以双向 LSTM 为网路基本组件，以 Language Model 为训练目标，通过预训练得到通用的语义表示，将通用的语义表示作为 Feature 迁移到下游 NLP 任务中，会显著提升下游任务的模型性能。 &nbsp; ELMo模型核心是一个双层双向的LSTM网络，与传统的word2vec算法中词向量一成不变相比，ELMo会根据上下文改变语义embedding。 一个简单的例子就是 “苹果”的词向量： 句子1：“我 买了 1斤 苹果” 句子2：“我 新 买了 1个 苹果 X” 在word2vec算法中，“苹果”的词向量固定，无法区分这两句话的区别，而ELMo可以解决语言中的二义性问题，可以带来性能的显著提升。 ELMo项目的飞桨（PaddlePaddle）实现 为了方便广大的开发者，飞桨（PaddlePaddle） 完成了ELMo的开源实现（依赖于 Paddle Fluid 1.4），发布要点如下。 注意啦，下面划重点！！！ &nbsp; 接下来，我们看看怎么可以快速把ELMo用到我们的项目中来吧！ &nbsp; ELMo训练过程介绍 &nbsp; （1）数据预处理 &nbsp; 将文档按照句号、问号、感叹以及内容分词预处理。预处理后的数据文件，每行为一个分词后的句子。给出了训练数据 data/train 和测试数据 data/dev的数据示例如下： 本 书 介绍 了 中国 经济 发展 的 内外 平衡问题 、 亚洲 金融 危机 十 周年 回顾 与 反思 、 实践 中 的 城乡 统筹 发展 、 未来 十 年 中国 需要 研究 的 重大 课题 、 科学 发展 与新型 工业 化 等 方面 。 吴 敬 琏 曾经 提出 中国 股市 “ 赌场 论 ” ， 主张 维护 市场 规则 ， 保护 草根 阶层 生计， 被 誉 为 “ 中国 经济 学界 良心 ” ， 是 媒体 和公众 眼中 的 学术 明星 &nbsp; （2）模型训练 &nbsp; 利用提供的示例训练数据和测试数据，进行单机多卡预训练。在开始预训练之前，需要把 CUDA、cuDNN、NCCL2 等动态库路径加入到环境变量 LD_LIBRARY_PATH 之中，然后执行run.sh即可开始单机多卡预训练，run.sh文件内容如下： &nbsp; &nbsp; &nbsp; export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train.py \ --train_path=&#39;data/train/sentence_file_*&#39; \ --test_path=&#39;data/dev/sentence_file_*&#39; \ --vocab_path data/vocabulary_min5k.txt \ --learning_rate 0.2 \ --use_gpu True \ --all_train_tokens 35479 \ --local True $@ 其中，all_train_tokens为train和dev统计出来的tokens总量，训练过程中，默认每个epoch后，将模型参数写入到 checkpoints 路径下，可以用于迁移到下游NLP任务。 （3）ELMo模型迁移 &nbsp; 以 LAC 任务为示例, 将 ELMo 预训练模型的语义表示迁移到 LAC 任务的主要步骤如下： #step1:&nbsp;在已经搭建好的LAC 网络结构之后，加载 ELMo 预训练模型参数： &nbsp; &nbsp; from bilm import init_pretraining_params init_pretraining_params(exe,args.pretrain_elmo_model_path, fluid.default_main_program()) &nbsp; #step2:&nbsp;基于ELMo 字典 将输入数据转化为 word_ids，利用 elmo_encoder接口获取 ELMo embedding： from bilm import elmo_encoder elmo_embedding = elmo_encoder(word_ids) #step3:&nbsp;ELMoembedding与 LAC 原有 word_embedding 拼接得到最终的 embedding： &nbsp; word_embedding=fluid.layers.concat(input=[elmo_embedding, word_embedding], axis=1) 好的，到这里，模型的迁移就完成了，再来回顾一下加入ELMo后对性能的提升，心动不如行动，赶紧用起来吧！ ERNIE模型简介 学习完了ELMo，我们再来了解一下LARK家族的学习成绩最好的重磅成员ERNIE，在多项NLP中文任务上表现非凡。 ERNIE通过建模海量数据中的实体概念等先验语义知识，学习真实世界的语义关系。具体来说，ERNIE 模型通过对词、实体等语义单元的掩码，使得模型学习完整概念的语义表示。相较于BERT 学习原始语言信号，ERNIE 直接对先验语义知识单元进行建模，增强了模型语义表示能力。 ERNIE在多个公开的中文数据集上进行了效果验证，包括语言推断、语义相似度、命名实体识别、情感分析、问答匹配等自然语言处理各类任务上，均超越了语义表示模型 BERT 的效果。 更多详细内容请点击文末阅读原文或参见： https://github.com/PaddlePaddle/LARK" />
<meta property="og:description" content="在NLP世界里，有一支很重要的家族，英文叫做LARK（LAnguage Representations Kit），翻译成中文是语言表示工具箱。目前LARK家族最新最重要的三种算法，分别是ELMo，BERT和ERNIE。 你一定不知道，这三个普通的名字，竟然包含着一个有趣的秘密。 真相，即将揭开！ 我们先从算法模型的名字寻找一些蛛丝马迹 第一位，ELMo： 来自英文Embedding from Language Models 的缩写，来自论文名为Deep contextualized word representation 第二位，BERT: 来自英文Bidirectional Encoder Representations from Transformers的缩写，来自论文名为Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding 第三位，ERNIE: 来自英文Enhanced Representation through kNowledge IntEgration) 的缩，来自论文名为Enhanced Representation through Knowledge Integration 看完了，是不是，还是一头雾水，哪里有什么秘密？ 不卖关子了，直接上图！ What？？ 再回头看看，你还记得那三个算法的名字么？ &nbsp; ELMo，BERT，ERNIE 竟然都是美国经典动画片，《Sesame Street（芝麻街）》里面的卡通人物！！！ &nbsp; 好吧，如果你说，没看过这个动画片，没感觉啊。那我举个例子，如果把《芝麻街》类比成中文《舒克和贝塔》。那么，第一篇论文把模型取做“舒克”，第二篇很有爱的就叫做“贝塔”，第三篇就硬把模型叫做“皮皮鲁”，也许不久的下一个模型就命名为“鲁西西”啦。 &nbsp; 谁说科学家们很无聊，是不是也很童趣？ 好了，扯远了，今天我们先给大家介绍LARK家族的ELMo！ 提出它的论文获得2018年NAACL最佳paper，它在NLP领域可是有着响当当的名头，让我们来认识它！ ELMo模型简介 ELMo(Embeddings from Language Models) 是重要的通用语义表示模型之一，以双向 LSTM 为网路基本组件，以 Language Model 为训练目标，通过预训练得到通用的语义表示，将通用的语义表示作为 Feature 迁移到下游 NLP 任务中，会显著提升下游任务的模型性能。 &nbsp; ELMo模型核心是一个双层双向的LSTM网络，与传统的word2vec算法中词向量一成不变相比，ELMo会根据上下文改变语义embedding。 一个简单的例子就是 “苹果”的词向量： 句子1：“我 买了 1斤 苹果” 句子2：“我 新 买了 1个 苹果 X” 在word2vec算法中，“苹果”的词向量固定，无法区分这两句话的区别，而ELMo可以解决语言中的二义性问题，可以带来性能的显著提升。 ELMo项目的飞桨（PaddlePaddle）实现 为了方便广大的开发者，飞桨（PaddlePaddle） 完成了ELMo的开源实现（依赖于 Paddle Fluid 1.4），发布要点如下。 注意啦，下面划重点！！！ &nbsp; 接下来，我们看看怎么可以快速把ELMo用到我们的项目中来吧！ &nbsp; ELMo训练过程介绍 &nbsp; （1）数据预处理 &nbsp; 将文档按照句号、问号、感叹以及内容分词预处理。预处理后的数据文件，每行为一个分词后的句子。给出了训练数据 data/train 和测试数据 data/dev的数据示例如下： 本 书 介绍 了 中国 经济 发展 的 内外 平衡问题 、 亚洲 金融 危机 十 周年 回顾 与 反思 、 实践 中 的 城乡 统筹 发展 、 未来 十 年 中国 需要 研究 的 重大 课题 、 科学 发展 与新型 工业 化 等 方面 。 吴 敬 琏 曾经 提出 中国 股市 “ 赌场 论 ” ， 主张 维护 市场 规则 ， 保护 草根 阶层 生计， 被 誉 为 “ 中国 经济 学界 良心 ” ， 是 媒体 和公众 眼中 的 学术 明星 &nbsp; （2）模型训练 &nbsp; 利用提供的示例训练数据和测试数据，进行单机多卡预训练。在开始预训练之前，需要把 CUDA、cuDNN、NCCL2 等动态库路径加入到环境变量 LD_LIBRARY_PATH 之中，然后执行run.sh即可开始单机多卡预训练，run.sh文件内容如下： &nbsp; &nbsp; &nbsp; export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train.py \ --train_path=&#39;data/train/sentence_file_*&#39; \ --test_path=&#39;data/dev/sentence_file_*&#39; \ --vocab_path data/vocabulary_min5k.txt \ --learning_rate 0.2 \ --use_gpu True \ --all_train_tokens 35479 \ --local True $@ 其中，all_train_tokens为train和dev统计出来的tokens总量，训练过程中，默认每个epoch后，将模型参数写入到 checkpoints 路径下，可以用于迁移到下游NLP任务。 （3）ELMo模型迁移 &nbsp; 以 LAC 任务为示例, 将 ELMo 预训练模型的语义表示迁移到 LAC 任务的主要步骤如下： #step1:&nbsp;在已经搭建好的LAC 网络结构之后，加载 ELMo 预训练模型参数： &nbsp; &nbsp; from bilm import init_pretraining_params init_pretraining_params(exe,args.pretrain_elmo_model_path, fluid.default_main_program()) &nbsp; #step2:&nbsp;基于ELMo 字典 将输入数据转化为 word_ids，利用 elmo_encoder接口获取 ELMo embedding： from bilm import elmo_encoder elmo_embedding = elmo_encoder(word_ids) #step3:&nbsp;ELMoembedding与 LAC 原有 word_embedding 拼接得到最终的 embedding： &nbsp; word_embedding=fluid.layers.concat(input=[elmo_embedding, word_embedding], axis=1) 好的，到这里，模型的迁移就完成了，再来回顾一下加入ELMo后对性能的提升，心动不如行动，赶紧用起来吧！ ERNIE模型简介 学习完了ELMo，我们再来了解一下LARK家族的学习成绩最好的重磅成员ERNIE，在多项NLP中文任务上表现非凡。 ERNIE通过建模海量数据中的实体概念等先验语义知识，学习真实世界的语义关系。具体来说，ERNIE 模型通过对词、实体等语义单元的掩码，使得模型学习完整概念的语义表示。相较于BERT 学习原始语言信号，ERNIE 直接对先验语义知识单元进行建模，增强了模型语义表示能力。 ERNIE在多个公开的中文数据集上进行了效果验证，包括语言推断、语义相似度、命名实体识别、情感分析、问答匹配等自然语言处理各类任务上，均超越了语义表示模型 BERT 的效果。 更多详细内容请点击文末阅读原文或参见： https://github.com/PaddlePaddle/LARK" />
<link rel="canonical" href="https://uzzz.org/2019/06/06/788481.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/06/788481.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-06T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"在NLP世界里，有一支很重要的家族，英文叫做LARK（LAnguage Representations Kit），翻译成中文是语言表示工具箱。目前LARK家族最新最重要的三种算法，分别是ELMo，BERT和ERNIE。 你一定不知道，这三个普通的名字，竟然包含着一个有趣的秘密。 真相，即将揭开！ 我们先从算法模型的名字寻找一些蛛丝马迹 第一位，ELMo： 来自英文Embedding from Language Models 的缩写，来自论文名为Deep contextualized word representation 第二位，BERT: 来自英文Bidirectional Encoder Representations from Transformers的缩写，来自论文名为Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding 第三位，ERNIE: 来自英文Enhanced Representation through kNowledge IntEgration) 的缩，来自论文名为Enhanced Representation through Knowledge Integration 看完了，是不是，还是一头雾水，哪里有什么秘密？ 不卖关子了，直接上图！ What？？ 再回头看看，你还记得那三个算法的名字么？ &nbsp; ELMo，BERT，ERNIE 竟然都是美国经典动画片，《Sesame Street（芝麻街）》里面的卡通人物！！！ &nbsp; 好吧，如果你说，没看过这个动画片，没感觉啊。那我举个例子，如果把《芝麻街》类比成中文《舒克和贝塔》。那么，第一篇论文把模型取做“舒克”，第二篇很有爱的就叫做“贝塔”，第三篇就硬把模型叫做“皮皮鲁”，也许不久的下一个模型就命名为“鲁西西”啦。 &nbsp; 谁说科学家们很无聊，是不是也很童趣？ 好了，扯远了，今天我们先给大家介绍LARK家族的ELMo！ 提出它的论文获得2018年NAACL最佳paper，它在NLP领域可是有着响当当的名头，让我们来认识它！ ELMo模型简介 ELMo(Embeddings from Language Models) 是重要的通用语义表示模型之一，以双向 LSTM 为网路基本组件，以 Language Model 为训练目标，通过预训练得到通用的语义表示，将通用的语义表示作为 Feature 迁移到下游 NLP 任务中，会显著提升下游任务的模型性能。 &nbsp; ELMo模型核心是一个双层双向的LSTM网络，与传统的word2vec算法中词向量一成不变相比，ELMo会根据上下文改变语义embedding。 一个简单的例子就是 “苹果”的词向量： 句子1：“我 买了 1斤 苹果” 句子2：“我 新 买了 1个 苹果 X” 在word2vec算法中，“苹果”的词向量固定，无法区分这两句话的区别，而ELMo可以解决语言中的二义性问题，可以带来性能的显著提升。 ELMo项目的飞桨（PaddlePaddle）实现 为了方便广大的开发者，飞桨（PaddlePaddle） 完成了ELMo的开源实现（依赖于 Paddle Fluid 1.4），发布要点如下。 注意啦，下面划重点！！！ &nbsp; 接下来，我们看看怎么可以快速把ELMo用到我们的项目中来吧！ &nbsp; ELMo训练过程介绍 &nbsp; （1）数据预处理 &nbsp; 将文档按照句号、问号、感叹以及内容分词预处理。预处理后的数据文件，每行为一个分词后的句子。给出了训练数据 data/train 和测试数据 data/dev的数据示例如下： 本 书 介绍 了 中国 经济 发展 的 内外 平衡问题 、 亚洲 金融 危机 十 周年 回顾 与 反思 、 实践 中 的 城乡 统筹 发展 、 未来 十 年 中国 需要 研究 的 重大 课题 、 科学 发展 与新型 工业 化 等 方面 。 吴 敬 琏 曾经 提出 中国 股市 “ 赌场 论 ” ， 主张 维护 市场 规则 ， 保护 草根 阶层 生计， 被 誉 为 “ 中国 经济 学界 良心 ” ， 是 媒体 和公众 眼中 的 学术 明星 &nbsp; （2）模型训练 &nbsp; 利用提供的示例训练数据和测试数据，进行单机多卡预训练。在开始预训练之前，需要把 CUDA、cuDNN、NCCL2 等动态库路径加入到环境变量 LD_LIBRARY_PATH 之中，然后执行run.sh即可开始单机多卡预训练，run.sh文件内容如下： &nbsp; &nbsp; &nbsp; export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train.py \\ --train_path=&#39;data/train/sentence_file_*&#39; \\ --test_path=&#39;data/dev/sentence_file_*&#39; \\ --vocab_path data/vocabulary_min5k.txt \\ --learning_rate 0.2 \\ --use_gpu True \\ --all_train_tokens 35479 \\ --local True $@ 其中，all_train_tokens为train和dev统计出来的tokens总量，训练过程中，默认每个epoch后，将模型参数写入到 checkpoints 路径下，可以用于迁移到下游NLP任务。 （3）ELMo模型迁移 &nbsp; 以 LAC 任务为示例, 将 ELMo 预训练模型的语义表示迁移到 LAC 任务的主要步骤如下： #step1:&nbsp;在已经搭建好的LAC 网络结构之后，加载 ELMo 预训练模型参数： &nbsp; &nbsp; from bilm import init_pretraining_params init_pretraining_params(exe,args.pretrain_elmo_model_path, fluid.default_main_program()) &nbsp; #step2:&nbsp;基于ELMo 字典 将输入数据转化为 word_ids，利用 elmo_encoder接口获取 ELMo embedding： from bilm import elmo_encoder elmo_embedding = elmo_encoder(word_ids) #step3:&nbsp;ELMoembedding与 LAC 原有 word_embedding 拼接得到最终的 embedding： &nbsp; word_embedding=fluid.layers.concat(input=[elmo_embedding, word_embedding], axis=1) 好的，到这里，模型的迁移就完成了，再来回顾一下加入ELMo后对性能的提升，心动不如行动，赶紧用起来吧！ ERNIE模型简介 学习完了ELMo，我们再来了解一下LARK家族的学习成绩最好的重磅成员ERNIE，在多项NLP中文任务上表现非凡。 ERNIE通过建模海量数据中的实体概念等先验语义知识，学习真实世界的语义关系。具体来说，ERNIE 模型通过对词、实体等语义单元的掩码，使得模型学习完整概念的语义表示。相较于BERT 学习原始语言信号，ERNIE 直接对先验语义知识单元进行建模，增强了模型语义表示能力。 ERNIE在多个公开的中文数据集上进行了效果验证，包括语言推断、语义相似度、命名实体识别、情感分析、问答匹配等自然语言处理各类任务上，均超越了语义表示模型 BERT 的效果。 更多详细内容请点击文末阅读原文或参见： https://github.com/PaddlePaddle/LARK","@type":"BlogPosting","url":"https://uzzz.org/2019/06/06/788481.html","headline":"飞桨带你了解：基于百科类数据训练的 ELMo 中文预训练模型","dateModified":"2019-06-06T00:00:00+08:00","datePublished":"2019-06-06T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/06/788481.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>飞桨带你了解：基于百科类数据训练的 ELMo 中文预训练模型</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>在NLP世界里，有一支很重要的家族，英文叫做LARK（LAnguage Representations Kit），翻译成中文是语言表示工具箱。目前LARK家族最新最重要的三种算法，分别是ELMo，BERT和ERNIE。</p> 
  <p>你一定不知道，这三个普通的名字，竟然包含着一个有趣的秘密。</p> 
  <p><strong>真相，即将揭开！</strong></p> 
  <p><img alt="" class="has" src="http://pic.rmb.bdstatic.com/a7a0550a167653c26c817177189d22d7.jpeg@wm_2,t_55m+5a625Y+3L1BhZGRsZVBhZGRsZQ==,fc_ffffff,ff_U2ltSGVp,sz_6,x_4,y_4"></p> 
  <p><strong>我们先从算法模型的名字寻找一些蛛丝马迹</strong></p> 
  <p><strong>第一位，ELMo：</strong></p> 
  <p>来自英文Embedding from Language Models 的缩写，来自论文名为Deep contextualized word representation</p> 
  <p><strong>第二位，BERT:</strong></p> 
  <p>来自英文Bidirectional Encoder Representations from Transformers的缩写，来自论文名为Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding</p> 
  <p><strong>第三位，ERNIE:</strong></p> 
  <p>来自英文Enhanced Representation through kNowledge IntEgration) 的缩，来自论文名为Enhanced Representation through Knowledge Integration</p> 
  <p>看完了，是不是，还是一头雾水，哪里有什么秘密？</p> 
  <p><img alt="" class="has" src="http://pic.rmb.bdstatic.com/6ce2e85a75c98e63179f635a34a9cbbb.png@wm_2,t_55m+5a625Y+3L1BhZGRsZVBhZGRsZQ==,fc_ffffff,ff_U2ltSGVp,sz_3,x_2,y_2"></p> 
  <p><strong>不卖关子了，直接上图！</strong></p> 
  <p><img alt="" class="has" src="http://pic.rmb.bdstatic.com/a57ae6160c026952551981693c4fc785.jpeg@wm_2,t_55m+5a625Y+3L1BhZGRsZVBhZGRsZQ==,fc_ffffff,ff_U2ltSGVp,sz_27,x_17,y_17"></p> 
  <p><strong>What？？</strong></p> 
  <p><strong>再回头看看，你还记得那三个算法的名字么？</strong></p> 
  <p>&nbsp;</p> 
  <p><strong>ELMo，BERT，ERNIE</strong></p> 
  <p>竟然都是美国经典动画片，《Sesame Street（芝麻街）》里面的卡通人物！！！</p> 
  <p>&nbsp;</p> 
  <p>好吧，如果你说，没看过这个动画片，没感觉啊。那我举个例子，如果把《芝麻街》类比成中文《舒克和贝塔》。那么，第一篇论文把模型取做“舒克”，第二篇很有爱的就叫做“贝塔”，第三篇就硬把模型叫做“皮皮鲁”，也许不久的下一个模型就命名为“鲁西西”啦。</p> 
  <p>&nbsp;</p> 
  <p>谁说科学家们很无聊，是不是也很童趣？</p> 
  <p>好了，扯远了，今天我们先给大家介绍LARK家族的ELMo！ 提出它的论文获得2018年NAACL最佳paper，它在NLP领域可是有着响当当的名头，让我们来认识它！</p> 
  <p><strong>ELMo</strong><strong>模型简介</strong></p> 
  <p>ELMo(Embeddings from Language Models) 是重要的通用语义表示模型之一，以双向 LSTM 为网路基本组件，以 Language Model 为训练目标，通过预训练得到通用的语义表示，将通用的语义表示作为 Feature 迁移到下游 NLP 任务中，会显著提升下游任务的模型性能。</p> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" src="http://pic.rmb.bdstatic.com/99cabc40d07e148fe4bd06f4b97123bc.jpeg@wm_2,t_55m+5a625Y+3L1BhZGRsZVBhZGRsZQ==,fc_ffffff,ff_U2ltSGVp,sz_21,x_14,y_14"></p> 
  <p>ELMo模型核心是一个双层双向的LSTM网络，与传统的word2vec算法中词向量一成不变相比，ELMo会根据上下文改变语义embedding。</p> 
  <p>一个简单的例子就是 “苹果”的词向量：</p> 
  <p>句子1：“我 买了 1斤 苹果”</p> 
  <p>句子2：“我 新 买了 1个 苹果 X”</p> 
  <p>在word2vec算法中，“苹果”的词向量固定，无法区分这两句话的区别，而ELMo可以解决语言中的二义性问题，可以带来性能的显著提升。</p> 
  <p><strong>ELMo</strong><strong>项目的飞桨（Paddle</strong><strong>Paddle</strong><strong>）实现</strong></p> 
  <p>为了方便广大的开发者，飞桨（PaddlePaddle） 完成了ELMo的开源实现（依赖于 Paddle Fluid 1.4），发布要点如下。</p> 
  <p><strong>注意啦，下面划重点！！！</strong></p> 
  <p><img alt="" class="has" src="http://pic.rmb.bdstatic.com/26fa24bb1e65ad66393653b19d2a05c71728.jpeg"><img alt="" class="has" src="http://pic.rmb.bdstatic.com/261a989495871b8ee40effd703e513b87655.jpeg"></p> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" src="http://pic.rmb.bdstatic.com/7bbd36170d3aa17cf628e01ced999e0a4103.jpeg"></p> 
  <p>接下来，我们看看怎么可以快速把ELMo用到我们的项目中来吧！</p> 
  <p>&nbsp;</p> 
  <p><strong>ELMo</strong><strong>训练过程介绍</strong></p> 
  <p>&nbsp;</p> 
  <p><strong>（1）数据预处理</strong></p> 
  <p>&nbsp;</p> 
  <p>将文档按照句号、问号、感叹以及内容分词预处理。预处理后的数据文件，每行为一个分词后的句子。给出了训练数据 data/train 和测试数据 data/dev的数据示例如下：</p> 
  <p>本 书 介绍 了 中国 经济 发展 的 内外 平衡问题 、 亚洲 金融 危机 十 周年 回顾 与 反思 、 实践 中 的 城乡 统筹 发展 、 未来 十 年 中国 需要 研究 的 重大 课题 、 科学 发展 与新型 工业 化 等 方面 。</p> 
  <p>吴 敬 琏 曾经 提出 中国 股市 “ 赌场 论 ” ， 主张 维护 市场 规则 ， 保护 草根 阶层 生计， 被 誉 为 “ 中国 经济 学界 良心 ” ， 是 媒体 和公众 眼中 的 学术 明星</p> 
  <p>&nbsp;</p> 
  <p><strong>（2）模型训练</strong></p> 
  <p>&nbsp;</p> 
  <p>利用提供的示例训练数据和测试数据，进行单机多卡预训练。在开始预训练之前，需要把 CUDA、cuDNN、NCCL2 等动态库路径加入到环境变量 LD_LIBRARY_PATH 之中，然后执行run.sh即可开始单机多卡预训练，run.sh文件内容如下：</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <ul>
   <li> <p>&nbsp;</p> </li> 
  </ul>
  <p>export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7</p> 
  <p>python train.py \</p> 
  <p>--train_path='data/train/sentence_file_*' \</p> 
  <p>--test_path='data/dev/sentence_file_*' \</p> 
  <p>--vocab_path data/vocabulary_min5k.txt \</p> 
  <p>--learning_rate 0.2 \</p> 
  <p>--use_gpu True \</p> 
  <p>--all_train_tokens 35479 \</p> 
  <p>--local True $@</p> 
  <p>其中，all_train_tokens为train和dev统计出来的tokens总量，训练过程中，默认每个epoch后，将模型参数写入到 checkpoints 路径下，可以用于迁移到下游NLP任务。</p> 
  <p><strong>（3）ELMo模型迁移</strong></p> 
  <p>&nbsp;</p> 
  <p>以 LAC 任务为示例, 将 ELMo 预训练模型的语义表示迁移到 LAC 任务的主要步骤如下：</p> 
  <p><strong>#step1:</strong>&nbsp;在已经搭建好的LAC 网络结构之后，加载 ELMo 预训练模型参数：</p> 
  <p>&nbsp;</p> 
  <ul>
   <li> <p>&nbsp;</p> </li> 
  </ul>
  <p>from bilm import</p> 
  <p>init_pretraining_params</p> 
  <p>init_pretraining_params(exe,args.pretrain_elmo_model_path, fluid.default_main_program())</p> 
  <p>&nbsp;</p> 
  <p><strong>#step2:&nbsp;</strong>基于ELMo 字典 将输入数据转化为 word_ids，利用 elmo_encoder接口获取 ELMo embedding：</p> 
  <p>from bilm import elmo_encoder</p> 
  <p>elmo_embedding = elmo_encoder(word_ids)</p> 
  <p><strong>#step3:&nbsp;</strong>ELMoembedding与 LAC 原有 word_embedding 拼接得到最终的 embedding：</p> 
  <p>&nbsp;</p> 
  <ul>
   <li> <p>word_embedding=fluid.layers.concat(input=[elmo_embedding, word_embedding], axis=1)</p> </li> 
  </ul>
  <p>好的，到这里，模型的迁移就完成了，再来回顾一下加入ELMo后对性能的提升，心动不如行动，赶紧用起来吧！</p> 
  <p><img alt="" class="has" src="http://pic.rmb.bdstatic.com/77689dc49ff2d9ea766d36e0ae445fdb.png@wm_2,t_55m+5a625Y+3L1BhZGRsZVBhZGRsZQ==,fc_ffffff,ff_U2ltSGVp,sz_27,x_17,y_17"></p> 
  <p><strong>ERNIE</strong><strong>模型简介</strong></p> 
  <p>学习完了ELMo，我们再来了解一下LARK家族的学习成绩最好的重磅成员ERNIE，在多项NLP中文任务上表现非凡。</p> 
  <p>ERNIE通过建模海量数据中的实体概念等先验语义知识，学习真实世界的语义关系。具体来说，ERNIE 模型通过对词、实体等语义单元的掩码，使得模型学习完整概念的语义表示。相较于BERT 学习原始语言信号，ERNIE 直接对先验语义知识单元进行建模，增强了模型语义表示能力。</p> 
  <p>ERNIE在多个公开的中文数据集上进行了效果验证，包括语言推断、语义相似度、命名实体识别、情感分析、问答匹配等自然语言处理各类任务上，均超越了语义表示模型 BERT 的效果。</p> 
  <p><img alt="" class="has" src="http://pic.rmb.bdstatic.com/5dc53bd1348913419047a7a807099e50.jpeg@wm_2,t_55m+5a625Y+3L1BhZGRsZVBhZGRsZQ==,fc_ffffff,ff_U2ltSGVp,sz_21,x_14,y_14"></p> 
  <p>更多详细内容请点击文末<strong>阅读原文</strong>或参见：</p> 
  <p>https://github.com/PaddlePaddle/LARK</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
