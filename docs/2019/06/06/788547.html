<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>用TensorFlow实现简单线性回归 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="用TensorFlow实现简单线性回归" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="使用TensorFlow 构造一个神经元，简单的线性回归网络。 问题： 现有一组有噪声的样本数据，共2000个，每一个样本 x 有 3 个特征， 对应一个标签 y 值。从数据样本中学习 y = w × x + b y=w\times x + b y=w×x+b 中的参数 首先我们来生成样本数据，w_real 和 b_real 是控制样本数据的参数的真实值， np.random.randn(num, shape): 生成指定数量的随机数，使用默认的高斯分布（mean = 0, stddev = 1) np.matmul(a, b): 矩阵乘法，左行（由axis 0 索引）对右列（由axis 1 索引） x_data = np.random.randn(2000, 4) w_real = [0.2, 0.3, 0.1, 0.3] b_real = -0.3 noise = np.random.randn(1, 2000)*0.1 y_data = np.matmul(w_real, x_data.T) + b_real + noise 编写神经网络 下面会用到的 Tensorflow API tf.reset_default_graph() : 清除默认图中的内容 tf.Graph(): 创建一张图 with g.as_default() : 在with 语句块范围内， 将 g 设置为默认图， as_default() 返回对象是 python ContextManager tf.placeholder(dtype, shape, name=None): 在默认图上放置一个 placeholder 节点， 用以接收输入数据 tf.Variable(init_val, dtype, name=None): 创建变量型Tensor, 每次执行都会创建新的，可用tf.get_variable(name, shape, dtype, initializer) tf.reduce_mean(x, name=None) 计算成员平均值，reduce 的意思是输出降维，默认输出是标量（常量） tf.transpose(x): 转置， 可以加 perm = [3, 1, 0, 2] 参数控制数轴交换顺序。 tf.add() : 张量加法 tf.square(): 张量的平方 tf.train.GradientDescentOptimizer(lr, name=None) : 在图上构造SGD优化器 optimizer.minimize(loss_op, name=None): 在图上加入基于loss_op 的优化运算节点 tf.global_variables_initializer() : 创建全局初始化节点，它会自动收集并信赖所有的Tensor 的初始化节点 tf.Session(): 创建一个会话 session.run(fetches, feed_dict=None): 运行 fetches 指定的节点， 自动分析节点的依赖情况并依次运行，fetches 节点的依赖如果有重复，只会运行一次，后续会直接取之前的运算结果，用相同的fetches 多次调用.run() 则会每次重新计算。 官方TensorFlow 文档 全部源代码实现： import tensorflow as tf import numpy as np import matplotlib.pyplot as plt # 创建数据模拟 x_data = np.random.randn(2000, 4) w_real = [0.2, 0.3, 0.1, 0.3] b_real = -0.3 noise = np.random.randn(1, 2000)*0.1 y_data = np.matmul(w_real, x_data.T) + b_real + noise # 清除默认图中的内容 tf.reset_default_graph() # 设置步数 NUM_STEP = 10 # 学习率 learning_rate = 0.5 # 创建图 g = tf.Graph() # 存储wb wb_sess = [] with g.as_default(): # x, y_true 占位符 x = tf.placeholder(tf.float32, name = &#39;x&#39;) y_true = tf.placeholder(tf.float32, name = &#39;y_true&#39;) # w, b 变量 w = tf.Variable([[0,0,0,0]], dtype = tf.float32, name = &#39;w&#39;) b = tf.Variable(0, dtype = tf.float32, name = &#39;b&#39;) # 预测值 y = w * x + b y_pred = tf.add(tf.matmul(w, tf.transpose(x)), b, name = &#39;y_pred&#39;) # 损失 计算成员平均值 loss = tf.reduce_mean(tf.square(y_true - y_pred), name = &#39;loss&#39;) # 优化器,SGD optimizer = tf.train.GradientDescentOptimizer(learning_rate, name=&#39;SGD&#39;) train = optimizer.minimize(loss, name = &#39;train&#39;) # 全局初始化节点 init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) for step in range(NUM_STEP): sess.run(train, {x: x_data, y_true: y_data}) wb_sess.append(sess.run([w, b])) if (step % 5 == 0): print(step + 1, sess.run([w, b])) print(NUM_STEP, sess.run([w, b])) 总结： 这只会让你了解TensorFlow的一些API 特性，加强使用这些API，简单模型。" />
<meta property="og:description" content="使用TensorFlow 构造一个神经元，简单的线性回归网络。 问题： 现有一组有噪声的样本数据，共2000个，每一个样本 x 有 3 个特征， 对应一个标签 y 值。从数据样本中学习 y = w × x + b y=w\times x + b y=w×x+b 中的参数 首先我们来生成样本数据，w_real 和 b_real 是控制样本数据的参数的真实值， np.random.randn(num, shape): 生成指定数量的随机数，使用默认的高斯分布（mean = 0, stddev = 1) np.matmul(a, b): 矩阵乘法，左行（由axis 0 索引）对右列（由axis 1 索引） x_data = np.random.randn(2000, 4) w_real = [0.2, 0.3, 0.1, 0.3] b_real = -0.3 noise = np.random.randn(1, 2000)*0.1 y_data = np.matmul(w_real, x_data.T) + b_real + noise 编写神经网络 下面会用到的 Tensorflow API tf.reset_default_graph() : 清除默认图中的内容 tf.Graph(): 创建一张图 with g.as_default() : 在with 语句块范围内， 将 g 设置为默认图， as_default() 返回对象是 python ContextManager tf.placeholder(dtype, shape, name=None): 在默认图上放置一个 placeholder 节点， 用以接收输入数据 tf.Variable(init_val, dtype, name=None): 创建变量型Tensor, 每次执行都会创建新的，可用tf.get_variable(name, shape, dtype, initializer) tf.reduce_mean(x, name=None) 计算成员平均值，reduce 的意思是输出降维，默认输出是标量（常量） tf.transpose(x): 转置， 可以加 perm = [3, 1, 0, 2] 参数控制数轴交换顺序。 tf.add() : 张量加法 tf.square(): 张量的平方 tf.train.GradientDescentOptimizer(lr, name=None) : 在图上构造SGD优化器 optimizer.minimize(loss_op, name=None): 在图上加入基于loss_op 的优化运算节点 tf.global_variables_initializer() : 创建全局初始化节点，它会自动收集并信赖所有的Tensor 的初始化节点 tf.Session(): 创建一个会话 session.run(fetches, feed_dict=None): 运行 fetches 指定的节点， 自动分析节点的依赖情况并依次运行，fetches 节点的依赖如果有重复，只会运行一次，后续会直接取之前的运算结果，用相同的fetches 多次调用.run() 则会每次重新计算。 官方TensorFlow 文档 全部源代码实现： import tensorflow as tf import numpy as np import matplotlib.pyplot as plt # 创建数据模拟 x_data = np.random.randn(2000, 4) w_real = [0.2, 0.3, 0.1, 0.3] b_real = -0.3 noise = np.random.randn(1, 2000)*0.1 y_data = np.matmul(w_real, x_data.T) + b_real + noise # 清除默认图中的内容 tf.reset_default_graph() # 设置步数 NUM_STEP = 10 # 学习率 learning_rate = 0.5 # 创建图 g = tf.Graph() # 存储wb wb_sess = [] with g.as_default(): # x, y_true 占位符 x = tf.placeholder(tf.float32, name = &#39;x&#39;) y_true = tf.placeholder(tf.float32, name = &#39;y_true&#39;) # w, b 变量 w = tf.Variable([[0,0,0,0]], dtype = tf.float32, name = &#39;w&#39;) b = tf.Variable(0, dtype = tf.float32, name = &#39;b&#39;) # 预测值 y = w * x + b y_pred = tf.add(tf.matmul(w, tf.transpose(x)), b, name = &#39;y_pred&#39;) # 损失 计算成员平均值 loss = tf.reduce_mean(tf.square(y_true - y_pred), name = &#39;loss&#39;) # 优化器,SGD optimizer = tf.train.GradientDescentOptimizer(learning_rate, name=&#39;SGD&#39;) train = optimizer.minimize(loss, name = &#39;train&#39;) # 全局初始化节点 init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) for step in range(NUM_STEP): sess.run(train, {x: x_data, y_true: y_data}) wb_sess.append(sess.run([w, b])) if (step % 5 == 0): print(step + 1, sess.run([w, b])) print(NUM_STEP, sess.run([w, b])) 总结： 这只会让你了解TensorFlow的一些API 特性，加强使用这些API，简单模型。" />
<link rel="canonical" href="https://uzzz.org/2019/06/06/788547.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/06/788547.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-06T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"使用TensorFlow 构造一个神经元，简单的线性回归网络。 问题： 现有一组有噪声的样本数据，共2000个，每一个样本 x 有 3 个特征， 对应一个标签 y 值。从数据样本中学习 y = w × x + b y=w\\times x + b y=w×x+b 中的参数 首先我们来生成样本数据，w_real 和 b_real 是控制样本数据的参数的真实值， np.random.randn(num, shape): 生成指定数量的随机数，使用默认的高斯分布（mean = 0, stddev = 1) np.matmul(a, b): 矩阵乘法，左行（由axis 0 索引）对右列（由axis 1 索引） x_data = np.random.randn(2000, 4) w_real = [0.2, 0.3, 0.1, 0.3] b_real = -0.3 noise = np.random.randn(1, 2000)*0.1 y_data = np.matmul(w_real, x_data.T) + b_real + noise 编写神经网络 下面会用到的 Tensorflow API tf.reset_default_graph() : 清除默认图中的内容 tf.Graph(): 创建一张图 with g.as_default() : 在with 语句块范围内， 将 g 设置为默认图， as_default() 返回对象是 python ContextManager tf.placeholder(dtype, shape, name=None): 在默认图上放置一个 placeholder 节点， 用以接收输入数据 tf.Variable(init_val, dtype, name=None): 创建变量型Tensor, 每次执行都会创建新的，可用tf.get_variable(name, shape, dtype, initializer) tf.reduce_mean(x, name=None) 计算成员平均值，reduce 的意思是输出降维，默认输出是标量（常量） tf.transpose(x): 转置， 可以加 perm = [3, 1, 0, 2] 参数控制数轴交换顺序。 tf.add() : 张量加法 tf.square(): 张量的平方 tf.train.GradientDescentOptimizer(lr, name=None) : 在图上构造SGD优化器 optimizer.minimize(loss_op, name=None): 在图上加入基于loss_op 的优化运算节点 tf.global_variables_initializer() : 创建全局初始化节点，它会自动收集并信赖所有的Tensor 的初始化节点 tf.Session(): 创建一个会话 session.run(fetches, feed_dict=None): 运行 fetches 指定的节点， 自动分析节点的依赖情况并依次运行，fetches 节点的依赖如果有重复，只会运行一次，后续会直接取之前的运算结果，用相同的fetches 多次调用.run() 则会每次重新计算。 官方TensorFlow 文档 全部源代码实现： import tensorflow as tf import numpy as np import matplotlib.pyplot as plt # 创建数据模拟 x_data = np.random.randn(2000, 4) w_real = [0.2, 0.3, 0.1, 0.3] b_real = -0.3 noise = np.random.randn(1, 2000)*0.1 y_data = np.matmul(w_real, x_data.T) + b_real + noise # 清除默认图中的内容 tf.reset_default_graph() # 设置步数 NUM_STEP = 10 # 学习率 learning_rate = 0.5 # 创建图 g = tf.Graph() # 存储wb wb_sess = [] with g.as_default(): # x, y_true 占位符 x = tf.placeholder(tf.float32, name = &#39;x&#39;) y_true = tf.placeholder(tf.float32, name = &#39;y_true&#39;) # w, b 变量 w = tf.Variable([[0,0,0,0]], dtype = tf.float32, name = &#39;w&#39;) b = tf.Variable(0, dtype = tf.float32, name = &#39;b&#39;) # 预测值 y = w * x + b y_pred = tf.add(tf.matmul(w, tf.transpose(x)), b, name = &#39;y_pred&#39;) # 损失 计算成员平均值 loss = tf.reduce_mean(tf.square(y_true - y_pred), name = &#39;loss&#39;) # 优化器,SGD optimizer = tf.train.GradientDescentOptimizer(learning_rate, name=&#39;SGD&#39;) train = optimizer.minimize(loss, name = &#39;train&#39;) # 全局初始化节点 init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) for step in range(NUM_STEP): sess.run(train, {x: x_data, y_true: y_data}) wb_sess.append(sess.run([w, b])) if (step % 5 == 0): print(step + 1, sess.run([w, b])) print(NUM_STEP, sess.run([w, b])) 总结： 这只会让你了解TensorFlow的一些API 特性，加强使用这些API，简单模型。","@type":"BlogPosting","url":"https://uzzz.org/2019/06/06/788547.html","headline":"用TensorFlow实现简单线性回归","dateModified":"2019-06-06T00:00:00+08:00","datePublished":"2019-06-06T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/06/788547.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>用TensorFlow实现简单线性回归</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>使用TensorFlow 构造一个神经元，简单的线性回归网络。</p> 
  <p>问题：</p> 
  <p>现有一组有噪声的样本数据，共2000个，每一个样本 x 有 3 个特征， 对应一个标签 y 值。从数据样本中学习 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          y
         </mi>
         <mo>
          =
         </mo>
         <mi>
          w
         </mi>
         <mo>
          ×
         </mo>
         <mi>
          x
         </mi>
         <mo>
          +
         </mo>
         <mi>
          b
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         y=w\times x + b
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathit" style="margin-right: 0.02691em;">w</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathit">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathit">b</span></span></span></span></span> 中的参数</p> 
  <p>首先我们来生成样本数据，w_real 和 b_real 是控制样本数据的参数的真实值，</p> 
  <ul> 
   <li><strong>np.random.randn(num, shape)</strong>: 生成指定数量的随机数，使用默认的高斯分布（mean = 0, stddev = 1)</li> 
   <li><strong>np.matmul(a, b)</strong>: 矩阵乘法，左行（由axis 0 索引）对右列（由axis 1 索引）</li> 
  </ul> 
  <pre><code class="prism language-python">x_data <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2000</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
w_real <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span>
b_real <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">0.3</span>
noise <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2000</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.1</span>
y_data <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>w_real<span class="token punctuation">,</span> x_data<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">+</span> b_real <span class="token operator">+</span> noise
</code></pre> 
  <p>编写神经网络</p> 
  <p>下面会用到的 Tensorflow API</p> 
  <ul> 
   <li><strong>tf.reset_default_graph() :</strong> 清除默认图中的内容</li> 
   <li><strong>tf.Graph():</strong> 创建一张图</li> 
   <li><strong>with g.as_default() :</strong> 在with 语句块范围内， 将 g 设置为默认图， as_default() 返回对象是 python ContextManager</li> 
   <li><strong>tf.placeholder(dtype, shape, name=None):</strong> 在默认图上放置一个 placeholder 节点， 用以接收输入数据</li> 
   <li><strong>tf.Variable(init_val, dtype, name=None)</strong>: 创建变量型Tensor, 每次执行都会创建新的，可用<strong>tf.get_variable(name, shape, dtype, initializer)</strong></li> 
   <li><strong>tf.reduce_mean(x, name=None)</strong> 计算成员平均值，reduce 的意思是输出降维，默认输出是标量（常量）</li> 
   <li><strong>tf.transpose(x):</strong> 转置， 可以加 perm = [3, 1, 0, 2] 参数控制数轴交换顺序。</li> 
   <li><strong>tf.add()</strong> : 张量加法</li> 
   <li><strong>tf.square()</strong>: 张量的平方</li> 
   <li><strong>tf.train.GradientDescentOptimizer(lr, name=None)</strong> : 在图上构造SGD优化器</li> 
   <li><strong>optimizer.minimize(loss_op, name=None)</strong>: 在图上加入基于loss_op 的优化运算节点</li> 
   <li><strong>tf.global_variables_initializer()</strong> : 创建全局初始化节点，它会自动收集并信赖所有的Tensor 的初始化节点</li> 
   <li><strong>tf.Session()</strong>: 创建一个会话</li> 
   <li><strong>session.run(fetches, feed_dict=None):</strong> 运行 fetches 指定的节点， 自动分析节点的依赖情况并依次运行，fetches 节点的依赖如果有重复，只会运行一次，后续会直接取之前的运算结果，用相同的fetches 多次调用.run() 则会每次重新计算。</li> 
  </ul> 
  <p>官方<a href="https://tensorflow.google.cn/" rel="nofollow">TensorFlow</a> 文档</p> 
  <p>全部源代码实现：</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># 创建数据模拟</span>
x_data <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2000</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
w_real <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span>
b_real <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">0.3</span>
noise <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2000</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.1</span>
y_data <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>w_real<span class="token punctuation">,</span> x_data<span class="token punctuation">.</span>T<span class="token punctuation">)</span> <span class="token operator">+</span> b_real <span class="token operator">+</span> noise

<span class="token comment"># 清除默认图中的内容</span>
tf<span class="token punctuation">.</span>reset_default_graph<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 设置步数</span>
NUM_STEP <span class="token operator">=</span> <span class="token number">10</span>

<span class="token comment"># 学习率</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.5</span>

<span class="token comment"># 创建图</span>
g <span class="token operator">=</span> tf<span class="token punctuation">.</span>Graph<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 存储wb</span>
wb_sess <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">with</span> g<span class="token punctuation">.</span>as_default<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># x, y_true 占位符</span>
    x <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">'x'</span><span class="token punctuation">)</span>
    y_true <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">'y_true'</span><span class="token punctuation">)</span>
    
    <span class="token comment"># w, b 变量</span>
    w <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">'w'</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">'b'</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 预测值 y = w * x + b</span>
    y_pred <span class="token operator">=</span> tf<span class="token punctuation">.</span>add<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>w<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> b<span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">'y_pred'</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 损失 计算成员平均值</span>
    loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>y_true <span class="token operator">-</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">'loss'</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 优化器,SGD</span>
    optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>GradientDescentOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'SGD'</span><span class="token punctuation">)</span>
    train <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">'train'</span><span class="token punctuation">)</span>
   
    <span class="token comment"># 全局初始化节点</span>
    init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
        sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>init<span class="token punctuation">)</span>
        
        <span class="token keyword">for</span> step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>NUM_STEP<span class="token punctuation">)</span><span class="token punctuation">:</span>
            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>train<span class="token punctuation">,</span> <span class="token punctuation">{</span>x<span class="token punctuation">:</span> x_data<span class="token punctuation">,</span> y_true<span class="token punctuation">:</span> y_data<span class="token punctuation">}</span><span class="token punctuation">)</span>
            
            wb_sess<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            
            <span class="token keyword">if</span> <span class="token punctuation">(</span>step <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                
        <span class="token keyword">print</span><span class="token punctuation">(</span>NUM_STEP<span class="token punctuation">,</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <p>总结：<br> 这只会让你了解TensorFlow的一些API 特性，加强使用这些API，简单模型。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
