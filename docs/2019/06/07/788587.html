<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>深度学习（DL）理论梳理（二）DNN、前向传播算法、BP算法 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="深度学习（DL）理论梳理（二）DNN、前向传播算法、BP算法" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1、神经网络     深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型。 感知机的模型，它是一个有若干输入和一个输出的模型，如下图:          输出和输入之间学习到一个线性关系，得到中间输出结果：          接着是一个神经元激活函数:          从而得到我们想要的输出结果1或者-1。     这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。 而神经网络则在感知机的模型上做了扩展，总结下主要有三点： （1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。      （2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。      （3） 对激活函数做扩展，感知机的激活函数是sign(z)，虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：          还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。 2、DNN模型 DNN可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准, 多层神经网络和深度神经网络DNN其实也是指的一个东西，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）。 从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。          层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系z=∑wixi+b加上一个激活函数σ(z)。     由于DNN层数多，则我们的线性关系系数ww和偏倚bb的数量也就是很多了。 3、DNN前向传播算法数学原理     假设我们选择的激活函数是σ(z)，隐藏层和输出层的输出值为a，则对于下图的三层DNN,利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。          对于第二层的的输出，有：          对于第三层的的输出，有：          将上面的例子一般化，假设第L−1层共有m个神经元，则对于第L层的第j个神经元的输出，有：          使用矩阵法则的简洁展示，第l层的输出为：      4、DNN前向传播算法     所谓的DNN的前向传播算法也就是利用我们的若干个权重系数矩阵W,偏倚向量b来和输入值向量x进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为值。     输入: 总层数L，所有隐藏层和输出层对应的矩阵W,偏倚向量b，输入值向量x     输出：输出层的输出         （1）初始化         （2）for l=2 to L，计算：                          最后的结果即为输出。 单独看DNN前向传播算法，似乎没有什么大用处，而且这一大堆的矩阵W，偏倚向量b对应的参数怎么获得呢？怎么得到最优的矩阵W，偏倚向量b呢？这需要反向传播算法求最优解。 5、DNN反向传播算法（BP）要解决的问题     回到监督学习的一般问题，假设我们有m个训练样本：{(x1,y1),(x2,y2),…,(xm,ym)}{(x1,y1),(x2,y2),…,(xm,ym)},其中x为输入向量，特征维度为n_in,而y为输出向量，特征维度为n_out。我们需要利用这m个样本训练出一个模型，当有一个新的测试样本(xtest，?)来到时, 我们可以预测ytest向量的输出。     如果我们采用DNN的模型，即我们使输入层有n_in个神经元，而输出层有n_out个神经元。再加上一些含有若干神经元的隐藏层。此时我们需要找到合适的所有隐藏层和输出层对应的线性系数矩阵W，偏倚向量b。让所有的训练样本输入计算出的输出尽可能的等于或很接近样本输出。怎么找到合适的参数呢？     这里就很容易联想到可以用一个合适的损失函数来度量训练样本的输出损失，接着对这个损失函数进行优化求最小化的极值，对应的一系列线性系数矩阵W,偏倚向量b即为我们的最终结果。在DNN中，损失函数优化极值求解的过程最常见的一般是通过梯度下降法来一步步迭代完成的，当然也可以是其他的迭代方法比如牛顿法与拟牛顿法。 对DNN的损失函数用梯度下降法进行迭代优化求极小值的过程即为我们的反向传播算法。 6、DNN反向传播算法（BP）的基本思路     在进行DNN反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。也许会问：训练样本计算出的输出是怎么得来的？这个输出是随机选择一系列W，b，用前向传播算法计算出来的。即通过一系列的计算：          回到损失函数，DNN可选择的损失函数有不少（第8节介绍），为专注算法，这里我们使用最常见的均方差来度量损失。即对于每个样本，我们期望最小化下式：          其中，和y为特征维度为n_out的向量,而为S的L2范数。     损失函数有了，现在我们开始用梯度下降法迭代求解每一层的W，b。     首先是输出层第L层。注意到输出层的W，b满足下式：          这样对于输出层的参数，我们的损失函数变为：          这样求解W,b的梯度就简单了：          注意上式中有一个符号⊙,它代表Hadamard积。     我们注意到在求解输出层的W,b的时候，有公共的部分，因此我们可以把公共的部分即对先算出来，记为：          现在终于把输出层的梯度算出来了，那么如何计算上一层L−1层的梯度，上上层L−2层的梯度呢？这里需要一步步的递推，注意到对于第ll层的未激活输出，它的梯度可以表示为:          如果我们可以依次计算出第ll层的，则该层的，很容易计算？为什么呢？注意到根据前向传播算法，有：          所以根据上式我们可以很方便的计算出第l层的，的梯度如下:          那么现在问题的关键就是要求出了。这里我们用数学归纳法，第L层的上面我们已经求出， 假设第l+1层的已经求出来了，那么我们如何求出第l层的呢？我们注意到：          可见，用归纳法递推和的关键在于求解。     而和的关系其实很容易找出：          这样很容易求出：          将上式带入上面和关系式我们得到：          现在我们得到了的递推关系式，只要求出了某一层的，求解，的对应梯度就很简单的。 7、DNN反向传播算法（BP）过程     现在我们总结下DNN反向传播算法的过程。由于梯度下降法有批量（Batch），小批量(mini-Batch)，随机三个变种，为了简化描述，这里我们以最基本的批量梯度下降法为例来描述反向传播算法。实际上在业界使用最多的是mini-Batch的梯度下降法。不过区别仅仅在于迭代时训练样本的选择而已。     输入: 总层数L，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长,最大迭代次数MAX与停止迭代阈值，输入的m个训练样本{(x_1,y_1), (x_2,y_2), …, (x_m,y_m)}     输出：各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b         （1）初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b的值为一个随机值。         （2）for iter to 1 to MAX：             (2-1) for i =1 to m：                 （a）将DNN输入设置为。                 （b） for l=2 to L，进行前向传播算法计算：                 （c）通过损失函数计算输出层的                 （d）for l= L-1 to 2, 进行反向传播算法计算                   for l = 2 to L，更新第l层的，:                                          如果所有W，b的变化值都小于停止迭代阈值，则跳出迭代循环到步骤3。         （3） 输出各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b。 上一篇：深度学习（DL）理论梳理（一）总括 下一篇：深度学习（DL）理论梳理（三）DNN损失函数和激活函数的选择" />
<meta property="og:description" content="1、神经网络     深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型。 感知机的模型，它是一个有若干输入和一个输出的模型，如下图:          输出和输入之间学习到一个线性关系，得到中间输出结果：          接着是一个神经元激活函数:          从而得到我们想要的输出结果1或者-1。     这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。 而神经网络则在感知机的模型上做了扩展，总结下主要有三点： （1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。      （2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。      （3） 对激活函数做扩展，感知机的激活函数是sign(z)，虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：          还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。 2、DNN模型 DNN可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准, 多层神经网络和深度神经网络DNN其实也是指的一个东西，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）。 从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。          层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系z=∑wixi+b加上一个激活函数σ(z)。     由于DNN层数多，则我们的线性关系系数ww和偏倚bb的数量也就是很多了。 3、DNN前向传播算法数学原理     假设我们选择的激活函数是σ(z)，隐藏层和输出层的输出值为a，则对于下图的三层DNN,利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。          对于第二层的的输出，有：          对于第三层的的输出，有：          将上面的例子一般化，假设第L−1层共有m个神经元，则对于第L层的第j个神经元的输出，有：          使用矩阵法则的简洁展示，第l层的输出为：      4、DNN前向传播算法     所谓的DNN的前向传播算法也就是利用我们的若干个权重系数矩阵W,偏倚向量b来和输入值向量x进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为值。     输入: 总层数L，所有隐藏层和输出层对应的矩阵W,偏倚向量b，输入值向量x     输出：输出层的输出         （1）初始化         （2）for l=2 to L，计算：                          最后的结果即为输出。 单独看DNN前向传播算法，似乎没有什么大用处，而且这一大堆的矩阵W，偏倚向量b对应的参数怎么获得呢？怎么得到最优的矩阵W，偏倚向量b呢？这需要反向传播算法求最优解。 5、DNN反向传播算法（BP）要解决的问题     回到监督学习的一般问题，假设我们有m个训练样本：{(x1,y1),(x2,y2),…,(xm,ym)}{(x1,y1),(x2,y2),…,(xm,ym)},其中x为输入向量，特征维度为n_in,而y为输出向量，特征维度为n_out。我们需要利用这m个样本训练出一个模型，当有一个新的测试样本(xtest，?)来到时, 我们可以预测ytest向量的输出。     如果我们采用DNN的模型，即我们使输入层有n_in个神经元，而输出层有n_out个神经元。再加上一些含有若干神经元的隐藏层。此时我们需要找到合适的所有隐藏层和输出层对应的线性系数矩阵W，偏倚向量b。让所有的训练样本输入计算出的输出尽可能的等于或很接近样本输出。怎么找到合适的参数呢？     这里就很容易联想到可以用一个合适的损失函数来度量训练样本的输出损失，接着对这个损失函数进行优化求最小化的极值，对应的一系列线性系数矩阵W,偏倚向量b即为我们的最终结果。在DNN中，损失函数优化极值求解的过程最常见的一般是通过梯度下降法来一步步迭代完成的，当然也可以是其他的迭代方法比如牛顿法与拟牛顿法。 对DNN的损失函数用梯度下降法进行迭代优化求极小值的过程即为我们的反向传播算法。 6、DNN反向传播算法（BP）的基本思路     在进行DNN反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。也许会问：训练样本计算出的输出是怎么得来的？这个输出是随机选择一系列W，b，用前向传播算法计算出来的。即通过一系列的计算：          回到损失函数，DNN可选择的损失函数有不少（第8节介绍），为专注算法，这里我们使用最常见的均方差来度量损失。即对于每个样本，我们期望最小化下式：          其中，和y为特征维度为n_out的向量,而为S的L2范数。     损失函数有了，现在我们开始用梯度下降法迭代求解每一层的W，b。     首先是输出层第L层。注意到输出层的W，b满足下式：          这样对于输出层的参数，我们的损失函数变为：          这样求解W,b的梯度就简单了：          注意上式中有一个符号⊙,它代表Hadamard积。     我们注意到在求解输出层的W,b的时候，有公共的部分，因此我们可以把公共的部分即对先算出来，记为：          现在终于把输出层的梯度算出来了，那么如何计算上一层L−1层的梯度，上上层L−2层的梯度呢？这里需要一步步的递推，注意到对于第ll层的未激活输出，它的梯度可以表示为:          如果我们可以依次计算出第ll层的，则该层的，很容易计算？为什么呢？注意到根据前向传播算法，有：          所以根据上式我们可以很方便的计算出第l层的，的梯度如下:          那么现在问题的关键就是要求出了。这里我们用数学归纳法，第L层的上面我们已经求出， 假设第l+1层的已经求出来了，那么我们如何求出第l层的呢？我们注意到：          可见，用归纳法递推和的关键在于求解。     而和的关系其实很容易找出：          这样很容易求出：          将上式带入上面和关系式我们得到：          现在我们得到了的递推关系式，只要求出了某一层的，求解，的对应梯度就很简单的。 7、DNN反向传播算法（BP）过程     现在我们总结下DNN反向传播算法的过程。由于梯度下降法有批量（Batch），小批量(mini-Batch)，随机三个变种，为了简化描述，这里我们以最基本的批量梯度下降法为例来描述反向传播算法。实际上在业界使用最多的是mini-Batch的梯度下降法。不过区别仅仅在于迭代时训练样本的选择而已。     输入: 总层数L，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长,最大迭代次数MAX与停止迭代阈值，输入的m个训练样本{(x_1,y_1), (x_2,y_2), …, (x_m,y_m)}     输出：各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b         （1）初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b的值为一个随机值。         （2）for iter to 1 to MAX：             (2-1) for i =1 to m：                 （a）将DNN输入设置为。                 （b） for l=2 to L，进行前向传播算法计算：                 （c）通过损失函数计算输出层的                 （d）for l= L-1 to 2, 进行反向传播算法计算                   for l = 2 to L，更新第l层的，:                                          如果所有W，b的变化值都小于停止迭代阈值，则跳出迭代循环到步骤3。         （3） 输出各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b。 上一篇：深度学习（DL）理论梳理（一）总括 下一篇：深度学习（DL）理论梳理（三）DNN损失函数和激活函数的选择" />
<link rel="canonical" href="https://uzzz.org/2019/06/07/788587.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/07/788587.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-07T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"1、神经网络     深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型。 感知机的模型，它是一个有若干输入和一个输出的模型，如下图:          输出和输入之间学习到一个线性关系，得到中间输出结果：          接着是一个神经元激活函数:          从而得到我们想要的输出结果1或者-1。     这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。 而神经网络则在感知机的模型上做了扩展，总结下主要有三点： （1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。      （2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。      （3） 对激活函数做扩展，感知机的激活函数是sign(z)，虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：          还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。 2、DNN模型 DNN可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准, 多层神经网络和深度神经网络DNN其实也是指的一个东西，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）。 从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。          层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系z=∑wixi+b加上一个激活函数σ(z)。     由于DNN层数多，则我们的线性关系系数ww和偏倚bb的数量也就是很多了。 3、DNN前向传播算法数学原理     假设我们选择的激活函数是σ(z)，隐藏层和输出层的输出值为a，则对于下图的三层DNN,利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。          对于第二层的的输出，有：          对于第三层的的输出，有：          将上面的例子一般化，假设第L−1层共有m个神经元，则对于第L层的第j个神经元的输出，有：          使用矩阵法则的简洁展示，第l层的输出为：      4、DNN前向传播算法     所谓的DNN的前向传播算法也就是利用我们的若干个权重系数矩阵W,偏倚向量b来和输入值向量x进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为值。     输入: 总层数L，所有隐藏层和输出层对应的矩阵W,偏倚向量b，输入值向量x     输出：输出层的输出         （1）初始化         （2）for l=2 to L，计算：                          最后的结果即为输出。 单独看DNN前向传播算法，似乎没有什么大用处，而且这一大堆的矩阵W，偏倚向量b对应的参数怎么获得呢？怎么得到最优的矩阵W，偏倚向量b呢？这需要反向传播算法求最优解。 5、DNN反向传播算法（BP）要解决的问题     回到监督学习的一般问题，假设我们有m个训练样本：{(x1,y1),(x2,y2),…,(xm,ym)}{(x1,y1),(x2,y2),…,(xm,ym)},其中x为输入向量，特征维度为n_in,而y为输出向量，特征维度为n_out。我们需要利用这m个样本训练出一个模型，当有一个新的测试样本(xtest，?)来到时, 我们可以预测ytest向量的输出。     如果我们采用DNN的模型，即我们使输入层有n_in个神经元，而输出层有n_out个神经元。再加上一些含有若干神经元的隐藏层。此时我们需要找到合适的所有隐藏层和输出层对应的线性系数矩阵W，偏倚向量b。让所有的训练样本输入计算出的输出尽可能的等于或很接近样本输出。怎么找到合适的参数呢？     这里就很容易联想到可以用一个合适的损失函数来度量训练样本的输出损失，接着对这个损失函数进行优化求最小化的极值，对应的一系列线性系数矩阵W,偏倚向量b即为我们的最终结果。在DNN中，损失函数优化极值求解的过程最常见的一般是通过梯度下降法来一步步迭代完成的，当然也可以是其他的迭代方法比如牛顿法与拟牛顿法。 对DNN的损失函数用梯度下降法进行迭代优化求极小值的过程即为我们的反向传播算法。 6、DNN反向传播算法（BP）的基本思路     在进行DNN反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。也许会问：训练样本计算出的输出是怎么得来的？这个输出是随机选择一系列W，b，用前向传播算法计算出来的。即通过一系列的计算：          回到损失函数，DNN可选择的损失函数有不少（第8节介绍），为专注算法，这里我们使用最常见的均方差来度量损失。即对于每个样本，我们期望最小化下式：          其中，和y为特征维度为n_out的向量,而为S的L2范数。     损失函数有了，现在我们开始用梯度下降法迭代求解每一层的W，b。     首先是输出层第L层。注意到输出层的W，b满足下式：          这样对于输出层的参数，我们的损失函数变为：          这样求解W,b的梯度就简单了：          注意上式中有一个符号⊙,它代表Hadamard积。     我们注意到在求解输出层的W,b的时候，有公共的部分，因此我们可以把公共的部分即对先算出来，记为：          现在终于把输出层的梯度算出来了，那么如何计算上一层L−1层的梯度，上上层L−2层的梯度呢？这里需要一步步的递推，注意到对于第ll层的未激活输出，它的梯度可以表示为:          如果我们可以依次计算出第ll层的，则该层的，很容易计算？为什么呢？注意到根据前向传播算法，有：          所以根据上式我们可以很方便的计算出第l层的，的梯度如下:          那么现在问题的关键就是要求出了。这里我们用数学归纳法，第L层的上面我们已经求出， 假设第l+1层的已经求出来了，那么我们如何求出第l层的呢？我们注意到：          可见，用归纳法递推和的关键在于求解。     而和的关系其实很容易找出：          这样很容易求出：          将上式带入上面和关系式我们得到：          现在我们得到了的递推关系式，只要求出了某一层的，求解，的对应梯度就很简单的。 7、DNN反向传播算法（BP）过程     现在我们总结下DNN反向传播算法的过程。由于梯度下降法有批量（Batch），小批量(mini-Batch)，随机三个变种，为了简化描述，这里我们以最基本的批量梯度下降法为例来描述反向传播算法。实际上在业界使用最多的是mini-Batch的梯度下降法。不过区别仅仅在于迭代时训练样本的选择而已。     输入: 总层数L，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长,最大迭代次数MAX与停止迭代阈值，输入的m个训练样本{(x_1,y_1), (x_2,y_2), …, (x_m,y_m)}     输出：各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b         （1）初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b的值为一个随机值。         （2）for iter to 1 to MAX：             (2-1) for i =1 to m：                 （a）将DNN输入设置为。                 （b） for l=2 to L，进行前向传播算法计算：                 （c）通过损失函数计算输出层的                 （d）for l= L-1 to 2, 进行反向传播算法计算                   for l = 2 to L，更新第l层的，:                                          如果所有W，b的变化值都小于停止迭代阈值，则跳出迭代循环到步骤3。         （3） 输出各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b。 上一篇：深度学习（DL）理论梳理（一）总括 下一篇：深度学习（DL）理论梳理（三）DNN损失函数和激活函数的选择","@type":"BlogPosting","url":"https://uzzz.org/2019/06/07/788587.html","headline":"深度学习（DL）理论梳理（二）DNN、前向传播算法、BP算法","dateModified":"2019-06-07T00:00:00+08:00","datePublished":"2019-06-07T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/07/788587.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>深度学习（DL）理论梳理（二）DNN、前向传播算法、BP算法</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h4><a id="1_0"></a>1、神经网络</h4> 
  <p>    深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型。</p> 
  <ul> 
   <li>感知机的模型，它是一个有若干输入和一个输出的模型，如下图:</li> 
  </ul> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200418510.png" alt="在这里插入图片描述"><br>     输出和输入之间学习到一个线性关系，得到中间输出结果：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200458314.png" alt="在这里插入图片描述"><br>     接着是一个神经元激活函数:</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200516890.png" alt="在这里插入图片描述"><br>     从而得到我们想要的输出结果1或者-1。</p> 
  <p>    这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。</p> 
  <ul> 
   <li> <p>而神经网络则在感知机的模型上做了扩展，总结下主要有三点：</p> <p>（1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。</p> </li> 
  </ul> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200551751.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY1Nzc2MA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> （2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200619386.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY1Nzc2MA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> （3） 对激活函数做扩展，感知机的激活函数是sign(z)，虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200634227.png" alt="在这里插入图片描述"><br>     还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。</p> 
  <h4><a id="2DNN_30"></a>2、DNN模型</h4> 
  <ul> 
   <li> <p>DNN可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准, 多层神经网络和深度神经网络DNN其实也是指的一个东西，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）。</p> </li> 
   <li> <p>从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。</p> </li> 
  </ul> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200655646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY1Nzc2MA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>     层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系z=∑wixi+b加上一个激活函数σ(z)。</p> 
  <p>    由于DNN层数多，则我们的线性关系系数ww和偏倚bb的数量也就是很多了。</p> 
  <h4><a id="3DNN_41"></a>3、DNN前向传播算法数学原理</h4> 
  <p>    假设我们选择的激活函数是σ(z)，隐藏层和输出层的输出值为a，则对于下图的三层DNN,利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200711687.png" alt="在这里插入图片描述"><br>     对于第二层的的输出，有：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200737290.png" alt="在这里插入图片描述"><br>     对于第三层的的输出，有：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200755274.png" alt="在这里插入图片描述"><br>     将上面的例子一般化，假设第L−1层共有m个神经元，则对于第L层的第j个神经元的输出，有：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200806899.png" alt="在这里插入图片描述"><br>     使用矩阵法则的简洁展示，第l层的输出为：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200816785.png" alt="在这里插入图片描述"></p> 
  <h4><a id="4DNN_58"></a>4、DNN前向传播算法</h4> 
  <p>    所谓的DNN的前向传播算法也就是利用我们的若干个权重系数矩阵W,偏倚向量b来和输入值向量x进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为值。<br>     输入: 总层数L，所有隐藏层和输出层对应的矩阵W,偏倚向量b，输入值向量x<br>     输出：输出层的输出<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200827968.png" alt="在这里插入图片描述"><br>         （1）初始化<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200902489.png" alt="在这里插入图片描述"><br>         （2）for l=2 to L，计算：<br>             <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200920294.png" alt="在这里插入图片描述"><br>             最后的结果即为输出<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607200944860.png" alt="在这里插入图片描述">。</p> 
  <ul> 
   <li>单独看DNN前向传播算法，似乎没有什么大用处，而且这一大堆的矩阵W，偏倚向量b对应的参数怎么获得呢？怎么得到最优的矩阵W，偏倚向量b呢？这需要反向传播算法求最优解。</li> 
  </ul> 
  <h4><a id="5DNNBP_70"></a>5、DNN反向传播算法（BP）要解决的问题</h4> 
  <p>    回到监督学习的一般问题，假设我们有m个训练样本：{(x1,y1),(x2,y2),…,(xm,ym)}{(x1,y1),(x2,y2),…,(xm,ym)},其中x为输入向量，特征维度为n_in,而y为输出向量，特征维度为n_out。我们需要利用这m个样本训练出一个模型，当有一个新的测试样本(xtest，?)来到时, 我们可以预测ytest向量的输出。</p> 
  <p>    如果我们采用DNN的模型，即我们使输入层有n_in个神经元，而输出层有n_out个神经元。再加上一些含有若干神经元的隐藏层。此时我们需要找到合适的所有隐藏层和输出层对应的线性系数矩阵W，偏倚向量b。让所有的训练样本输入计算出的输出尽可能的等于或很接近样本输出。怎么找到合适的参数呢？</p> 
  <p>    这里就很容易联想到可以用一个合适的损失函数来度量训练样本的输出损失，接着对这个损失函数进行优化求最小化的极值，对应的一系列线性系数矩阵W,偏倚向量b即为我们的最终结果。在DNN中，损失函数优化极值求解的过程最常见的一般是通过梯度下降法来一步步迭代完成的，当然也可以是其他的迭代方法比如牛顿法与拟牛顿法。</p> 
  <ul> 
   <li>对DNN的损失函数用梯度下降法进行迭代优化求极小值的过程即为我们的反向传播算法。</li> 
  </ul> 
  <h4><a id="6DNNBP_80"></a>6、DNN反向传播算法（BP）的基本思路</h4> 
  <p>    在进行DNN反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。<strong>也许会问：训练样本计算出的输出是怎么得来的？<strong>这个输出是</strong>随机选择一系列W，b，用前向传播算法计算出来的</strong>。即通过一系列的计算：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201052743.png" alt="在这里插入图片描述"><br>     回到损失函数，DNN可选择的损失函数有不少（第8节介绍），为专注算法，这里我们使用最常见的均方差来度量损失。即对于每个样本，我们期望最小化下式：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201106146.png" alt="在这里插入图片描述"><br>     其中，<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201117978.png" alt="在这里插入图片描述">和y为特征维度为n_out的向量,而<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201130876.png" alt="在这里插入图片描述">为S的L2范数。</p> 
  <p>    损失函数有了，现在我们开始用梯度下降法迭代求解每一层的W，b。</p> 
  <p>    首先是输出层第L层。注意到输出层的W，b满足下式：</p> 
  <p>    </p> 
  <p>    这样对于输出层的参数，我们的损失函数变为：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201326839.png" alt="在这里插入图片描述"><br>     这样求解W,b的梯度就简单了：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201343296.png" alt="在这里插入图片描述"><br>     注意上式中有一个符号⊙,它代表Hadamard积。<br>     我们注意到在求解输出层的W,b的时候，有公共的部分<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201400955.png" alt="在这里插入图片描述">，因此我们可以把公共的部分即对<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201415227.png" alt="在这里插入图片描述">先算出来，记为：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201428361.png" alt="在这里插入图片描述"><br>     现在终于把输出层的梯度算出来了，那么如何计算上一层L−1层的梯度，上上层L−2层的梯度呢？这里需要一步步的递推，注意到对于第ll层的未激活输出<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201443940.png" alt="在这里插入图片描述">，它的梯度可以表示为:</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201501209.png" alt="在这里插入图片描述"><br>     如果我们可以依次计算出第ll层的<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201519606.png" alt="在这里插入图片描述">，则该层的<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201532447.png" alt="在这里插入图片描述">，<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201544396.png" alt="在这里插入图片描述">很容易计算？为什么呢？注意到根据前向传播算法，有：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201555329.png" alt="在这里插入图片描述"></p> 
  <p>    所以根据上式我们可以很方便的计算出第l层的<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201613348.png" alt="在这里插入图片描述">，<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201627898.png" alt="在这里插入图片描述">的梯度如下:</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201644235.png" alt="在这里插入图片描述"><br>     那么现在问题的关键就是要求出<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201656281.png" alt="在这里插入图片描述">了。这里我们用数学归纳法，第L层的<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201709782.png" alt="在这里插入图片描述">上面我们已经求出， 假设第l+1层的<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201725823.png" alt="在这里插入图片描述">已经求出来了，那么我们如何求出第l层的<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201740729.png" alt="在这里插入图片描述">呢？我们注意到：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201752102.png" alt="在这里插入图片描述"><br>     可见，用归纳法递推<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201818280.png" alt="在这里插入图片描述">和<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201833716.png" alt="在这里插入图片描述">的关键在于求解<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201848430.png" alt="在这里插入图片描述">。<br>     而<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201902291.png" alt="在这里插入图片描述">和<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201915916.png" alt="在这里插入图片描述">的关系其实很容易找出：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201927679.png" alt="在这里插入图片描述"><br>     这样很容易求出：</p> 
  <p>    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201943361.png" alt="在这里插入图片描述"><br>     将上式带入上面<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607201957975.png" alt="在这里插入图片描述">和<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202007961.png" alt="在这里插入图片描述">关系式我们得到：</p> 
  <p>    </p> 
  <p>    现在我们得到了<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202054966.png" alt="在这里插入图片描述">的递推关系式，只要求出了某一层的<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202114980.png" alt="在这里插入图片描述">，求解<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019060720213065.png" alt="在这里插入图片描述">，<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/201906072021445.png" alt="在这里插入图片描述">的对应梯度就很简单的。</p> 
  <h4><a id="7DNNBP_132"></a>7、DNN反向传播算法（BP）过程</h4> 
  <p>    现在我们总结下DNN反向传播算法的过程。由于梯度下降法有批量（Batch），小批量(mini-Batch)，随机三个变种，为了简化描述，这里我们以最基本的批量梯度下降法为例来描述反向传播算法。实际上在业界使用最多的是mini-Batch的梯度下降法。不过区别仅仅在于迭代时训练样本的选择而已。</p> 
  <p>    输入: 总层数L，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202208329.png" alt="在这里插入图片描述">,最大迭代次数MAX与停止迭代阈值，<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019060720222898.png" alt="在这里插入图片描述">输入的m个训练样本{(x_1,y_1), (x_2,y_2), …, (x_m,y_m)}</p> 
  <p>    输出：各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b</p> 
  <p>        （1）初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b的值为一个随机值。</p> 
  <p>        （2）for iter to 1 to MAX：<br>             (2-1) for i =1 to m：</p> 
  <p>                （a）将DNN输入<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202255177.png" alt="在这里插入图片描述">设置为<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202311669.png" alt="在这里插入图片描述">。<br>                 （b） for l=2 to L，进行前向传播算法计算：<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202333176.png" alt="在这里插入图片描述"><br>                 （c）通过损失函数计算输出层的<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202355213.jpg" alt="在这里插入图片描述"><br>                 （d）for l= L-1 to 2, 进行反向传播算法计算<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019060720241247.png" alt="在这里插入图片描述"><br>                   for l = 2 to L，更新第l层的<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202426825.png" alt="在这里插入图片描述">，<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202441754.png" alt="在这里插入图片描述">:<br>                       <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190607202500508.png" alt="在这里插入图片描述"><br>                   如果所有W，b的变化值都小于停止迭代阈值<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019060720251532.png" alt="在这里插入图片描述">，则跳出迭代循环到步骤3。<br>         （3） 输出各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b。</p> 
  <hr> 
  <p><strong>上一篇：</strong><a href="https://blog.csdn.net/weixin_41657760/article/details/91129240" rel="nofollow">深度学习（DL）理论梳理（一）总括</a><br> <strong>下一篇：</strong><a href="https://blog.csdn.net/weixin_41657760/article/details/91284016" rel="nofollow">深度学习（DL）理论梳理（三）DNN损失函数和激活函数的选择</a></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
