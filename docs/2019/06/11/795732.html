<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>【自然语言处理】tf.contrib.seq2seq.BasicDecoder源码解析 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="【自然语言处理】tf.contrib.seq2seq.BasicDecoder源码解析" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="前言 tf.contrib.seq2seq.dynamic_decode源码分析本文衔接上文。 首先tf.contrib.seq2seq.dynamic_decode主要作用是接收一个Decoder类，然后依据Encoder进行解码，实现序列的生成（映射）。其中，这个函数主要的一个思想是一步一步地调用Decoder的step函数（该函数接收当前的输入和隐层状态会生成下一个词），实现最后的一句话的生成。该函数类似tf.nn.dynamic_rnn。 该函数用到的Decoder类就是今天所要解析的类。 源码解析 class BasicDecoder(decoder.Decoder): &quot;&quot;&quot;Basic sampling decoder.&quot;&quot;&quot; def __init__(self, cell, helper, initial_state, output_layer=None): &quot;&quot;&quot;Initialize BasicDecoder. Args: cell: RNN实例 helper: Helper类，用于训练和推理 initial_state: 初始状态 output_layer: 输出层 Raises: TypeError: 如果`cell`, `helper` or `output_layer`没有正确的类型 &quot;&quot;&quot; rnn_cell_impl.assert_like_rnncell(&quot;cell&quot;, cell) if not isinstance(helper, helper_py.Helper): raise TypeError(&quot;helper must be a Helper, received: %s&quot; % type(helper)) if (output_layer is not None and not isinstance(output_layer, layers_base.Layer)): raise TypeError( &quot;output_layer must be a Layer, received: %s&quot; % type(output_layer)) self._cell = cell self._helper = helper self._initial_state = initial_state self._output_layer = output_layer BasicDecoder是继承于Decoder类，这个类只是个抽象类，定义了几个抽象方法。首先这个cell, helper, initial_state, output_layer这几个参数，cell一般就是个RNN（及其衍生类，比如LSTM）实例，initial_state一般是用Encoder的最后一个隐层状态，也就是标准Seq2seq的做法，output_layer是输出层，很自然。 那helper是啥可能有些抽象。这里简单的说就是文本生成分为两个阶段，一个是训练，一个是推理。那么我们希望得到训练的output（输出层的输出），推理的采样样本。而这里也是采用了一个策略模式（设计模式的内容，不懂的可以看看），把Helper分为两大类，一种是TrainingHelper，一种是InferenceHelper。 @property def batch_size(self): return self._helper.batch_size 构造getter。 def _rnn_output_size(self): size = self._cell.output_size if self._output_layer is None: return size else: # To use layer&#39;s compute_output_shape, we need to convert the # RNNCell&#39;s output_size entries into shapes with an unknown # batch size. We then pass this through the layer&#39;s # compute_output_shape and read off all but the first (batch) # dimensions to get the output size of the rnn with the layer # applied to the top. output_shape_with_unknown_batch = nest.map_structure( lambda s: tensor_shape.TensorShape([None]).concatenate(s), size) layer_output_shape = self._output_layer.compute_output_shape( output_shape_with_unknown_batch) return nest.map_structure(lambda s: s[1:], layer_output_shape) 这里也是判断是否给于输出层，如果有的话，返回全连接层的之后的输出大小。 @property def output_size(self): # Return the cell output and the id return BasicDecoderOutput( rnn_output=self._rnn_output_size(), sample_id=self._helper.sample_ids_shape) def step(self, time, inputs, state, name=None): &quot;&quot;&quot;Perform a decoding step. Args: time: scalar `int32` tensor. inputs: A (structure of) input tensors. state: A (structure of) state tensors and TensorArrays. name: Name scope for any created operations. Returns: `(outputs, next_state, next_inputs, finished)`. &quot;&quot;&quot; with ops.name_scope(name, &quot;BasicDecoderStep&quot;, (time, inputs, state)): cell_outputs, cell_state = self._cell(inputs, state) if self._output_layer is not None: cell_outputs = self._output_layer(cell_outputs) sample_ids = self._helper.sample( time=time, outputs=cell_outputs, state=cell_state) (finished, next_inputs, next_state) = self._helper.next_inputs( time=time, outputs=cell_outputs, state=cell_state, sample_ids=sample_ids) outputs = BasicDecoderOutput(cell_outputs, sample_ids) return (outputs, next_state, next_inputs, finished) 最重要的一步，这步其实类似LSTM的call方法，接受前一个隐层状态，当前的输入，返回一个输出状态，下一个状态，下一个输出，和finished。 总结 Decoder类有点类似RNN的call方法，接受前一个隐含状态以及当前时刻的输入返回当前的隐含状态和输出。" />
<meta property="og:description" content="前言 tf.contrib.seq2seq.dynamic_decode源码分析本文衔接上文。 首先tf.contrib.seq2seq.dynamic_decode主要作用是接收一个Decoder类，然后依据Encoder进行解码，实现序列的生成（映射）。其中，这个函数主要的一个思想是一步一步地调用Decoder的step函数（该函数接收当前的输入和隐层状态会生成下一个词），实现最后的一句话的生成。该函数类似tf.nn.dynamic_rnn。 该函数用到的Decoder类就是今天所要解析的类。 源码解析 class BasicDecoder(decoder.Decoder): &quot;&quot;&quot;Basic sampling decoder.&quot;&quot;&quot; def __init__(self, cell, helper, initial_state, output_layer=None): &quot;&quot;&quot;Initialize BasicDecoder. Args: cell: RNN实例 helper: Helper类，用于训练和推理 initial_state: 初始状态 output_layer: 输出层 Raises: TypeError: 如果`cell`, `helper` or `output_layer`没有正确的类型 &quot;&quot;&quot; rnn_cell_impl.assert_like_rnncell(&quot;cell&quot;, cell) if not isinstance(helper, helper_py.Helper): raise TypeError(&quot;helper must be a Helper, received: %s&quot; % type(helper)) if (output_layer is not None and not isinstance(output_layer, layers_base.Layer)): raise TypeError( &quot;output_layer must be a Layer, received: %s&quot; % type(output_layer)) self._cell = cell self._helper = helper self._initial_state = initial_state self._output_layer = output_layer BasicDecoder是继承于Decoder类，这个类只是个抽象类，定义了几个抽象方法。首先这个cell, helper, initial_state, output_layer这几个参数，cell一般就是个RNN（及其衍生类，比如LSTM）实例，initial_state一般是用Encoder的最后一个隐层状态，也就是标准Seq2seq的做法，output_layer是输出层，很自然。 那helper是啥可能有些抽象。这里简单的说就是文本生成分为两个阶段，一个是训练，一个是推理。那么我们希望得到训练的output（输出层的输出），推理的采样样本。而这里也是采用了一个策略模式（设计模式的内容，不懂的可以看看），把Helper分为两大类，一种是TrainingHelper，一种是InferenceHelper。 @property def batch_size(self): return self._helper.batch_size 构造getter。 def _rnn_output_size(self): size = self._cell.output_size if self._output_layer is None: return size else: # To use layer&#39;s compute_output_shape, we need to convert the # RNNCell&#39;s output_size entries into shapes with an unknown # batch size. We then pass this through the layer&#39;s # compute_output_shape and read off all but the first (batch) # dimensions to get the output size of the rnn with the layer # applied to the top. output_shape_with_unknown_batch = nest.map_structure( lambda s: tensor_shape.TensorShape([None]).concatenate(s), size) layer_output_shape = self._output_layer.compute_output_shape( output_shape_with_unknown_batch) return nest.map_structure(lambda s: s[1:], layer_output_shape) 这里也是判断是否给于输出层，如果有的话，返回全连接层的之后的输出大小。 @property def output_size(self): # Return the cell output and the id return BasicDecoderOutput( rnn_output=self._rnn_output_size(), sample_id=self._helper.sample_ids_shape) def step(self, time, inputs, state, name=None): &quot;&quot;&quot;Perform a decoding step. Args: time: scalar `int32` tensor. inputs: A (structure of) input tensors. state: A (structure of) state tensors and TensorArrays. name: Name scope for any created operations. Returns: `(outputs, next_state, next_inputs, finished)`. &quot;&quot;&quot; with ops.name_scope(name, &quot;BasicDecoderStep&quot;, (time, inputs, state)): cell_outputs, cell_state = self._cell(inputs, state) if self._output_layer is not None: cell_outputs = self._output_layer(cell_outputs) sample_ids = self._helper.sample( time=time, outputs=cell_outputs, state=cell_state) (finished, next_inputs, next_state) = self._helper.next_inputs( time=time, outputs=cell_outputs, state=cell_state, sample_ids=sample_ids) outputs = BasicDecoderOutput(cell_outputs, sample_ids) return (outputs, next_state, next_inputs, finished) 最重要的一步，这步其实类似LSTM的call方法，接受前一个隐层状态，当前的输入，返回一个输出状态，下一个状态，下一个输出，和finished。 总结 Decoder类有点类似RNN的call方法，接受前一个隐含状态以及当前时刻的输入返回当前的隐含状态和输出。" />
<link rel="canonical" href="https://uzzz.org/2019/06/11/795732.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/11/795732.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-11T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"前言 tf.contrib.seq2seq.dynamic_decode源码分析本文衔接上文。 首先tf.contrib.seq2seq.dynamic_decode主要作用是接收一个Decoder类，然后依据Encoder进行解码，实现序列的生成（映射）。其中，这个函数主要的一个思想是一步一步地调用Decoder的step函数（该函数接收当前的输入和隐层状态会生成下一个词），实现最后的一句话的生成。该函数类似tf.nn.dynamic_rnn。 该函数用到的Decoder类就是今天所要解析的类。 源码解析 class BasicDecoder(decoder.Decoder): &quot;&quot;&quot;Basic sampling decoder.&quot;&quot;&quot; def __init__(self, cell, helper, initial_state, output_layer=None): &quot;&quot;&quot;Initialize BasicDecoder. Args: cell: RNN实例 helper: Helper类，用于训练和推理 initial_state: 初始状态 output_layer: 输出层 Raises: TypeError: 如果`cell`, `helper` or `output_layer`没有正确的类型 &quot;&quot;&quot; rnn_cell_impl.assert_like_rnncell(&quot;cell&quot;, cell) if not isinstance(helper, helper_py.Helper): raise TypeError(&quot;helper must be a Helper, received: %s&quot; % type(helper)) if (output_layer is not None and not isinstance(output_layer, layers_base.Layer)): raise TypeError( &quot;output_layer must be a Layer, received: %s&quot; % type(output_layer)) self._cell = cell self._helper = helper self._initial_state = initial_state self._output_layer = output_layer BasicDecoder是继承于Decoder类，这个类只是个抽象类，定义了几个抽象方法。首先这个cell, helper, initial_state, output_layer这几个参数，cell一般就是个RNN（及其衍生类，比如LSTM）实例，initial_state一般是用Encoder的最后一个隐层状态，也就是标准Seq2seq的做法，output_layer是输出层，很自然。 那helper是啥可能有些抽象。这里简单的说就是文本生成分为两个阶段，一个是训练，一个是推理。那么我们希望得到训练的output（输出层的输出），推理的采样样本。而这里也是采用了一个策略模式（设计模式的内容，不懂的可以看看），把Helper分为两大类，一种是TrainingHelper，一种是InferenceHelper。 @property def batch_size(self): return self._helper.batch_size 构造getter。 def _rnn_output_size(self): size = self._cell.output_size if self._output_layer is None: return size else: # To use layer&#39;s compute_output_shape, we need to convert the # RNNCell&#39;s output_size entries into shapes with an unknown # batch size. We then pass this through the layer&#39;s # compute_output_shape and read off all but the first (batch) # dimensions to get the output size of the rnn with the layer # applied to the top. output_shape_with_unknown_batch = nest.map_structure( lambda s: tensor_shape.TensorShape([None]).concatenate(s), size) layer_output_shape = self._output_layer.compute_output_shape( output_shape_with_unknown_batch) return nest.map_structure(lambda s: s[1:], layer_output_shape) 这里也是判断是否给于输出层，如果有的话，返回全连接层的之后的输出大小。 @property def output_size(self): # Return the cell output and the id return BasicDecoderOutput( rnn_output=self._rnn_output_size(), sample_id=self._helper.sample_ids_shape) def step(self, time, inputs, state, name=None): &quot;&quot;&quot;Perform a decoding step. Args: time: scalar `int32` tensor. inputs: A (structure of) input tensors. state: A (structure of) state tensors and TensorArrays. name: Name scope for any created operations. Returns: `(outputs, next_state, next_inputs, finished)`. &quot;&quot;&quot; with ops.name_scope(name, &quot;BasicDecoderStep&quot;, (time, inputs, state)): cell_outputs, cell_state = self._cell(inputs, state) if self._output_layer is not None: cell_outputs = self._output_layer(cell_outputs) sample_ids = self._helper.sample( time=time, outputs=cell_outputs, state=cell_state) (finished, next_inputs, next_state) = self._helper.next_inputs( time=time, outputs=cell_outputs, state=cell_state, sample_ids=sample_ids) outputs = BasicDecoderOutput(cell_outputs, sample_ids) return (outputs, next_state, next_inputs, finished) 最重要的一步，这步其实类似LSTM的call方法，接受前一个隐层状态，当前的输入，返回一个输出状态，下一个状态，下一个输出，和finished。 总结 Decoder类有点类似RNN的call方法，接受前一个隐含状态以及当前时刻的输入返回当前的隐含状态和输出。","@type":"BlogPosting","url":"https://uzzz.org/2019/06/11/795732.html","headline":"【自然语言处理】tf.contrib.seq2seq.BasicDecoder源码解析","dateModified":"2019-06-11T00:00:00+08:00","datePublished":"2019-06-11T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/11/795732.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>【自然语言处理】tf.contrib.seq2seq.BasicDecoder源码解析</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h2><a id="_0"></a>前言</h2> 
  <p><a href="https://blog.csdn.net/tudaodiaozhale/article/details/90737077" rel="nofollow" data-token="2a17e7658c582ec178049fb0a00dfc51">tf.contrib.seq2seq.dynamic_decode源码分析</a>本文衔接上文。</p> 
  <blockquote> 
   <p>首先tf.contrib.seq2seq.dynamic_decode主要作用是接收一个Decoder类，然后依据Encoder进行解码，实现序列的生成（映射）。其中，这个函数主要的一个思想是一步一步地调用Decoder的step函数（该函数接收当前的输入和隐层状态会生成下一个词），实现最后的一句话的生成。该函数类似tf.nn.dynamic_rnn。</p> 
  </blockquote> 
  <p>该函数用到的Decoder类就是今天所要解析的类。</p> 
  <h2><a id="_6"></a>源码解析</h2> 
  <pre><code>class BasicDecoder(decoder.Decoder):
  """Basic sampling decoder."""
    def __init__(self, cell, helper, initial_state, output_layer=None):
    """Initialize BasicDecoder.

    Args:
      cell: RNN实例
      helper: Helper类，用于训练和推理
      initial_state: 初始状态
      output_layer: 输出层

    Raises:
      TypeError: 如果`cell`, `helper` or `output_layer`没有正确的类型
    """
    rnn_cell_impl.assert_like_rnncell("cell", cell)
    if not isinstance(helper, helper_py.Helper):
      raise TypeError("helper must be a Helper, received: %s" % type(helper))
    if (output_layer is not None
        and not isinstance(output_layer, layers_base.Layer)):
      raise TypeError(
          "output_layer must be a Layer, received: %s" % type(output_layer))
    self._cell = cell
    self._helper = helper
    self._initial_state = initial_state
    self._output_layer = output_layer
</code></pre> 
  <p>BasicDecoder是继承于Decoder类，这个类只是个抽象类，定义了几个抽象方法。首先这个cell, helper, initial_state, output_layer这几个参数，cell一般就是个RNN（及其衍生类，比如LSTM）实例，initial_state一般是用Encoder的最后一个隐层状态，也就是标准Seq2seq的做法，output_layer是输出层，很自然。<br> 那helper是啥可能有些抽象。这里简单的说就是文本生成分为两个阶段，一个是训练，一个是推理。那么我们希望得到训练的output（输出层的输出），推理的采样样本。而这里也是采用了一个<a href="https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/strategy.html" rel="nofollow" data-token="27f7cdb18c742c2523250a9d7550f6c3">策略模式</a>（设计模式的内容，不懂的可以看看），把Helper分为两大类，一种是TrainingHelper，一种是InferenceHelper。</p> 
  <pre><code>  @property
  def batch_size(self):
    return self._helper.batch_size
</code></pre> 
  <p>构造getter。</p> 
  <pre><code>  def _rnn_output_size(self):
    size = self._cell.output_size
    if self._output_layer is None:
      return size
    else:
      # To use layer's compute_output_shape, we need to convert the
      # RNNCell's output_size entries into shapes with an unknown
      # batch size.  We then pass this through the layer's
      # compute_output_shape and read off all but the first (batch)
      # dimensions to get the output size of the rnn with the layer
      # applied to the top.
      output_shape_with_unknown_batch = nest.map_structure(
          lambda s: tensor_shape.TensorShape([None]).concatenate(s),
          size)
      layer_output_shape = self._output_layer.compute_output_shape(
          output_shape_with_unknown_batch)
      return nest.map_structure(lambda s: s[1:], layer_output_shape)
</code></pre> 
  <p>这里也是判断是否给于输出层，如果有的话，返回全连接层的之后的输出大小。</p> 
  <pre><code>@property
  def output_size(self):
    # Return the cell output and the id
    return BasicDecoderOutput(
        rnn_output=self._rnn_output_size(),
        sample_id=self._helper.sample_ids_shape)
</code></pre> 
  <pre><code>  def step(self, time, inputs, state, name=None):
    """Perform a decoding step.

    Args:
      time: scalar `int32` tensor.
      inputs: A (structure of) input tensors.
      state: A (structure of) state tensors and TensorArrays.
      name: Name scope for any created operations.

    Returns:
      `(outputs, next_state, next_inputs, finished)`.
    """
    with ops.name_scope(name, "BasicDecoderStep", (time, inputs, state)):
      cell_outputs, cell_state = self._cell(inputs, state)
      if self._output_layer is not None:
        cell_outputs = self._output_layer(cell_outputs)
      sample_ids = self._helper.sample(
          time=time, outputs=cell_outputs, state=cell_state)
      (finished, next_inputs, next_state) = self._helper.next_inputs(
          time=time,
          outputs=cell_outputs,
          state=cell_state,
          sample_ids=sample_ids)
    outputs = BasicDecoderOutput(cell_outputs, sample_ids)
    return (outputs, next_state, next_inputs, finished)
</code></pre> 
  <p>最重要的一步，这步其实类似LSTM的call方法，接受前一个隐层状态，当前的输入，返回一个输出状态，下一个状态，下一个输出，和finished。</p> 
  <h2><a id="_103"></a>总结</h2> 
  <p>Decoder类有点类似RNN的call方法，接受前一个隐含状态以及当前时刻的输入返回当前的隐含状态和输出。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
