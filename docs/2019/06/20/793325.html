<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>TensorFlow基础知识学习笔记 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="TensorFlow基础知识学习笔记" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1.TensorFlow系统架构 数据操作层：主要包括卷积神经网络、激活函数等操作 图计算层：（核心）包括图的创建、编译、优化和执行 2.设计理念 将图的定义和图的运行完全分开。因此TensorFlow被认为是一个&quot;符号主义&quot;的库。符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量之间的计算关系，最后需要对数据流图进行编译，但此时的数据流图还是一个空壳，里面没有任何实际数据，只有把需要计算的输入放进去后，才能在整个模型中形成数据流，才能在整个模型中形成数据流，从而形成输出值 # 传统程序操作 t = 8 + 9 print(t) # 定义了t的运算，在运行时就执行了，并输出17, # tensorflow import tensorflow as tf t = tf.add(8, 9) print(t) # 输出 Tensor(&quot;Add_1:0&quot;, shape=(), dtype=int32) # 数据流图中的节点，实际上对应的是TensorFlow API中的一个操作，并没有真正去运行 TensorFlow中涉及的运算都要放在图中，而图的运行只发生在会话（session）中，开启会话后，就可以用数据去填充节点，进行运算，关闭会话后，就不能进行计算了，因此，会话提供了操作运行和Tensor求值的环境。 import tensorflow as tf # 创建图 a = tf.constant([1.0, 2.0]) b = tf.constant([3.0, 4.0]) c = a * b # 创建会话 sess = tf.Session() # 计算c print(sess.run(c)) sess.close() 3.TensorFlow的编程模型 ​ TensorFlow是用数据流图做计算的，因此需要先创建一个数据流图。一个简单的回归模型中的元素有：输入（input）、塑形（reshape）、Relu层（Relu Layer）、Logit层（Logit Layer）、Softmax、交叉熵（cross entropy）、梯度（gradient）、SGD训练（SGD Trainer）等部分。 ​ 它的计算过程是，首先从输入开始，经过塑形后，一层一层进行前向传播运算。Relu层里会有两个参数，即Wh1和bh1，在输出前是用ReLU（Rectified Linear Units）激活函数做非线性处理。然后进入Logit层（输出层），学习两个参数Wsm和bsm。用Softmax来计算输出结果中各个类别的概率分布，用交叉熵来度量两个概率分布（源样本的概率分布和输出结果的概率分布）之间的相似性，然后开始计算梯度，这里是需要前面的四个参数，以及交叉熵后的结果。随后进入SGD训练，也就是反向传播的过程，从上往下计算每一层的参数，依次进行更新。 ​ 顾名思义，TensorFlow是指“张量的流动”。TensorFlow的数据流图是由节点（node）和边（edge）组成的有向无环图。TensorFlow由Tensor和Flow组成，Tensor（张量）代表了数据流图中的边，Flow（流动）这个动作就代表了数据流图中节点所做的操作。 3.1边 ​ TensorFlow的边有两种连接关系：数据依赖和控制依赖。其中，实线边表示数据依赖，代表数据，即张量。任意维度的数据统称为张量。在机器学习算法中，张量在数据流图中从前往后流动一遍就完成了一次前向传播，而残差（在数理统计中，残差是指实际观察值与训练的估计值之间的差）从后向前流动一遍就完成了一次反向传播。TensorFlow支持的张量具有下表所示的数据属性 数据类型 Python类型 描述 DT_FLOAT tf.float32 32位浮点型 DT_DOUBLE tf.float64 64位浮点型 DT_INT64 tf.int64 64位有符号整型 DT_INT32 tf.int32 32位有符号整型 DT_INT16 tf.int16 16位有符号整型 DT_INT8 tf.int8 8位有符号整型 DT_UINT8 tf.uint8 8位无符号整型 DT_STRING tf.string 可变长度的字节数组，每一个张量元素都是一个字节数组 DT_BOOL tf.bool 布尔型 DT_COMPLEX64 tf.complex64 由两个32位浮点数组成的复数：实部和虚部 DT_QINT32 tf.qint32 用于量化操作的32位有符号整型 DT_QINT8 tf.qint8 用于量化操作的8位有符号整型 DT_QUINT8 tf.quint8 用于量化操作的8位无符号整型 量化是数字信号处理领域的一个概念，是指将信号的连续取值（或者大量可能的离散取值）金思维有限多个（或较少的）离散值的过程 3.2节点 ​ 节点又称为算子，它代表一个操作，一般用来表示施加的数学运算，也可以表示数据输入的起点以及输出的终点，或者是读取/写入持久变量的终点。下表列举了一些TensorFlow实现的算子，算子支持下表所示的张量的各种数据属性，并且需要在建立图的时候确定下来。 类别 示例 数学运算操作 Add、Substract、Multiply、Div、Exp、Log、Greater、Less、Equal 数组运算操作 Concat、Slice、Split、Constant、Rank、Shape、Shuffle 矩阵运算操作 MatMul、MatrixInverse、MatrixDeterminant 有状态的操作 Variable、Assign、AssignAdd 神经网络构建操作 SoftMax、Sigmoid、ReLU、Convolution2D、MaxPool 检查点操作 Save、Restore 队列和同步操作 Enqueue、Dequeue、MutexAcquire、MutexRelease 控制张量流动的操作 Merge、Switch、Enter、Leave、NextIteration 3.3其它概念 图 操作任务可以描述成一个有向无环图，如何构建一个图，第一步是创建各个节点 import tensorflow as tf # 创建一个常量运算操作，产生一个1x2矩阵 matrix1 = tf.constant([[3., 3.]]) # 创建另外一个常量运算操作，产生一个2x1矩阵 matrix2 = tf.constant([[2.], [2.]]) # 创建一个矩阵乘法运算，把matrix1和matrix2作为输入 # 返回值result代表矩阵乘法的结果 result = tf.matmul(matrix1, matrix2) 2. 会话 ​ 启动图的第一步是创建一个Session对象，会话（Session）提供在途中执行操作的一些方法。一般的模式是，建立会话，此时会生成一张空图，在会话中添加节点和边，形成一张图，然后执行。 with tf.Session() as sess: result = sess.run(result) print(result) ​ 在调用Session对象的run()方法来执行图时，传入一些tensor，这个过程叫填充，返回的结果类型根据输入的类型而定，这个过程叫取回。 ​ 会话是图交互的一个桥梁，一个会话可以有多个图，会话可以修改图的结构，也可以往图中注入数据进行计算。因此会话主要有两个API接口：Extend和Run。Extend操作是在图中添加节点和边，Run操作是输入计算的节点和填充必要的数据后，进行运算，并输出结果 设备 ​ 设备是指一块可以用来运算并且拥有自己的地址空间的硬件，如GPU和CPU，TensorFlow为了实现分布式执行操作，充分利用资源，可以明确指定操作在哪个设备上执行 with tf.Session() as sess: # 指定在第二个gpu上执行 with tf.device(&quot;/gpu:1&quot;): ... 变量 ​ 变量是一种特殊的数据，它在图中有固定的位置，不像普通张量那样可以流动。创建一个变量张量，使用tf.Variable()构造函数，这个构造函数需要一个初始值，初始值的形状和类型决定了这个变量的形状和类型 # 创建一个变量，初始化为标量0 state = tf.Variable(0, name=&#39;counter&#39;) ​ TensorFlow还提供了填充机制，可以在构建图时使用tf.placeholder()临时替代任意操作的张量，在调用Session对象的run()方法去执行图时，使用填充数据作为调用的参数，调用之后，填充数据就消失 4.常用API 4.1图，操作和张量 ​ TensorFlow的计算表现为数据流图，所以tf.Graph类中包含一系列表示计算的操作对象（tf.Operation），以及在操作之间流动的数据–张量对象（tf.Tensor）。与图相关的API均位于tf.Graph类中 操作 描述 tf.Graph.__ init__() 创建一个空图 tf.Graph.as_default() 将某图设置成默认图，并返回一个上下文管理器，如果不显式添加一个默认图，系统会自动设置一个全局的默认图，所设置的默认图，在模块范围内定义的节点都将默认加入默认图中 tf.Graph.device(deice_name_or_function) 定义运行图所使用的的设备，并返回一个上下文管理器 tf.Graph.name_scope(name) 为节点创建层次化的名称，并返回一个上下文管理器 ​ tf.Operation类代表图中的一个节点，用于计算张量数据。该类型由节点构造器（如tf.matmul()或者Graph.create_op()）产生。与操作相关的API均位于tf.Operation类中 操作 描述 tf.Operation.name 操作的名称 tf.Operation.type 操作的类型 tf.Operation.inputs/outputs 操作的输入与输出 tf.Operation.control_inputs 操作的依赖 tf.Operation.run(feed_dict=None, session=None) 在会话中运行该操作 tf.Operation.get_attr(name) 获取操作的属性值 ​ tf.Tensor类是操作输出的符号句柄，它不包含操作输出的值，而是提供了一种在tf.Session中计算这些值的方法。这样就可以在操作之间构建一个数据流连接，使TensorFlow能够执行一个表示大量多步计算的图形。与张量相关的API均位于tf.Tensor类中 操作 描述 tf.Tensor.dtype 张量的数据类型 tf.Tensor.name 张量的名称 tf.Tensor.value_index 张量在操作输出中的索引 tf.Tensor.graph 张量所在的图 tf.Tensor.op 产生该张量的操作 tf.Tensor.consumers() 返回使用该张量的操作列表 tf.Tensor.eval(feed_dict=None, session=None) 在会话中求张量的值，需要使用sess.as_default或eval(session=sess) tf.Tensor.get_shape() 返回用于表示张量的形状（维度）的类TensorShape tf.Tensor.set_shape(shape) 更新张量的形状 tf.Tensor.device 设置计算该张量的设备 4.2可视化 ​ 可视化时，需要在程序中给必要的节点添加摘要（summary），摘要会收集该节点的数据，并标记上第几步、时间戳等标识，写入事件文件（event file）中。tf.summary.FileWriter类用于在目录中创建事件文件，并且向文件中添加摘要和事件，用来在TensorBoard中展示 API 描述 tf.summary.FileWriter.__ init__(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None) 创建FileWriter和事件文件，会在logdir中创建一个新的事件文件 tf.summary.FileWriter.add_summary(summary, global_step=None) 将摘要添加到事件文件 tf.summary.FileWriter.add_event(event) 向事件文件中添加一个事件 tf.summary.FileWriter.add_graph(graph, global_step=None, graph_def=None) 向事件中添加一个图 tf.summary.FileWriter.get_logdir() 获取事件文件的路径 tf.summary.FileWriter.flush() 将所有事件都写入磁盘 tf.summary.FileWriter.close() 将事件写入磁盘，并关闭文件操作符 tf.summary.scalar(name, tensor, collections=None) 输出包含单个标量值的摘要 tf.summary.histogram(name, values, collections=None) 输出包含直方图的摘要 tf.summary.audio(name, tensor, sample_rate, max_outputs=3, collections=None) 输出包含音频的摘要 tf.summary.image(name, tensor, max_outputs=3, collections=None) 输出包含图片的摘要 tf.summary.merge(inputs, collections=None, name=None) 合并摘要，包含所有输入摘要的值 5.变量作用域 ​ 在TensorFlow中有两个作用域（scope），一个是name_scope，一个是variable_scope。variable_scope主要是给variable_name加前缀，也可以给op_name加前缀；name_scope是给op_name加前缀。 5.1variable_scope v = tf.get_variable(name, shape, dtype, initializer) # 通过所给的名字创建或返回一个变量 tf.variable_scope(&lt;scope_name&gt;) # 为变量指定命名空间 当tf.get_variable_scope().reuse==False时，variable_scope作用域只能用来创建新变量： import tensorflow as tf with tf.variable_scope(&quot;foo&quot;): v = tf.get_variable(&quot;v&quot;, [1]) v2 = tf.get_variable(&quot;v&quot;, [1]) assert v.name == &quot;foo/v:0&quot; 上述程序会抛出ValueError错误，因为v这个变量已经被定义过了，但tf.get_variable_scope().reuse默认为False，故不能重用 当tf.get_variable_scope().reuse==True时，作用域可以共享变量： import tensorflow as tf with tf.variable_scope(&quot;foo&quot;) as scope: v = tf.get_variable(&quot;v&quot;, [1]) with tf.variable_scope(&quot;foo&quot;, reuse=True): # 也可以写成: # scope.reuse_variables() v1 = tf.get_variable(&quot;v&quot;, [1]) assert v1 == v 5.1.1获取变量作用域 可以直接通过tf.get_variable_scope()来获取变量作用域 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;) as foo_scope: v = tf.get_variable(&quot;v&quot;, [1]) with tf.variable_scope(foo_scope): w = tf.get_variable(&quot;w&quot;, [1]) 如果在开启的一个变量作用域里使用之前预先定义的一个作用域，则会跳过当前变量的作用域，保持预先存在的作用域不变 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;) as foo_scope: assert foo_scope.name == &quot;foo&quot; with tf.variable_scope(&quot;bar&quot;): with tf.variable_scope(&quot;baz&quot;) as other_scope: assert other_scope.name == &quot;bar/baz&quot; with tf.variable_scope(foo_scope) as foo_scope2: assert foo_scope2.name == &quot;foo&quot; # 保持不变 5.1.2变量作用域的初始化 变量作用域可以默认携带一个初始化器，在这个作用域中的子作用域或变量都可以继承或者重写父作用域初始化器中的值 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;, initializer=tf.constant_initializer(0.4)): v = tf.get_variable(&quot;v&quot;, [1]) assert v.eval() == 0.4 # 被作用域初始化 w = tf.get_variable(&quot;w&quot;, [1], initializer=tf.constant_initializer(0.3)) assert w.eval() == 0.3 # 重写初始化器的值 with tf.variable_scope(&quot;bar&quot;): v = tf.get_variable(&quot;v&quot;, [1]) assert v.eval() == 0.4 # 继承默认的初始化器 with tf.variable_scope(&quot;baz&quot;, initializer=tf.constant_initializer(0.2)): v = tf.get_variable(&quot;v&quot;, [1]) assert v.eval() == 0.2 # 重写父作用域的初始化器的值 上面讲的是variable_name，对于op_name，在variable_scope作用域下的操作，也会被加上前缀 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;): x = 1.0 + tf.get_variable(&quot;v&quot;, [1]) assert x.op.name == &quot;foo/add&quot; 5.2name_scope示例 TensorFlow中常常有数以千计的节点，在可视化的过程中很难一下子展示出来，因此用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。name_scope会影响op_name，不会影响用get_variable()创建的变量，而会影响通过Variable()创建的变量 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;): with tf.name_scope(&quot;bar&quot;): v = tf.get_variable(&quot;v&quot;, [1]) b = tf.Variable(tf.zeros([1]), name=&#39;b&#39;) x = 1.0 + v assert v.name == &quot;foo/v:0&quot; assert b.name == &quot;foo/bar/b:0&quot; assert x.op.name == &quot;foo/bar/add&quot; 6.批标准化 ​ 批标准化（batch normalization, BN）是为了克服神经网络层数加深导致难以训练而诞生的。随着神经网络的深度加深，训练会越来越困难，收敛速度会很慢，常常会导致梯度消失问题。 梯度消失问题：在神经网络中，当前隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫梯度消失问题 ​ 传统机器学习中有一个ICS理论，这是一个经典假设：源域（source domain）和目标域（target domain）的数据分布是一致的，也就是说，训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障 ​ Covariate Shift是指训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好地泛化（泛化：通俗来讲就是指学习到的模型对位置数据的预知能力）。它是分布不一致假设之下的一个分支问题，也就是指源域和目标域的条件概率是一致的，但是边缘概率不同。对于神经网络的各层输出，在经过了层内的操作后，各层输出分布就会与对应的输入信号分布不同，而且差异会随着网络深度增大而增大，但是每一层所指向的样本标记仍然是不变的。 ​ 解决思路一般是根据训练样本的比例对训练样本做一个矫正，因此，通过引入批标准化来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差。 6.1方法 ​ 批标准化一般用在非线性映射（激活函数）之前，对x=Wu+b做规范化，使结果（输出信号各个维度）的均值为0，方差为1。让每一层的输入有一个稳定的分布会有利于网络的训练 6.2优点 ​ 批标准化通过规范化让激活函数分布在线性区间，结果就是加大了梯度，让模型更加大胆地进行梯度下降，于是有如下优点： 加大探索的步长，加快收敛的速度 更容易跳出局部最小值 破坏原来的数据分布， 一定程度上缓解过拟合（过拟合：为了得到一致假设而使假设变得过度严格称为过拟合，出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少） 6.3示例 对每一层的Wx_plus_b进行批标准化，这个步骤放在激活函数之前 # 计算Wx_plus_b的均值和方差，其中axes = [0]表示想要标准化的维度 fc_mean, fc_var = tf.nn.moments(Wx_plus_b, axes=[0], ) scale = tf.Variable(tf.ones([out_size])) shift = tf.Variable(tf.zeros([out_size])) epsilon = 0.001 Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, fc_mean, fc_var, shift, scale, epsilon) # 也就是在做： # Wx_plus_b = (Wx_plus_b - fc_mean)/tf.sqrt(fc_var + 0.001) # Wx_plus_b = Wx_plus_b * scale + shift 7.神经元函数及优化方法 7.1激活函数 ​ 激活函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经网络。神经网络之所以能解决非线性问题（如语音、图像识别），本质上就是激活函数加入了非线性因素，弥补了线性模型的表达力，把“激活的神经元的特征”通过函数保留并映射到下一层 ​ 激活函数不会更改输入数据的维度，也就是输入和输出的维度是相同的。TensorFlow中有如下激活函数，输入均为要计算的x（一个张量），输出均为与x数据类型相同的张量。常见的有sigmoid、tanh、relu和softplus sigmoid函数： S ( x ) = 1 1 + e − x S(x)=\frac{1}{1+e^-x} S(x)=1+e−x1​ 使用方法： a = tf.constant([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]]) sess = tf.Session() print(sess.run(tf.sigmoid(a))) ​ sigmoid函数优点在于，它的输出映射在（0, 1）内，单调连续，非常适合于用作输出层，并且求导比较容易。缺点在于，因为软饱和性（软饱和性：是指激活函数h(x)在取值趋于无穷大时，它的一阶导数趋于0.硬饱和是指当|x|&gt;c时，其中c为常数，f’(x)=0），一旦落入软饱和区，f’(x)就会变得接近于0，很容易产生梯度消失。（梯度消失：是指在更新模型参数时采用链式求导法则反向求导，越往前梯度越小。最终的结果是到达一定深度后梯度对于模型的更新就没有任何贡献了） tanh函数： S ( x ) = 1 − e − 2 x 1 + e − 2 x S(x)=\frac{1-e^-2x}{1+e^-2x} S(x)=1+e−2x1−e−2x​ tanh函数也具有软饱和性，因为它的输出以0为中心，收敛速度比sigmoid要快，但是仍无法解决梯度消失的问题 relu函数 目前最受欢迎的激活函数。softplus可以看作是relu的平滑版本。relu定义为f(x)=max(x, 0)。softplus定义为f(x)=log(1+exp(x))。使用示例如下： a = tf.constant([-1.0, 2.0]) with tf.Session() as sess: b = tf.nn.relu(a) print(sess.run(b)) 除了relu本身之外，TensorFlow还定义了relu6以及crelu。 dropout函数 ​ 一个神经元将以概率keep_prob决定是否被抑制。如果被抑制，该神经元的输出就为0；如果不被抑制，那么该神经元的输出值将被放大到原来的1/keep_prob倍。 ​ 在默认情况下，每个神经元是否被抑制是相互独立的。但是否被抑制也可以通过noise_shape来调节。当noise_shape[i] == shape(x)[i]时，x中的元素是相互独立的。如果shape(x) = [k, l, m, n]，x中的维度的顺序分别为批、行、列和通道，如果noise_shape = [k, l, l, n]，那么每个批和通道相互独立，行跟列相互关联，也就是说，要不都为0，要不都还是原来的值。使用示例如下： a = tf.constant([[-1.0, 2.0, 3.0, 4.0]]) with tf.Session() as sess: b = tf.nn.dropout(a, 0.5, noise_shape=[1, 4]) print(sess.run(b)) b = tf.nn.dropout(a, 0.5, noise_shape=[1, 1]) print(sess.run(b)) 7.2卷积函数 ​ 卷积函数是构建神经网络的重要支架，是在一批图像上扫描的二维过滤器。 tf.nn.convolution(input, filter, padding, strides=None, dilation_rate=None, name=None, data_format=None) 这个函数是计算N维卷积的和 tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)这个函数的作用是对一个四维的输入数据input和四维的卷积核filter进行操作，然后对输入数据进行一个二维的卷积操作，最后得到卷积之后的结果 tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None) # 输入： # input：一个tensor。数据类型必须是float32或者float64 # filter：一个tensor。数据类型必须与input相同 # strides：一个长度是4的一维整数类型数组，每一维度对应的是input中每一维的对应移动步数 # padding：一个字符串，取值为SAME或VALID # padding=&#39;SAME&#39;：仅适用于全尺寸操作，即输入数据维度和输出数据维度相同 # padding=&#39;VALID&#39;：适用于部分窗口，即输入数据维度和输出数据维度不同 # use_cudnn_on_gpu：一个可选布尔值，默认是True # name：（可选）为该操作取一个名字 使用示例如下： input_data = tf.Variable(np.random.rand(10,9,9,3), dtype = np.float32) filter_data = tf.Variable(np.random.rand(2,2,3,2), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1,1,1,1], padding=&#39;SAME&#39;) tf.nn.depthwise_conv2d(input, filter, strides, padding, rate=None, name=None, data_format=None)这个函数输入张量的数据维度是[batch, in_height, in_width, in_channels]，卷积核的维度是[filter_height, filter_width, in_channels, channel_multiplier]，在通道in_channels上面的卷积深度是1，depthwise_conv2d函数将不同的卷积核独立地应用在in_channels的每个通道上（从通道1到通道channel_multiplier），然后把所有的结果进行汇总。最后输出通道的总数是in_channels*channel_multiplier。 使用示例如下： input_data = tf.Variable(np.random.rand(10, 9, 9, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 5), dtype=np.float32) y = tf.nn.depthwise_conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate=None, name=None, data_format=None)是利用几个分离的卷积核去做卷积。在这个API中，将应用一个二维的卷积核，在每个通道上，以深度channel_multiplier进行卷积 def separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate=None, name=None, data_format=None) # 特殊参数： # depthwise_filter：一个张量。数据维度是四维[filter_height, filter_width, in_channels, channel_multiplier]。其中，in_channels的卷积深度是1 # pointwise_filter：一个张量，数据维度是四维[1, 1, channel_multiplier*in_channels, out_channels]。其中，pointwise_filter是在depthwise_filter卷积之后的混合卷积 使用示例如下： input_data = tf.Variable(np.random.rand(10, 9, 9, 3), dtype=np.float32) depthwise_filter = tf.Variable(np.random.rand(2, 2, 3, 5), dtype=np.float32) pointwise_filter = tf.Variable(np.random.rand(1, 1, 15, 20), dtype=np.float32) y = tf.nn.separable_conv2d(input_data, depthwise_filter, pointwise_filter, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.atrous_conv2d(value, filters, rate, padding, name=None)计算Atrous卷积，又称孔卷积或者扩张卷积。 使用示例如下： input_data = tf.Variable(np.random.rand(1, 5, 5, 1), dtype=np.float32) filters = tf.Variable(np.random.rand(3, 3, 1, 1), dtype=np.float32) y = tf.nn.atrous_conv2d(input_data, filters, 2, padding=&#39;SAME&#39;) tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding=‘SAME’, data_format=‘NHWC’, name=None)在解卷积网络中有时称为“反卷积”，但实际上是conv2d的转置，而不是实际的反卷积 def conv2d_transpose(value, filter, output_shape, strides, padding=&#39;SAME&#39;, data_format=&#39;NHWC&#39;, name=None) # 特殊参数： # output_shape：一维的张量，表示反卷积运算后输出的形状 # 输出：和value一样维度的Tensor 使用示例如下： x = tf.random_normal(shape=[1, 3, 3, 1]) kernel = tf.random_normal(shape=[2, 2, 3, 1]) y = tf.nn.conv2d_transpose(x, kernel, output_shape=[1, 5, 5, 3], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)和二维卷积类似。这个函数是用来计算给定三维的输入和过滤器的情况下的一维卷积。不同的是，它的输入是三维，如[batch, in_width, in_channels]。卷积核的维度也是三维，少了一维filter_height，如[filter_width, in_channels, out_channels]。stride是一个正整数，代表卷积核向右移动每一步的长度 tf.nn.conv3d(input, filter, strides, padding, name=None)和二维卷积类似，这个函数用来计算给定五维的输入和过滤器的情况下的三维卷积。和二维卷积相对比： input的shape中多了一维in_depth，形状为Shape[batch, in_depth, in_height, in_width, in_channels] filter的shape中多了一维filter_depth，由filter_depth，filter_height，filter_width构成了卷积核的大小 strides中多了一维，变为[strides_batch， strides_depth， strides_height， strides_width， strides_channel]，必须保证strides[0] = strides[4] = 1 tf.nn.conv3d_transpose(value, filter, output_shape, strides, padding=‘SAME’, name=None)和二维反卷积类似 7.3池化函数 ​ 在神经网络中，池化函数一般跟在卷积函数的下一层，池化操作是利用一个矩阵窗口在张量上进行扫描，将每个矩阵窗口中的值通过取最大值或平均值来减少元素个数。每个池化操作的矩阵窗口大小是由ksize指定的，并且根据步长strides决定移动步长 tf.nn.avg_pool(value, ksize, strides, padding, data_format=‘NHWC’, name=None)这个函数计算池化区域中元素的平均值 def avg_pool(value, ksize, strides, padding, data_format=&#39;NHWC&#39;, name=None) # value：一个四维的张量。数据维度是[batch, height, width, channels] # ksize：一个长度不小于4的整形数组。每一位上的值对应于输入数据张量中每一维的窗口对应值 # strides：一个长度不小于4的整形数组，该参数指定滑动窗口在输入数据张量每一维上的步长 # padding：一个字符串，取值为SAME或者VALID # data_format：&#39;NHWC&#39;代表输入张量维度的顺序，N为个数，H为高度，W为宽度，C为通道数（RGB三通道或者灰度单通道） # name：为这个操作取一个名字 # 输出：一个张量，数据类型和value相同 使用示例如下： input_data = tf.Variable(np.random.rand(10, 6, 6, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 10), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) out_put = tf.nn.avg_pool(value=y, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.max_pool(value, ksize, strides, padding, data_format=‘NHWC’, name=None)这个函数是计算池化区域中元素的最大值。使用示例如下： input_data = tf.Variable(np.random.rand(10, 6, 6, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 10), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) out_put = tf.nn.max_pool(value=y, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax, name=None)这个函数的作用是计算池化区域中元素的最大值和该最大值所在的位置 在计算位置argmax的时候，我们将input平铺了进行计算，所以，如果input = [b, y, x, c]，那么索引位置是 ((b * height + y) * width + x) * channels + c。使用示例如下： input_data = tf.Variable(np.random.rand(10, 6, 6, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 10), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) out_put, argmax = tf.nn.max_pool_with_argmax(input=y, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # 返回结果是一个张量组成的元组（output, argmax），output表示池化区域的最大值，argmax的数据类型是Targmax，维度是四维 tf.nn.avg_pool3d()和tf.nn.max_pool3d()分别是在三维下的平均池化和最大池化 tf.nn.fractional_avg_pool()和tf.nn.fractional_max_pool()分别是Benjamin Graham在论文中提出的池化技术，池化后的图片大小可以成非整数倍缩小，如$ \sqrt 2 ， ， ， \sqrt 3 $ tf.nn.pool(input, window_shape, pooling_type, padding, dilation_rate=None, strides=None, name=None, data_format=None)这个函数执行一个N维的池化操作 7.4分类函数 tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None) tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None) # 输入：logits：[batch_size, num_classes]，targets：[batch_size, size].logits用最后一层的输入即可 # 最后一层不需要进行sigmoid运算，此函数内部进行了sigmoid操作 # 输出：loss[batch_size, num_classes] 这个函数的输入要格外注意。如果采用此函数作为损失函数，在神经网络的最后一层不需要进行sigmoid运算 tf.nn.softmax(logits, dim=-1, name=None)计算SoftMax激活，也就是 softmax = exp(logits) / reduce_sum(exp(logits), dim) tf.nn.log_softmax(logits, dim=-1, name=None)计算log softmax激活，也就是 logsoftmax = logits - log(reduce_sum(exp(logits), dim)) tf.nn.softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None) softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None) # 输入：logits and labels均为[batch_size, num_classes] # 输出：loss[batch_size]，里面保存的是batch中每个样本的交叉熵 tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None) tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None) # 输入：logits：[batch_size, num_classes] labels：[batch_size]，必须在[0, num_classes] # 输出：loss[batch_size]，里面保存的是batch中每个样本的交叉熵 7.5优化方法 ​ 目前加速训练的优化方法基本都是基于梯度下降的，只是细节上有差异。梯度下降是求函数极值的一种方法，学习到最后就是求损失函数的极值问题 BGD法 ​ BGD的全称是batch gradient descent，即批梯度下降。这种方法是利用现有参数对训练集中每一个输入生成一个估计输出yi，然后跟实际输出yi比较，统计所有误差，求平均后得到平均误差，以此作为更新参数的一句。它的迭代过程为： 提取训练集中的所有内容{x1, ···, xn}，以及相关的输出yi 计算梯度和误差并更新参数 ​ 这个方法的优点是，使用所有训练数据计算，能够保证收敛，并且不需要逐渐减少学习率；缺点是，每一步都需要使用所有的训练集，随着训练的进行，速度会越来越慢 SGD法 ​ SGD的全称是stochastic gradient descent，即随机梯度下降。因为这种方法的主要思想是将数据集拆分成一个个批次（batch），随机抽取一个批次来计算并更新参数 ​ SGD在每一次迭代计算mini-batch的梯度，然后对参数进行更新。与BGD相比，SGD在训练数据集很大时，仍能以较快的速度收敛，但仍有以下两个缺点 由于抽取不可避免地梯度会有误差，需要手动调整学习率，但是选择合适的学习率又很困难 SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点 Momentum法 ​ Momentum是模拟物理学中动量的概念，更新时会在一定程度上保留之前的更新方向，利用当前批次再微调本次的更新参数，因此引入了一个新的变量v（速度），作为前几次梯度的累加。因此，Momentum能够更新学习率，在下降初期，前后梯度方向一致时，能够加速学习，在下降的中后期，在局部最小值的附近来回震荡时，能够抑制震荡，加快收敛 Nesterov Momentum法 ​ 是对Momentum的改进，Momentum法首先计算一个梯度，然后在加速更新梯度的方向进行一个大的跳跃；Nesterov项首先在原来加速的梯度方向进行一个大的跳跃，然后在该位置计算梯度值，然后用这个梯度值修正最终的更新方向。 Adagrad法 ​ Adagrad法能够自适应地为各个参数分配不同的学习率，能够控制每个维度的梯度方向。这种方法的优点是能够实现学习率的自动更改；如果本次更新时梯度大，学习率就衰减得快一些；如果这一次更新时梯度小，学习率就衰减得慢一些。 Adadelta法 ​ Adadelta法仍然存在一些问题：其学习率单调递减没在训练的后期学习率非常小，并且需要手动设置一个全局的初始学习率 RMSProp法 ​ RMSProp法与Momentum法类似，通过引入一个衰减系数，使每一回合都衰减一定比例，在实践中，对循环神经网络（RNN）效果很好 Adam法 ​ Adam的名称来源于自适应矩估计（adaptive moment estimation）。Adam法根据损失函数针对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。（矩估计就是利用样本来估计总体中相应的参数）" />
<meta property="og:description" content="1.TensorFlow系统架构 数据操作层：主要包括卷积神经网络、激活函数等操作 图计算层：（核心）包括图的创建、编译、优化和执行 2.设计理念 将图的定义和图的运行完全分开。因此TensorFlow被认为是一个&quot;符号主义&quot;的库。符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量之间的计算关系，最后需要对数据流图进行编译，但此时的数据流图还是一个空壳，里面没有任何实际数据，只有把需要计算的输入放进去后，才能在整个模型中形成数据流，才能在整个模型中形成数据流，从而形成输出值 # 传统程序操作 t = 8 + 9 print(t) # 定义了t的运算，在运行时就执行了，并输出17, # tensorflow import tensorflow as tf t = tf.add(8, 9) print(t) # 输出 Tensor(&quot;Add_1:0&quot;, shape=(), dtype=int32) # 数据流图中的节点，实际上对应的是TensorFlow API中的一个操作，并没有真正去运行 TensorFlow中涉及的运算都要放在图中，而图的运行只发生在会话（session）中，开启会话后，就可以用数据去填充节点，进行运算，关闭会话后，就不能进行计算了，因此，会话提供了操作运行和Tensor求值的环境。 import tensorflow as tf # 创建图 a = tf.constant([1.0, 2.0]) b = tf.constant([3.0, 4.0]) c = a * b # 创建会话 sess = tf.Session() # 计算c print(sess.run(c)) sess.close() 3.TensorFlow的编程模型 ​ TensorFlow是用数据流图做计算的，因此需要先创建一个数据流图。一个简单的回归模型中的元素有：输入（input）、塑形（reshape）、Relu层（Relu Layer）、Logit层（Logit Layer）、Softmax、交叉熵（cross entropy）、梯度（gradient）、SGD训练（SGD Trainer）等部分。 ​ 它的计算过程是，首先从输入开始，经过塑形后，一层一层进行前向传播运算。Relu层里会有两个参数，即Wh1和bh1，在输出前是用ReLU（Rectified Linear Units）激活函数做非线性处理。然后进入Logit层（输出层），学习两个参数Wsm和bsm。用Softmax来计算输出结果中各个类别的概率分布，用交叉熵来度量两个概率分布（源样本的概率分布和输出结果的概率分布）之间的相似性，然后开始计算梯度，这里是需要前面的四个参数，以及交叉熵后的结果。随后进入SGD训练，也就是反向传播的过程，从上往下计算每一层的参数，依次进行更新。 ​ 顾名思义，TensorFlow是指“张量的流动”。TensorFlow的数据流图是由节点（node）和边（edge）组成的有向无环图。TensorFlow由Tensor和Flow组成，Tensor（张量）代表了数据流图中的边，Flow（流动）这个动作就代表了数据流图中节点所做的操作。 3.1边 ​ TensorFlow的边有两种连接关系：数据依赖和控制依赖。其中，实线边表示数据依赖，代表数据，即张量。任意维度的数据统称为张量。在机器学习算法中，张量在数据流图中从前往后流动一遍就完成了一次前向传播，而残差（在数理统计中，残差是指实际观察值与训练的估计值之间的差）从后向前流动一遍就完成了一次反向传播。TensorFlow支持的张量具有下表所示的数据属性 数据类型 Python类型 描述 DT_FLOAT tf.float32 32位浮点型 DT_DOUBLE tf.float64 64位浮点型 DT_INT64 tf.int64 64位有符号整型 DT_INT32 tf.int32 32位有符号整型 DT_INT16 tf.int16 16位有符号整型 DT_INT8 tf.int8 8位有符号整型 DT_UINT8 tf.uint8 8位无符号整型 DT_STRING tf.string 可变长度的字节数组，每一个张量元素都是一个字节数组 DT_BOOL tf.bool 布尔型 DT_COMPLEX64 tf.complex64 由两个32位浮点数组成的复数：实部和虚部 DT_QINT32 tf.qint32 用于量化操作的32位有符号整型 DT_QINT8 tf.qint8 用于量化操作的8位有符号整型 DT_QUINT8 tf.quint8 用于量化操作的8位无符号整型 量化是数字信号处理领域的一个概念，是指将信号的连续取值（或者大量可能的离散取值）金思维有限多个（或较少的）离散值的过程 3.2节点 ​ 节点又称为算子，它代表一个操作，一般用来表示施加的数学运算，也可以表示数据输入的起点以及输出的终点，或者是读取/写入持久变量的终点。下表列举了一些TensorFlow实现的算子，算子支持下表所示的张量的各种数据属性，并且需要在建立图的时候确定下来。 类别 示例 数学运算操作 Add、Substract、Multiply、Div、Exp、Log、Greater、Less、Equal 数组运算操作 Concat、Slice、Split、Constant、Rank、Shape、Shuffle 矩阵运算操作 MatMul、MatrixInverse、MatrixDeterminant 有状态的操作 Variable、Assign、AssignAdd 神经网络构建操作 SoftMax、Sigmoid、ReLU、Convolution2D、MaxPool 检查点操作 Save、Restore 队列和同步操作 Enqueue、Dequeue、MutexAcquire、MutexRelease 控制张量流动的操作 Merge、Switch、Enter、Leave、NextIteration 3.3其它概念 图 操作任务可以描述成一个有向无环图，如何构建一个图，第一步是创建各个节点 import tensorflow as tf # 创建一个常量运算操作，产生一个1x2矩阵 matrix1 = tf.constant([[3., 3.]]) # 创建另外一个常量运算操作，产生一个2x1矩阵 matrix2 = tf.constant([[2.], [2.]]) # 创建一个矩阵乘法运算，把matrix1和matrix2作为输入 # 返回值result代表矩阵乘法的结果 result = tf.matmul(matrix1, matrix2) 2. 会话 ​ 启动图的第一步是创建一个Session对象，会话（Session）提供在途中执行操作的一些方法。一般的模式是，建立会话，此时会生成一张空图，在会话中添加节点和边，形成一张图，然后执行。 with tf.Session() as sess: result = sess.run(result) print(result) ​ 在调用Session对象的run()方法来执行图时，传入一些tensor，这个过程叫填充，返回的结果类型根据输入的类型而定，这个过程叫取回。 ​ 会话是图交互的一个桥梁，一个会话可以有多个图，会话可以修改图的结构，也可以往图中注入数据进行计算。因此会话主要有两个API接口：Extend和Run。Extend操作是在图中添加节点和边，Run操作是输入计算的节点和填充必要的数据后，进行运算，并输出结果 设备 ​ 设备是指一块可以用来运算并且拥有自己的地址空间的硬件，如GPU和CPU，TensorFlow为了实现分布式执行操作，充分利用资源，可以明确指定操作在哪个设备上执行 with tf.Session() as sess: # 指定在第二个gpu上执行 with tf.device(&quot;/gpu:1&quot;): ... 变量 ​ 变量是一种特殊的数据，它在图中有固定的位置，不像普通张量那样可以流动。创建一个变量张量，使用tf.Variable()构造函数，这个构造函数需要一个初始值，初始值的形状和类型决定了这个变量的形状和类型 # 创建一个变量，初始化为标量0 state = tf.Variable(0, name=&#39;counter&#39;) ​ TensorFlow还提供了填充机制，可以在构建图时使用tf.placeholder()临时替代任意操作的张量，在调用Session对象的run()方法去执行图时，使用填充数据作为调用的参数，调用之后，填充数据就消失 4.常用API 4.1图，操作和张量 ​ TensorFlow的计算表现为数据流图，所以tf.Graph类中包含一系列表示计算的操作对象（tf.Operation），以及在操作之间流动的数据–张量对象（tf.Tensor）。与图相关的API均位于tf.Graph类中 操作 描述 tf.Graph.__ init__() 创建一个空图 tf.Graph.as_default() 将某图设置成默认图，并返回一个上下文管理器，如果不显式添加一个默认图，系统会自动设置一个全局的默认图，所设置的默认图，在模块范围内定义的节点都将默认加入默认图中 tf.Graph.device(deice_name_or_function) 定义运行图所使用的的设备，并返回一个上下文管理器 tf.Graph.name_scope(name) 为节点创建层次化的名称，并返回一个上下文管理器 ​ tf.Operation类代表图中的一个节点，用于计算张量数据。该类型由节点构造器（如tf.matmul()或者Graph.create_op()）产生。与操作相关的API均位于tf.Operation类中 操作 描述 tf.Operation.name 操作的名称 tf.Operation.type 操作的类型 tf.Operation.inputs/outputs 操作的输入与输出 tf.Operation.control_inputs 操作的依赖 tf.Operation.run(feed_dict=None, session=None) 在会话中运行该操作 tf.Operation.get_attr(name) 获取操作的属性值 ​ tf.Tensor类是操作输出的符号句柄，它不包含操作输出的值，而是提供了一种在tf.Session中计算这些值的方法。这样就可以在操作之间构建一个数据流连接，使TensorFlow能够执行一个表示大量多步计算的图形。与张量相关的API均位于tf.Tensor类中 操作 描述 tf.Tensor.dtype 张量的数据类型 tf.Tensor.name 张量的名称 tf.Tensor.value_index 张量在操作输出中的索引 tf.Tensor.graph 张量所在的图 tf.Tensor.op 产生该张量的操作 tf.Tensor.consumers() 返回使用该张量的操作列表 tf.Tensor.eval(feed_dict=None, session=None) 在会话中求张量的值，需要使用sess.as_default或eval(session=sess) tf.Tensor.get_shape() 返回用于表示张量的形状（维度）的类TensorShape tf.Tensor.set_shape(shape) 更新张量的形状 tf.Tensor.device 设置计算该张量的设备 4.2可视化 ​ 可视化时，需要在程序中给必要的节点添加摘要（summary），摘要会收集该节点的数据，并标记上第几步、时间戳等标识，写入事件文件（event file）中。tf.summary.FileWriter类用于在目录中创建事件文件，并且向文件中添加摘要和事件，用来在TensorBoard中展示 API 描述 tf.summary.FileWriter.__ init__(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None) 创建FileWriter和事件文件，会在logdir中创建一个新的事件文件 tf.summary.FileWriter.add_summary(summary, global_step=None) 将摘要添加到事件文件 tf.summary.FileWriter.add_event(event) 向事件文件中添加一个事件 tf.summary.FileWriter.add_graph(graph, global_step=None, graph_def=None) 向事件中添加一个图 tf.summary.FileWriter.get_logdir() 获取事件文件的路径 tf.summary.FileWriter.flush() 将所有事件都写入磁盘 tf.summary.FileWriter.close() 将事件写入磁盘，并关闭文件操作符 tf.summary.scalar(name, tensor, collections=None) 输出包含单个标量值的摘要 tf.summary.histogram(name, values, collections=None) 输出包含直方图的摘要 tf.summary.audio(name, tensor, sample_rate, max_outputs=3, collections=None) 输出包含音频的摘要 tf.summary.image(name, tensor, max_outputs=3, collections=None) 输出包含图片的摘要 tf.summary.merge(inputs, collections=None, name=None) 合并摘要，包含所有输入摘要的值 5.变量作用域 ​ 在TensorFlow中有两个作用域（scope），一个是name_scope，一个是variable_scope。variable_scope主要是给variable_name加前缀，也可以给op_name加前缀；name_scope是给op_name加前缀。 5.1variable_scope v = tf.get_variable(name, shape, dtype, initializer) # 通过所给的名字创建或返回一个变量 tf.variable_scope(&lt;scope_name&gt;) # 为变量指定命名空间 当tf.get_variable_scope().reuse==False时，variable_scope作用域只能用来创建新变量： import tensorflow as tf with tf.variable_scope(&quot;foo&quot;): v = tf.get_variable(&quot;v&quot;, [1]) v2 = tf.get_variable(&quot;v&quot;, [1]) assert v.name == &quot;foo/v:0&quot; 上述程序会抛出ValueError错误，因为v这个变量已经被定义过了，但tf.get_variable_scope().reuse默认为False，故不能重用 当tf.get_variable_scope().reuse==True时，作用域可以共享变量： import tensorflow as tf with tf.variable_scope(&quot;foo&quot;) as scope: v = tf.get_variable(&quot;v&quot;, [1]) with tf.variable_scope(&quot;foo&quot;, reuse=True): # 也可以写成: # scope.reuse_variables() v1 = tf.get_variable(&quot;v&quot;, [1]) assert v1 == v 5.1.1获取变量作用域 可以直接通过tf.get_variable_scope()来获取变量作用域 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;) as foo_scope: v = tf.get_variable(&quot;v&quot;, [1]) with tf.variable_scope(foo_scope): w = tf.get_variable(&quot;w&quot;, [1]) 如果在开启的一个变量作用域里使用之前预先定义的一个作用域，则会跳过当前变量的作用域，保持预先存在的作用域不变 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;) as foo_scope: assert foo_scope.name == &quot;foo&quot; with tf.variable_scope(&quot;bar&quot;): with tf.variable_scope(&quot;baz&quot;) as other_scope: assert other_scope.name == &quot;bar/baz&quot; with tf.variable_scope(foo_scope) as foo_scope2: assert foo_scope2.name == &quot;foo&quot; # 保持不变 5.1.2变量作用域的初始化 变量作用域可以默认携带一个初始化器，在这个作用域中的子作用域或变量都可以继承或者重写父作用域初始化器中的值 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;, initializer=tf.constant_initializer(0.4)): v = tf.get_variable(&quot;v&quot;, [1]) assert v.eval() == 0.4 # 被作用域初始化 w = tf.get_variable(&quot;w&quot;, [1], initializer=tf.constant_initializer(0.3)) assert w.eval() == 0.3 # 重写初始化器的值 with tf.variable_scope(&quot;bar&quot;): v = tf.get_variable(&quot;v&quot;, [1]) assert v.eval() == 0.4 # 继承默认的初始化器 with tf.variable_scope(&quot;baz&quot;, initializer=tf.constant_initializer(0.2)): v = tf.get_variable(&quot;v&quot;, [1]) assert v.eval() == 0.2 # 重写父作用域的初始化器的值 上面讲的是variable_name，对于op_name，在variable_scope作用域下的操作，也会被加上前缀 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;): x = 1.0 + tf.get_variable(&quot;v&quot;, [1]) assert x.op.name == &quot;foo/add&quot; 5.2name_scope示例 TensorFlow中常常有数以千计的节点，在可视化的过程中很难一下子展示出来，因此用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。name_scope会影响op_name，不会影响用get_variable()创建的变量，而会影响通过Variable()创建的变量 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;): with tf.name_scope(&quot;bar&quot;): v = tf.get_variable(&quot;v&quot;, [1]) b = tf.Variable(tf.zeros([1]), name=&#39;b&#39;) x = 1.0 + v assert v.name == &quot;foo/v:0&quot; assert b.name == &quot;foo/bar/b:0&quot; assert x.op.name == &quot;foo/bar/add&quot; 6.批标准化 ​ 批标准化（batch normalization, BN）是为了克服神经网络层数加深导致难以训练而诞生的。随着神经网络的深度加深，训练会越来越困难，收敛速度会很慢，常常会导致梯度消失问题。 梯度消失问题：在神经网络中，当前隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫梯度消失问题 ​ 传统机器学习中有一个ICS理论，这是一个经典假设：源域（source domain）和目标域（target domain）的数据分布是一致的，也就是说，训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障 ​ Covariate Shift是指训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好地泛化（泛化：通俗来讲就是指学习到的模型对位置数据的预知能力）。它是分布不一致假设之下的一个分支问题，也就是指源域和目标域的条件概率是一致的，但是边缘概率不同。对于神经网络的各层输出，在经过了层内的操作后，各层输出分布就会与对应的输入信号分布不同，而且差异会随着网络深度增大而增大，但是每一层所指向的样本标记仍然是不变的。 ​ 解决思路一般是根据训练样本的比例对训练样本做一个矫正，因此，通过引入批标准化来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差。 6.1方法 ​ 批标准化一般用在非线性映射（激活函数）之前，对x=Wu+b做规范化，使结果（输出信号各个维度）的均值为0，方差为1。让每一层的输入有一个稳定的分布会有利于网络的训练 6.2优点 ​ 批标准化通过规范化让激活函数分布在线性区间，结果就是加大了梯度，让模型更加大胆地进行梯度下降，于是有如下优点： 加大探索的步长，加快收敛的速度 更容易跳出局部最小值 破坏原来的数据分布， 一定程度上缓解过拟合（过拟合：为了得到一致假设而使假设变得过度严格称为过拟合，出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少） 6.3示例 对每一层的Wx_plus_b进行批标准化，这个步骤放在激活函数之前 # 计算Wx_plus_b的均值和方差，其中axes = [0]表示想要标准化的维度 fc_mean, fc_var = tf.nn.moments(Wx_plus_b, axes=[0], ) scale = tf.Variable(tf.ones([out_size])) shift = tf.Variable(tf.zeros([out_size])) epsilon = 0.001 Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, fc_mean, fc_var, shift, scale, epsilon) # 也就是在做： # Wx_plus_b = (Wx_plus_b - fc_mean)/tf.sqrt(fc_var + 0.001) # Wx_plus_b = Wx_plus_b * scale + shift 7.神经元函数及优化方法 7.1激活函数 ​ 激活函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经网络。神经网络之所以能解决非线性问题（如语音、图像识别），本质上就是激活函数加入了非线性因素，弥补了线性模型的表达力，把“激活的神经元的特征”通过函数保留并映射到下一层 ​ 激活函数不会更改输入数据的维度，也就是输入和输出的维度是相同的。TensorFlow中有如下激活函数，输入均为要计算的x（一个张量），输出均为与x数据类型相同的张量。常见的有sigmoid、tanh、relu和softplus sigmoid函数： S ( x ) = 1 1 + e − x S(x)=\frac{1}{1+e^-x} S(x)=1+e−x1​ 使用方法： a = tf.constant([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]]) sess = tf.Session() print(sess.run(tf.sigmoid(a))) ​ sigmoid函数优点在于，它的输出映射在（0, 1）内，单调连续，非常适合于用作输出层，并且求导比较容易。缺点在于，因为软饱和性（软饱和性：是指激活函数h(x)在取值趋于无穷大时，它的一阶导数趋于0.硬饱和是指当|x|&gt;c时，其中c为常数，f’(x)=0），一旦落入软饱和区，f’(x)就会变得接近于0，很容易产生梯度消失。（梯度消失：是指在更新模型参数时采用链式求导法则反向求导，越往前梯度越小。最终的结果是到达一定深度后梯度对于模型的更新就没有任何贡献了） tanh函数： S ( x ) = 1 − e − 2 x 1 + e − 2 x S(x)=\frac{1-e^-2x}{1+e^-2x} S(x)=1+e−2x1−e−2x​ tanh函数也具有软饱和性，因为它的输出以0为中心，收敛速度比sigmoid要快，但是仍无法解决梯度消失的问题 relu函数 目前最受欢迎的激活函数。softplus可以看作是relu的平滑版本。relu定义为f(x)=max(x, 0)。softplus定义为f(x)=log(1+exp(x))。使用示例如下： a = tf.constant([-1.0, 2.0]) with tf.Session() as sess: b = tf.nn.relu(a) print(sess.run(b)) 除了relu本身之外，TensorFlow还定义了relu6以及crelu。 dropout函数 ​ 一个神经元将以概率keep_prob决定是否被抑制。如果被抑制，该神经元的输出就为0；如果不被抑制，那么该神经元的输出值将被放大到原来的1/keep_prob倍。 ​ 在默认情况下，每个神经元是否被抑制是相互独立的。但是否被抑制也可以通过noise_shape来调节。当noise_shape[i] == shape(x)[i]时，x中的元素是相互独立的。如果shape(x) = [k, l, m, n]，x中的维度的顺序分别为批、行、列和通道，如果noise_shape = [k, l, l, n]，那么每个批和通道相互独立，行跟列相互关联，也就是说，要不都为0，要不都还是原来的值。使用示例如下： a = tf.constant([[-1.0, 2.0, 3.0, 4.0]]) with tf.Session() as sess: b = tf.nn.dropout(a, 0.5, noise_shape=[1, 4]) print(sess.run(b)) b = tf.nn.dropout(a, 0.5, noise_shape=[1, 1]) print(sess.run(b)) 7.2卷积函数 ​ 卷积函数是构建神经网络的重要支架，是在一批图像上扫描的二维过滤器。 tf.nn.convolution(input, filter, padding, strides=None, dilation_rate=None, name=None, data_format=None) 这个函数是计算N维卷积的和 tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)这个函数的作用是对一个四维的输入数据input和四维的卷积核filter进行操作，然后对输入数据进行一个二维的卷积操作，最后得到卷积之后的结果 tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None) # 输入： # input：一个tensor。数据类型必须是float32或者float64 # filter：一个tensor。数据类型必须与input相同 # strides：一个长度是4的一维整数类型数组，每一维度对应的是input中每一维的对应移动步数 # padding：一个字符串，取值为SAME或VALID # padding=&#39;SAME&#39;：仅适用于全尺寸操作，即输入数据维度和输出数据维度相同 # padding=&#39;VALID&#39;：适用于部分窗口，即输入数据维度和输出数据维度不同 # use_cudnn_on_gpu：一个可选布尔值，默认是True # name：（可选）为该操作取一个名字 使用示例如下： input_data = tf.Variable(np.random.rand(10,9,9,3), dtype = np.float32) filter_data = tf.Variable(np.random.rand(2,2,3,2), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1,1,1,1], padding=&#39;SAME&#39;) tf.nn.depthwise_conv2d(input, filter, strides, padding, rate=None, name=None, data_format=None)这个函数输入张量的数据维度是[batch, in_height, in_width, in_channels]，卷积核的维度是[filter_height, filter_width, in_channels, channel_multiplier]，在通道in_channels上面的卷积深度是1，depthwise_conv2d函数将不同的卷积核独立地应用在in_channels的每个通道上（从通道1到通道channel_multiplier），然后把所有的结果进行汇总。最后输出通道的总数是in_channels*channel_multiplier。 使用示例如下： input_data = tf.Variable(np.random.rand(10, 9, 9, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 5), dtype=np.float32) y = tf.nn.depthwise_conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate=None, name=None, data_format=None)是利用几个分离的卷积核去做卷积。在这个API中，将应用一个二维的卷积核，在每个通道上，以深度channel_multiplier进行卷积 def separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate=None, name=None, data_format=None) # 特殊参数： # depthwise_filter：一个张量。数据维度是四维[filter_height, filter_width, in_channels, channel_multiplier]。其中，in_channels的卷积深度是1 # pointwise_filter：一个张量，数据维度是四维[1, 1, channel_multiplier*in_channels, out_channels]。其中，pointwise_filter是在depthwise_filter卷积之后的混合卷积 使用示例如下： input_data = tf.Variable(np.random.rand(10, 9, 9, 3), dtype=np.float32) depthwise_filter = tf.Variable(np.random.rand(2, 2, 3, 5), dtype=np.float32) pointwise_filter = tf.Variable(np.random.rand(1, 1, 15, 20), dtype=np.float32) y = tf.nn.separable_conv2d(input_data, depthwise_filter, pointwise_filter, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.atrous_conv2d(value, filters, rate, padding, name=None)计算Atrous卷积，又称孔卷积或者扩张卷积。 使用示例如下： input_data = tf.Variable(np.random.rand(1, 5, 5, 1), dtype=np.float32) filters = tf.Variable(np.random.rand(3, 3, 1, 1), dtype=np.float32) y = tf.nn.atrous_conv2d(input_data, filters, 2, padding=&#39;SAME&#39;) tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding=‘SAME’, data_format=‘NHWC’, name=None)在解卷积网络中有时称为“反卷积”，但实际上是conv2d的转置，而不是实际的反卷积 def conv2d_transpose(value, filter, output_shape, strides, padding=&#39;SAME&#39;, data_format=&#39;NHWC&#39;, name=None) # 特殊参数： # output_shape：一维的张量，表示反卷积运算后输出的形状 # 输出：和value一样维度的Tensor 使用示例如下： x = tf.random_normal(shape=[1, 3, 3, 1]) kernel = tf.random_normal(shape=[2, 2, 3, 1]) y = tf.nn.conv2d_transpose(x, kernel, output_shape=[1, 5, 5, 3], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)和二维卷积类似。这个函数是用来计算给定三维的输入和过滤器的情况下的一维卷积。不同的是，它的输入是三维，如[batch, in_width, in_channels]。卷积核的维度也是三维，少了一维filter_height，如[filter_width, in_channels, out_channels]。stride是一个正整数，代表卷积核向右移动每一步的长度 tf.nn.conv3d(input, filter, strides, padding, name=None)和二维卷积类似，这个函数用来计算给定五维的输入和过滤器的情况下的三维卷积。和二维卷积相对比： input的shape中多了一维in_depth，形状为Shape[batch, in_depth, in_height, in_width, in_channels] filter的shape中多了一维filter_depth，由filter_depth，filter_height，filter_width构成了卷积核的大小 strides中多了一维，变为[strides_batch， strides_depth， strides_height， strides_width， strides_channel]，必须保证strides[0] = strides[4] = 1 tf.nn.conv3d_transpose(value, filter, output_shape, strides, padding=‘SAME’, name=None)和二维反卷积类似 7.3池化函数 ​ 在神经网络中，池化函数一般跟在卷积函数的下一层，池化操作是利用一个矩阵窗口在张量上进行扫描，将每个矩阵窗口中的值通过取最大值或平均值来减少元素个数。每个池化操作的矩阵窗口大小是由ksize指定的，并且根据步长strides决定移动步长 tf.nn.avg_pool(value, ksize, strides, padding, data_format=‘NHWC’, name=None)这个函数计算池化区域中元素的平均值 def avg_pool(value, ksize, strides, padding, data_format=&#39;NHWC&#39;, name=None) # value：一个四维的张量。数据维度是[batch, height, width, channels] # ksize：一个长度不小于4的整形数组。每一位上的值对应于输入数据张量中每一维的窗口对应值 # strides：一个长度不小于4的整形数组，该参数指定滑动窗口在输入数据张量每一维上的步长 # padding：一个字符串，取值为SAME或者VALID # data_format：&#39;NHWC&#39;代表输入张量维度的顺序，N为个数，H为高度，W为宽度，C为通道数（RGB三通道或者灰度单通道） # name：为这个操作取一个名字 # 输出：一个张量，数据类型和value相同 使用示例如下： input_data = tf.Variable(np.random.rand(10, 6, 6, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 10), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) out_put = tf.nn.avg_pool(value=y, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.max_pool(value, ksize, strides, padding, data_format=‘NHWC’, name=None)这个函数是计算池化区域中元素的最大值。使用示例如下： input_data = tf.Variable(np.random.rand(10, 6, 6, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 10), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) out_put = tf.nn.max_pool(value=y, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax, name=None)这个函数的作用是计算池化区域中元素的最大值和该最大值所在的位置 在计算位置argmax的时候，我们将input平铺了进行计算，所以，如果input = [b, y, x, c]，那么索引位置是 ((b * height + y) * width + x) * channels + c。使用示例如下： input_data = tf.Variable(np.random.rand(10, 6, 6, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 10), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) out_put, argmax = tf.nn.max_pool_with_argmax(input=y, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # 返回结果是一个张量组成的元组（output, argmax），output表示池化区域的最大值，argmax的数据类型是Targmax，维度是四维 tf.nn.avg_pool3d()和tf.nn.max_pool3d()分别是在三维下的平均池化和最大池化 tf.nn.fractional_avg_pool()和tf.nn.fractional_max_pool()分别是Benjamin Graham在论文中提出的池化技术，池化后的图片大小可以成非整数倍缩小，如$ \sqrt 2 ， ， ， \sqrt 3 $ tf.nn.pool(input, window_shape, pooling_type, padding, dilation_rate=None, strides=None, name=None, data_format=None)这个函数执行一个N维的池化操作 7.4分类函数 tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None) tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None) # 输入：logits：[batch_size, num_classes]，targets：[batch_size, size].logits用最后一层的输入即可 # 最后一层不需要进行sigmoid运算，此函数内部进行了sigmoid操作 # 输出：loss[batch_size, num_classes] 这个函数的输入要格外注意。如果采用此函数作为损失函数，在神经网络的最后一层不需要进行sigmoid运算 tf.nn.softmax(logits, dim=-1, name=None)计算SoftMax激活，也就是 softmax = exp(logits) / reduce_sum(exp(logits), dim) tf.nn.log_softmax(logits, dim=-1, name=None)计算log softmax激活，也就是 logsoftmax = logits - log(reduce_sum(exp(logits), dim)) tf.nn.softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None) softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None) # 输入：logits and labels均为[batch_size, num_classes] # 输出：loss[batch_size]，里面保存的是batch中每个样本的交叉熵 tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None) tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None) # 输入：logits：[batch_size, num_classes] labels：[batch_size]，必须在[0, num_classes] # 输出：loss[batch_size]，里面保存的是batch中每个样本的交叉熵 7.5优化方法 ​ 目前加速训练的优化方法基本都是基于梯度下降的，只是细节上有差异。梯度下降是求函数极值的一种方法，学习到最后就是求损失函数的极值问题 BGD法 ​ BGD的全称是batch gradient descent，即批梯度下降。这种方法是利用现有参数对训练集中每一个输入生成一个估计输出yi，然后跟实际输出yi比较，统计所有误差，求平均后得到平均误差，以此作为更新参数的一句。它的迭代过程为： 提取训练集中的所有内容{x1, ···, xn}，以及相关的输出yi 计算梯度和误差并更新参数 ​ 这个方法的优点是，使用所有训练数据计算，能够保证收敛，并且不需要逐渐减少学习率；缺点是，每一步都需要使用所有的训练集，随着训练的进行，速度会越来越慢 SGD法 ​ SGD的全称是stochastic gradient descent，即随机梯度下降。因为这种方法的主要思想是将数据集拆分成一个个批次（batch），随机抽取一个批次来计算并更新参数 ​ SGD在每一次迭代计算mini-batch的梯度，然后对参数进行更新。与BGD相比，SGD在训练数据集很大时，仍能以较快的速度收敛，但仍有以下两个缺点 由于抽取不可避免地梯度会有误差，需要手动调整学习率，但是选择合适的学习率又很困难 SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点 Momentum法 ​ Momentum是模拟物理学中动量的概念，更新时会在一定程度上保留之前的更新方向，利用当前批次再微调本次的更新参数，因此引入了一个新的变量v（速度），作为前几次梯度的累加。因此，Momentum能够更新学习率，在下降初期，前后梯度方向一致时，能够加速学习，在下降的中后期，在局部最小值的附近来回震荡时，能够抑制震荡，加快收敛 Nesterov Momentum法 ​ 是对Momentum的改进，Momentum法首先计算一个梯度，然后在加速更新梯度的方向进行一个大的跳跃；Nesterov项首先在原来加速的梯度方向进行一个大的跳跃，然后在该位置计算梯度值，然后用这个梯度值修正最终的更新方向。 Adagrad法 ​ Adagrad法能够自适应地为各个参数分配不同的学习率，能够控制每个维度的梯度方向。这种方法的优点是能够实现学习率的自动更改；如果本次更新时梯度大，学习率就衰减得快一些；如果这一次更新时梯度小，学习率就衰减得慢一些。 Adadelta法 ​ Adadelta法仍然存在一些问题：其学习率单调递减没在训练的后期学习率非常小，并且需要手动设置一个全局的初始学习率 RMSProp法 ​ RMSProp法与Momentum法类似，通过引入一个衰减系数，使每一回合都衰减一定比例，在实践中，对循环神经网络（RNN）效果很好 Adam法 ​ Adam的名称来源于自适应矩估计（adaptive moment estimation）。Adam法根据损失函数针对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。（矩估计就是利用样本来估计总体中相应的参数）" />
<link rel="canonical" href="https://uzzz.org/2019/06/20/793325.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/20/793325.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-20T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"1.TensorFlow系统架构 数据操作层：主要包括卷积神经网络、激活函数等操作 图计算层：（核心）包括图的创建、编译、优化和执行 2.设计理念 将图的定义和图的运行完全分开。因此TensorFlow被认为是一个&quot;符号主义&quot;的库。符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量之间的计算关系，最后需要对数据流图进行编译，但此时的数据流图还是一个空壳，里面没有任何实际数据，只有把需要计算的输入放进去后，才能在整个模型中形成数据流，才能在整个模型中形成数据流，从而形成输出值 # 传统程序操作 t = 8 + 9 print(t) # 定义了t的运算，在运行时就执行了，并输出17, # tensorflow import tensorflow as tf t = tf.add(8, 9) print(t) # 输出 Tensor(&quot;Add_1:0&quot;, shape=(), dtype=int32) # 数据流图中的节点，实际上对应的是TensorFlow API中的一个操作，并没有真正去运行 TensorFlow中涉及的运算都要放在图中，而图的运行只发生在会话（session）中，开启会话后，就可以用数据去填充节点，进行运算，关闭会话后，就不能进行计算了，因此，会话提供了操作运行和Tensor求值的环境。 import tensorflow as tf # 创建图 a = tf.constant([1.0, 2.0]) b = tf.constant([3.0, 4.0]) c = a * b # 创建会话 sess = tf.Session() # 计算c print(sess.run(c)) sess.close() 3.TensorFlow的编程模型 ​ TensorFlow是用数据流图做计算的，因此需要先创建一个数据流图。一个简单的回归模型中的元素有：输入（input）、塑形（reshape）、Relu层（Relu Layer）、Logit层（Logit Layer）、Softmax、交叉熵（cross entropy）、梯度（gradient）、SGD训练（SGD Trainer）等部分。 ​ 它的计算过程是，首先从输入开始，经过塑形后，一层一层进行前向传播运算。Relu层里会有两个参数，即Wh1和bh1，在输出前是用ReLU（Rectified Linear Units）激活函数做非线性处理。然后进入Logit层（输出层），学习两个参数Wsm和bsm。用Softmax来计算输出结果中各个类别的概率分布，用交叉熵来度量两个概率分布（源样本的概率分布和输出结果的概率分布）之间的相似性，然后开始计算梯度，这里是需要前面的四个参数，以及交叉熵后的结果。随后进入SGD训练，也就是反向传播的过程，从上往下计算每一层的参数，依次进行更新。 ​ 顾名思义，TensorFlow是指“张量的流动”。TensorFlow的数据流图是由节点（node）和边（edge）组成的有向无环图。TensorFlow由Tensor和Flow组成，Tensor（张量）代表了数据流图中的边，Flow（流动）这个动作就代表了数据流图中节点所做的操作。 3.1边 ​ TensorFlow的边有两种连接关系：数据依赖和控制依赖。其中，实线边表示数据依赖，代表数据，即张量。任意维度的数据统称为张量。在机器学习算法中，张量在数据流图中从前往后流动一遍就完成了一次前向传播，而残差（在数理统计中，残差是指实际观察值与训练的估计值之间的差）从后向前流动一遍就完成了一次反向传播。TensorFlow支持的张量具有下表所示的数据属性 数据类型 Python类型 描述 DT_FLOAT tf.float32 32位浮点型 DT_DOUBLE tf.float64 64位浮点型 DT_INT64 tf.int64 64位有符号整型 DT_INT32 tf.int32 32位有符号整型 DT_INT16 tf.int16 16位有符号整型 DT_INT8 tf.int8 8位有符号整型 DT_UINT8 tf.uint8 8位无符号整型 DT_STRING tf.string 可变长度的字节数组，每一个张量元素都是一个字节数组 DT_BOOL tf.bool 布尔型 DT_COMPLEX64 tf.complex64 由两个32位浮点数组成的复数：实部和虚部 DT_QINT32 tf.qint32 用于量化操作的32位有符号整型 DT_QINT8 tf.qint8 用于量化操作的8位有符号整型 DT_QUINT8 tf.quint8 用于量化操作的8位无符号整型 量化是数字信号处理领域的一个概念，是指将信号的连续取值（或者大量可能的离散取值）金思维有限多个（或较少的）离散值的过程 3.2节点 ​ 节点又称为算子，它代表一个操作，一般用来表示施加的数学运算，也可以表示数据输入的起点以及输出的终点，或者是读取/写入持久变量的终点。下表列举了一些TensorFlow实现的算子，算子支持下表所示的张量的各种数据属性，并且需要在建立图的时候确定下来。 类别 示例 数学运算操作 Add、Substract、Multiply、Div、Exp、Log、Greater、Less、Equal 数组运算操作 Concat、Slice、Split、Constant、Rank、Shape、Shuffle 矩阵运算操作 MatMul、MatrixInverse、MatrixDeterminant 有状态的操作 Variable、Assign、AssignAdd 神经网络构建操作 SoftMax、Sigmoid、ReLU、Convolution2D、MaxPool 检查点操作 Save、Restore 队列和同步操作 Enqueue、Dequeue、MutexAcquire、MutexRelease 控制张量流动的操作 Merge、Switch、Enter、Leave、NextIteration 3.3其它概念 图 操作任务可以描述成一个有向无环图，如何构建一个图，第一步是创建各个节点 import tensorflow as tf # 创建一个常量运算操作，产生一个1x2矩阵 matrix1 = tf.constant([[3., 3.]]) # 创建另外一个常量运算操作，产生一个2x1矩阵 matrix2 = tf.constant([[2.], [2.]]) # 创建一个矩阵乘法运算，把matrix1和matrix2作为输入 # 返回值result代表矩阵乘法的结果 result = tf.matmul(matrix1, matrix2) 2. 会话 ​ 启动图的第一步是创建一个Session对象，会话（Session）提供在途中执行操作的一些方法。一般的模式是，建立会话，此时会生成一张空图，在会话中添加节点和边，形成一张图，然后执行。 with tf.Session() as sess: result = sess.run(result) print(result) ​ 在调用Session对象的run()方法来执行图时，传入一些tensor，这个过程叫填充，返回的结果类型根据输入的类型而定，这个过程叫取回。 ​ 会话是图交互的一个桥梁，一个会话可以有多个图，会话可以修改图的结构，也可以往图中注入数据进行计算。因此会话主要有两个API接口：Extend和Run。Extend操作是在图中添加节点和边，Run操作是输入计算的节点和填充必要的数据后，进行运算，并输出结果 设备 ​ 设备是指一块可以用来运算并且拥有自己的地址空间的硬件，如GPU和CPU，TensorFlow为了实现分布式执行操作，充分利用资源，可以明确指定操作在哪个设备上执行 with tf.Session() as sess: # 指定在第二个gpu上执行 with tf.device(&quot;/gpu:1&quot;): ... 变量 ​ 变量是一种特殊的数据，它在图中有固定的位置，不像普通张量那样可以流动。创建一个变量张量，使用tf.Variable()构造函数，这个构造函数需要一个初始值，初始值的形状和类型决定了这个变量的形状和类型 # 创建一个变量，初始化为标量0 state = tf.Variable(0, name=&#39;counter&#39;) ​ TensorFlow还提供了填充机制，可以在构建图时使用tf.placeholder()临时替代任意操作的张量，在调用Session对象的run()方法去执行图时，使用填充数据作为调用的参数，调用之后，填充数据就消失 4.常用API 4.1图，操作和张量 ​ TensorFlow的计算表现为数据流图，所以tf.Graph类中包含一系列表示计算的操作对象（tf.Operation），以及在操作之间流动的数据–张量对象（tf.Tensor）。与图相关的API均位于tf.Graph类中 操作 描述 tf.Graph.__ init__() 创建一个空图 tf.Graph.as_default() 将某图设置成默认图，并返回一个上下文管理器，如果不显式添加一个默认图，系统会自动设置一个全局的默认图，所设置的默认图，在模块范围内定义的节点都将默认加入默认图中 tf.Graph.device(deice_name_or_function) 定义运行图所使用的的设备，并返回一个上下文管理器 tf.Graph.name_scope(name) 为节点创建层次化的名称，并返回一个上下文管理器 ​ tf.Operation类代表图中的一个节点，用于计算张量数据。该类型由节点构造器（如tf.matmul()或者Graph.create_op()）产生。与操作相关的API均位于tf.Operation类中 操作 描述 tf.Operation.name 操作的名称 tf.Operation.type 操作的类型 tf.Operation.inputs/outputs 操作的输入与输出 tf.Operation.control_inputs 操作的依赖 tf.Operation.run(feed_dict=None, session=None) 在会话中运行该操作 tf.Operation.get_attr(name) 获取操作的属性值 ​ tf.Tensor类是操作输出的符号句柄，它不包含操作输出的值，而是提供了一种在tf.Session中计算这些值的方法。这样就可以在操作之间构建一个数据流连接，使TensorFlow能够执行一个表示大量多步计算的图形。与张量相关的API均位于tf.Tensor类中 操作 描述 tf.Tensor.dtype 张量的数据类型 tf.Tensor.name 张量的名称 tf.Tensor.value_index 张量在操作输出中的索引 tf.Tensor.graph 张量所在的图 tf.Tensor.op 产生该张量的操作 tf.Tensor.consumers() 返回使用该张量的操作列表 tf.Tensor.eval(feed_dict=None, session=None) 在会话中求张量的值，需要使用sess.as_default或eval(session=sess) tf.Tensor.get_shape() 返回用于表示张量的形状（维度）的类TensorShape tf.Tensor.set_shape(shape) 更新张量的形状 tf.Tensor.device 设置计算该张量的设备 4.2可视化 ​ 可视化时，需要在程序中给必要的节点添加摘要（summary），摘要会收集该节点的数据，并标记上第几步、时间戳等标识，写入事件文件（event file）中。tf.summary.FileWriter类用于在目录中创建事件文件，并且向文件中添加摘要和事件，用来在TensorBoard中展示 API 描述 tf.summary.FileWriter.__ init__(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None) 创建FileWriter和事件文件，会在logdir中创建一个新的事件文件 tf.summary.FileWriter.add_summary(summary, global_step=None) 将摘要添加到事件文件 tf.summary.FileWriter.add_event(event) 向事件文件中添加一个事件 tf.summary.FileWriter.add_graph(graph, global_step=None, graph_def=None) 向事件中添加一个图 tf.summary.FileWriter.get_logdir() 获取事件文件的路径 tf.summary.FileWriter.flush() 将所有事件都写入磁盘 tf.summary.FileWriter.close() 将事件写入磁盘，并关闭文件操作符 tf.summary.scalar(name, tensor, collections=None) 输出包含单个标量值的摘要 tf.summary.histogram(name, values, collections=None) 输出包含直方图的摘要 tf.summary.audio(name, tensor, sample_rate, max_outputs=3, collections=None) 输出包含音频的摘要 tf.summary.image(name, tensor, max_outputs=3, collections=None) 输出包含图片的摘要 tf.summary.merge(inputs, collections=None, name=None) 合并摘要，包含所有输入摘要的值 5.变量作用域 ​ 在TensorFlow中有两个作用域（scope），一个是name_scope，一个是variable_scope。variable_scope主要是给variable_name加前缀，也可以给op_name加前缀；name_scope是给op_name加前缀。 5.1variable_scope v = tf.get_variable(name, shape, dtype, initializer) # 通过所给的名字创建或返回一个变量 tf.variable_scope(&lt;scope_name&gt;) # 为变量指定命名空间 当tf.get_variable_scope().reuse==False时，variable_scope作用域只能用来创建新变量： import tensorflow as tf with tf.variable_scope(&quot;foo&quot;): v = tf.get_variable(&quot;v&quot;, [1]) v2 = tf.get_variable(&quot;v&quot;, [1]) assert v.name == &quot;foo/v:0&quot; 上述程序会抛出ValueError错误，因为v这个变量已经被定义过了，但tf.get_variable_scope().reuse默认为False，故不能重用 当tf.get_variable_scope().reuse==True时，作用域可以共享变量： import tensorflow as tf with tf.variable_scope(&quot;foo&quot;) as scope: v = tf.get_variable(&quot;v&quot;, [1]) with tf.variable_scope(&quot;foo&quot;, reuse=True): # 也可以写成: # scope.reuse_variables() v1 = tf.get_variable(&quot;v&quot;, [1]) assert v1 == v 5.1.1获取变量作用域 可以直接通过tf.get_variable_scope()来获取变量作用域 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;) as foo_scope: v = tf.get_variable(&quot;v&quot;, [1]) with tf.variable_scope(foo_scope): w = tf.get_variable(&quot;w&quot;, [1]) 如果在开启的一个变量作用域里使用之前预先定义的一个作用域，则会跳过当前变量的作用域，保持预先存在的作用域不变 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;) as foo_scope: assert foo_scope.name == &quot;foo&quot; with tf.variable_scope(&quot;bar&quot;): with tf.variable_scope(&quot;baz&quot;) as other_scope: assert other_scope.name == &quot;bar/baz&quot; with tf.variable_scope(foo_scope) as foo_scope2: assert foo_scope2.name == &quot;foo&quot; # 保持不变 5.1.2变量作用域的初始化 变量作用域可以默认携带一个初始化器，在这个作用域中的子作用域或变量都可以继承或者重写父作用域初始化器中的值 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;, initializer=tf.constant_initializer(0.4)): v = tf.get_variable(&quot;v&quot;, [1]) assert v.eval() == 0.4 # 被作用域初始化 w = tf.get_variable(&quot;w&quot;, [1], initializer=tf.constant_initializer(0.3)) assert w.eval() == 0.3 # 重写初始化器的值 with tf.variable_scope(&quot;bar&quot;): v = tf.get_variable(&quot;v&quot;, [1]) assert v.eval() == 0.4 # 继承默认的初始化器 with tf.variable_scope(&quot;baz&quot;, initializer=tf.constant_initializer(0.2)): v = tf.get_variable(&quot;v&quot;, [1]) assert v.eval() == 0.2 # 重写父作用域的初始化器的值 上面讲的是variable_name，对于op_name，在variable_scope作用域下的操作，也会被加上前缀 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;): x = 1.0 + tf.get_variable(&quot;v&quot;, [1]) assert x.op.name == &quot;foo/add&quot; 5.2name_scope示例 TensorFlow中常常有数以千计的节点，在可视化的过程中很难一下子展示出来，因此用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。name_scope会影响op_name，不会影响用get_variable()创建的变量，而会影响通过Variable()创建的变量 import tensorflow as tf with tf.variable_scope(&quot;foo&quot;): with tf.name_scope(&quot;bar&quot;): v = tf.get_variable(&quot;v&quot;, [1]) b = tf.Variable(tf.zeros([1]), name=&#39;b&#39;) x = 1.0 + v assert v.name == &quot;foo/v:0&quot; assert b.name == &quot;foo/bar/b:0&quot; assert x.op.name == &quot;foo/bar/add&quot; 6.批标准化 ​ 批标准化（batch normalization, BN）是为了克服神经网络层数加深导致难以训练而诞生的。随着神经网络的深度加深，训练会越来越困难，收敛速度会很慢，常常会导致梯度消失问题。 梯度消失问题：在神经网络中，当前隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫梯度消失问题 ​ 传统机器学习中有一个ICS理论，这是一个经典假设：源域（source domain）和目标域（target domain）的数据分布是一致的，也就是说，训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障 ​ Covariate Shift是指训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好地泛化（泛化：通俗来讲就是指学习到的模型对位置数据的预知能力）。它是分布不一致假设之下的一个分支问题，也就是指源域和目标域的条件概率是一致的，但是边缘概率不同。对于神经网络的各层输出，在经过了层内的操作后，各层输出分布就会与对应的输入信号分布不同，而且差异会随着网络深度增大而增大，但是每一层所指向的样本标记仍然是不变的。 ​ 解决思路一般是根据训练样本的比例对训练样本做一个矫正，因此，通过引入批标准化来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差。 6.1方法 ​ 批标准化一般用在非线性映射（激活函数）之前，对x=Wu+b做规范化，使结果（输出信号各个维度）的均值为0，方差为1。让每一层的输入有一个稳定的分布会有利于网络的训练 6.2优点 ​ 批标准化通过规范化让激活函数分布在线性区间，结果就是加大了梯度，让模型更加大胆地进行梯度下降，于是有如下优点： 加大探索的步长，加快收敛的速度 更容易跳出局部最小值 破坏原来的数据分布， 一定程度上缓解过拟合（过拟合：为了得到一致假设而使假设变得过度严格称为过拟合，出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少） 6.3示例 对每一层的Wx_plus_b进行批标准化，这个步骤放在激活函数之前 # 计算Wx_plus_b的均值和方差，其中axes = [0]表示想要标准化的维度 fc_mean, fc_var = tf.nn.moments(Wx_plus_b, axes=[0], ) scale = tf.Variable(tf.ones([out_size])) shift = tf.Variable(tf.zeros([out_size])) epsilon = 0.001 Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, fc_mean, fc_var, shift, scale, epsilon) # 也就是在做： # Wx_plus_b = (Wx_plus_b - fc_mean)/tf.sqrt(fc_var + 0.001) # Wx_plus_b = Wx_plus_b * scale + shift 7.神经元函数及优化方法 7.1激活函数 ​ 激活函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经网络。神经网络之所以能解决非线性问题（如语音、图像识别），本质上就是激活函数加入了非线性因素，弥补了线性模型的表达力，把“激活的神经元的特征”通过函数保留并映射到下一层 ​ 激活函数不会更改输入数据的维度，也就是输入和输出的维度是相同的。TensorFlow中有如下激活函数，输入均为要计算的x（一个张量），输出均为与x数据类型相同的张量。常见的有sigmoid、tanh、relu和softplus sigmoid函数： S ( x ) = 1 1 + e − x S(x)=\\frac{1}{1+e^-x} S(x)=1+e−x1​ 使用方法： a = tf.constant([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]]) sess = tf.Session() print(sess.run(tf.sigmoid(a))) ​ sigmoid函数优点在于，它的输出映射在（0, 1）内，单调连续，非常适合于用作输出层，并且求导比较容易。缺点在于，因为软饱和性（软饱和性：是指激活函数h(x)在取值趋于无穷大时，它的一阶导数趋于0.硬饱和是指当|x|&gt;c时，其中c为常数，f’(x)=0），一旦落入软饱和区，f’(x)就会变得接近于0，很容易产生梯度消失。（梯度消失：是指在更新模型参数时采用链式求导法则反向求导，越往前梯度越小。最终的结果是到达一定深度后梯度对于模型的更新就没有任何贡献了） tanh函数： S ( x ) = 1 − e − 2 x 1 + e − 2 x S(x)=\\frac{1-e^-2x}{1+e^-2x} S(x)=1+e−2x1−e−2x​ tanh函数也具有软饱和性，因为它的输出以0为中心，收敛速度比sigmoid要快，但是仍无法解决梯度消失的问题 relu函数 目前最受欢迎的激活函数。softplus可以看作是relu的平滑版本。relu定义为f(x)=max(x, 0)。softplus定义为f(x)=log(1+exp(x))。使用示例如下： a = tf.constant([-1.0, 2.0]) with tf.Session() as sess: b = tf.nn.relu(a) print(sess.run(b)) 除了relu本身之外，TensorFlow还定义了relu6以及crelu。 dropout函数 ​ 一个神经元将以概率keep_prob决定是否被抑制。如果被抑制，该神经元的输出就为0；如果不被抑制，那么该神经元的输出值将被放大到原来的1/keep_prob倍。 ​ 在默认情况下，每个神经元是否被抑制是相互独立的。但是否被抑制也可以通过noise_shape来调节。当noise_shape[i] == shape(x)[i]时，x中的元素是相互独立的。如果shape(x) = [k, l, m, n]，x中的维度的顺序分别为批、行、列和通道，如果noise_shape = [k, l, l, n]，那么每个批和通道相互独立，行跟列相互关联，也就是说，要不都为0，要不都还是原来的值。使用示例如下： a = tf.constant([[-1.0, 2.0, 3.0, 4.0]]) with tf.Session() as sess: b = tf.nn.dropout(a, 0.5, noise_shape=[1, 4]) print(sess.run(b)) b = tf.nn.dropout(a, 0.5, noise_shape=[1, 1]) print(sess.run(b)) 7.2卷积函数 ​ 卷积函数是构建神经网络的重要支架，是在一批图像上扫描的二维过滤器。 tf.nn.convolution(input, filter, padding, strides=None, dilation_rate=None, name=None, data_format=None) 这个函数是计算N维卷积的和 tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)这个函数的作用是对一个四维的输入数据input和四维的卷积核filter进行操作，然后对输入数据进行一个二维的卷积操作，最后得到卷积之后的结果 tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None) # 输入： # input：一个tensor。数据类型必须是float32或者float64 # filter：一个tensor。数据类型必须与input相同 # strides：一个长度是4的一维整数类型数组，每一维度对应的是input中每一维的对应移动步数 # padding：一个字符串，取值为SAME或VALID # padding=&#39;SAME&#39;：仅适用于全尺寸操作，即输入数据维度和输出数据维度相同 # padding=&#39;VALID&#39;：适用于部分窗口，即输入数据维度和输出数据维度不同 # use_cudnn_on_gpu：一个可选布尔值，默认是True # name：（可选）为该操作取一个名字 使用示例如下： input_data = tf.Variable(np.random.rand(10,9,9,3), dtype = np.float32) filter_data = tf.Variable(np.random.rand(2,2,3,2), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1,1,1,1], padding=&#39;SAME&#39;) tf.nn.depthwise_conv2d(input, filter, strides, padding, rate=None, name=None, data_format=None)这个函数输入张量的数据维度是[batch, in_height, in_width, in_channels]，卷积核的维度是[filter_height, filter_width, in_channels, channel_multiplier]，在通道in_channels上面的卷积深度是1，depthwise_conv2d函数将不同的卷积核独立地应用在in_channels的每个通道上（从通道1到通道channel_multiplier），然后把所有的结果进行汇总。最后输出通道的总数是in_channels*channel_multiplier。 使用示例如下： input_data = tf.Variable(np.random.rand(10, 9, 9, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 5), dtype=np.float32) y = tf.nn.depthwise_conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate=None, name=None, data_format=None)是利用几个分离的卷积核去做卷积。在这个API中，将应用一个二维的卷积核，在每个通道上，以深度channel_multiplier进行卷积 def separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate=None, name=None, data_format=None) # 特殊参数： # depthwise_filter：一个张量。数据维度是四维[filter_height, filter_width, in_channels, channel_multiplier]。其中，in_channels的卷积深度是1 # pointwise_filter：一个张量，数据维度是四维[1, 1, channel_multiplier*in_channels, out_channels]。其中，pointwise_filter是在depthwise_filter卷积之后的混合卷积 使用示例如下： input_data = tf.Variable(np.random.rand(10, 9, 9, 3), dtype=np.float32) depthwise_filter = tf.Variable(np.random.rand(2, 2, 3, 5), dtype=np.float32) pointwise_filter = tf.Variable(np.random.rand(1, 1, 15, 20), dtype=np.float32) y = tf.nn.separable_conv2d(input_data, depthwise_filter, pointwise_filter, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.atrous_conv2d(value, filters, rate, padding, name=None)计算Atrous卷积，又称孔卷积或者扩张卷积。 使用示例如下： input_data = tf.Variable(np.random.rand(1, 5, 5, 1), dtype=np.float32) filters = tf.Variable(np.random.rand(3, 3, 1, 1), dtype=np.float32) y = tf.nn.atrous_conv2d(input_data, filters, 2, padding=&#39;SAME&#39;) tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding=‘SAME’, data_format=‘NHWC’, name=None)在解卷积网络中有时称为“反卷积”，但实际上是conv2d的转置，而不是实际的反卷积 def conv2d_transpose(value, filter, output_shape, strides, padding=&#39;SAME&#39;, data_format=&#39;NHWC&#39;, name=None) # 特殊参数： # output_shape：一维的张量，表示反卷积运算后输出的形状 # 输出：和value一样维度的Tensor 使用示例如下： x = tf.random_normal(shape=[1, 3, 3, 1]) kernel = tf.random_normal(shape=[2, 2, 3, 1]) y = tf.nn.conv2d_transpose(x, kernel, output_shape=[1, 5, 5, 3], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)和二维卷积类似。这个函数是用来计算给定三维的输入和过滤器的情况下的一维卷积。不同的是，它的输入是三维，如[batch, in_width, in_channels]。卷积核的维度也是三维，少了一维filter_height，如[filter_width, in_channels, out_channels]。stride是一个正整数，代表卷积核向右移动每一步的长度 tf.nn.conv3d(input, filter, strides, padding, name=None)和二维卷积类似，这个函数用来计算给定五维的输入和过滤器的情况下的三维卷积。和二维卷积相对比： input的shape中多了一维in_depth，形状为Shape[batch, in_depth, in_height, in_width, in_channels] filter的shape中多了一维filter_depth，由filter_depth，filter_height，filter_width构成了卷积核的大小 strides中多了一维，变为[strides_batch， strides_depth， strides_height， strides_width， strides_channel]，必须保证strides[0] = strides[4] = 1 tf.nn.conv3d_transpose(value, filter, output_shape, strides, padding=‘SAME’, name=None)和二维反卷积类似 7.3池化函数 ​ 在神经网络中，池化函数一般跟在卷积函数的下一层，池化操作是利用一个矩阵窗口在张量上进行扫描，将每个矩阵窗口中的值通过取最大值或平均值来减少元素个数。每个池化操作的矩阵窗口大小是由ksize指定的，并且根据步长strides决定移动步长 tf.nn.avg_pool(value, ksize, strides, padding, data_format=‘NHWC’, name=None)这个函数计算池化区域中元素的平均值 def avg_pool(value, ksize, strides, padding, data_format=&#39;NHWC&#39;, name=None) # value：一个四维的张量。数据维度是[batch, height, width, channels] # ksize：一个长度不小于4的整形数组。每一位上的值对应于输入数据张量中每一维的窗口对应值 # strides：一个长度不小于4的整形数组，该参数指定滑动窗口在输入数据张量每一维上的步长 # padding：一个字符串，取值为SAME或者VALID # data_format：&#39;NHWC&#39;代表输入张量维度的顺序，N为个数，H为高度，W为宽度，C为通道数（RGB三通道或者灰度单通道） # name：为这个操作取一个名字 # 输出：一个张量，数据类型和value相同 使用示例如下： input_data = tf.Variable(np.random.rand(10, 6, 6, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 10), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) out_put = tf.nn.avg_pool(value=y, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.max_pool(value, ksize, strides, padding, data_format=‘NHWC’, name=None)这个函数是计算池化区域中元素的最大值。使用示例如下： input_data = tf.Variable(np.random.rand(10, 6, 6, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 10), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) out_put = tf.nn.max_pool(value=y, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax, name=None)这个函数的作用是计算池化区域中元素的最大值和该最大值所在的位置 在计算位置argmax的时候，我们将input平铺了进行计算，所以，如果input = [b, y, x, c]，那么索引位置是 ((b * height + y) * width + x) * channels + c。使用示例如下： input_data = tf.Variable(np.random.rand(10, 6, 6, 3), dtype=np.float32) filter_data = tf.Variable(np.random.rand(2, 2, 3, 10), dtype=np.float32) y = tf.nn.conv2d(input_data, filter_data, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) out_put, argmax = tf.nn.max_pool_with_argmax(input=y, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # 返回结果是一个张量组成的元组（output, argmax），output表示池化区域的最大值，argmax的数据类型是Targmax，维度是四维 tf.nn.avg_pool3d()和tf.nn.max_pool3d()分别是在三维下的平均池化和最大池化 tf.nn.fractional_avg_pool()和tf.nn.fractional_max_pool()分别是Benjamin Graham在论文中提出的池化技术，池化后的图片大小可以成非整数倍缩小，如$ \\sqrt 2 ， ， ， \\sqrt 3 $ tf.nn.pool(input, window_shape, pooling_type, padding, dilation_rate=None, strides=None, name=None, data_format=None)这个函数执行一个N维的池化操作 7.4分类函数 tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None) tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None) # 输入：logits：[batch_size, num_classes]，targets：[batch_size, size].logits用最后一层的输入即可 # 最后一层不需要进行sigmoid运算，此函数内部进行了sigmoid操作 # 输出：loss[batch_size, num_classes] 这个函数的输入要格外注意。如果采用此函数作为损失函数，在神经网络的最后一层不需要进行sigmoid运算 tf.nn.softmax(logits, dim=-1, name=None)计算SoftMax激活，也就是 softmax = exp(logits) / reduce_sum(exp(logits), dim) tf.nn.log_softmax(logits, dim=-1, name=None)计算log softmax激活，也就是 logsoftmax = logits - log(reduce_sum(exp(logits), dim)) tf.nn.softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None) softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None) # 输入：logits and labels均为[batch_size, num_classes] # 输出：loss[batch_size]，里面保存的是batch中每个样本的交叉熵 tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None) tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None) # 输入：logits：[batch_size, num_classes] labels：[batch_size]，必须在[0, num_classes] # 输出：loss[batch_size]，里面保存的是batch中每个样本的交叉熵 7.5优化方法 ​ 目前加速训练的优化方法基本都是基于梯度下降的，只是细节上有差异。梯度下降是求函数极值的一种方法，学习到最后就是求损失函数的极值问题 BGD法 ​ BGD的全称是batch gradient descent，即批梯度下降。这种方法是利用现有参数对训练集中每一个输入生成一个估计输出yi，然后跟实际输出yi比较，统计所有误差，求平均后得到平均误差，以此作为更新参数的一句。它的迭代过程为： 提取训练集中的所有内容{x1, ···, xn}，以及相关的输出yi 计算梯度和误差并更新参数 ​ 这个方法的优点是，使用所有训练数据计算，能够保证收敛，并且不需要逐渐减少学习率；缺点是，每一步都需要使用所有的训练集，随着训练的进行，速度会越来越慢 SGD法 ​ SGD的全称是stochastic gradient descent，即随机梯度下降。因为这种方法的主要思想是将数据集拆分成一个个批次（batch），随机抽取一个批次来计算并更新参数 ​ SGD在每一次迭代计算mini-batch的梯度，然后对参数进行更新。与BGD相比，SGD在训练数据集很大时，仍能以较快的速度收敛，但仍有以下两个缺点 由于抽取不可避免地梯度会有误差，需要手动调整学习率，但是选择合适的学习率又很困难 SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点 Momentum法 ​ Momentum是模拟物理学中动量的概念，更新时会在一定程度上保留之前的更新方向，利用当前批次再微调本次的更新参数，因此引入了一个新的变量v（速度），作为前几次梯度的累加。因此，Momentum能够更新学习率，在下降初期，前后梯度方向一致时，能够加速学习，在下降的中后期，在局部最小值的附近来回震荡时，能够抑制震荡，加快收敛 Nesterov Momentum法 ​ 是对Momentum的改进，Momentum法首先计算一个梯度，然后在加速更新梯度的方向进行一个大的跳跃；Nesterov项首先在原来加速的梯度方向进行一个大的跳跃，然后在该位置计算梯度值，然后用这个梯度值修正最终的更新方向。 Adagrad法 ​ Adagrad法能够自适应地为各个参数分配不同的学习率，能够控制每个维度的梯度方向。这种方法的优点是能够实现学习率的自动更改；如果本次更新时梯度大，学习率就衰减得快一些；如果这一次更新时梯度小，学习率就衰减得慢一些。 Adadelta法 ​ Adadelta法仍然存在一些问题：其学习率单调递减没在训练的后期学习率非常小，并且需要手动设置一个全局的初始学习率 RMSProp法 ​ RMSProp法与Momentum法类似，通过引入一个衰减系数，使每一回合都衰减一定比例，在实践中，对循环神经网络（RNN）效果很好 Adam法 ​ Adam的名称来源于自适应矩估计（adaptive moment estimation）。Adam法根据损失函数针对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。（矩估计就是利用样本来估计总体中相应的参数）","@type":"BlogPosting","url":"https://uzzz.org/2019/06/20/793325.html","headline":"TensorFlow基础知识学习笔记","dateModified":"2019-06-20T00:00:00+08:00","datePublished":"2019-06-20T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/20/793325.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>TensorFlow基础知识学习笔记</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h3><a id="1TensorFlow_1"></a>1.TensorFlow系统架构</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019062016390259.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Nzc3Nzc3NfQQ==,size_16,color_FFFFFF,t_70" alt="TensorFlow系统架构图"></p> 
  <ul> 
   <li>数据操作层：主要包括卷积神经网络、激活函数等操作</li> 
   <li>图计算层：（核心）包括图的创建、编译、优化和执行</li> 
  </ul> 
  <h3><a id="2_7"></a>2.设计理念</h3> 
  <ol> 
   <li> <p>将图的定义和图的运行完全分开。因此TensorFlow被认为是一个"符号主义"的库。符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量之间的计算关系，最后需要对数据流图进行编译，但此时的数据流图还是一个空壳，里面没有任何实际数据，只有把需要计算的输入放进去后，才能在整个模型中形成数据流，才能在整个模型中形成数据流，从而形成输出值</p> <pre><code class="prism language-python"><span class="token comment"># 传统程序操作</span>
t <span class="token operator">=</span> <span class="token number">8</span> <span class="token operator">+</span> <span class="token number">9</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span> <span class="token comment"># 定义了t的运算，在运行时就执行了，并输出17,</span>
<span class="token comment"># tensorflow</span>
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
t <span class="token operator">=</span> tf<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span> <span class="token comment"># 输出 Tensor("Add_1:0", shape=(), dtype=int32)</span>
<span class="token comment"># 数据流图中的节点，实际上对应的是TensorFlow API中的一个操作，并没有真正去运行</span>
</code></pre> </li> 
   <li> <p>TensorFlow中<strong>涉及的运算都要放在图中</strong>，而<strong>图的运行只发生在会话（session）中</strong>，开启会话后，就可以用数据去填充节点，进行运算，关闭会话后，就不能进行计算了，因此，会话提供了操作运行和Tensor求值的环境。</p> <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token comment"># 创建图</span>
a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> a <span class="token operator">*</span> b
<span class="token comment"># 创建会话</span>
sess <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 计算c</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>c<span class="token punctuation">)</span><span class="token punctuation">)</span>
sess<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> </li> 
  </ol> 
  <h3><a id="3TensorFlow_37"></a>3.TensorFlow的编程模型</h3> 
  <p>​ TensorFlow是用数据流图做计算的，因此需要先创建一个数据流图。一个简单的回归模型中的元素有：输入（input）、塑形（reshape）、Relu层（Relu Layer）、Logit层（Logit Layer）、Softmax、交叉熵（cross entropy）、梯度（gradient）、SGD训练（SGD Trainer）等部分。</p> 
  <p>​ 它的计算过程是，首先从输入开始，经过塑形后，一层一层进行前向传播运算。Relu层里会有两个参数，即W<sub>h1</sub>和b<sub>h1</sub>，在输出前是用ReLU（Rectified Linear Units）激活函数做非线性处理。然后进入Logit层（输出层），学习两个参数W<sub>sm</sub>和b<sub>sm</sub>。用Softmax来计算输出结果中各个类别的概率分布，用交叉熵来度量两个概率分布（源样本的概率分布和输出结果的概率分布）之间的相似性，然后开始计算梯度，这里是需要前面的四个参数，以及交叉熵后的结果。随后进入SGD训练，也就是反向传播的过程，从上往下计算每一层的参数，依次进行更新。</p> 
  <p>​ 顾名思义，TensorFlow是指“张量的流动”。TensorFlow的数据流图是由节点（node）和边（edge）组成的有向无环图。TensorFlow由Tensor和Flow组成，Tensor（张量）代表了数据流图中的边，Flow（流动）这个动作就代表了数据流图中节点所做的操作。</p> 
  <h4><a id="31_45"></a>3.1边</h4> 
  <p>​ TensorFlow的边有两种连接关系：<strong>数据依赖</strong>和<strong>控制依赖</strong>。其中，实线边表示数据依赖，代表数据，即张量。任意维度的数据统称为张量。在机器学习算法中，张量在数据流图中从前往后流动一遍就完成了一次前向传播，而残差（在数理统计中，残差是指实际观察值与训练的估计值之间的差）从后向前流动一遍就完成了一次反向传播。TensorFlow支持的张量具有下表所示的数据属性</p> 
  <table> 
   <thead> 
    <tr> 
     <th>数据类型</th> 
     <th>Python类型</th> 
     <th>描述</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>DT_FLOAT</td> 
     <td>tf.float32</td> 
     <td>32位浮点型</td> 
    </tr> 
    <tr> 
     <td>DT_DOUBLE</td> 
     <td>tf.float64</td> 
     <td>64位浮点型</td> 
    </tr> 
    <tr> 
     <td>DT_INT64</td> 
     <td>tf.int64</td> 
     <td>64位有符号整型</td> 
    </tr> 
    <tr> 
     <td>DT_INT32</td> 
     <td>tf.int32</td> 
     <td>32位有符号整型</td> 
    </tr> 
    <tr> 
     <td>DT_INT16</td> 
     <td>tf.int16</td> 
     <td>16位有符号整型</td> 
    </tr> 
    <tr> 
     <td>DT_INT8</td> 
     <td>tf.int8</td> 
     <td>8位有符号整型</td> 
    </tr> 
    <tr> 
     <td>DT_UINT8</td> 
     <td>tf.uint8</td> 
     <td>8位无符号整型</td> 
    </tr> 
    <tr> 
     <td>DT_STRING</td> 
     <td>tf.string</td> 
     <td>可变长度的字节数组，每一个张量元素都是一个字节数组</td> 
    </tr> 
    <tr> 
     <td>DT_BOOL</td> 
     <td>tf.bool</td> 
     <td>布尔型</td> 
    </tr> 
    <tr> 
     <td>DT_COMPLEX64</td> 
     <td>tf.complex64</td> 
     <td>由两个32位浮点数组成的复数：实部和虚部</td> 
    </tr> 
    <tr> 
     <td>DT_QINT32</td> 
     <td>tf.qint32</td> 
     <td>用于量化操作的32位有符号整型</td> 
    </tr> 
    <tr> 
     <td>DT_QINT8</td> 
     <td>tf.qint8</td> 
     <td>用于量化操作的8位有符号整型</td> 
    </tr> 
    <tr> 
     <td>DT_QUINT8</td> 
     <td>tf.quint8</td> 
     <td>用于量化操作的8位无符号整型</td> 
    </tr> 
   </tbody> 
  </table>
  <ul> 
   <li>量化是数字信号处理领域的一个概念，是指将信号的连续取值（或者大量可能的离散取值）金思维有限多个（或较少的）离散值的过程</li> 
  </ul> 
  <h4><a id="32_67"></a>3.2节点</h4> 
  <p>​ 节点又称为算子，它代表一个操作，一般用来表示施加的数学运算，也可以表示数据输入的起点以及输出的终点，或者是读取/写入持久变量的终点。下表列举了一些TensorFlow实现的算子，算子支持下表所示的张量的各种数据属性，并且需要在建立图的时候确定下来。</p> 
  <table> 
   <thead> 
    <tr> 
     <th>类别</th> 
     <th>示例</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>数学运算操作</td> 
     <td>Add、Substract、Multiply、Div、Exp、Log、Greater、Less、Equal</td> 
    </tr> 
    <tr> 
     <td>数组运算操作</td> 
     <td>Concat、Slice、Split、Constant、Rank、Shape、Shuffle</td> 
    </tr> 
    <tr> 
     <td>矩阵运算操作</td> 
     <td>MatMul、MatrixInverse、MatrixDeterminant</td> 
    </tr> 
    <tr> 
     <td>有状态的操作</td> 
     <td>Variable、Assign、AssignAdd</td> 
    </tr> 
    <tr> 
     <td>神经网络构建操作</td> 
     <td>SoftMax、Sigmoid、ReLU、Convolution2D、MaxPool</td> 
    </tr> 
    <tr> 
     <td>检查点操作</td> 
     <td>Save、Restore</td> 
    </tr> 
    <tr> 
     <td>队列和同步操作</td> 
     <td>Enqueue、Dequeue、MutexAcquire、MutexRelease</td> 
    </tr> 
    <tr> 
     <td>控制张量流动的操作</td> 
     <td>Merge、Switch、Enter、Leave、NextIteration</td> 
    </tr> 
   </tbody> 
  </table>
  <h4><a id="33_82"></a>3.3其它概念</h4> 
  <ol> 
   <li> <p>图</p> <p>操作任务可以描述成一个有向无环图，如何构建一个图，第一步是创建各个节点</p> <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token comment"># 创建一个常量运算操作，产生一个1x2矩阵</span>
matrix1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 创建另外一个常量运算操作，产生一个2x1矩阵</span>
matrix2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 创建一个矩阵乘法运算，把matrix1和matrix2作为输入</span>
<span class="token comment"># 返回值result代表矩阵乘法的结果</span>
result <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>matrix1<span class="token punctuation">,</span> matrix2<span class="token punctuation">)</span>
</code></pre> </li> 
  </ol> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190620164005331.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Nzc3Nzc3NfQQ==,size_16,color_FFFFFF,t_70" alt="构建图的基本操作"><br> 2. 会话</p> 
  <p>​ 启动图的第一步是创建一个Session对象，会话（Session）提供在途中执行操作的一些方法。一般的模式是，建立会话，此时会生成一张空图，在会话中添加节点和边，形成一张图，然后执行。</p> 
  <pre><code class="prism language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
    result <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>result<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>
</code></pre> 
  <p>​ 在调用Session对象的run()方法来执行图时，传入一些tensor，这个过程叫填充，返回的结果类型根据输入的类型而定，这个过程叫取回。</p> 
  <p>​ 会话是图交互的一个桥梁，一个会话可以有多个图，会话可以修改图的结构，也可以往图中注入数据进行计算。因此会话主要有两个API接口：Extend和Run。Extend操作是在图中添加节点和边，Run操作是输入计算的节点和填充必要的数据后，进行运算，并输出结果</p> 
  <ol start="3"> 
   <li> <p>设备</p> <p>​ 设备是指一块可以用来运算并且拥有自己的地址空间的硬件，如GPU和CPU，TensorFlow为了实现分布式执行操作，充分利用资源，可以明确指定操作在哪个设备上执行</p> <pre><code class="prism language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
    <span class="token comment"># 指定在第二个gpu上执行</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"/gpu:1"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> </li> 
   <li> <p>变量</p> <p>​ 变量是一种特殊的数据，它在图中有固定的位置，不像普通张量那样可以流动。创建一个变量张量，使用tf.Variable()构造函数，这个构造函数需要一个初始值，初始值的形状和类型决定了这个变量的形状和类型</p> <pre><code class="prism language-python"><span class="token comment"># 创建一个变量，初始化为标量0</span>
state <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'counter'</span><span class="token punctuation">)</span>
</code></pre> <p>​ TensorFlow还提供了填充机制，可以在构建图时使用tf.placeholder()临时替代任意操作的张量，在调用Session对象的run()方法去执行图时，使用填充数据作为调用的参数，调用之后，填充数据就消失<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190620164102452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Nzc3Nzc3NfQQ==,size_16,color_FFFFFF,t_70" alt="TensorFlow的填充机制"></p> </li> 
  </ol> 
  <h3><a id="4API_135"></a>4.常用API</h3> 
  <h4><a id="41_137"></a>4.1图，操作和张量</h4> 
  <p>​ TensorFlow的计算表现为数据流图，所以<strong>tf.Graph</strong>类中包含一系列表示计算的操作对象（tf.Operation），以及在操作之间流动的数据–张量对象（tf.Tensor）。与图相关的API均位于tf.Graph类中</p> 
  <table> 
   <thead> 
    <tr> 
     <th>操作</th> 
     <th>描述</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>tf.Graph.__ init__()</td> 
     <td>创建一个空图</td> 
    </tr> 
    <tr> 
     <td>tf.Graph.as_default()</td> 
     <td>将某图设置成默认图，并返回一个上下文管理器，如果不显式添加一个默认图，系统会自动设置一个全局的默认图，所设置的默认图，在模块范围内定义的节点都将默认加入默认图中</td> 
    </tr> 
    <tr> 
     <td>tf.Graph.device(deice_name_or_function)</td> 
     <td>定义运行图所使用的的设备，并返回一个上下文管理器</td> 
    </tr> 
    <tr> 
     <td>tf.Graph.name_scope(name)</td> 
     <td>为节点创建层次化的名称，并返回一个上下文管理器</td> 
    </tr> 
   </tbody> 
  </table>
  <p>​ <strong>tf.Operation</strong>类代表图中的一个节点，用于计算张量数据。该类型由节点构造器（如tf.matmul()或者Graph.create_op()）产生。与操作相关的API均位于tf.Operation类中</p> 
  <table> 
   <thead> 
    <tr> 
     <th>操作</th> 
     <th>描述</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td><a href="http://tf.Operation.name" rel="nofollow" data-token="69e8733b6ea8d1639f403349c357fb2b">tf.Operation.name</a></td> 
     <td>操作的名称</td> 
    </tr> 
    <tr> 
     <td>tf.Operation.type</td> 
     <td>操作的类型</td> 
    </tr> 
    <tr> 
     <td>tf.Operation.inputs/outputs</td> 
     <td>操作的输入与输出</td> 
    </tr> 
    <tr> 
     <td>tf.Operation.control_inputs</td> 
     <td>操作的依赖</td> 
    </tr> 
    <tr> 
     <td>tf.Operation.run(feed_dict=None, session=None)</td> 
     <td>在会话中运行该操作</td> 
    </tr> 
    <tr> 
     <td>tf.Operation.get_attr(name)</td> 
     <td>获取操作的属性值</td> 
    </tr> 
   </tbody> 
  </table>
  <p>​ <strong>tf.Tensor</strong>类是操作输出的符号句柄，它不包含操作输出的值，而是提供了一种在tf.Session中计算这些值的方法。这样就可以在操作之间构建一个数据流连接，使TensorFlow能够执行一个表示大量多步计算的图形。与张量相关的API均位于tf.Tensor类中</p> 
  <table> 
   <thead> 
    <tr> 
     <th>操作</th> 
     <th>描述</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>tf.Tensor.dtype</td> 
     <td>张量的数据类型</td> 
    </tr> 
    <tr> 
     <td><a href="http://tf.Tensor.name" rel="nofollow" data-token="50738c595ffb219c8d6c96277093144d">tf.Tensor.name</a></td> 
     <td>张量的名称</td> 
    </tr> 
    <tr> 
     <td>tf.Tensor.value_index</td> 
     <td>张量在操作输出中的索引</td> 
    </tr> 
    <tr> 
     <td>tf.Tensor.graph</td> 
     <td>张量所在的图</td> 
    </tr> 
    <tr> 
     <td>tf.Tensor.op</td> 
     <td>产生该张量的操作</td> 
    </tr> 
    <tr> 
     <td>tf.Tensor.consumers()</td> 
     <td>返回使用该张量的操作列表</td> 
    </tr> 
    <tr> 
     <td>tf.Tensor.eval(feed_dict=None, session=None)</td> 
     <td>在会话中求张量的值，需要使用sess.as_default或eval(session=sess)</td> 
    </tr> 
    <tr> 
     <td>tf.Tensor.get_shape()</td> 
     <td>返回用于表示张量的形状（维度）的类TensorShape</td> 
    </tr> 
    <tr> 
     <td>tf.Tensor.set_shape(shape)</td> 
     <td>更新张量的形状</td> 
    </tr> 
    <tr> 
     <td>tf.Tensor.device</td> 
     <td>设置计算该张量的设备</td> 
    </tr> 
   </tbody> 
  </table>
  <h4><a id="42_174"></a>4.2可视化</h4> 
  <p>​ 可视化时，需要在程序中给必要的节点添加摘要（summary），摘要会收集该节点的数据，并标记上第几步、时间戳等标识，写入事件文件（event file）中。tf.summary.FileWriter类用于在目录中创建事件文件，并且向文件中添加摘要和事件，用来在TensorBoard中展示</p> 
  <table> 
   <thead> 
    <tr> 
     <th>API</th> 
     <th>描述</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>tf.summary.FileWriter.__ init__(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None)</td> 
     <td>创建FileWriter和事件文件，会在logdir中创建一个新的事件文件</td> 
    </tr> 
    <tr> 
     <td>tf.summary.FileWriter.add_summary(summary, global_step=None)</td> 
     <td>将摘要添加到事件文件</td> 
    </tr> 
    <tr> 
     <td>tf.summary.FileWriter.add_event(event)</td> 
     <td>向事件文件中添加一个事件</td> 
    </tr> 
    <tr> 
     <td>tf.summary.FileWriter.add_graph(graph, global_step=None, graph_def=None)</td> 
     <td>向事件中添加一个图</td> 
    </tr> 
    <tr> 
     <td>tf.summary.FileWriter.get_logdir()</td> 
     <td>获取事件文件的路径</td> 
    </tr> 
    <tr> 
     <td>tf.summary.FileWriter.flush()</td> 
     <td>将所有事件都写入磁盘</td> 
    </tr> 
    <tr> 
     <td>tf.summary.FileWriter.close()</td> 
     <td>将事件写入磁盘，并关闭文件操作符</td> 
    </tr> 
    <tr> 
     <td>tf.summary.scalar(name, tensor, collections=None)</td> 
     <td>输出包含单个标量值的摘要</td> 
    </tr> 
    <tr> 
     <td>tf.summary.histogram(name, values, collections=None)</td> 
     <td>输出包含直方图的摘要</td> 
    </tr> 
    <tr> 
     <td>tf.summary.audio(name, tensor, sample_rate, max_outputs=3, collections=None)</td> 
     <td>输出包含音频的摘要</td> 
    </tr> 
    <tr> 
     <td>tf.summary.image(name, tensor, max_outputs=3, collections=None)</td> 
     <td>输出包含图片的摘要</td> 
    </tr> 
    <tr> 
     <td>tf.summary.merge(inputs, collections=None, name=None)</td> 
     <td>合并摘要，包含所有输入摘要的值</td> 
    </tr> 
   </tbody> 
  </table>
  <h3><a id="5_193"></a>5.变量作用域</h3> 
  <p>​ 在TensorFlow中有两个作用域（scope），一个是name_scope，一个是variable_scope。variable_scope主要是给variable_name加前缀，也可以给op_name加前缀；name_scope是给op_name加前缀。</p> 
  <h4><a id="51variable_scope_197"></a>5.1variable_scope</h4> 
  <pre><code class="prism language-python">v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span>name<span class="token punctuation">,</span> shape<span class="token punctuation">,</span> dtype<span class="token punctuation">,</span> initializer<span class="token punctuation">)</span> <span class="token comment"># 通过所给的名字创建或返回一个变量</span>
tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token operator">&lt;</span>scope_name<span class="token operator">&gt;</span><span class="token punctuation">)</span> <span class="token comment"># 为变量指定命名空间</span>
</code></pre> 
  <p>当tf.get_variable_scope().reuse==False时，variable_scope作用域只能用来创建新变量：</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    v2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">assert</span> v<span class="token punctuation">.</span>name <span class="token operator">==</span> <span class="token string">"foo/v:0"</span>
</code></pre> 
  <p>上述程序会抛出ValueError错误，因为v这个变量已经被定义过了，但tf.get_variable_scope().reuse默认为False，故不能重用</p> 
  <p>当tf.get_variable_scope().reuse==True时，作用域可以共享变量：</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> scope<span class="token punctuation">:</span>
    v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">,</span> reuse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 也可以写成:</span>
    <span class="token comment"># scope.reuse_variables()</span>
    v1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">assert</span> v1 <span class="token operator">==</span> v
</code></pre> 
  <h5><a id="511_231"></a>5.1.1获取变量作用域</h5> 
  <p>可以直接通过tf.get_variable_scope()来获取变量作用域</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> foo_scope<span class="token punctuation">:</span>
    v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>foo_scope<span class="token punctuation">)</span><span class="token punctuation">:</span>
    w <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"w"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
  <p>如果在开启的一个变量作用域里使用之前预先定义的一个作用域，则会跳过当前变量的作用域，保持预先存在的作用域不变</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> foo_scope<span class="token punctuation">:</span>
    <span class="token keyword">assert</span> foo_scope<span class="token punctuation">.</span>name <span class="token operator">==</span> <span class="token string">"foo"</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"bar"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"baz"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> other_scope<span class="token punctuation">:</span>
        <span class="token keyword">assert</span> other_scope<span class="token punctuation">.</span>name <span class="token operator">==</span> <span class="token string">"bar/baz"</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>foo_scope<span class="token punctuation">)</span> <span class="token keyword">as</span> foo_scope2<span class="token punctuation">:</span>
            <span class="token keyword">assert</span> foo_scope2<span class="token punctuation">.</span>name <span class="token operator">==</span> <span class="token string">"foo"</span>  <span class="token comment"># 保持不变</span>
</code></pre> 
  <h5><a id="512_258"></a>5.1.2变量作用域的初始化</h5> 
  <p>变量作用域可以默认携带一个初始化器，在这个作用域中的子作用域或变量都可以继承或者重写父作用域初始化器中的值</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span><span class="token number">0.4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> v<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0.4</span>  <span class="token comment"># 被作用域初始化</span>
    w <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"w"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> w<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0.3</span>  <span class="token comment"># 重写初始化器的值</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"bar"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> v<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0.4</span>  <span class="token comment"># 继承默认的初始化器</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"baz"</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> v<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0.2</span>  <span class="token comment"># 重写父作用域的初始化器的值</span>
</code></pre> 
  <p>上面讲的是variable_name，对于op_name，在variable_scope作用域下的操作，也会被加上前缀</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> x<span class="token punctuation">.</span>op<span class="token punctuation">.</span>name <span class="token operator">==</span> <span class="token string">"foo/add"</span>
</code></pre> 
  <h4><a id="52name_scope_288"></a>5.2name_scope示例</h4> 
  <p>TensorFlow中常常有数以千计的节点，在可视化的过程中很难一下子展示出来，因此用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。<em><strong>name_scope会影响op_name，不会影响用get_variable()创建的变量，而会影响通过Variable()创建的变量</strong></em></p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"bar"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        b <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">+</span> v
<span class="token keyword">assert</span> v<span class="token punctuation">.</span>name <span class="token operator">==</span> <span class="token string">"foo/v:0"</span>
<span class="token keyword">assert</span> b<span class="token punctuation">.</span>name <span class="token operator">==</span> <span class="token string">"foo/bar/b:0"</span>
<span class="token keyword">assert</span> x<span class="token punctuation">.</span>op<span class="token punctuation">.</span>name <span class="token operator">==</span> <span class="token string">"foo/bar/add"</span>
</code></pre> 
  <h3><a id="6_305"></a>6.批标准化</h3> 
  <p>​ 批标准化（batch normalization, BN）是为了克服神经网络层数加深导致难以训练而诞生的。随着神经网络的深度加深，训练会越来越困难，收敛速度会很慢，常常会导致梯度消失问题。</p> 
  <ul> 
   <li> <p>梯度消失问题：在神经网络中，当前隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫梯度消失问题</p> <p>​ 传统机器学习中有一个ICS理论，这是一个经典假设：源域（source domain）和目标域（target domain）的数据分布是一致的，也就是说，训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障</p> <p>​ Covariate Shift是指训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好地泛化（泛化：通俗来讲就是指学习到的模型对位置数据的预知能力）。它是分布不一致假设之下的一个分支问题，也就是指源域和目标域的条件概率是一致的，但是边缘概率不同。对于神经网络的各层输出，在经过了层内的操作后，各层输出分布就会与对应的输入信号分布不同，而且差异会随着网络深度增大而增大，但是每一层所指向的样本标记仍然是不变的。</p> <p>​ 解决思路一般是根据训练样本的比例对训练样本做一个矫正，因此，通过引入批标准化来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差。</p> </li> 
  </ul> 
  <h4><a id="61_317"></a>6.1方法</h4> 
  <p>​ 批标准化一般用在非线性映射（激活函数）之前，对x=W<sub>u</sub>+b做规范化，使结果（输出信号各个维度）的均值为0，方差为1。让每一层的输入有一个稳定的分布会有利于网络的训练</p> 
  <h4><a id="62_321"></a>6.2优点</h4> 
  <p>​ 批标准化通过规范化让激活函数分布在线性区间，结果就是加大了梯度，让模型更加大胆地进行梯度下降，于是有如下优点：</p> 
  <ul> 
   <li>加大探索的步长，加快收敛的速度</li> 
   <li>更容易跳出局部最小值</li> 
   <li>破坏原来的数据分布， 一定程度上缓解过拟合（过拟合：为了得到一致假设而使假设变得过度严格称为过拟合，出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少）</li> 
  </ul> 
  <h4><a id="63_329"></a>6.3示例</h4> 
  <p>对每一层的Wx_plus_b进行批标准化，这个步骤放在激活函数之前</p> 
  <pre><code class="prism language-python"><span class="token comment"># 计算Wx_plus_b的均值和方差，其中axes = [0]表示想要标准化的维度</span>
fc_mean<span class="token punctuation">,</span> fc_var <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>moments<span class="token punctuation">(</span>Wx_plus_b<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">)</span>
scale <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span>out_size<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
shift <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>out_size<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
epsilon <span class="token operator">=</span> <span class="token number">0.001</span>
Wx_plus_b <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>batch_normalization<span class="token punctuation">(</span>Wx_plus_b<span class="token punctuation">,</span> fc_mean<span class="token punctuation">,</span> fc_var<span class="token punctuation">,</span> shift<span class="token punctuation">,</span> scale<span class="token punctuation">,</span> epsilon<span class="token punctuation">)</span>
<span class="token comment"># 也就是在做：</span>
<span class="token comment"># Wx_plus_b = (Wx_plus_b - fc_mean)/tf.sqrt(fc_var + 0.001)</span>
<span class="token comment"># Wx_plus_b = Wx_plus_b * scale + shift</span>
</code></pre> 
  <h3><a id="7_345"></a>7.神经元函数及优化方法</h3> 
  <h4><a id="71_347"></a>7.1激活函数</h4> 
  <p>​ 激活函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经网络。神经网络之所以能解决非线性问题（如语音、图像识别），本质上就是激活函数加入了非线性因素，弥补了线性模型的表达力，把“激活的神经元的特征”通过函数保留并映射到下一层</p> 
  <p>​ 激活函数不会更改输入数据的维度，也就是输入和输出的维度是相同的。TensorFlow中有如下激活函数，输入均为要计算的x（一个张量），输出均为与x数据类型相同的张量。常见的有sigmoid、tanh、relu和softplus</p> 
  <ol> 
   <li> <p>sigmoid函数：</p> <p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
        <math>
         <semantics>
          <mrow>
           <mi>
            S
           </mi>
           <mo>
            (
           </mo>
           <mi>
            x
           </mi>
           <mo>
            )
           </mo>
           <mo>
            =
           </mo>
           <mfrac>
            <mn>
             1
            </mn>
            <mrow>
             <mn>
              1
             </mn>
             <mo>
              +
             </mo>
             <msup>
              <mi>
               e
              </mi>
              <mo>
               −
              </mo>
             </msup>
             <mi>
              x
             </mi>
            </mrow>
           </mfrac>
          </mrow>
          <annotation encoding="application/x-tex">
           S(x)=\frac{1}{1+e^-x}
          </annotation>
         </semantics>
        </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.05764em;">S</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.24844em; vertical-align: -0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathit mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.702664em;"><span class="" style="top: -2.786em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mord mathit mtight">x</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.403331em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p> <p>使用方法：</p> <pre><code class="prism language-python">a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
sess <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <p>​ sigmoid函数优点在于，它的输出映射在（0, 1）内，单调连续，非常适合于用作输出层，并且求导比较容易。缺点在于，因为软饱和性（软饱和性：是指激活函数h(x)在取值趋于无穷大时，它的一阶导数趋于0.硬饱和是指当|x|&gt;c时，其中c为常数，f’(x)=0），一旦落入软饱和区，f’(x)就会变得接近于0，很容易产生梯度消失。（梯度消失：是指在更新模型参数时采用链式求导法则反向求导，越往前梯度越小。最终的结果是到达一定深度后梯度对于模型的更新就没有任何贡献了）</p> </li> 
   <li> <p>tanh函数：<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml">
        <math>
         <semantics>
          <mrow>
           <mi>
            S
           </mi>
           <mo>
            (
           </mo>
           <mi>
            x
           </mi>
           <mo>
            )
           </mo>
           <mo>
            =
           </mo>
           <mfrac>
            <mrow>
             <mn>
              1
             </mn>
             <mo>
              −
             </mo>
             <msup>
              <mi>
               e
              </mi>
              <mo>
               −
              </mo>
             </msup>
             <mn>
              2
             </mn>
             <mi>
              x
             </mi>
            </mrow>
            <mrow>
             <mn>
              1
             </mn>
             <mo>
              +
             </mo>
             <msup>
              <mi>
               e
              </mi>
              <mo>
               −
              </mo>
             </msup>
             <mn>
              2
             </mn>
             <mi>
              x
             </mi>
            </mrow>
           </mfrac>
          </mrow>
          <annotation encoding="application/x-tex">
           S(x)=\frac{1-e^-2x}{1+e^-2x}
          </annotation>
         </semantics>
        </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.05764em;">S</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.3907em; vertical-align: -0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.987365em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathit mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.702664em;"><span class="" style="top: -2.786em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mord mtight">2</span><span class="mord mathit mtight">x</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathit mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.847664em;"><span class="" style="top: -2.931em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mord mtight">2</span><span class="mord mathit mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.403331em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p> </li> 
  </ol> 
  <p>tanh函数也具有软饱和性，因为它的输出以0为中心，收敛速度比sigmoid要快，但是仍无法解决梯度消失的问题</p> 
  <ol start="3"> 
   <li> <p>relu函数</p> <p>目前最受欢迎的激活函数。softplus可以看作是relu的平滑版本。relu定义为f(x)=max(x, 0)。softplus定义为f(x)=log(1+exp(x))。使用示例如下：</p> <pre><code class="prism language-python">a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
    b <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <p>除了relu本身之外，TensorFlow还定义了relu6以及crelu。</p> </li> 
   <li> <p>dropout函数</p> <p>​ 一个神经元将以概率keep_prob决定是否被抑制。如果被抑制，该神经元的输出就为0；如果不被抑制，那么该神经元的输出值将被放大到原来的1/keep_prob倍。</p> <p>​ 在默认情况下，每个神经元是否被抑制是相互独立的。但是否被抑制也可以通过noise_shape来调节。当noise_shape[i] == shape(x)[i]时，x中的元素是相互独立的。如果shape(x) = [k, l, m, n]，x中的维度的顺序分别为批、行、列和通道，如果noise_shape = [k, l, l, n]，那么每个批和通道相互独立，行跟列相互关联，也就是说，要不都为0，要不都还是原来的值。使用示例如下：</p> <pre><code class="prism language-python">a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
    b <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> noise_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> noise_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li> 
  </ol> 
  <h4><a id="72_399"></a>7.2卷积函数</h4> 
  <p>​ 卷积函数是构建神经网络的重要支架，是在一批图像上扫描的二维过滤器。</p> 
  <ol> 
   <li> <p>tf.nn.convolution(input, filter, padding, strides=None, dilation_rate=None, name=None, data_format=None) 这个函数是计算N维卷积的和</p> </li> 
   <li> <p>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)这个函数的作用是对一个四维的输入数据input和四维的卷积核filter进行操作，然后对输入数据进行一个二维的卷积操作，最后得到卷积之后的结果</p> <pre><code class="prism language-python">tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token builtin">filter</span><span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">,</span> use_cudnn_on_gpu<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> data_format<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 输入：</span>
<span class="token comment"># input：一个tensor。数据类型必须是float32或者float64</span>
<span class="token comment"># filter：一个tensor。数据类型必须与input相同</span>
<span class="token comment"># strides：一个长度是4的一维整数类型数组，每一维度对应的是input中每一维的对应移动步数</span>
<span class="token comment"># padding：一个字符串，取值为SAME或VALID</span>
<span class="token comment"># padding='SAME'：仅适用于全尺寸操作，即输入数据维度和输出数据维度相同</span>
<span class="token comment"># padding='VALID'：适用于部分窗口，即输入数据维度和输出数据维度不同</span>
<span class="token comment"># use_cudnn_on_gpu：一个可选布尔值，默认是True</span>
<span class="token comment"># name：（可选）为该操作取一个名字</span>
</code></pre> <p>使用示例如下：</p> <pre><code class="prism language-python">input_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
filter_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> filter_data<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>tf.nn.depthwise_conv2d(input, filter, strides, padding, rate=None, name=None, data_format=None)这个函数输入张量的数据维度是[batch, in_height, in_width, in_channels]，卷积核的维度是[filter_height, filter_width, in_channels, channel_multiplier]，在通道in_channels上面的卷积深度是1，depthwise_conv2d函数将不同的卷积核独立地应用在in_channels的每个通道上（从通道1到通道channel_multiplier），然后把所有的结果进行汇总。最后输出通道的总数是in_channels*channel_multiplier。</p> <p>使用示例如下：</p> <pre><code class="prism language-python">input_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
filter_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>depthwise_conv2d<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> filter_data<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate=None, name=None, data_format=None)是利用几个分离的卷积核去做卷积。在这个API中，将应用一个二维的卷积核，在每个通道上，以深度channel_multiplier进行卷积</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">separable_conv2d</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> depthwise_filter<span class="token punctuation">,</span> pointwise_filter<span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">,</span> rate<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> data_format<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 特殊参数：</span>
<span class="token comment"># depthwise_filter：一个张量。数据维度是四维[filter_height, filter_width, in_channels, channel_multiplier]。其中，in_channels的卷积深度是1</span>
<span class="token comment"># pointwise_filter：一个张量，数据维度是四维[1, 1, channel_multiplier*in_channels, out_channels]。其中，pointwise_filter是在depthwise_filter卷积之后的混合卷积</span>
</code></pre> <p>使用示例如下：</p> <pre><code class="prism language-python">input_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
depthwise_filter <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
pointwise_filter <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>separable_conv2d<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> depthwise_filter<span class="token punctuation">,</span> pointwise_filter<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>tf.nn.atrous_conv2d(value, filters, rate, padding, name=None)计算Atrous卷积，又称孔卷积或者扩张卷积。</p> <p>使用示例如下：</p> <pre><code class="prism language-python">input_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
filters <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>atrous_conv2d<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> filters<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding=‘SAME’, data_format=‘NHWC’, name=None)在解卷积网络中有时称为“反卷积”，但实际上是conv2d的转置，而不是实际的反卷积</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">conv2d_transpose</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span> <span class="token builtin">filter</span><span class="token punctuation">,</span> output_shape<span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">,</span> data_format<span class="token operator">=</span><span class="token string">'NHWC'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 特殊参数：</span>
<span class="token comment"># output_shape：一维的张量，表示反卷积运算后输出的形状</span>
<span class="token comment"># 输出：和value一样维度的Tensor</span>
</code></pre> <p>使用示例如下：</p> <pre><code class="prism language-python">x <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
kernel <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d_transpose<span class="token punctuation">(</span>x<span class="token punctuation">,</span> kernel<span class="token punctuation">,</span> output_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)和二维卷积类似。这个函数是用来计算给定三维的输入和过滤器的情况下的一维卷积。不同的是，它的输入是三维，如[batch, in_width, in_channels]。卷积核的维度也是三维，少了一维filter_height，如[filter_width, in_channels, out_channels]。stride是一个正整数，代表卷积核向右移动每一步的长度</p> </li> 
   <li> <p>tf.nn.conv3d(input, filter, strides, padding, name=None)和二维卷积类似，这个函数用来计算给定五维的输入和过滤器的情况下的三维卷积。和二维卷积相对比：</p> 
    <ul> 
     <li>input的shape中多了一维in_depth，形状为Shape[batch, in_depth, in_height, in_width, in_channels]</li> 
     <li>filter的shape中多了一维filter_depth，由filter_depth，filter_height，filter_width构成了卷积核的大小</li> 
     <li>strides中多了一维，变为[strides_batch， strides_depth， strides_height， strides_width， strides_channel]，必须保证strides[0] = strides[4] = 1</li> 
    </ul> </li> 
   <li> <p>tf.nn.conv3d_transpose(value, filter, output_shape, strides, padding=‘SAME’, name=None)和二维反卷积类似</p> </li> 
  </ol> 
  <h4><a id="73_491"></a>7.3池化函数</h4> 
  <p>​ 在神经网络中，池化函数一般跟在卷积函数的下一层，池化操作是利用一个矩阵窗口在张量上进行扫描，将每个矩阵窗口中的值通过取最大值或平均值来减少元素个数。每个池化操作的矩阵窗口大小是由ksize指定的，并且根据步长strides决定移动步长</p> 
  <ol> 
   <li> <p>tf.nn.avg_pool(value, ksize, strides, padding, data_format=‘NHWC’, name=None)这个函数计算池化区域中元素的平均值</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">avg_pool</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span> ksize<span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">,</span> data_format<span class="token operator">=</span><span class="token string">'NHWC'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># value：一个四维的张量。数据维度是[batch, height, width, channels]</span>
<span class="token comment"># ksize：一个长度不小于4的整形数组。每一位上的值对应于输入数据张量中每一维的窗口对应值</span>
<span class="token comment"># strides：一个长度不小于4的整形数组，该参数指定滑动窗口在输入数据张量每一维上的步长</span>
<span class="token comment"># padding：一个字符串，取值为SAME或者VALID</span>
<span class="token comment"># data_format：'NHWC'代表输入张量维度的顺序，N为个数，H为高度，W为宽度，C为通道数（RGB三通道或者灰度单通道）</span>
<span class="token comment"># name：为这个操作取一个名字</span>
<span class="token comment"># 输出：一个张量，数据类型和value相同</span>
</code></pre> <p>使用示例如下：</p> <pre><code class="prism language-python">input_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
filter_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> filter_data<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
out_put <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>value<span class="token operator">=</span>y<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>tf.nn.max_pool(value, ksize, strides, padding, data_format=‘NHWC’, name=None)这个函数是计算池化区域中元素的最大值。使用示例如下：</p> <pre><code class="prism language-python">input_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
filter_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> filter_data<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
out_put <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>value<span class="token operator">=</span>y<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax, name=None)这个函数的作用是计算池化区域中元素的最大值和该最大值所在的位置</p> <p>在计算位置argmax的时候，我们将input平铺了进行计算，所以，如果input = [b, y, x, c]，那么索引位置是</p> <p>((b * height + y) * width + x) * channels + c。使用示例如下：</p> <pre><code class="prism language-python">input_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
filter_data <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> filter_data<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
out_put<span class="token punctuation">,</span> argmax <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool_with_argmax<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>y<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
<span class="token comment"># 返回结果是一个张量组成的元组（output, argmax），output表示池化区域的最大值，argmax的数据类型是Targmax，维度是四维</span>
</code></pre> </li> 
   <li> <p>tf.nn.avg_pool3d()和tf.nn.max_pool3d()分别是在三维下的平均池化和最大池化</p> </li> 
   <li> <p>tf.nn.fractional_avg_pool()和tf.nn.fractional_max_pool()分别是Benjamin Graham在论文中提出的池化技术，池化后的图片大小可以成非整数倍缩小，如$ \sqrt 2<span class="katex--inline"><span class="katex"><span class="katex-mathml">
        <math>
         <semantics>
          <mrow>
           <mi mathvariant="normal">
            ，
           </mi>
          </mrow>
          <annotation encoding="application/x-tex">
           ，
          </annotation>
         </semantics>
        </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0em; vertical-align: 0em;"></span><span class="mord cjk_fallback">，</span></span></span></span></span> \sqrt 3 $</p> </li> 
   <li> <p>tf.nn.pool(input, window_shape, pooling_type, padding, dilation_rate=None, strides=None, name=None, data_format=None)这个函数执行一个N维的池化操作</p> </li> 
  </ol> 
  <h4><a id="74_546"></a>7.4分类函数</h4> 
  <ol> 
   <li> <p>tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)</p> <pre><code class="prism language-python">tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 输入：logits：[batch_size, num_classes]，targets：[batch_size, size].logits用最后一层的输入即可</span>
<span class="token comment"># 最后一层不需要进行sigmoid运算，此函数内部进行了sigmoid操作</span>
<span class="token comment"># 输出：loss[batch_size, num_classes]</span>
</code></pre> <p>这个函数的输入要格外注意。如果采用此函数作为损失函数，在神经网络的最后一层不需要进行sigmoid运算</p> </li> 
   <li> <p>tf.nn.softmax(logits, dim=-1, name=None)计算SoftMax激活，也就是</p> <p>softmax = exp(logits) / reduce_sum(exp(logits), dim)</p> </li> 
   <li> <p>tf.nn.log_softmax(logits, dim=-1, name=None)计算log softmax激活，也就是</p> <p>logsoftmax = logits - log(reduce_sum(exp(logits), dim))</p> </li> 
   <li> <p>tf.nn.softmax_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, dim=-1, name=None)</p> <pre><code class="prism language-python">softmax_cross_entropy_with_logits<span class="token punctuation">(</span>_sentinel<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> labels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> logits<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 输入：logits and labels均为[batch_size, num_classes]</span>
<span class="token comment"># 输出：loss[batch_size]，里面保存的是batch中每个样本的交叉熵</span>
</code></pre> </li> 
   <li> <p>tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)</p> <pre><code class="prism language-python">tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sparse_softmax_cross_entropy_with_logits<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 输入：logits：[batch_size, num_classes] labels：[batch_size]，必须在[0, num_classes]</span>
<span class="token comment"># 输出：loss[batch_size]，里面保存的是batch中每个样本的交叉熵</span>
</code></pre> </li> 
  </ol> 
  <h4><a id="75_583"></a>7.5优化方法</h4> 
  <p>​ 目前加速训练的优化方法基本都是基于梯度下降的，只是细节上有差异。梯度下降是求函数极值的一种方法，学习到最后就是求损失函数的极值问题</p> 
  <ol> 
   <li> <p>BGD法</p> <p>​ BGD的全称是batch gradient descent，即批梯度下降。这种方法是利用现有参数对训练集中每一个输入生成一个估计输出y<sub>i</sub>，然后跟实际输出y<sub>i</sub>比较，统计所有误差，求平均后得到平均误差，以此作为更新参数的一句。它的迭代过程为：</p> 
    <ul> 
     <li> <p>提取训练集中的所有内容{x<sub>1</sub>, ···, x<sub>n</sub>}，以及相关的输出y<sub>i</sub></p> </li> 
     <li> <p>计算梯度和误差并更新参数</p> <p>​ 这个方法的优点是，使用所有训练数据计算，能够保证收敛，并且不需要逐渐减少学习率；缺点是，每一步都需要使用所有的训练集，随着训练的进行，速度会越来越慢</p> </li> 
    </ul> </li> 
   <li> <p>SGD法</p> <p>​ SGD的全称是stochastic gradient descent，即随机梯度下降。因为这种方法的主要思想是将数据集拆分成一个个批次（batch），随机抽取一个批次来计算并更新参数</p> <p>​ SGD在每一次迭代计算mini-batch的梯度，然后对参数进行更新。与BGD相比，SGD在训练数据集很大时，仍能以较快的速度收敛，但仍有以下两个缺点</p> 
    <ul> 
     <li>由于抽取不可避免地梯度会有误差，需要手动调整学习率，但是选择合适的学习率又很困难</li> 
     <li>SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点</li> 
    </ul> </li> 
   <li> <p>Momentum法</p> <p>​ Momentum是模拟物理学中动量的概念，更新时会在一定程度上保留之前的更新方向，利用当前批次再微调本次的更新参数，因此引入了一个新的变量v（速度），作为前几次梯度的累加。因此，Momentum能够更新学习率，在下降初期，前后梯度方向一致时，能够加速学习，在下降的中后期，在局部最小值的附近来回震荡时，能够抑制震荡，加快收敛</p> </li> 
   <li> <p>Nesterov Momentum法</p> <p>​ 是对Momentum的改进，Momentum法首先计算一个梯度，然后在加速更新梯度的方向进行一个大的跳跃；Nesterov项首先在原来加速的梯度方向进行一个大的跳跃，然后在该位置计算梯度值，然后用这个梯度值修正最终的更新方向。</p> </li> 
   <li> <p>Adagrad法</p> <p>​ Adagrad法能够自适应地为各个参数分配不同的学习率，能够控制每个维度的梯度方向。这种方法的优点是能够实现学习率的自动更改；如果本次更新时梯度大，学习率就衰减得快一些；如果这一次更新时梯度小，学习率就衰减得慢一些。</p> </li> 
   <li> <p>Adadelta法</p> <p>​ Adadelta法仍然存在一些问题：其学习率单调递减没在训练的后期学习率非常小，并且需要手动设置一个全局的初始学习率</p> </li> 
   <li> <p>RMSProp法</p> <p>​ RMSProp法与Momentum法类似，通过引入一个衰减系数，使每一回合都衰减一定比例，在实践中，对循环神经网络（RNN）效果很好</p> </li> 
   <li> <p>Adam法</p> <p>​ Adam的名称来源于自适应矩估计（adaptive moment estimation）。Adam法根据损失函数针对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。（矩估计就是利用样本来估计总体中相应的参数）</p> </li> 
  </ol> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
