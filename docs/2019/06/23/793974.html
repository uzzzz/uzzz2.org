<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>大数据hadoop | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="大数据hadoop" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="大数据的由来 • 大数据 – 随着计算机技术的发展,互联网的普及,信息的积累 已经到了一个非常庞大的地步,信息的增长也在不断 的加快,随着互联网、物联网建设的加快,信息更是 爆炸是增长,收集、检索、统计这些信息越发困难, 必须使用新的技术来解决这些问题什么是大数据 • 大数据的定义 – 大数据指无法在一定时间范围内用常规软件工具进行捕捉、 管理和处理的数据集合,需要新处理模式才能具有更强的 决策力、洞察发现力和流程优化能力的海量、高增长率和 多样化的信息资产 – 是指从各种各样类型的数据中,快速获得有价值的信息 &nbsp; • 大数据能做什么 – 企业组织利用相关数据分析帮助他们降低成本、提高 效率、开发新产品、做出更明智的业务决策等 – 把数据集合并后进行分析得出的信息和数据关系性, 用来察觉商业趋势、判定研究质量、避免疾病扩散、 打击犯罪或测定即时交通路况等 – 大规模并行处理数据库,数据挖掘电网,分布式文件 系统或数据库,云计算平和可扩展的存储系统等 &nbsp; 特性 大体量– (V)olume (大体量) 可从数百TB到数十数百PB、甚至EB的规模 速度快– (V)elocity(时效性) 很多大数据需要在一定的时间限度下得到及时处理 不限种类– (V)ariety(多样性) 大数据包括各种格式和形态的数据 统计分析，预测性– (V)alue(大价值) 大数据包含很多深度的价值,大数据分析挖掘和利用将带来巨大 的商业价值 真实性– (V)eracity(准确性) 处理的结果要保证一定的准确性 &nbsp; &nbsp; &nbsp; hadoop &nbsp; &nbsp; &nbsp;&nbsp; 开源,基于java开发,提供分布式基础架构, 特点:高可靠性、高扩展性、高效性、高容错性、低成本 • 2003年开始Google陆续发表了3篇论文 – GFS,MapReduce,BigTable • GFS – GFS是一个可扩展的分布式文件系统,用于大型的、分布式 的、对大量数据进行访问的应用 – 可以运行于廉价的普通硬件上,提供容错功能 • MapReduce – MapReduce是针对分布式并行计算的一套编程模型,由 Map和Reduce组成,Map是映射,把指令分发到多个 worker上,Reduce是规约,把worker计算出的结果合并 • BigTable – BigTable是存储结构化数据 – BigTable建立在GFS,Scheduler,Lock Service和 MapReduce之上 – 每个Table都是一个多维的稀疏图 &nbsp; • GFS、MapReduce和BigTable三大技术被称为 Google的三驾马车,虽然没有公布源码,但发布了 这三个产品的详细设计论 • Yahoo资助的Hadoop,是按照这三篇论文的开源 Java实现的,但在性能上Hadoop比Google要差很多 – GFS - - -&gt; HDFS – MapReduce - - -&gt; MapReduce – BigTable - - -&gt; HbaseHadoop组件 &nbsp; 组件 HDFS: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Hadoop分布式文件系统(核心组件) &nbsp; &nbsp; &nbsp;存储 MapReduce: 分布式计算框架(核心组件) Yarn: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 集群资源管理系统(核心组件) Zookeeper: &nbsp; 分布式协作服务 Hbase: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;分布式列存数据库 Hive: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 基于Hadoop的数据仓库 Sqoop: &nbsp; &nbsp; &nbsp; &nbsp; 数据同步工具 Pig: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 基于hadoop的数据流系统 Mahout: &nbsp; &nbsp; &nbsp; &nbsp;数据挖掘算法库 Flume: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;日志收集工具 &nbsp; &nbsp; &nbsp; &nbsp; HDFS 结构 &nbsp; Hadoop体系中数据存储管理的基础,是一个高度容错的系统,用于在低成本的通用硬件上运行 角色 client namenode secondarynode datanode • NameNode – Master节点,管理HDFS的名称空间和数据块映射信 息(fsimage),配置副本策略,处理所有客户端请求 • Secondary NameNode – 定期合并fsimage 和fsedits,推送给NameNode – 紧急情况下,可辅助恢复NameNode fsedits 变更日志(打补丁) • 但Secondary NameNode并非NameNode的热备 &nbsp; • DataNode – 数据存储节点,存储实际的数据 – 汇报存储信息给NameNode • Client – 切分文件 – 访问HDFS – 与NameNode交互,获取文件位置信息 – 与DataNode交互,读取和写入数据 block 每块缺省128MB大小,每块可以多个副本 &nbsp; &nbsp; MapReduce &nbsp; 结构 &nbsp; • 源自于Google的MapReduce论文,JAVA实现的分布式计算框架 • 角色和概念– JobTracker &nbsp;Master节点只有一个,管理所有作业/任务的监控、错误处理等 将任务分解成一系列任务,并分派给TaskTracker – TaskTracker Slave节点,一般是多台,运行Map Task和Reduce Task 并与JobTracker交互,汇报任务状态 – Map Task 解析每条数据记录,传递给用户编写的 map()并执行,将输出结果写入本地磁盘 – 如果为map-only作业,直接写入HDFS – Reducer Task 从Map Task的执行结果中,远程读 取输入数据,对数据进行排序,将数据按照分组传递 给用户编写的reduce函数执行 &nbsp; &nbsp; &nbsp; Yarn 结构 &nbsp; Yarn是Hadoop的一个通用的资源管理系统 角色 – Resourcemanager – 处理客户端请求 – 启动/监控ApplicationMaster – 监控NodeManager – 资源分配与调度 – Nodemanager – 单个节点上的资源管理 – 处理来自ResourceManager的命令 – 处理来自ApplicationMaster的命令 – ApplicationMaster – 数据切分 – 为应用程序申请资源,并分配给内部任务 – 任务监控与容错 – Container – 对任务运行行环境的抽象,封装了CPU 、内存等 – 多维资源以及环境变量、启动命令等任务运行相关的信息资源分配与调度 – Client – 用户与Yarn交互的客户端程序 – 提交应用程序、监控应用程序状态,杀死应用程序等 &nbsp; • Yarn的核心思想 • 将JobTracker和TaskTacker进行分离,它由下面几大构成组件 – ResourceManager一个全局的资源管理器 – NodeManager每个节点(RM)代理 – ApplicationMaster表示每个应用 – 每一个ApplicationMaster有多个Container在NodeManager上运行 &nbsp; &nbsp; • Hadoop的部署模式有三种 – 单机 – 伪分布式 – 完全分布式 单机模式 新虚拟机 192.168.5.61 node1 安装&nbsp;java-1.8.0-openjdk-devel [root@node1 ~]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#测试是否成功 1446 Jps &nbsp; [root@node1 ~]# tar -xf hadoop-2.7.7.tar.gz&nbsp; [root@node1 ~]# mv hadoop-2.7.7/ &nbsp; &nbsp; &nbsp; /usr/local/hadoop &nbsp; &nbsp;#保证所有者和所属组为root [root@node1 ~]# cd /usr/local/hadoop [root@node1 hadoop]# rpm -ql java-1.8.0-openjdk/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre/ &nbsp;截取前面相同的路径 [root@node1 hadoop]# ./bin/hadoop verion Error: JAVA_HOME is not set and could not be found. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#找不到java你装哪了,需定义变量指定 &nbsp; [root@node1 hadoop]# vim etc/hadoop/hadoop-env.sh&nbsp; # The java implementation to use. export JAVA_HOME=&quot;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre&quot; export HADOOP_CONF_DIR=&quot;/usr/local/hadoop/etc/hadoop&quot; [root@node1 hadoop]# ./bin/hadoop version &nbsp; &nbsp; [root@node1 ~]# cd /usr/local/hadoop/ [root@node1 hadoop]# mkdir input [root@node1 hadoop]# lsbin &nbsp;include &nbsp;lib &nbsp; &nbsp; &nbsp;LICENSE.txt &nbsp;&nbsp; README.txt &nbsp;shareetc &nbsp;input &nbsp; &nbsp;libexec &nbsp;NOTICE.txt &nbsp; output &nbsp;sbin [root@node1 hadoop]# cp *.txt input/ [root@node1 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount input output &nbsp; [root@node1 hadoop]# ls &nbsp;output/ part-r-00000 &nbsp;_SUCCESS &nbsp; &nbsp; &nbsp; &nbsp; HDFS分布式文件系统 需修改的配置文件 – Hadoop-env.sh JAVA_HOME HADOOP_CONF_DIR &nbsp; – xml文件配置格式 &lt;property&gt; &lt;name&gt;关键字&lt;/name&gt; &lt;value&gt;变量值&lt;/value&gt; &lt;description&gt; 描述 &lt;/description&gt; &lt;/property&gt; &nbsp; &nbsp; &nbsp; &nbsp; 环境准备 192.168.5.60 nn01 192.168.5.61 node1 &nbsp; &nbsp; 前面已配置 192.168.5.62 node2 192.168.5.63 node3 &nbsp; 修改 &nbsp;/etc/hosts # ::1&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 &nbsp; &nbsp; &nbsp; 全部装 &nbsp; &nbsp;&nbsp;java-1.8.0-openjdk-devel selinux关闭,禁用firewalld 所有机器能被nn01无密码ssh &nbsp;ssh-keygen &nbsp;生成密钥, ssh-copy-id &nbsp; nn01/node1/node2/node3 &nbsp; [root@nn01 ~]# vim /etc/ssh/ssh_config&nbsp; Host * &nbsp; &nbsp; &nbsp; &nbsp; StrictHostKeyChecking no &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;设置第一次ssh时不需要输入yes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; https://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-common/core-default.xml tar -xf hadoop-2.7.7.tar.gz &nbsp;chown -R root:root &nbsp;hadoop-2.7.7/ mv hadoop-2.7.7/ &nbsp;/usr/local/hadoop cd &nbsp;/usr/local/hadoop [root@nn01 hadoop]# vim &nbsp;etc/hadoop/core-site.xml &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;核心配置文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;文件系统配置参数 &lt;value&gt;hdfs://nn01:9000&lt;/value&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;数据根目录配置参数 &lt;value&gt;/var/hadoop&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &nbsp; [root@nn01 hadoop]# &nbsp;vim etc/hadoop/hdfs-site.xml&nbsp; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 地址声明(谁是namenode) &lt;value&gt;nn01:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 地址声明(SecondaryNameNode) &lt;value&gt;nn01:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 文件冗余份数() &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [root@nn01 hadoop]# cd etc/hadoop [root@nn01 hadoop]# vim slaves &nbsp; &nbsp; &nbsp;#只写DataNode节点的主机名称 node1 node2 node3 &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# vim etc/hadoop/hadoop-env.sh&nbsp; export JAVA_HOME=&quot;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre&quot; export HADOOP_CONF_DIR=&quot;/usr/local/hadoop/etc/hadoop&quot; Hadoop所有节点的配置参数完全一样,在一台配置好 后,把配置文件同步到其它所有主机上 &nbsp; rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node1:/usr/local/ rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node2:/usr/local/ rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node3:/usr/local/ &nbsp; [root@nn01 hadoop]# cd /usr/local/hadoop/ [root@nn01 hadoop]# mkdir /var/hadoop [root@nn01 hadoop]# ./bin/hdfs namenode -format [root@nn01 hadoop]# ./sbin/start-dfs.sh&nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# jps 30871 SecondaryNameNode 30680 NameNode 31017 Jps &nbsp; &nbsp; [root@node1 logs]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;其他节点上查看 1093 Jps 998 DataNode &nbsp; &nbsp; &nbsp; 核心手段:拆() 不同服务合在一台机器上 &nbsp; 主机 角色 软件 192.168.1.60 Master NameNode SecondaryNameNode ResourceManager HDFS YARN 192.168.1.61 node1 DataNode NodeManager HDFS YARN 192.168.1.62 node2 DataNode NodeManager HDFS YARN 192.168.1.63 node3 DataNode NodeManager HDFS YARN &nbsp; [root@nn01 hadoop]# cd &nbsp;/usr/local/hadoop/etc/hadoop &nbsp; [root@nn01 hadoop]# mv mapred-site.xml.template &nbsp; mapred-site.xml[root@nn01 hadoop]# vim mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &nbsp; [root@nn01 hadoop]# vim yarn-site.xml&nbsp; &lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;nn01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &nbsp; &nbsp; [root@nn01 hadoop]# for i in node1 node2 node3; do rsync -avXSH --delete /usr/local/hadoop &nbsp;${i}:/usr/local; done [root@nn01 hadoop]# ./sbin/start-yarn.sh&nbsp; [root@nn01 hadoop]# ./bin/yarn node -list 19/06/21 09:54:01 INFO client.RMProxy: Connecting to ResourceManager at nn01/192.168.5.60:8032 Total Nodes:3 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Node-Id&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; Node-State&nbsp;&nbsp; &nbsp;Node-Http-Address&nbsp;&nbsp; &nbsp;Number-of-Running-Containers &nbsp; &nbsp; &nbsp;node1:38135&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node1:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp;node3:39287&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node3:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp;node2:33541&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node2:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; [root@nn01 hadoop]# jps 1684 NameNode 1876 SecondaryNameNode 2775 Jps 2206 ResourceManager &nbsp; &nbsp; [root@node1 ~]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 914 DataNode 1474 NodeManager 1634 Jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; http://192.168.5.60:8088/&nbsp; &nbsp; resourcemanager &nbsp;WEB页面(nn01) http://192.168.5.61:8042/&nbsp; &nbsp; &nbsp; nodemanager WEB页面(node1 node2 node3) http://192.168.5.60:50090/&nbsp; &nbsp; &nbsp; &nbsp;secondory &nbsp; namenode WEB页面 &nbsp; http://192.168.5.60:50070/&nbsp; &nbsp; &nbsp; &nbsp; namenode &nbsp;WEB页面&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; http://192.168.5.61:50075/&nbsp; &nbsp; &nbsp; &nbsp;datanode(node1 &nbsp;node2 node3) &nbsp;WEB页面 &nbsp; [root@nn01 hadoop]# ./bin/hadoop fs -ls / &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# ./bin/hadoop fs -mkdir /abc [root@nn01 hadoop]# ./bin/hadoop fs &nbsp;-ls / [root@nn01 hadoop]# ./bin/hadoop fs -touchz /abc/f1 &nbsp; &nbsp; &nbsp;创建 [root@nn01 hadoop]# ./bin/hadoop fs -put &nbsp;*.txt &nbsp;/abc &nbsp; &nbsp;&nbsp;上传文件 [root@nn01 hadoop]# ./bin/hadoop fs -get /abc/f1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 下载文件 [root@nn01 hadoop]# ls f1&nbsp; &nbsp; hdfs安全模式相关命令(不是必要操作) hdfs &nbsp; dfsadmin -report &nbsp; &nbsp;查看 hadoop &nbsp;dfsadmin &nbsp;safemode &nbsp;leave &nbsp;强制namenode退出安全模式 &nbsp; &nbsp; [root@nn01 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar &nbsp;wordcount &nbsp;/abc/* &nbsp; /bcd &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#提交分析作业 [root@nn01 hadoop]# ./bin/hadoop fs -cat /bcd/* &nbsp; &nbsp; &nbsp;#查看结果 &nbsp; &nbsp; 扩容 新增192.168.5.64 &nbsp; &nbsp;newnode – 启动一个新的系统,设置SSH免密码登录 [root@nn01 #] ssh-copy-id newnode – 在所有节点修改 /etc/hosts,增加新节点的主机信息 vim /etc/hosts # ::1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1 &nbsp; &nbsp; &nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 192.168.5.64 newnode &nbsp; ]# &nbsp;for i in node{1..3}; do scp /etc/hosts &nbsp; ${i}:/etc/; done &nbsp;]# scp /etc/hosts &nbsp; newnode:/etc/ – 安装java运行环境(java-1.8.0-openjdk-devel) – 修改NameNode的slaves文件增加该节点 [root@nn01 hadoop]# vim /usr/local/hadoop/etc/hadoop/ node1 node2 node3 newnode #] &nbsp;for i in node1 node2 node3 ; do scp ./slaves &nbsp;${i}:/usr/local/hadoop/etc/hadoop; done – 拷贝NamNode的/usr/local/hadoop到本机 #] &nbsp; scp -r /usr/local/hadoop &nbsp; newnode:/usr/local – 在该节点启动DataNode ./sbin/hadoop-daemon.sh start datanode &nbsp; – 设置同步带宽,并同步数据 # ./bin/hdfs dfsadmin -setBalancerBandwidth 60000000 # ./sbin/start-balancer.sh – 查看集群状态 # ./bin/hdfs dfsadmin -report &nbsp; HDFS 修复节点比较简单,与增加节点基本一致 --注意:新节点的ip和主机名要与损坏节点的一致 --启动服务: &nbsp; /usr/local/hadoop/sbin/hadoop-daemon.sh &nbsp;start datanode --数据恢复是自动的 --上线以后会自动恢复数据,如果数据量非常巨大,可能需要一段时间 &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# cd &nbsp;/usr/local/hadoop/ [root@nn01 hadoop]#&nbsp;&nbsp;./bin/hadoop fs -put /root/CentOS7-1804.iso &nbsp;/bcd [root@nn01 ~]# vim etc/hadoop/hdfs-site.xml&nbsp; &lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt; &lt;/property&gt; &nbsp; [root@nn01 hadoop]# ./bin/hdfs dfsadmin -report &nbsp; &nbsp; NFS网关 • NFS 网关用途 – 用户可以通过操作系统兼容的本地NFSv3客户端来浏 览HDFS文件系统 – 用户可以从HDFS文件系统下载文档到本地文件系统 – 用户可以通过挂载点直接流化数据,支持文件附加, 但是不支持随机写 – NFS网关支持NFSv3和允许HDFS作为客户端文件系统 的一部分被挂载 &nbsp; &nbsp; 新机器192.168.5.65 &nbsp;nfsgw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 192.168.5.66 &nbsp;localhost 添加/etc/hosts,然后同步给其他集群主机 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 192.168.5.64 newnode 192.168.5.65 nfsgw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 安装&nbsp;&nbsp;java-1.8.0-openjdk-devel 卸载rpcbind &nbsp;nfs-utils nfsgw服务器和nn01上配置用户 ]# useradd nfsgw ; useradd nn01 ]# groupadd -g 800 nfsuser ]# &nbsp;useradd &nbsp;-u &nbsp;800 -g 800 -r -d /var/hadoop &nbsp;nfsuser &nbsp; [root@nn01 hadoop]# ./sbin/stop-all.sh &nbsp; &nbsp; 停止集群 &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# vim etc/hadoop/core-site.xml &lt;property&gt; &lt;name&gt;hadoop.proxyuser.nfsuser.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.nfsuser.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &nbsp; 同步/usr/local/hadoop给其他集群主机(node1,node2,node3) 启动集群 jps &nbsp; hdfs &nbsp;dfsadmin &nbsp;-report &nbsp; [root@nfsgw ~]# rsync -aSH --delete &nbsp;nn01:/usr/local/hadoop &nbsp;/usr/local &nbsp; [root@nfsgw ~]# vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml&nbsp; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;nfs.exports.allowed.hosts&lt;/name&gt; &lt;value&gt;* rw&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;nfs.dump.dir&lt;/name&gt; &lt;value&gt;/var/nfstmp&lt;/value&gt; &lt;/property&gt; &nbsp; &lt;/configuration&gt; &nbsp; [root@nfsgw ~]# mkdir /var/nfstmp [root@nfsgw ~]# chown 800.800 &nbsp;/var/nfstmp [root@nfsgw ~]# ls -ld /var/nfstmp drwxr-xr-x 2 nfsuser nfsuser 6 6月 &nbsp;21 16:47 /var/nfstmp &nbsp; [root@nfsgw ~]# cd /usr/local/hadoop/ [root@nfsgw hadoop]# rm -rf logs/* [root@nfsgw hadoop]# setfacl -m nfsuser:rwx logs/ [root@nfsgw hadoop]# ./sbin/hadoop-daemon.sh --script ./bin/hdfs start portmap &nbsp; 使用root启动portmap服务 [root@nfsgw hadoop]# sudo -u nfsuser ./sbin/hadoop-daemon.sh --script &nbsp;./bin/hdfs start nfs3 &nbsp; &nbsp;使用代理用户启动nfs3 [root@nfsgw hadoop]# jps 23659 Nfs3 23596 Portmap 23710 Jps &nbsp; &nbsp; pormap(root用户先起服务,停的话后停,先停nfs) nfs3(nfsuser用户后起服务,) atime &nbsp;访问的时间 ctmie &nbsp;改变的时间 mtime &nbsp;修改的时间 &nbsp; 客户端192.168.5.66测试 [root@localhost ~]# yum -y install nfs-utils [root@localhost ~]# mount -t nfs -o vers=3,proto=tcp,noatime,nolock,sync,noacl 192.168.5.65:/ &nbsp;/mnt [root@localhost ~]# ls /mnt &nbsp; &nbsp; zookeeper 开源的分布式应用程序协调服务 保证数据在集群间的事务一致性 &nbsp; 应用场景 – 集群分布式锁 – 集群统一命名服务 – 分布式协调服务 &nbsp; &nbsp; &nbsp; • Zookeeper角色与特性 – Leader:接受所有Follower的提案请求并统一协调发起 提案的投票,负责与所有的Follower进行内部数据交换 – Follower:直接为客户端服务并参与提案的投票,同时 与Leader进行数据交换 – Observer:直接为客户端服务但并不参与提案的投票, 同时也与Leader进行数据交换 &nbsp; • Zookeeper角色与选举 – 服务在启动的时候是没有角色的(LOOKING) – 角色是通过选举产生的 – 选举产生一个Leader,剩下的是Follower • 选举Leader原则 – 集群中超过半数机器投票选择Leader – 假如集群中拥有n台服务器,那么Leader必须得到 n/2+1台服务器的投票 &nbsp; &nbsp; • Zookeeper角色与选举 – 如果Leader死亡,重新选举Leader – 如果死亡的机器数量达到一半,则集群挂掉 – 如果无法得到足够的投票数量,就重新发起投票,如果参与投票的机器不足n/2+1,则集群停止工作 – 如果Follower死亡过多,剩余机器不足n/2+1,则集群也会停止工作 – Observer不计算在投票总设备数量里面 &nbsp; &nbsp; • Zookeeper可伸缩扩展性原理与设计 – Leader所有写相关操作 – Follower读操作与响应Leader提议 – 在Observer出现以前,Zookeeper的伸缩性由Follower 来实现,我们可以通过添加Follower节点的数量来保证 Zookeeper服务的读性能,但是随着Follower节点数量 的增加,Zookeeper服务的写性能受到了影响 &nbsp; • Zookeeper可伸缩扩展性原理与设计 – 客户端提交一个请求,若是读请求,则由每台Server的本地 副本数据库直接响应。若是写请求,需要通过一致性协议 (Zab)来处理 – Zab协议规定:来自Client的所有写请求都要转发给ZK服务 中唯一的Leader,由Leader根据该请求发起一个Proposal。 然后其他的Server对该Proposal进行Vote。之后Leader对 Vote进行收集,当Vote数量过半时Leader会向所有的 Server发送一个通知消息。最后当Client所连接的Server收 到该消息时,会把该操作更新到内存中并对Client的写请求 做出回应 &nbsp; – ZooKeeper在上述协议中实际扮演了两个职能。一方面从 客户端接受连接与操作请求,另一方面对操作结果进行投票。 这两个职能在Zookeeper集群扩展的时候彼此制约 – 从Zab协议对写请求的处理过程中可以发现,增加Follower 的数量,则增加了协议投票过程的压力。因为Leader节点 必须等待集群中过半Server响应投票,是节点的增加使得部 分计算机运行较慢,从而拖慢整个投票过程的可能性也随之 提高,随着集群变大,写操作也会随之下降 &nbsp; – 所以,我们不得不在增加Client数量的期望和我们希望保 持较好吞吐性能的期望间进行权衡。要打破这一耦合关系, 我们引入了不参与投票的服务器Observer。Observer可 以接受客户端的连接,并将写请求转发给Leader节点。但 Leader节点不会要求Observer参加投票,仅仅在上述第3 歩那样,和其他服务节点一起得到投票结果 &nbsp; – Observer的扩展,给Zookeeper的可伸缩性带来了全 新的景象。加入很多Observer节点,无须担心严重影 响写吞吐量。但并非是无懈可击,因为协议中的通知 阶段,仍然与服务器的数量呈线性关系。但是这里的 串行开销非常低。因此,我们可以认为在通知服务器 阶段的开销不会成为瓶颈 – Observer提升读性能的可伸缩性 – Observer提供了广域网能力 &nbsp; &nbsp; &nbsp;[root@nn01 ~]# tar -xf zookeeper-3.4.13.tar.gz&nbsp; [root@nn01 ~]# mv &nbsp;~/zookeeper-3.4.13 &nbsp; /usr/local/zookeeper [root@nn01 ~]# cd /usr/local/zookeeper/ [root@nn01 zookeeper]#cd conf/ [root@nn01 conf]# mv zoo_sample.cfg &nbsp;zoo.cfg [root@nn01 conf]# vim zoo.cfg server.1=node1:2888:3888 server.2=node2:2888:3888 server.3=node3:2888:3888 server.4=nn01:2888:3888:observer &nbsp; &nbsp;scp -r /usr/local/zookeeper &nbsp; node1:/usr/local &nbsp;scp -r /usr/local/zookeeper &nbsp; node2:/usr/local &nbsp;scp -r /usr/local/zookeeper &nbsp; node3:/usr/local &nbsp; &nbsp; mkdir /tmp/zookeeper &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #所有集群上创建 node1]# echo &nbsp;1 &gt; /tmp/zookeeper/myid &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;请确保每个server的myid文件中id数字不同,server.id中的id与myid中的id必须一致&nbsp; &nbsp; node2]# echo &nbsp;2 &gt; /tmp/zookeeper/myid &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;id的范围是1~255 node3]# echo &nbsp;3 &gt; /tmp/zookeeper/myid nn01]# &nbsp; echo 4 &nbsp;&gt; /tmp/zookeeper/myid &nbsp; &nbsp; 启动集群,查看验证(在所有集群节点执行) /usr/local/zookeeper/bin/zkServer.sh &nbsp;start /usr/local/zookeeper/bin/zkServer.sh status &nbsp; &nbsp; &nbsp; &nbsp;#刚启动一个查看状态为不运行的,因为还需投票,等全部起完了就运行了 &nbsp; &nbsp; Zookeeper管理文档 http://zookeeper.apache.org/doc/r3.4.10/zookeeperAdmin.html man &nbsp; bash &nbsp;---&gt; &nbsp; &nbsp; 搜索/dev/tcp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/fd/fd &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If fd is a valid integer, file descriptor fd is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stdin &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 0 is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stdout &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 1 is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stderr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 2 is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/tcp/host/port &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If &nbsp;host &nbsp;is &nbsp;a valid hostname or Internet address, and port is an integer port number or service name, bash &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;attempts to open a TCP connection to the corresponding socket. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/udp/host/port &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If host is a valid hostname or Internet address, and port is an integer port number or &nbsp;service &nbsp;name, &nbsp;bash &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;attempts to open a UDP connection to the corresponding socket. &nbsp; [root@nn01 ~]# vim zkstats &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #检测zookeeper状态脚本 #!/bin/bash function getzkstat(){ &nbsp; &nbsp; exec 2&gt;/dev/null &nbsp; &nbsp; exec 8&lt;&gt;/dev/tcp/$1/2181 &nbsp; &nbsp; echo stat &gt;&amp;8 &nbsp; &nbsp; Msg=$(cat &lt;&amp;8 |grep -P &quot;^Mode:&quot;) &nbsp; &nbsp; echo -e &quot;$1\t${Msg:-Mode: \x1b[31mNULL\x1b[0m}&quot; &nbsp; &nbsp; exec 8&lt;&amp;- } if (( $# == 0 ));then &nbsp; &nbsp; echo &quot;${0##*/} zk1 zk2 zk3 ... ...&quot; else &nbsp; &nbsp; for i in $@;do &nbsp; &nbsp; &nbsp; &nbsp; getzkstat ${i} &nbsp; &nbsp; done fi [nn01 ~]# &nbsp;./zkstats &nbsp;node{1..3} &nbsp; kafka &nbsp; &nbsp; – Kafka是由LinkedIn开发的一个分布式的消息系统 – Kafka是使用Scala编写 – Kafka是一种消息中间件 • 为什么要使用Kafka – 解耦、冗余、提高扩展性、缓冲 – 保证顺序,灵活,削峰填谷 – 异步通信 &nbsp; &nbsp; • Kafka角色与集群结构 – producer:生产者,负责发布消息 – consumer:消费者,负责读取处理消息 – topic:消息的类别 – Parition:每个Topic包含一个或多个Partition – Broker:Kafka集群包含一个或多个服务器 • Kafka通过Zookeeper管理集群配置,选举Leader &nbsp; • Kafka集群的安装配置 – Kafka集群的安装配置依赖Zookeeper,搭建Kafka集群之前,请先创建好一个可用的Zookeeper集群 – 安装OpenJDK运行环境 – 同步Kafka拷贝到所有集群主机 – 修改配置文件 – 启动与验证 &nbsp; &nbsp; &nbsp; vim /usr/local/kafka/config/server.properties&nbsp; broker.id=1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#每台服务器的broker.id都不能相同 zookeeper.connect=node1:2181,node2:2181,node3:2181 &nbsp;#zookeeper集群地址,不用都列出,写一部分即可 [root@nn01 kafka]# for i in node{1..3}; do scp -r /usr/local/kafka &nbsp; ${i}:/usr/local &nbsp;; done &nbsp;/usr/local/kafka/bin/kafka-server-start.sh -daemon &nbsp;/usr/local/kafka/config/server.properties &nbsp; &nbsp; &nbsp;#所有主机启动服务 – jps命令应该能看到Kafka模块 – netstat应该能看到9092在监听 [root@node1 ~]# /usr/local/kafka/bin/kafka-topics.sh --create --partitions 2 --replication-factor 2 --zookeeper localhost:2181 --topic mymsg &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#创建一个topic Created topic &quot;mymsg&quot;. &nbsp; [root@node2 ~]# /usr/local/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mymsg &nbsp; &nbsp; &nbsp; &nbsp; #生产者 发布消息,随便打一些字符串 [root@node3 kafka]# /usr/local/kafka/bin/kafka-console-consumer.sh &nbsp;--bootstrap-server localhost:9092 --topic mymsg &nbsp; #消费者 接收消息,上面打的字符串会在这同步显示 &nbsp; &nbsp; &nbsp; Haddop高可用 &nbsp; &nbsp; 为什么需要NameNode • 原因 – NameNode是HDFS的核心配置,HDFS又是Hadoop核心组件,NameNode在Hadoop集群中至关重要 – NameNode宕机,将导致集群不可用,如果NameNode数据丢失将导致整个集群的数据丢失,而NameNode的数据的更新又比较频繁,实现NameNode高可用势在必行 &nbsp; • 官方提供了两种解决方案 – HDFS with NFS – HDFS with QJM • 两种方案异同NFS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; QJMNN &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; NN ZK &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; ZK ZKFailoverController &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;ZKFailoverController NFS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp;&nbsp; JournalNode &nbsp; • HA方案对比 – 都能实现热备 – 都是一个Active NN和一个Standby NN – 都使用Zookeeper和ZKFC来实现自动失效恢复 – 失效切换都使用Fencin配置的方法来Active NN – NFS数据共享变更方案把数据存储在共享存储里,我们还需要考虑NFS的高可用设计 – QJM不需要共享存储,但需要让每一个DN都知道两个NN的位置,并把块信息和心跳包发送给Active和Standby这两个NN &nbsp; • 使用原因(QJM) – 解决NameNode单点故障问题 – Hadoop给出了HDFS的高可用HA方案:HDFS通常由两 个NameNode组成,一个处于Active状态,另一个处于 Standby状态。Active NameNode对外提供服务,比如 处理来自客户端的RPC请求,而Standby NameNode则 不对外提供服务,仅同步Active NameNode的状态,以 便能够在它失败时进行切换 • 典型的HA集群 – NameNode会被配置在两台独立的机器上,在任何时 候 , 一 个 NameNode 处 于 活 动 状 态 , 而 另 一 个 NameNode则处于备份状态 – 活动状态的NameNode会响应集群中所有的客户端, 备份状态的NameNode只是作为一个副本,保证在必 要的时候提供一个快速的转移 &nbsp; • NameNode高可用架构 – 为了让Standby Node与Active Node保持同步,这两个 Node 都 与 一 组 称 为 JNS 的 互 相 独 立 的 进 程 保 持 通 信 (Journal Nodes)。当Active Node更新了namespace, 它将记录修改日志发送给JNS的多数派。Standby Node将 会从JNS中读取这些edits,并持续关注它们对日志的变更 – Standby Node将日志变更应用在自己的namespace中, 当Failover发生时,Standby将会在提升自己为Active之前, 确保能够从JNS中读取所有的edits,即在Failover发生之前 Standy持有的namespace与Active保持完全同步 – NameNode更新很频繁,为了保持主备数据的一致性, 为了支持快速Failover,Standby Node持有集群中 blocks的最新位置是非常必要的。为了达到这一目的, DataNodes上需要同时配置这两个Namenode的地址, 同时和它们都建立心跳连接,并把block位置发送给它们 &nbsp; – 任何时刻,只能有一个Active NameNode,否则会导致 集群操作混乱,两个NameNode将会有两种不同的数据 状态,可能会导致数据丢失或状态异常,这种情况通常 称为&quot;split-brain&quot;(脑裂,三节点通讯阻断,即集群中不 同的DataNode看到了不同的Active NameNodes) – 对于JNS而言,任何时候只允许一个NameNode作为 writer;在Failover期间,原来的Standby Node将会接 管Active的所有职能,并负责向JNS写入日志记录,这种 机制阻止了其他NameNode处于Active状态的问题 &nbsp; 系统规划: &nbsp; 新准备机器 192.168.5.66 nn02 免密码登录 [root@nn02 ~]# scp -r 192.168.5.60:/root/.ssh &nbsp; &nbsp;/root [root@nn02 ~]# cd ./.ssh [root@nn02 .ssh]# ls authorized_keys &nbsp;id_rsa &nbsp;id_rsa.pub &nbsp;known_hosts [root@nn02 .ssh]# rm &nbsp;-rf known_hosts&nbsp; [root@nn01 ~]# vim &nbsp;/etc/hosts # ::1&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 192.168.5.65 nfsgw 192.168.5.66 nn02 ]# &nbsp;for i in 61 62 63 66; do scp &nbsp;/etc/hosts &nbsp; &nbsp;192.168.5.$i:/etc; done &nbsp; &nbsp; 关机重启(全部) 再开启zookeeper /usr/local/zookeeper/bin/zkServer.sh start &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #node{1..3},nn01都起zookeeper服务 ]# jps &nbsp; [root@nn01 ~]# rm -rf /var/hadoop/* [root@nn01 ~]# ssh node1 rm -rf /var/hadoop/* [root@nn01 ~]# ssh node2 rm -rf /var/hadoop/* [root@nn01 ~]# ssh node3 rm -rf /var/hadoop/* [root@nn01 ~]# rm -rf /usr/local/hadoop/logs/* &nbsp; &nbsp; [root@nn02 ~]# mkdir /var/hadoop &nbsp; &nbsp; yarn高可用 • ResourceManager高可用 – RM的高可用原理与NN一样,需要依赖ZK来实现,这里配置文件的关键部分,感兴趣的同学可以自己学习和测试 – yarn.resourcemanager.hostname – 同理因为使用集群模式,该选项应该关闭 &nbsp; [root@nn01 hadoop]# cd&nbsp;&nbsp;/usr/local/hadoop/etc/hadoop/ 0. &nbsp;[root@nn01 hadoop]# vim hadoop-env.sh&nbsp; export JAVA_HOME=&quot;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre&quot; export HADOOP_CONF_DIR=&quot;/usr/local/hadoop/etc/hadoop&quot; &nbsp; &nbsp; 1. &nbsp;[root@root hadoop]# vim core-site.xml&nbsp; &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;fs.defaultFS&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;hdfs://nsd1902&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/var/hadoop&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;node1:2181,node2:2181,node3:2181&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.proxyuser.nfsuser.groups&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;*&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.proxyuser.nfsuser.hosts&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;*&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; ~ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2. &nbsp;vim hdfs-site.xml&nbsp; &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.nameservices&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#指定hdfs的nameservices名称 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nsd1902&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.namenodes.nsd1902&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp;#指定集群的两个namenode的名称分别为nn1,nn2 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn1,nn2&lt;/value&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #这里不是指主机名 &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.rpc-address.nsd1902.nn1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置nn1,nn2的rpc通信端口 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01:8020&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.rpc-address.nsd1902.nn2&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02:8020&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.http-address.nsd1902.nn1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置nn1,nn2的http通信端口 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01:50070&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.http-address.nsd1902.nn2&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02:50070&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;#指定namenode元数据存储在journalnode中的路径 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;qjournal://node1:8485;node2:8485;node3:8485/nsd1902&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#指定journalnode日志文件存储路径 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/var/hadoop/journal&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.client.failover.proxy.provider.nsd1902&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; #指定HDFS客户端连接Active NameNode的java类 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置隔离机制为ssh &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;sshfence&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; #指定密钥的位置 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #开启自动故障转移 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.replication&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;2&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; &nbsp; 3. &nbsp;vim mapred-site.xml &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;yarn&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; &nbsp; &nbsp; 4. &nbsp;vim slaves node1 node2 node3 &nbsp; &nbsp; 5. &nbsp; vim yarn-site.xml&nbsp; &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;rm1,rm2&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;node1:2181,node2:2181,node3:2181&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;yarn-ha&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;!-- Site specific YARN configuration properties --&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; &nbsp; 6. &nbsp; vim &nbsp;exclude &nbsp; 把里面的内容删除 &nbsp; &nbsp; 集群初始化: &nbsp; – ALL: 代表所有机器 – nodeX: 代表node1 &nbsp;node2 &nbsp;node3 &nbsp;(标示方便全部一起操作) &nbsp; – ALL: 同步配置到所有集群机器 for i in node{1..3} nn02; do scp -r /usr/local/hadoop &nbsp;${i}:/usr/local &amp; done – NN1: 初始化ZK集群 # ./bin/hdfs zkfc -formatZK – nodeX: 启动journalnode服务 # ./sbin/hadoop-daemon.sh start journalnode &nbsp; • 初始化 – NN1: 格式化 # ./bin/hdfs namenode -format – NN2: 数据同步到本地/var/hadoop/dfs # rsync -aSH nn01:/var/hadoop/dfs /var/hadoop/ – NN1: 初始化JNS # ./bin/hdfs namenode -initializeSharedEdits – nodeX: 停止journalnode服务 # ./sbin/hadoop-daemon.sh stop journalnode &nbsp; &nbsp; &nbsp; • 启动集群 – NN1: 启动hdfs # ./sbin/start-dfs.sh – NN1: 启动yarn # ./sbin/start-yarn.sh – NN2: 启动热备ResourceManager # ./sbin/yarn-daemon.sh start resourcemanager &nbsp; &nbsp; &nbsp; • 查看集群状态 – 获取NameNode状态 # ./bin/hdfs haadmin -getServiceState nn1 # ./bin/hdfs haadmin -getServiceState nn2 – 获取ResourceManager状态 # ./bin/yarn rmadmin -getServiceState rm1 # ./bin/yarn rmadmin -getServiceState rm2 &nbsp; &nbsp; – 获取节点信息 # ./bin/hdfs dfsadmin -report &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#查看有3台集群节点 # ./bin/yarn node -list – 访问集群文件 # ./bin/hadoop fs -mkdir /input # ./bin/hadoop fs -ls hdfs://mycluster/ – 主从切换Activate # ./sbin/hadoop-daemon.sh stop namenode &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(把Activate的那台namenode停掉服务) &nbsp; ./bin/hadoop fs -put *.txt &nbsp;/input &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;./sbin/hadoop-daemon.sh start&nbsp;namenode ./bin/hadoop fs &nbsp;-ls /input &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="大数据的由来 • 大数据 – 随着计算机技术的发展,互联网的普及,信息的积累 已经到了一个非常庞大的地步,信息的增长也在不断 的加快,随着互联网、物联网建设的加快,信息更是 爆炸是增长,收集、检索、统计这些信息越发困难, 必须使用新的技术来解决这些问题什么是大数据 • 大数据的定义 – 大数据指无法在一定时间范围内用常规软件工具进行捕捉、 管理和处理的数据集合,需要新处理模式才能具有更强的 决策力、洞察发现力和流程优化能力的海量、高增长率和 多样化的信息资产 – 是指从各种各样类型的数据中,快速获得有价值的信息 &nbsp; • 大数据能做什么 – 企业组织利用相关数据分析帮助他们降低成本、提高 效率、开发新产品、做出更明智的业务决策等 – 把数据集合并后进行分析得出的信息和数据关系性, 用来察觉商业趋势、判定研究质量、避免疾病扩散、 打击犯罪或测定即时交通路况等 – 大规模并行处理数据库,数据挖掘电网,分布式文件 系统或数据库,云计算平和可扩展的存储系统等 &nbsp; 特性 大体量– (V)olume (大体量) 可从数百TB到数十数百PB、甚至EB的规模 速度快– (V)elocity(时效性) 很多大数据需要在一定的时间限度下得到及时处理 不限种类– (V)ariety(多样性) 大数据包括各种格式和形态的数据 统计分析，预测性– (V)alue(大价值) 大数据包含很多深度的价值,大数据分析挖掘和利用将带来巨大 的商业价值 真实性– (V)eracity(准确性) 处理的结果要保证一定的准确性 &nbsp; &nbsp; &nbsp; hadoop &nbsp; &nbsp; &nbsp;&nbsp; 开源,基于java开发,提供分布式基础架构, 特点:高可靠性、高扩展性、高效性、高容错性、低成本 • 2003年开始Google陆续发表了3篇论文 – GFS,MapReduce,BigTable • GFS – GFS是一个可扩展的分布式文件系统,用于大型的、分布式 的、对大量数据进行访问的应用 – 可以运行于廉价的普通硬件上,提供容错功能 • MapReduce – MapReduce是针对分布式并行计算的一套编程模型,由 Map和Reduce组成,Map是映射,把指令分发到多个 worker上,Reduce是规约,把worker计算出的结果合并 • BigTable – BigTable是存储结构化数据 – BigTable建立在GFS,Scheduler,Lock Service和 MapReduce之上 – 每个Table都是一个多维的稀疏图 &nbsp; • GFS、MapReduce和BigTable三大技术被称为 Google的三驾马车,虽然没有公布源码,但发布了 这三个产品的详细设计论 • Yahoo资助的Hadoop,是按照这三篇论文的开源 Java实现的,但在性能上Hadoop比Google要差很多 – GFS - - -&gt; HDFS – MapReduce - - -&gt; MapReduce – BigTable - - -&gt; HbaseHadoop组件 &nbsp; 组件 HDFS: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Hadoop分布式文件系统(核心组件) &nbsp; &nbsp; &nbsp;存储 MapReduce: 分布式计算框架(核心组件) Yarn: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 集群资源管理系统(核心组件) Zookeeper: &nbsp; 分布式协作服务 Hbase: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;分布式列存数据库 Hive: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 基于Hadoop的数据仓库 Sqoop: &nbsp; &nbsp; &nbsp; &nbsp; 数据同步工具 Pig: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 基于hadoop的数据流系统 Mahout: &nbsp; &nbsp; &nbsp; &nbsp;数据挖掘算法库 Flume: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;日志收集工具 &nbsp; &nbsp; &nbsp; &nbsp; HDFS 结构 &nbsp; Hadoop体系中数据存储管理的基础,是一个高度容错的系统,用于在低成本的通用硬件上运行 角色 client namenode secondarynode datanode • NameNode – Master节点,管理HDFS的名称空间和数据块映射信 息(fsimage),配置副本策略,处理所有客户端请求 • Secondary NameNode – 定期合并fsimage 和fsedits,推送给NameNode – 紧急情况下,可辅助恢复NameNode fsedits 变更日志(打补丁) • 但Secondary NameNode并非NameNode的热备 &nbsp; • DataNode – 数据存储节点,存储实际的数据 – 汇报存储信息给NameNode • Client – 切分文件 – 访问HDFS – 与NameNode交互,获取文件位置信息 – 与DataNode交互,读取和写入数据 block 每块缺省128MB大小,每块可以多个副本 &nbsp; &nbsp; MapReduce &nbsp; 结构 &nbsp; • 源自于Google的MapReduce论文,JAVA实现的分布式计算框架 • 角色和概念– JobTracker &nbsp;Master节点只有一个,管理所有作业/任务的监控、错误处理等 将任务分解成一系列任务,并分派给TaskTracker – TaskTracker Slave节点,一般是多台,运行Map Task和Reduce Task 并与JobTracker交互,汇报任务状态 – Map Task 解析每条数据记录,传递给用户编写的 map()并执行,将输出结果写入本地磁盘 – 如果为map-only作业,直接写入HDFS – Reducer Task 从Map Task的执行结果中,远程读 取输入数据,对数据进行排序,将数据按照分组传递 给用户编写的reduce函数执行 &nbsp; &nbsp; &nbsp; Yarn 结构 &nbsp; Yarn是Hadoop的一个通用的资源管理系统 角色 – Resourcemanager – 处理客户端请求 – 启动/监控ApplicationMaster – 监控NodeManager – 资源分配与调度 – Nodemanager – 单个节点上的资源管理 – 处理来自ResourceManager的命令 – 处理来自ApplicationMaster的命令 – ApplicationMaster – 数据切分 – 为应用程序申请资源,并分配给内部任务 – 任务监控与容错 – Container – 对任务运行行环境的抽象,封装了CPU 、内存等 – 多维资源以及环境变量、启动命令等任务运行相关的信息资源分配与调度 – Client – 用户与Yarn交互的客户端程序 – 提交应用程序、监控应用程序状态,杀死应用程序等 &nbsp; • Yarn的核心思想 • 将JobTracker和TaskTacker进行分离,它由下面几大构成组件 – ResourceManager一个全局的资源管理器 – NodeManager每个节点(RM)代理 – ApplicationMaster表示每个应用 – 每一个ApplicationMaster有多个Container在NodeManager上运行 &nbsp; &nbsp; • Hadoop的部署模式有三种 – 单机 – 伪分布式 – 完全分布式 单机模式 新虚拟机 192.168.5.61 node1 安装&nbsp;java-1.8.0-openjdk-devel [root@node1 ~]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#测试是否成功 1446 Jps &nbsp; [root@node1 ~]# tar -xf hadoop-2.7.7.tar.gz&nbsp; [root@node1 ~]# mv hadoop-2.7.7/ &nbsp; &nbsp; &nbsp; /usr/local/hadoop &nbsp; &nbsp;#保证所有者和所属组为root [root@node1 ~]# cd /usr/local/hadoop [root@node1 hadoop]# rpm -ql java-1.8.0-openjdk/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre/ &nbsp;截取前面相同的路径 [root@node1 hadoop]# ./bin/hadoop verion Error: JAVA_HOME is not set and could not be found. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#找不到java你装哪了,需定义变量指定 &nbsp; [root@node1 hadoop]# vim etc/hadoop/hadoop-env.sh&nbsp; # The java implementation to use. export JAVA_HOME=&quot;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre&quot; export HADOOP_CONF_DIR=&quot;/usr/local/hadoop/etc/hadoop&quot; [root@node1 hadoop]# ./bin/hadoop version &nbsp; &nbsp; [root@node1 ~]# cd /usr/local/hadoop/ [root@node1 hadoop]# mkdir input [root@node1 hadoop]# lsbin &nbsp;include &nbsp;lib &nbsp; &nbsp; &nbsp;LICENSE.txt &nbsp;&nbsp; README.txt &nbsp;shareetc &nbsp;input &nbsp; &nbsp;libexec &nbsp;NOTICE.txt &nbsp; output &nbsp;sbin [root@node1 hadoop]# cp *.txt input/ [root@node1 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount input output &nbsp; [root@node1 hadoop]# ls &nbsp;output/ part-r-00000 &nbsp;_SUCCESS &nbsp; &nbsp; &nbsp; &nbsp; HDFS分布式文件系统 需修改的配置文件 – Hadoop-env.sh JAVA_HOME HADOOP_CONF_DIR &nbsp; – xml文件配置格式 &lt;property&gt; &lt;name&gt;关键字&lt;/name&gt; &lt;value&gt;变量值&lt;/value&gt; &lt;description&gt; 描述 &lt;/description&gt; &lt;/property&gt; &nbsp; &nbsp; &nbsp; &nbsp; 环境准备 192.168.5.60 nn01 192.168.5.61 node1 &nbsp; &nbsp; 前面已配置 192.168.5.62 node2 192.168.5.63 node3 &nbsp; 修改 &nbsp;/etc/hosts # ::1&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 &nbsp; &nbsp; &nbsp; 全部装 &nbsp; &nbsp;&nbsp;java-1.8.0-openjdk-devel selinux关闭,禁用firewalld 所有机器能被nn01无密码ssh &nbsp;ssh-keygen &nbsp;生成密钥, ssh-copy-id &nbsp; nn01/node1/node2/node3 &nbsp; [root@nn01 ~]# vim /etc/ssh/ssh_config&nbsp; Host * &nbsp; &nbsp; &nbsp; &nbsp; StrictHostKeyChecking no &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;设置第一次ssh时不需要输入yes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; https://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-common/core-default.xml tar -xf hadoop-2.7.7.tar.gz &nbsp;chown -R root:root &nbsp;hadoop-2.7.7/ mv hadoop-2.7.7/ &nbsp;/usr/local/hadoop cd &nbsp;/usr/local/hadoop [root@nn01 hadoop]# vim &nbsp;etc/hadoop/core-site.xml &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;核心配置文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;文件系统配置参数 &lt;value&gt;hdfs://nn01:9000&lt;/value&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;数据根目录配置参数 &lt;value&gt;/var/hadoop&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &nbsp; [root@nn01 hadoop]# &nbsp;vim etc/hadoop/hdfs-site.xml&nbsp; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 地址声明(谁是namenode) &lt;value&gt;nn01:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 地址声明(SecondaryNameNode) &lt;value&gt;nn01:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 文件冗余份数() &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [root@nn01 hadoop]# cd etc/hadoop [root@nn01 hadoop]# vim slaves &nbsp; &nbsp; &nbsp;#只写DataNode节点的主机名称 node1 node2 node3 &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# vim etc/hadoop/hadoop-env.sh&nbsp; export JAVA_HOME=&quot;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre&quot; export HADOOP_CONF_DIR=&quot;/usr/local/hadoop/etc/hadoop&quot; Hadoop所有节点的配置参数完全一样,在一台配置好 后,把配置文件同步到其它所有主机上 &nbsp; rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node1:/usr/local/ rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node2:/usr/local/ rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node3:/usr/local/ &nbsp; [root@nn01 hadoop]# cd /usr/local/hadoop/ [root@nn01 hadoop]# mkdir /var/hadoop [root@nn01 hadoop]# ./bin/hdfs namenode -format [root@nn01 hadoop]# ./sbin/start-dfs.sh&nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# jps 30871 SecondaryNameNode 30680 NameNode 31017 Jps &nbsp; &nbsp; [root@node1 logs]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;其他节点上查看 1093 Jps 998 DataNode &nbsp; &nbsp; &nbsp; 核心手段:拆() 不同服务合在一台机器上 &nbsp; 主机 角色 软件 192.168.1.60 Master NameNode SecondaryNameNode ResourceManager HDFS YARN 192.168.1.61 node1 DataNode NodeManager HDFS YARN 192.168.1.62 node2 DataNode NodeManager HDFS YARN 192.168.1.63 node3 DataNode NodeManager HDFS YARN &nbsp; [root@nn01 hadoop]# cd &nbsp;/usr/local/hadoop/etc/hadoop &nbsp; [root@nn01 hadoop]# mv mapred-site.xml.template &nbsp; mapred-site.xml[root@nn01 hadoop]# vim mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &nbsp; [root@nn01 hadoop]# vim yarn-site.xml&nbsp; &lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;nn01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &nbsp; &nbsp; [root@nn01 hadoop]# for i in node1 node2 node3; do rsync -avXSH --delete /usr/local/hadoop &nbsp;${i}:/usr/local; done [root@nn01 hadoop]# ./sbin/start-yarn.sh&nbsp; [root@nn01 hadoop]# ./bin/yarn node -list 19/06/21 09:54:01 INFO client.RMProxy: Connecting to ResourceManager at nn01/192.168.5.60:8032 Total Nodes:3 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Node-Id&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; Node-State&nbsp;&nbsp; &nbsp;Node-Http-Address&nbsp;&nbsp; &nbsp;Number-of-Running-Containers &nbsp; &nbsp; &nbsp;node1:38135&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node1:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp;node3:39287&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node3:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp;node2:33541&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node2:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; [root@nn01 hadoop]# jps 1684 NameNode 1876 SecondaryNameNode 2775 Jps 2206 ResourceManager &nbsp; &nbsp; [root@node1 ~]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 914 DataNode 1474 NodeManager 1634 Jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; http://192.168.5.60:8088/&nbsp; &nbsp; resourcemanager &nbsp;WEB页面(nn01) http://192.168.5.61:8042/&nbsp; &nbsp; &nbsp; nodemanager WEB页面(node1 node2 node3) http://192.168.5.60:50090/&nbsp; &nbsp; &nbsp; &nbsp;secondory &nbsp; namenode WEB页面 &nbsp; http://192.168.5.60:50070/&nbsp; &nbsp; &nbsp; &nbsp; namenode &nbsp;WEB页面&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; http://192.168.5.61:50075/&nbsp; &nbsp; &nbsp; &nbsp;datanode(node1 &nbsp;node2 node3) &nbsp;WEB页面 &nbsp; [root@nn01 hadoop]# ./bin/hadoop fs -ls / &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# ./bin/hadoop fs -mkdir /abc [root@nn01 hadoop]# ./bin/hadoop fs &nbsp;-ls / [root@nn01 hadoop]# ./bin/hadoop fs -touchz /abc/f1 &nbsp; &nbsp; &nbsp;创建 [root@nn01 hadoop]# ./bin/hadoop fs -put &nbsp;*.txt &nbsp;/abc &nbsp; &nbsp;&nbsp;上传文件 [root@nn01 hadoop]# ./bin/hadoop fs -get /abc/f1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 下载文件 [root@nn01 hadoop]# ls f1&nbsp; &nbsp; hdfs安全模式相关命令(不是必要操作) hdfs &nbsp; dfsadmin -report &nbsp; &nbsp;查看 hadoop &nbsp;dfsadmin &nbsp;safemode &nbsp;leave &nbsp;强制namenode退出安全模式 &nbsp; &nbsp; [root@nn01 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar &nbsp;wordcount &nbsp;/abc/* &nbsp; /bcd &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#提交分析作业 [root@nn01 hadoop]# ./bin/hadoop fs -cat /bcd/* &nbsp; &nbsp; &nbsp;#查看结果 &nbsp; &nbsp; 扩容 新增192.168.5.64 &nbsp; &nbsp;newnode – 启动一个新的系统,设置SSH免密码登录 [root@nn01 #] ssh-copy-id newnode – 在所有节点修改 /etc/hosts,增加新节点的主机信息 vim /etc/hosts # ::1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1 &nbsp; &nbsp; &nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 192.168.5.64 newnode &nbsp; ]# &nbsp;for i in node{1..3}; do scp /etc/hosts &nbsp; ${i}:/etc/; done &nbsp;]# scp /etc/hosts &nbsp; newnode:/etc/ – 安装java运行环境(java-1.8.0-openjdk-devel) – 修改NameNode的slaves文件增加该节点 [root@nn01 hadoop]# vim /usr/local/hadoop/etc/hadoop/ node1 node2 node3 newnode #] &nbsp;for i in node1 node2 node3 ; do scp ./slaves &nbsp;${i}:/usr/local/hadoop/etc/hadoop; done – 拷贝NamNode的/usr/local/hadoop到本机 #] &nbsp; scp -r /usr/local/hadoop &nbsp; newnode:/usr/local – 在该节点启动DataNode ./sbin/hadoop-daemon.sh start datanode &nbsp; – 设置同步带宽,并同步数据 # ./bin/hdfs dfsadmin -setBalancerBandwidth 60000000 # ./sbin/start-balancer.sh – 查看集群状态 # ./bin/hdfs dfsadmin -report &nbsp; HDFS 修复节点比较简单,与增加节点基本一致 --注意:新节点的ip和主机名要与损坏节点的一致 --启动服务: &nbsp; /usr/local/hadoop/sbin/hadoop-daemon.sh &nbsp;start datanode --数据恢复是自动的 --上线以后会自动恢复数据,如果数据量非常巨大,可能需要一段时间 &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# cd &nbsp;/usr/local/hadoop/ [root@nn01 hadoop]#&nbsp;&nbsp;./bin/hadoop fs -put /root/CentOS7-1804.iso &nbsp;/bcd [root@nn01 ~]# vim etc/hadoop/hdfs-site.xml&nbsp; &lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt; &lt;/property&gt; &nbsp; [root@nn01 hadoop]# ./bin/hdfs dfsadmin -report &nbsp; &nbsp; NFS网关 • NFS 网关用途 – 用户可以通过操作系统兼容的本地NFSv3客户端来浏 览HDFS文件系统 – 用户可以从HDFS文件系统下载文档到本地文件系统 – 用户可以通过挂载点直接流化数据,支持文件附加, 但是不支持随机写 – NFS网关支持NFSv3和允许HDFS作为客户端文件系统 的一部分被挂载 &nbsp; &nbsp; 新机器192.168.5.65 &nbsp;nfsgw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 192.168.5.66 &nbsp;localhost 添加/etc/hosts,然后同步给其他集群主机 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 192.168.5.64 newnode 192.168.5.65 nfsgw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 安装&nbsp;&nbsp;java-1.8.0-openjdk-devel 卸载rpcbind &nbsp;nfs-utils nfsgw服务器和nn01上配置用户 ]# useradd nfsgw ; useradd nn01 ]# groupadd -g 800 nfsuser ]# &nbsp;useradd &nbsp;-u &nbsp;800 -g 800 -r -d /var/hadoop &nbsp;nfsuser &nbsp; [root@nn01 hadoop]# ./sbin/stop-all.sh &nbsp; &nbsp; 停止集群 &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# vim etc/hadoop/core-site.xml &lt;property&gt; &lt;name&gt;hadoop.proxyuser.nfsuser.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.nfsuser.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &nbsp; 同步/usr/local/hadoop给其他集群主机(node1,node2,node3) 启动集群 jps &nbsp; hdfs &nbsp;dfsadmin &nbsp;-report &nbsp; [root@nfsgw ~]# rsync -aSH --delete &nbsp;nn01:/usr/local/hadoop &nbsp;/usr/local &nbsp; [root@nfsgw ~]# vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml&nbsp; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;nfs.exports.allowed.hosts&lt;/name&gt; &lt;value&gt;* rw&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;nfs.dump.dir&lt;/name&gt; &lt;value&gt;/var/nfstmp&lt;/value&gt; &lt;/property&gt; &nbsp; &lt;/configuration&gt; &nbsp; [root@nfsgw ~]# mkdir /var/nfstmp [root@nfsgw ~]# chown 800.800 &nbsp;/var/nfstmp [root@nfsgw ~]# ls -ld /var/nfstmp drwxr-xr-x 2 nfsuser nfsuser 6 6月 &nbsp;21 16:47 /var/nfstmp &nbsp; [root@nfsgw ~]# cd /usr/local/hadoop/ [root@nfsgw hadoop]# rm -rf logs/* [root@nfsgw hadoop]# setfacl -m nfsuser:rwx logs/ [root@nfsgw hadoop]# ./sbin/hadoop-daemon.sh --script ./bin/hdfs start portmap &nbsp; 使用root启动portmap服务 [root@nfsgw hadoop]# sudo -u nfsuser ./sbin/hadoop-daemon.sh --script &nbsp;./bin/hdfs start nfs3 &nbsp; &nbsp;使用代理用户启动nfs3 [root@nfsgw hadoop]# jps 23659 Nfs3 23596 Portmap 23710 Jps &nbsp; &nbsp; pormap(root用户先起服务,停的话后停,先停nfs) nfs3(nfsuser用户后起服务,) atime &nbsp;访问的时间 ctmie &nbsp;改变的时间 mtime &nbsp;修改的时间 &nbsp; 客户端192.168.5.66测试 [root@localhost ~]# yum -y install nfs-utils [root@localhost ~]# mount -t nfs -o vers=3,proto=tcp,noatime,nolock,sync,noacl 192.168.5.65:/ &nbsp;/mnt [root@localhost ~]# ls /mnt &nbsp; &nbsp; zookeeper 开源的分布式应用程序协调服务 保证数据在集群间的事务一致性 &nbsp; 应用场景 – 集群分布式锁 – 集群统一命名服务 – 分布式协调服务 &nbsp; &nbsp; &nbsp; • Zookeeper角色与特性 – Leader:接受所有Follower的提案请求并统一协调发起 提案的投票,负责与所有的Follower进行内部数据交换 – Follower:直接为客户端服务并参与提案的投票,同时 与Leader进行数据交换 – Observer:直接为客户端服务但并不参与提案的投票, 同时也与Leader进行数据交换 &nbsp; • Zookeeper角色与选举 – 服务在启动的时候是没有角色的(LOOKING) – 角色是通过选举产生的 – 选举产生一个Leader,剩下的是Follower • 选举Leader原则 – 集群中超过半数机器投票选择Leader – 假如集群中拥有n台服务器,那么Leader必须得到 n/2+1台服务器的投票 &nbsp; &nbsp; • Zookeeper角色与选举 – 如果Leader死亡,重新选举Leader – 如果死亡的机器数量达到一半,则集群挂掉 – 如果无法得到足够的投票数量,就重新发起投票,如果参与投票的机器不足n/2+1,则集群停止工作 – 如果Follower死亡过多,剩余机器不足n/2+1,则集群也会停止工作 – Observer不计算在投票总设备数量里面 &nbsp; &nbsp; • Zookeeper可伸缩扩展性原理与设计 – Leader所有写相关操作 – Follower读操作与响应Leader提议 – 在Observer出现以前,Zookeeper的伸缩性由Follower 来实现,我们可以通过添加Follower节点的数量来保证 Zookeeper服务的读性能,但是随着Follower节点数量 的增加,Zookeeper服务的写性能受到了影响 &nbsp; • Zookeeper可伸缩扩展性原理与设计 – 客户端提交一个请求,若是读请求,则由每台Server的本地 副本数据库直接响应。若是写请求,需要通过一致性协议 (Zab)来处理 – Zab协议规定:来自Client的所有写请求都要转发给ZK服务 中唯一的Leader,由Leader根据该请求发起一个Proposal。 然后其他的Server对该Proposal进行Vote。之后Leader对 Vote进行收集,当Vote数量过半时Leader会向所有的 Server发送一个通知消息。最后当Client所连接的Server收 到该消息时,会把该操作更新到内存中并对Client的写请求 做出回应 &nbsp; – ZooKeeper在上述协议中实际扮演了两个职能。一方面从 客户端接受连接与操作请求,另一方面对操作结果进行投票。 这两个职能在Zookeeper集群扩展的时候彼此制约 – 从Zab协议对写请求的处理过程中可以发现,增加Follower 的数量,则增加了协议投票过程的压力。因为Leader节点 必须等待集群中过半Server响应投票,是节点的增加使得部 分计算机运行较慢,从而拖慢整个投票过程的可能性也随之 提高,随着集群变大,写操作也会随之下降 &nbsp; – 所以,我们不得不在增加Client数量的期望和我们希望保 持较好吞吐性能的期望间进行权衡。要打破这一耦合关系, 我们引入了不参与投票的服务器Observer。Observer可 以接受客户端的连接,并将写请求转发给Leader节点。但 Leader节点不会要求Observer参加投票,仅仅在上述第3 歩那样,和其他服务节点一起得到投票结果 &nbsp; – Observer的扩展,给Zookeeper的可伸缩性带来了全 新的景象。加入很多Observer节点,无须担心严重影 响写吞吐量。但并非是无懈可击,因为协议中的通知 阶段,仍然与服务器的数量呈线性关系。但是这里的 串行开销非常低。因此,我们可以认为在通知服务器 阶段的开销不会成为瓶颈 – Observer提升读性能的可伸缩性 – Observer提供了广域网能力 &nbsp; &nbsp; &nbsp;[root@nn01 ~]# tar -xf zookeeper-3.4.13.tar.gz&nbsp; [root@nn01 ~]# mv &nbsp;~/zookeeper-3.4.13 &nbsp; /usr/local/zookeeper [root@nn01 ~]# cd /usr/local/zookeeper/ [root@nn01 zookeeper]#cd conf/ [root@nn01 conf]# mv zoo_sample.cfg &nbsp;zoo.cfg [root@nn01 conf]# vim zoo.cfg server.1=node1:2888:3888 server.2=node2:2888:3888 server.3=node3:2888:3888 server.4=nn01:2888:3888:observer &nbsp; &nbsp;scp -r /usr/local/zookeeper &nbsp; node1:/usr/local &nbsp;scp -r /usr/local/zookeeper &nbsp; node2:/usr/local &nbsp;scp -r /usr/local/zookeeper &nbsp; node3:/usr/local &nbsp; &nbsp; mkdir /tmp/zookeeper &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #所有集群上创建 node1]# echo &nbsp;1 &gt; /tmp/zookeeper/myid &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;请确保每个server的myid文件中id数字不同,server.id中的id与myid中的id必须一致&nbsp; &nbsp; node2]# echo &nbsp;2 &gt; /tmp/zookeeper/myid &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;id的范围是1~255 node3]# echo &nbsp;3 &gt; /tmp/zookeeper/myid nn01]# &nbsp; echo 4 &nbsp;&gt; /tmp/zookeeper/myid &nbsp; &nbsp; 启动集群,查看验证(在所有集群节点执行) /usr/local/zookeeper/bin/zkServer.sh &nbsp;start /usr/local/zookeeper/bin/zkServer.sh status &nbsp; &nbsp; &nbsp; &nbsp;#刚启动一个查看状态为不运行的,因为还需投票,等全部起完了就运行了 &nbsp; &nbsp; Zookeeper管理文档 http://zookeeper.apache.org/doc/r3.4.10/zookeeperAdmin.html man &nbsp; bash &nbsp;---&gt; &nbsp; &nbsp; 搜索/dev/tcp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/fd/fd &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If fd is a valid integer, file descriptor fd is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stdin &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 0 is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stdout &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 1 is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stderr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 2 is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/tcp/host/port &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If &nbsp;host &nbsp;is &nbsp;a valid hostname or Internet address, and port is an integer port number or service name, bash &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;attempts to open a TCP connection to the corresponding socket. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/udp/host/port &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If host is a valid hostname or Internet address, and port is an integer port number or &nbsp;service &nbsp;name, &nbsp;bash &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;attempts to open a UDP connection to the corresponding socket. &nbsp; [root@nn01 ~]# vim zkstats &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #检测zookeeper状态脚本 #!/bin/bash function getzkstat(){ &nbsp; &nbsp; exec 2&gt;/dev/null &nbsp; &nbsp; exec 8&lt;&gt;/dev/tcp/$1/2181 &nbsp; &nbsp; echo stat &gt;&amp;8 &nbsp; &nbsp; Msg=$(cat &lt;&amp;8 |grep -P &quot;^Mode:&quot;) &nbsp; &nbsp; echo -e &quot;$1\t${Msg:-Mode: \x1b[31mNULL\x1b[0m}&quot; &nbsp; &nbsp; exec 8&lt;&amp;- } if (( $# == 0 ));then &nbsp; &nbsp; echo &quot;${0##*/} zk1 zk2 zk3 ... ...&quot; else &nbsp; &nbsp; for i in $@;do &nbsp; &nbsp; &nbsp; &nbsp; getzkstat ${i} &nbsp; &nbsp; done fi [nn01 ~]# &nbsp;./zkstats &nbsp;node{1..3} &nbsp; kafka &nbsp; &nbsp; – Kafka是由LinkedIn开发的一个分布式的消息系统 – Kafka是使用Scala编写 – Kafka是一种消息中间件 • 为什么要使用Kafka – 解耦、冗余、提高扩展性、缓冲 – 保证顺序,灵活,削峰填谷 – 异步通信 &nbsp; &nbsp; • Kafka角色与集群结构 – producer:生产者,负责发布消息 – consumer:消费者,负责读取处理消息 – topic:消息的类别 – Parition:每个Topic包含一个或多个Partition – Broker:Kafka集群包含一个或多个服务器 • Kafka通过Zookeeper管理集群配置,选举Leader &nbsp; • Kafka集群的安装配置 – Kafka集群的安装配置依赖Zookeeper,搭建Kafka集群之前,请先创建好一个可用的Zookeeper集群 – 安装OpenJDK运行环境 – 同步Kafka拷贝到所有集群主机 – 修改配置文件 – 启动与验证 &nbsp; &nbsp; &nbsp; vim /usr/local/kafka/config/server.properties&nbsp; broker.id=1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#每台服务器的broker.id都不能相同 zookeeper.connect=node1:2181,node2:2181,node3:2181 &nbsp;#zookeeper集群地址,不用都列出,写一部分即可 [root@nn01 kafka]# for i in node{1..3}; do scp -r /usr/local/kafka &nbsp; ${i}:/usr/local &nbsp;; done &nbsp;/usr/local/kafka/bin/kafka-server-start.sh -daemon &nbsp;/usr/local/kafka/config/server.properties &nbsp; &nbsp; &nbsp;#所有主机启动服务 – jps命令应该能看到Kafka模块 – netstat应该能看到9092在监听 [root@node1 ~]# /usr/local/kafka/bin/kafka-topics.sh --create --partitions 2 --replication-factor 2 --zookeeper localhost:2181 --topic mymsg &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#创建一个topic Created topic &quot;mymsg&quot;. &nbsp; [root@node2 ~]# /usr/local/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mymsg &nbsp; &nbsp; &nbsp; &nbsp; #生产者 发布消息,随便打一些字符串 [root@node3 kafka]# /usr/local/kafka/bin/kafka-console-consumer.sh &nbsp;--bootstrap-server localhost:9092 --topic mymsg &nbsp; #消费者 接收消息,上面打的字符串会在这同步显示 &nbsp; &nbsp; &nbsp; Haddop高可用 &nbsp; &nbsp; 为什么需要NameNode • 原因 – NameNode是HDFS的核心配置,HDFS又是Hadoop核心组件,NameNode在Hadoop集群中至关重要 – NameNode宕机,将导致集群不可用,如果NameNode数据丢失将导致整个集群的数据丢失,而NameNode的数据的更新又比较频繁,实现NameNode高可用势在必行 &nbsp; • 官方提供了两种解决方案 – HDFS with NFS – HDFS with QJM • 两种方案异同NFS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; QJMNN &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; NN ZK &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; ZK ZKFailoverController &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;ZKFailoverController NFS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp;&nbsp; JournalNode &nbsp; • HA方案对比 – 都能实现热备 – 都是一个Active NN和一个Standby NN – 都使用Zookeeper和ZKFC来实现自动失效恢复 – 失效切换都使用Fencin配置的方法来Active NN – NFS数据共享变更方案把数据存储在共享存储里,我们还需要考虑NFS的高可用设计 – QJM不需要共享存储,但需要让每一个DN都知道两个NN的位置,并把块信息和心跳包发送给Active和Standby这两个NN &nbsp; • 使用原因(QJM) – 解决NameNode单点故障问题 – Hadoop给出了HDFS的高可用HA方案:HDFS通常由两 个NameNode组成,一个处于Active状态,另一个处于 Standby状态。Active NameNode对外提供服务,比如 处理来自客户端的RPC请求,而Standby NameNode则 不对外提供服务,仅同步Active NameNode的状态,以 便能够在它失败时进行切换 • 典型的HA集群 – NameNode会被配置在两台独立的机器上,在任何时 候 , 一 个 NameNode 处 于 活 动 状 态 , 而 另 一 个 NameNode则处于备份状态 – 活动状态的NameNode会响应集群中所有的客户端, 备份状态的NameNode只是作为一个副本,保证在必 要的时候提供一个快速的转移 &nbsp; • NameNode高可用架构 – 为了让Standby Node与Active Node保持同步,这两个 Node 都 与 一 组 称 为 JNS 的 互 相 独 立 的 进 程 保 持 通 信 (Journal Nodes)。当Active Node更新了namespace, 它将记录修改日志发送给JNS的多数派。Standby Node将 会从JNS中读取这些edits,并持续关注它们对日志的变更 – Standby Node将日志变更应用在自己的namespace中, 当Failover发生时,Standby将会在提升自己为Active之前, 确保能够从JNS中读取所有的edits,即在Failover发生之前 Standy持有的namespace与Active保持完全同步 – NameNode更新很频繁,为了保持主备数据的一致性, 为了支持快速Failover,Standby Node持有集群中 blocks的最新位置是非常必要的。为了达到这一目的, DataNodes上需要同时配置这两个Namenode的地址, 同时和它们都建立心跳连接,并把block位置发送给它们 &nbsp; – 任何时刻,只能有一个Active NameNode,否则会导致 集群操作混乱,两个NameNode将会有两种不同的数据 状态,可能会导致数据丢失或状态异常,这种情况通常 称为&quot;split-brain&quot;(脑裂,三节点通讯阻断,即集群中不 同的DataNode看到了不同的Active NameNodes) – 对于JNS而言,任何时候只允许一个NameNode作为 writer;在Failover期间,原来的Standby Node将会接 管Active的所有职能,并负责向JNS写入日志记录,这种 机制阻止了其他NameNode处于Active状态的问题 &nbsp; 系统规划: &nbsp; 新准备机器 192.168.5.66 nn02 免密码登录 [root@nn02 ~]# scp -r 192.168.5.60:/root/.ssh &nbsp; &nbsp;/root [root@nn02 ~]# cd ./.ssh [root@nn02 .ssh]# ls authorized_keys &nbsp;id_rsa &nbsp;id_rsa.pub &nbsp;known_hosts [root@nn02 .ssh]# rm &nbsp;-rf known_hosts&nbsp; [root@nn01 ~]# vim &nbsp;/etc/hosts # ::1&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 192.168.5.65 nfsgw 192.168.5.66 nn02 ]# &nbsp;for i in 61 62 63 66; do scp &nbsp;/etc/hosts &nbsp; &nbsp;192.168.5.$i:/etc; done &nbsp; &nbsp; 关机重启(全部) 再开启zookeeper /usr/local/zookeeper/bin/zkServer.sh start &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #node{1..3},nn01都起zookeeper服务 ]# jps &nbsp; [root@nn01 ~]# rm -rf /var/hadoop/* [root@nn01 ~]# ssh node1 rm -rf /var/hadoop/* [root@nn01 ~]# ssh node2 rm -rf /var/hadoop/* [root@nn01 ~]# ssh node3 rm -rf /var/hadoop/* [root@nn01 ~]# rm -rf /usr/local/hadoop/logs/* &nbsp; &nbsp; [root@nn02 ~]# mkdir /var/hadoop &nbsp; &nbsp; yarn高可用 • ResourceManager高可用 – RM的高可用原理与NN一样,需要依赖ZK来实现,这里配置文件的关键部分,感兴趣的同学可以自己学习和测试 – yarn.resourcemanager.hostname – 同理因为使用集群模式,该选项应该关闭 &nbsp; [root@nn01 hadoop]# cd&nbsp;&nbsp;/usr/local/hadoop/etc/hadoop/ 0. &nbsp;[root@nn01 hadoop]# vim hadoop-env.sh&nbsp; export JAVA_HOME=&quot;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre&quot; export HADOOP_CONF_DIR=&quot;/usr/local/hadoop/etc/hadoop&quot; &nbsp; &nbsp; 1. &nbsp;[root@root hadoop]# vim core-site.xml&nbsp; &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;fs.defaultFS&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;hdfs://nsd1902&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/var/hadoop&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;node1:2181,node2:2181,node3:2181&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.proxyuser.nfsuser.groups&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;*&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.proxyuser.nfsuser.hosts&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;*&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; ~ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2. &nbsp;vim hdfs-site.xml&nbsp; &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.nameservices&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#指定hdfs的nameservices名称 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nsd1902&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.namenodes.nsd1902&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp;#指定集群的两个namenode的名称分别为nn1,nn2 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn1,nn2&lt;/value&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #这里不是指主机名 &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.rpc-address.nsd1902.nn1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置nn1,nn2的rpc通信端口 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01:8020&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.rpc-address.nsd1902.nn2&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02:8020&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.http-address.nsd1902.nn1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置nn1,nn2的http通信端口 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01:50070&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.http-address.nsd1902.nn2&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02:50070&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;#指定namenode元数据存储在journalnode中的路径 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;qjournal://node1:8485;node2:8485;node3:8485/nsd1902&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#指定journalnode日志文件存储路径 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/var/hadoop/journal&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.client.failover.proxy.provider.nsd1902&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; #指定HDFS客户端连接Active NameNode的java类 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置隔离机制为ssh &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;sshfence&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; #指定密钥的位置 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #开启自动故障转移 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.replication&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;2&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; &nbsp; 3. &nbsp;vim mapred-site.xml &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;yarn&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; &nbsp; &nbsp; 4. &nbsp;vim slaves node1 node2 node3 &nbsp; &nbsp; 5. &nbsp; vim yarn-site.xml&nbsp; &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;rm1,rm2&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;node1:2181,node2:2181,node3:2181&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;yarn-ha&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;!-- Site specific YARN configuration properties --&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; &nbsp; 6. &nbsp; vim &nbsp;exclude &nbsp; 把里面的内容删除 &nbsp; &nbsp; 集群初始化: &nbsp; – ALL: 代表所有机器 – nodeX: 代表node1 &nbsp;node2 &nbsp;node3 &nbsp;(标示方便全部一起操作) &nbsp; – ALL: 同步配置到所有集群机器 for i in node{1..3} nn02; do scp -r /usr/local/hadoop &nbsp;${i}:/usr/local &amp; done – NN1: 初始化ZK集群 # ./bin/hdfs zkfc -formatZK – nodeX: 启动journalnode服务 # ./sbin/hadoop-daemon.sh start journalnode &nbsp; • 初始化 – NN1: 格式化 # ./bin/hdfs namenode -format – NN2: 数据同步到本地/var/hadoop/dfs # rsync -aSH nn01:/var/hadoop/dfs /var/hadoop/ – NN1: 初始化JNS # ./bin/hdfs namenode -initializeSharedEdits – nodeX: 停止journalnode服务 # ./sbin/hadoop-daemon.sh stop journalnode &nbsp; &nbsp; &nbsp; • 启动集群 – NN1: 启动hdfs # ./sbin/start-dfs.sh – NN1: 启动yarn # ./sbin/start-yarn.sh – NN2: 启动热备ResourceManager # ./sbin/yarn-daemon.sh start resourcemanager &nbsp; &nbsp; &nbsp; • 查看集群状态 – 获取NameNode状态 # ./bin/hdfs haadmin -getServiceState nn1 # ./bin/hdfs haadmin -getServiceState nn2 – 获取ResourceManager状态 # ./bin/yarn rmadmin -getServiceState rm1 # ./bin/yarn rmadmin -getServiceState rm2 &nbsp; &nbsp; – 获取节点信息 # ./bin/hdfs dfsadmin -report &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#查看有3台集群节点 # ./bin/yarn node -list – 访问集群文件 # ./bin/hadoop fs -mkdir /input # ./bin/hadoop fs -ls hdfs://mycluster/ – 主从切换Activate # ./sbin/hadoop-daemon.sh stop namenode &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(把Activate的那台namenode停掉服务) &nbsp; ./bin/hadoop fs -put *.txt &nbsp;/input &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;./sbin/hadoop-daemon.sh start&nbsp;namenode ./bin/hadoop fs &nbsp;-ls /input &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/06/23/793974.html" />
<meta property="og:url" content="https://uzzz.org/2019/06/23/793974.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-23T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"大数据的由来 • 大数据 – 随着计算机技术的发展,互联网的普及,信息的积累 已经到了一个非常庞大的地步,信息的增长也在不断 的加快,随着互联网、物联网建设的加快,信息更是 爆炸是增长,收集、检索、统计这些信息越发困难, 必须使用新的技术来解决这些问题什么是大数据 • 大数据的定义 – 大数据指无法在一定时间范围内用常规软件工具进行捕捉、 管理和处理的数据集合,需要新处理模式才能具有更强的 决策力、洞察发现力和流程优化能力的海量、高增长率和 多样化的信息资产 – 是指从各种各样类型的数据中,快速获得有价值的信息 &nbsp; • 大数据能做什么 – 企业组织利用相关数据分析帮助他们降低成本、提高 效率、开发新产品、做出更明智的业务决策等 – 把数据集合并后进行分析得出的信息和数据关系性, 用来察觉商业趋势、判定研究质量、避免疾病扩散、 打击犯罪或测定即时交通路况等 – 大规模并行处理数据库,数据挖掘电网,分布式文件 系统或数据库,云计算平和可扩展的存储系统等 &nbsp; 特性 大体量– (V)olume (大体量) 可从数百TB到数十数百PB、甚至EB的规模 速度快– (V)elocity(时效性) 很多大数据需要在一定的时间限度下得到及时处理 不限种类– (V)ariety(多样性) 大数据包括各种格式和形态的数据 统计分析，预测性– (V)alue(大价值) 大数据包含很多深度的价值,大数据分析挖掘和利用将带来巨大 的商业价值 真实性– (V)eracity(准确性) 处理的结果要保证一定的准确性 &nbsp; &nbsp; &nbsp; hadoop &nbsp; &nbsp; &nbsp;&nbsp; 开源,基于java开发,提供分布式基础架构, 特点:高可靠性、高扩展性、高效性、高容错性、低成本 • 2003年开始Google陆续发表了3篇论文 – GFS,MapReduce,BigTable • GFS – GFS是一个可扩展的分布式文件系统,用于大型的、分布式 的、对大量数据进行访问的应用 – 可以运行于廉价的普通硬件上,提供容错功能 • MapReduce – MapReduce是针对分布式并行计算的一套编程模型,由 Map和Reduce组成,Map是映射,把指令分发到多个 worker上,Reduce是规约,把worker计算出的结果合并 • BigTable – BigTable是存储结构化数据 – BigTable建立在GFS,Scheduler,Lock Service和 MapReduce之上 – 每个Table都是一个多维的稀疏图 &nbsp; • GFS、MapReduce和BigTable三大技术被称为 Google的三驾马车,虽然没有公布源码,但发布了 这三个产品的详细设计论 • Yahoo资助的Hadoop,是按照这三篇论文的开源 Java实现的,但在性能上Hadoop比Google要差很多 – GFS - - -&gt; HDFS – MapReduce - - -&gt; MapReduce – BigTable - - -&gt; HbaseHadoop组件 &nbsp; 组件 HDFS: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Hadoop分布式文件系统(核心组件) &nbsp; &nbsp; &nbsp;存储 MapReduce: 分布式计算框架(核心组件) Yarn: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 集群资源管理系统(核心组件) Zookeeper: &nbsp; 分布式协作服务 Hbase: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;分布式列存数据库 Hive: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 基于Hadoop的数据仓库 Sqoop: &nbsp; &nbsp; &nbsp; &nbsp; 数据同步工具 Pig: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 基于hadoop的数据流系统 Mahout: &nbsp; &nbsp; &nbsp; &nbsp;数据挖掘算法库 Flume: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;日志收集工具 &nbsp; &nbsp; &nbsp; &nbsp; HDFS 结构 &nbsp; Hadoop体系中数据存储管理的基础,是一个高度容错的系统,用于在低成本的通用硬件上运行 角色 client namenode secondarynode datanode • NameNode – Master节点,管理HDFS的名称空间和数据块映射信 息(fsimage),配置副本策略,处理所有客户端请求 • Secondary NameNode – 定期合并fsimage 和fsedits,推送给NameNode – 紧急情况下,可辅助恢复NameNode fsedits 变更日志(打补丁) • 但Secondary NameNode并非NameNode的热备 &nbsp; • DataNode – 数据存储节点,存储实际的数据 – 汇报存储信息给NameNode • Client – 切分文件 – 访问HDFS – 与NameNode交互,获取文件位置信息 – 与DataNode交互,读取和写入数据 block 每块缺省128MB大小,每块可以多个副本 &nbsp; &nbsp; MapReduce &nbsp; 结构 &nbsp; • 源自于Google的MapReduce论文,JAVA实现的分布式计算框架 • 角色和概念– JobTracker &nbsp;Master节点只有一个,管理所有作业/任务的监控、错误处理等 将任务分解成一系列任务,并分派给TaskTracker – TaskTracker Slave节点,一般是多台,运行Map Task和Reduce Task 并与JobTracker交互,汇报任务状态 – Map Task 解析每条数据记录,传递给用户编写的 map()并执行,将输出结果写入本地磁盘 – 如果为map-only作业,直接写入HDFS – Reducer Task 从Map Task的执行结果中,远程读 取输入数据,对数据进行排序,将数据按照分组传递 给用户编写的reduce函数执行 &nbsp; &nbsp; &nbsp; Yarn 结构 &nbsp; Yarn是Hadoop的一个通用的资源管理系统 角色 – Resourcemanager – 处理客户端请求 – 启动/监控ApplicationMaster – 监控NodeManager – 资源分配与调度 – Nodemanager – 单个节点上的资源管理 – 处理来自ResourceManager的命令 – 处理来自ApplicationMaster的命令 – ApplicationMaster – 数据切分 – 为应用程序申请资源,并分配给内部任务 – 任务监控与容错 – Container – 对任务运行行环境的抽象,封装了CPU 、内存等 – 多维资源以及环境变量、启动命令等任务运行相关的信息资源分配与调度 – Client – 用户与Yarn交互的客户端程序 – 提交应用程序、监控应用程序状态,杀死应用程序等 &nbsp; • Yarn的核心思想 • 将JobTracker和TaskTacker进行分离,它由下面几大构成组件 – ResourceManager一个全局的资源管理器 – NodeManager每个节点(RM)代理 – ApplicationMaster表示每个应用 – 每一个ApplicationMaster有多个Container在NodeManager上运行 &nbsp; &nbsp; • Hadoop的部署模式有三种 – 单机 – 伪分布式 – 完全分布式 单机模式 新虚拟机 192.168.5.61 node1 安装&nbsp;java-1.8.0-openjdk-devel [root@node1 ~]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#测试是否成功 1446 Jps &nbsp; [root@node1 ~]# tar -xf hadoop-2.7.7.tar.gz&nbsp; [root@node1 ~]# mv hadoop-2.7.7/ &nbsp; &nbsp; &nbsp; /usr/local/hadoop &nbsp; &nbsp;#保证所有者和所属组为root [root@node1 ~]# cd /usr/local/hadoop [root@node1 hadoop]# rpm -ql java-1.8.0-openjdk/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre/ &nbsp;截取前面相同的路径 [root@node1 hadoop]# ./bin/hadoop verion Error: JAVA_HOME is not set and could not be found. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#找不到java你装哪了,需定义变量指定 &nbsp; [root@node1 hadoop]# vim etc/hadoop/hadoop-env.sh&nbsp; # The java implementation to use. export JAVA_HOME=&quot;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre&quot; export HADOOP_CONF_DIR=&quot;/usr/local/hadoop/etc/hadoop&quot; [root@node1 hadoop]# ./bin/hadoop version &nbsp; &nbsp; [root@node1 ~]# cd /usr/local/hadoop/ [root@node1 hadoop]# mkdir input [root@node1 hadoop]# lsbin &nbsp;include &nbsp;lib &nbsp; &nbsp; &nbsp;LICENSE.txt &nbsp;&nbsp; README.txt &nbsp;shareetc &nbsp;input &nbsp; &nbsp;libexec &nbsp;NOTICE.txt &nbsp; output &nbsp;sbin [root@node1 hadoop]# cp *.txt input/ [root@node1 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount input output &nbsp; [root@node1 hadoop]# ls &nbsp;output/ part-r-00000 &nbsp;_SUCCESS &nbsp; &nbsp; &nbsp; &nbsp; HDFS分布式文件系统 需修改的配置文件 – Hadoop-env.sh JAVA_HOME HADOOP_CONF_DIR &nbsp; – xml文件配置格式 &lt;property&gt; &lt;name&gt;关键字&lt;/name&gt; &lt;value&gt;变量值&lt;/value&gt; &lt;description&gt; 描述 &lt;/description&gt; &lt;/property&gt; &nbsp; &nbsp; &nbsp; &nbsp; 环境准备 192.168.5.60 nn01 192.168.5.61 node1 &nbsp; &nbsp; 前面已配置 192.168.5.62 node2 192.168.5.63 node3 &nbsp; 修改 &nbsp;/etc/hosts # ::1&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 &nbsp; &nbsp; &nbsp; 全部装 &nbsp; &nbsp;&nbsp;java-1.8.0-openjdk-devel selinux关闭,禁用firewalld 所有机器能被nn01无密码ssh &nbsp;ssh-keygen &nbsp;生成密钥, ssh-copy-id &nbsp; nn01/node1/node2/node3 &nbsp; [root@nn01 ~]# vim /etc/ssh/ssh_config&nbsp; Host * &nbsp; &nbsp; &nbsp; &nbsp; StrictHostKeyChecking no &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;设置第一次ssh时不需要输入yes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; https://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-common/core-default.xml tar -xf hadoop-2.7.7.tar.gz &nbsp;chown -R root:root &nbsp;hadoop-2.7.7/ mv hadoop-2.7.7/ &nbsp;/usr/local/hadoop cd &nbsp;/usr/local/hadoop [root@nn01 hadoop]# vim &nbsp;etc/hadoop/core-site.xml &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;核心配置文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;文件系统配置参数 &lt;value&gt;hdfs://nn01:9000&lt;/value&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;数据根目录配置参数 &lt;value&gt;/var/hadoop&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &nbsp; [root@nn01 hadoop]# &nbsp;vim etc/hadoop/hdfs-site.xml&nbsp; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 地址声明(谁是namenode) &lt;value&gt;nn01:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 地址声明(SecondaryNameNode) &lt;value&gt;nn01:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 文件冗余份数() &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [root@nn01 hadoop]# cd etc/hadoop [root@nn01 hadoop]# vim slaves &nbsp; &nbsp; &nbsp;#只写DataNode节点的主机名称 node1 node2 node3 &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# vim etc/hadoop/hadoop-env.sh&nbsp; export JAVA_HOME=&quot;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre&quot; export HADOOP_CONF_DIR=&quot;/usr/local/hadoop/etc/hadoop&quot; Hadoop所有节点的配置参数完全一样,在一台配置好 后,把配置文件同步到其它所有主机上 &nbsp; rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node1:/usr/local/ rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node2:/usr/local/ rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node3:/usr/local/ &nbsp; [root@nn01 hadoop]# cd /usr/local/hadoop/ [root@nn01 hadoop]# mkdir /var/hadoop [root@nn01 hadoop]# ./bin/hdfs namenode -format [root@nn01 hadoop]# ./sbin/start-dfs.sh&nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# jps 30871 SecondaryNameNode 30680 NameNode 31017 Jps &nbsp; &nbsp; [root@node1 logs]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;其他节点上查看 1093 Jps 998 DataNode &nbsp; &nbsp; &nbsp; 核心手段:拆() 不同服务合在一台机器上 &nbsp; 主机 角色 软件 192.168.1.60 Master NameNode SecondaryNameNode ResourceManager HDFS YARN 192.168.1.61 node1 DataNode NodeManager HDFS YARN 192.168.1.62 node2 DataNode NodeManager HDFS YARN 192.168.1.63 node3 DataNode NodeManager HDFS YARN &nbsp; [root@nn01 hadoop]# cd &nbsp;/usr/local/hadoop/etc/hadoop &nbsp; [root@nn01 hadoop]# mv mapred-site.xml.template &nbsp; mapred-site.xml[root@nn01 hadoop]# vim mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &nbsp; [root@nn01 hadoop]# vim yarn-site.xml&nbsp; &lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;nn01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &nbsp; &nbsp; [root@nn01 hadoop]# for i in node1 node2 node3; do rsync -avXSH --delete /usr/local/hadoop &nbsp;${i}:/usr/local; done [root@nn01 hadoop]# ./sbin/start-yarn.sh&nbsp; [root@nn01 hadoop]# ./bin/yarn node -list 19/06/21 09:54:01 INFO client.RMProxy: Connecting to ResourceManager at nn01/192.168.5.60:8032 Total Nodes:3 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Node-Id&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; Node-State&nbsp;&nbsp; &nbsp;Node-Http-Address&nbsp;&nbsp; &nbsp;Number-of-Running-Containers &nbsp; &nbsp; &nbsp;node1:38135&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node1:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp;node3:39287&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node3:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp;node2:33541&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node2:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; [root@nn01 hadoop]# jps 1684 NameNode 1876 SecondaryNameNode 2775 Jps 2206 ResourceManager &nbsp; &nbsp; [root@node1 ~]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 914 DataNode 1474 NodeManager 1634 Jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; http://192.168.5.60:8088/&nbsp; &nbsp; resourcemanager &nbsp;WEB页面(nn01) http://192.168.5.61:8042/&nbsp; &nbsp; &nbsp; nodemanager WEB页面(node1 node2 node3) http://192.168.5.60:50090/&nbsp; &nbsp; &nbsp; &nbsp;secondory &nbsp; namenode WEB页面 &nbsp; http://192.168.5.60:50070/&nbsp; &nbsp; &nbsp; &nbsp; namenode &nbsp;WEB页面&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; http://192.168.5.61:50075/&nbsp; &nbsp; &nbsp; &nbsp;datanode(node1 &nbsp;node2 node3) &nbsp;WEB页面 &nbsp; [root@nn01 hadoop]# ./bin/hadoop fs -ls / &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# ./bin/hadoop fs -mkdir /abc [root@nn01 hadoop]# ./bin/hadoop fs &nbsp;-ls / [root@nn01 hadoop]# ./bin/hadoop fs -touchz /abc/f1 &nbsp; &nbsp; &nbsp;创建 [root@nn01 hadoop]# ./bin/hadoop fs -put &nbsp;*.txt &nbsp;/abc &nbsp; &nbsp;&nbsp;上传文件 [root@nn01 hadoop]# ./bin/hadoop fs -get /abc/f1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 下载文件 [root@nn01 hadoop]# ls f1&nbsp; &nbsp; hdfs安全模式相关命令(不是必要操作) hdfs &nbsp; dfsadmin -report &nbsp; &nbsp;查看 hadoop &nbsp;dfsadmin &nbsp;safemode &nbsp;leave &nbsp;强制namenode退出安全模式 &nbsp; &nbsp; [root@nn01 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar &nbsp;wordcount &nbsp;/abc/* &nbsp; /bcd &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#提交分析作业 [root@nn01 hadoop]# ./bin/hadoop fs -cat /bcd/* &nbsp; &nbsp; &nbsp;#查看结果 &nbsp; &nbsp; 扩容 新增192.168.5.64 &nbsp; &nbsp;newnode – 启动一个新的系统,设置SSH免密码登录 [root@nn01 #] ssh-copy-id newnode – 在所有节点修改 /etc/hosts,增加新节点的主机信息 vim /etc/hosts # ::1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1 &nbsp; &nbsp; &nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 192.168.5.64 newnode &nbsp; ]# &nbsp;for i in node{1..3}; do scp /etc/hosts &nbsp; ${i}:/etc/; done &nbsp;]# scp /etc/hosts &nbsp; newnode:/etc/ – 安装java运行环境(java-1.8.0-openjdk-devel) – 修改NameNode的slaves文件增加该节点 [root@nn01 hadoop]# vim /usr/local/hadoop/etc/hadoop/ node1 node2 node3 newnode #] &nbsp;for i in node1 node2 node3 ; do scp ./slaves &nbsp;${i}:/usr/local/hadoop/etc/hadoop; done – 拷贝NamNode的/usr/local/hadoop到本机 #] &nbsp; scp -r /usr/local/hadoop &nbsp; newnode:/usr/local – 在该节点启动DataNode ./sbin/hadoop-daemon.sh start datanode &nbsp; – 设置同步带宽,并同步数据 # ./bin/hdfs dfsadmin -setBalancerBandwidth 60000000 # ./sbin/start-balancer.sh – 查看集群状态 # ./bin/hdfs dfsadmin -report &nbsp; HDFS 修复节点比较简单,与增加节点基本一致 --注意:新节点的ip和主机名要与损坏节点的一致 --启动服务: &nbsp; /usr/local/hadoop/sbin/hadoop-daemon.sh &nbsp;start datanode --数据恢复是自动的 --上线以后会自动恢复数据,如果数据量非常巨大,可能需要一段时间 &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# cd &nbsp;/usr/local/hadoop/ [root@nn01 hadoop]#&nbsp;&nbsp;./bin/hadoop fs -put /root/CentOS7-1804.iso &nbsp;/bcd [root@nn01 ~]# vim etc/hadoop/hdfs-site.xml&nbsp; &lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt; &lt;/property&gt; &nbsp; [root@nn01 hadoop]# ./bin/hdfs dfsadmin -report &nbsp; &nbsp; NFS网关 • NFS 网关用途 – 用户可以通过操作系统兼容的本地NFSv3客户端来浏 览HDFS文件系统 – 用户可以从HDFS文件系统下载文档到本地文件系统 – 用户可以通过挂载点直接流化数据,支持文件附加, 但是不支持随机写 – NFS网关支持NFSv3和允许HDFS作为客户端文件系统 的一部分被挂载 &nbsp; &nbsp; 新机器192.168.5.65 &nbsp;nfsgw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 192.168.5.66 &nbsp;localhost 添加/etc/hosts,然后同步给其他集群主机 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 192.168.5.64 newnode 192.168.5.65 nfsgw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 安装&nbsp;&nbsp;java-1.8.0-openjdk-devel 卸载rpcbind &nbsp;nfs-utils nfsgw服务器和nn01上配置用户 ]# useradd nfsgw ; useradd nn01 ]# groupadd -g 800 nfsuser ]# &nbsp;useradd &nbsp;-u &nbsp;800 -g 800 -r -d /var/hadoop &nbsp;nfsuser &nbsp; [root@nn01 hadoop]# ./sbin/stop-all.sh &nbsp; &nbsp; 停止集群 &nbsp; &nbsp; &nbsp; [root@nn01 hadoop]# vim etc/hadoop/core-site.xml &lt;property&gt; &lt;name&gt;hadoop.proxyuser.nfsuser.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.nfsuser.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &nbsp; 同步/usr/local/hadoop给其他集群主机(node1,node2,node3) 启动集群 jps &nbsp; hdfs &nbsp;dfsadmin &nbsp;-report &nbsp; [root@nfsgw ~]# rsync -aSH --delete &nbsp;nn01:/usr/local/hadoop &nbsp;/usr/local &nbsp; [root@nfsgw ~]# vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml&nbsp; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;nfs.exports.allowed.hosts&lt;/name&gt; &lt;value&gt;* rw&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;nfs.dump.dir&lt;/name&gt; &lt;value&gt;/var/nfstmp&lt;/value&gt; &lt;/property&gt; &nbsp; &lt;/configuration&gt; &nbsp; [root@nfsgw ~]# mkdir /var/nfstmp [root@nfsgw ~]# chown 800.800 &nbsp;/var/nfstmp [root@nfsgw ~]# ls -ld /var/nfstmp drwxr-xr-x 2 nfsuser nfsuser 6 6月 &nbsp;21 16:47 /var/nfstmp &nbsp; [root@nfsgw ~]# cd /usr/local/hadoop/ [root@nfsgw hadoop]# rm -rf logs/* [root@nfsgw hadoop]# setfacl -m nfsuser:rwx logs/ [root@nfsgw hadoop]# ./sbin/hadoop-daemon.sh --script ./bin/hdfs start portmap &nbsp; 使用root启动portmap服务 [root@nfsgw hadoop]# sudo -u nfsuser ./sbin/hadoop-daemon.sh --script &nbsp;./bin/hdfs start nfs3 &nbsp; &nbsp;使用代理用户启动nfs3 [root@nfsgw hadoop]# jps 23659 Nfs3 23596 Portmap 23710 Jps &nbsp; &nbsp; pormap(root用户先起服务,停的话后停,先停nfs) nfs3(nfsuser用户后起服务,) atime &nbsp;访问的时间 ctmie &nbsp;改变的时间 mtime &nbsp;修改的时间 &nbsp; 客户端192.168.5.66测试 [root@localhost ~]# yum -y install nfs-utils [root@localhost ~]# mount -t nfs -o vers=3,proto=tcp,noatime,nolock,sync,noacl 192.168.5.65:/ &nbsp;/mnt [root@localhost ~]# ls /mnt &nbsp; &nbsp; zookeeper 开源的分布式应用程序协调服务 保证数据在集群间的事务一致性 &nbsp; 应用场景 – 集群分布式锁 – 集群统一命名服务 – 分布式协调服务 &nbsp; &nbsp; &nbsp; • Zookeeper角色与特性 – Leader:接受所有Follower的提案请求并统一协调发起 提案的投票,负责与所有的Follower进行内部数据交换 – Follower:直接为客户端服务并参与提案的投票,同时 与Leader进行数据交换 – Observer:直接为客户端服务但并不参与提案的投票, 同时也与Leader进行数据交换 &nbsp; • Zookeeper角色与选举 – 服务在启动的时候是没有角色的(LOOKING) – 角色是通过选举产生的 – 选举产生一个Leader,剩下的是Follower • 选举Leader原则 – 集群中超过半数机器投票选择Leader – 假如集群中拥有n台服务器,那么Leader必须得到 n/2+1台服务器的投票 &nbsp; &nbsp; • Zookeeper角色与选举 – 如果Leader死亡,重新选举Leader – 如果死亡的机器数量达到一半,则集群挂掉 – 如果无法得到足够的投票数量,就重新发起投票,如果参与投票的机器不足n/2+1,则集群停止工作 – 如果Follower死亡过多,剩余机器不足n/2+1,则集群也会停止工作 – Observer不计算在投票总设备数量里面 &nbsp; &nbsp; • Zookeeper可伸缩扩展性原理与设计 – Leader所有写相关操作 – Follower读操作与响应Leader提议 – 在Observer出现以前,Zookeeper的伸缩性由Follower 来实现,我们可以通过添加Follower节点的数量来保证 Zookeeper服务的读性能,但是随着Follower节点数量 的增加,Zookeeper服务的写性能受到了影响 &nbsp; • Zookeeper可伸缩扩展性原理与设计 – 客户端提交一个请求,若是读请求,则由每台Server的本地 副本数据库直接响应。若是写请求,需要通过一致性协议 (Zab)来处理 – Zab协议规定:来自Client的所有写请求都要转发给ZK服务 中唯一的Leader,由Leader根据该请求发起一个Proposal。 然后其他的Server对该Proposal进行Vote。之后Leader对 Vote进行收集,当Vote数量过半时Leader会向所有的 Server发送一个通知消息。最后当Client所连接的Server收 到该消息时,会把该操作更新到内存中并对Client的写请求 做出回应 &nbsp; – ZooKeeper在上述协议中实际扮演了两个职能。一方面从 客户端接受连接与操作请求,另一方面对操作结果进行投票。 这两个职能在Zookeeper集群扩展的时候彼此制约 – 从Zab协议对写请求的处理过程中可以发现,增加Follower 的数量,则增加了协议投票过程的压力。因为Leader节点 必须等待集群中过半Server响应投票,是节点的增加使得部 分计算机运行较慢,从而拖慢整个投票过程的可能性也随之 提高,随着集群变大,写操作也会随之下降 &nbsp; – 所以,我们不得不在增加Client数量的期望和我们希望保 持较好吞吐性能的期望间进行权衡。要打破这一耦合关系, 我们引入了不参与投票的服务器Observer。Observer可 以接受客户端的连接,并将写请求转发给Leader节点。但 Leader节点不会要求Observer参加投票,仅仅在上述第3 歩那样,和其他服务节点一起得到投票结果 &nbsp; – Observer的扩展,给Zookeeper的可伸缩性带来了全 新的景象。加入很多Observer节点,无须担心严重影 响写吞吐量。但并非是无懈可击,因为协议中的通知 阶段,仍然与服务器的数量呈线性关系。但是这里的 串行开销非常低。因此,我们可以认为在通知服务器 阶段的开销不会成为瓶颈 – Observer提升读性能的可伸缩性 – Observer提供了广域网能力 &nbsp; &nbsp; &nbsp;[root@nn01 ~]# tar -xf zookeeper-3.4.13.tar.gz&nbsp; [root@nn01 ~]# mv &nbsp;~/zookeeper-3.4.13 &nbsp; /usr/local/zookeeper [root@nn01 ~]# cd /usr/local/zookeeper/ [root@nn01 zookeeper]#cd conf/ [root@nn01 conf]# mv zoo_sample.cfg &nbsp;zoo.cfg [root@nn01 conf]# vim zoo.cfg server.1=node1:2888:3888 server.2=node2:2888:3888 server.3=node3:2888:3888 server.4=nn01:2888:3888:observer &nbsp; &nbsp;scp -r /usr/local/zookeeper &nbsp; node1:/usr/local &nbsp;scp -r /usr/local/zookeeper &nbsp; node2:/usr/local &nbsp;scp -r /usr/local/zookeeper &nbsp; node3:/usr/local &nbsp; &nbsp; mkdir /tmp/zookeeper &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #所有集群上创建 node1]# echo &nbsp;1 &gt; /tmp/zookeeper/myid &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;请确保每个server的myid文件中id数字不同,server.id中的id与myid中的id必须一致&nbsp; &nbsp; node2]# echo &nbsp;2 &gt; /tmp/zookeeper/myid &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;id的范围是1~255 node3]# echo &nbsp;3 &gt; /tmp/zookeeper/myid nn01]# &nbsp; echo 4 &nbsp;&gt; /tmp/zookeeper/myid &nbsp; &nbsp; 启动集群,查看验证(在所有集群节点执行) /usr/local/zookeeper/bin/zkServer.sh &nbsp;start /usr/local/zookeeper/bin/zkServer.sh status &nbsp; &nbsp; &nbsp; &nbsp;#刚启动一个查看状态为不运行的,因为还需投票,等全部起完了就运行了 &nbsp; &nbsp; Zookeeper管理文档 http://zookeeper.apache.org/doc/r3.4.10/zookeeperAdmin.html man &nbsp; bash &nbsp;---&gt; &nbsp; &nbsp; 搜索/dev/tcp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/fd/fd &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If fd is a valid integer, file descriptor fd is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stdin &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 0 is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stdout &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 1 is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stderr &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 2 is duplicated. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/tcp/host/port &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If &nbsp;host &nbsp;is &nbsp;a valid hostname or Internet address, and port is an integer port number or service name, bash &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;attempts to open a TCP connection to the corresponding socket. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/udp/host/port &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If host is a valid hostname or Internet address, and port is an integer port number or &nbsp;service &nbsp;name, &nbsp;bash &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;attempts to open a UDP connection to the corresponding socket. &nbsp; [root@nn01 ~]# vim zkstats &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #检测zookeeper状态脚本 #!/bin/bash function getzkstat(){ &nbsp; &nbsp; exec 2&gt;/dev/null &nbsp; &nbsp; exec 8&lt;&gt;/dev/tcp/$1/2181 &nbsp; &nbsp; echo stat &gt;&amp;8 &nbsp; &nbsp; Msg=$(cat &lt;&amp;8 |grep -P &quot;^Mode:&quot;) &nbsp; &nbsp; echo -e &quot;$1\\t${Msg:-Mode: \\x1b[31mNULL\\x1b[0m}&quot; &nbsp; &nbsp; exec 8&lt;&amp;- } if (( $# == 0 ));then &nbsp; &nbsp; echo &quot;${0##*/} zk1 zk2 zk3 ... ...&quot; else &nbsp; &nbsp; for i in $@;do &nbsp; &nbsp; &nbsp; &nbsp; getzkstat ${i} &nbsp; &nbsp; done fi [nn01 ~]# &nbsp;./zkstats &nbsp;node{1..3} &nbsp; kafka &nbsp; &nbsp; – Kafka是由LinkedIn开发的一个分布式的消息系统 – Kafka是使用Scala编写 – Kafka是一种消息中间件 • 为什么要使用Kafka – 解耦、冗余、提高扩展性、缓冲 – 保证顺序,灵活,削峰填谷 – 异步通信 &nbsp; &nbsp; • Kafka角色与集群结构 – producer:生产者,负责发布消息 – consumer:消费者,负责读取处理消息 – topic:消息的类别 – Parition:每个Topic包含一个或多个Partition – Broker:Kafka集群包含一个或多个服务器 • Kafka通过Zookeeper管理集群配置,选举Leader &nbsp; • Kafka集群的安装配置 – Kafka集群的安装配置依赖Zookeeper,搭建Kafka集群之前,请先创建好一个可用的Zookeeper集群 – 安装OpenJDK运行环境 – 同步Kafka拷贝到所有集群主机 – 修改配置文件 – 启动与验证 &nbsp; &nbsp; &nbsp; vim /usr/local/kafka/config/server.properties&nbsp; broker.id=1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#每台服务器的broker.id都不能相同 zookeeper.connect=node1:2181,node2:2181,node3:2181 &nbsp;#zookeeper集群地址,不用都列出,写一部分即可 [root@nn01 kafka]# for i in node{1..3}; do scp -r /usr/local/kafka &nbsp; ${i}:/usr/local &nbsp;; done &nbsp;/usr/local/kafka/bin/kafka-server-start.sh -daemon &nbsp;/usr/local/kafka/config/server.properties &nbsp; &nbsp; &nbsp;#所有主机启动服务 – jps命令应该能看到Kafka模块 – netstat应该能看到9092在监听 [root@node1 ~]# /usr/local/kafka/bin/kafka-topics.sh --create --partitions 2 --replication-factor 2 --zookeeper localhost:2181 --topic mymsg &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#创建一个topic Created topic &quot;mymsg&quot;. &nbsp; [root@node2 ~]# /usr/local/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mymsg &nbsp; &nbsp; &nbsp; &nbsp; #生产者 发布消息,随便打一些字符串 [root@node3 kafka]# /usr/local/kafka/bin/kafka-console-consumer.sh &nbsp;--bootstrap-server localhost:9092 --topic mymsg &nbsp; #消费者 接收消息,上面打的字符串会在这同步显示 &nbsp; &nbsp; &nbsp; Haddop高可用 &nbsp; &nbsp; 为什么需要NameNode • 原因 – NameNode是HDFS的核心配置,HDFS又是Hadoop核心组件,NameNode在Hadoop集群中至关重要 – NameNode宕机,将导致集群不可用,如果NameNode数据丢失将导致整个集群的数据丢失,而NameNode的数据的更新又比较频繁,实现NameNode高可用势在必行 &nbsp; • 官方提供了两种解决方案 – HDFS with NFS – HDFS with QJM • 两种方案异同NFS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; QJMNN &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; NN ZK &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; ZK ZKFailoverController &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;ZKFailoverController NFS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp;&nbsp; JournalNode &nbsp; • HA方案对比 – 都能实现热备 – 都是一个Active NN和一个Standby NN – 都使用Zookeeper和ZKFC来实现自动失效恢复 – 失效切换都使用Fencin配置的方法来Active NN – NFS数据共享变更方案把数据存储在共享存储里,我们还需要考虑NFS的高可用设计 – QJM不需要共享存储,但需要让每一个DN都知道两个NN的位置,并把块信息和心跳包发送给Active和Standby这两个NN &nbsp; • 使用原因(QJM) – 解决NameNode单点故障问题 – Hadoop给出了HDFS的高可用HA方案:HDFS通常由两 个NameNode组成,一个处于Active状态,另一个处于 Standby状态。Active NameNode对外提供服务,比如 处理来自客户端的RPC请求,而Standby NameNode则 不对外提供服务,仅同步Active NameNode的状态,以 便能够在它失败时进行切换 • 典型的HA集群 – NameNode会被配置在两台独立的机器上,在任何时 候 , 一 个 NameNode 处 于 活 动 状 态 , 而 另 一 个 NameNode则处于备份状态 – 活动状态的NameNode会响应集群中所有的客户端, 备份状态的NameNode只是作为一个副本,保证在必 要的时候提供一个快速的转移 &nbsp; • NameNode高可用架构 – 为了让Standby Node与Active Node保持同步,这两个 Node 都 与 一 组 称 为 JNS 的 互 相 独 立 的 进 程 保 持 通 信 (Journal Nodes)。当Active Node更新了namespace, 它将记录修改日志发送给JNS的多数派。Standby Node将 会从JNS中读取这些edits,并持续关注它们对日志的变更 – Standby Node将日志变更应用在自己的namespace中, 当Failover发生时,Standby将会在提升自己为Active之前, 确保能够从JNS中读取所有的edits,即在Failover发生之前 Standy持有的namespace与Active保持完全同步 – NameNode更新很频繁,为了保持主备数据的一致性, 为了支持快速Failover,Standby Node持有集群中 blocks的最新位置是非常必要的。为了达到这一目的, DataNodes上需要同时配置这两个Namenode的地址, 同时和它们都建立心跳连接,并把block位置发送给它们 &nbsp; – 任何时刻,只能有一个Active NameNode,否则会导致 集群操作混乱,两个NameNode将会有两种不同的数据 状态,可能会导致数据丢失或状态异常,这种情况通常 称为&quot;split-brain&quot;(脑裂,三节点通讯阻断,即集群中不 同的DataNode看到了不同的Active NameNodes) – 对于JNS而言,任何时候只允许一个NameNode作为 writer;在Failover期间,原来的Standby Node将会接 管Active的所有职能,并负责向JNS写入日志记录,这种 机制阻止了其他NameNode处于Active状态的问题 &nbsp; 系统规划: &nbsp; 新准备机器 192.168.5.66 nn02 免密码登录 [root@nn02 ~]# scp -r 192.168.5.60:/root/.ssh &nbsp; &nbsp;/root [root@nn02 ~]# cd ./.ssh [root@nn02 .ssh]# ls authorized_keys &nbsp;id_rsa &nbsp;id_rsa.pub &nbsp;known_hosts [root@nn02 .ssh]# rm &nbsp;-rf known_hosts&nbsp; [root@nn01 ~]# vim &nbsp;/etc/hosts # ::1&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost4 localhost4.localdomain4 192.168.5.60 nn01 192.168.5.61 node1 192.168.5.62 node2 192.168.5.63 node3 192.168.5.65 nfsgw 192.168.5.66 nn02 ]# &nbsp;for i in 61 62 63 66; do scp &nbsp;/etc/hosts &nbsp; &nbsp;192.168.5.$i:/etc; done &nbsp; &nbsp; 关机重启(全部) 再开启zookeeper /usr/local/zookeeper/bin/zkServer.sh start &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #node{1..3},nn01都起zookeeper服务 ]# jps &nbsp; [root@nn01 ~]# rm -rf /var/hadoop/* [root@nn01 ~]# ssh node1 rm -rf /var/hadoop/* [root@nn01 ~]# ssh node2 rm -rf /var/hadoop/* [root@nn01 ~]# ssh node3 rm -rf /var/hadoop/* [root@nn01 ~]# rm -rf /usr/local/hadoop/logs/* &nbsp; &nbsp; [root@nn02 ~]# mkdir /var/hadoop &nbsp; &nbsp; yarn高可用 • ResourceManager高可用 – RM的高可用原理与NN一样,需要依赖ZK来实现,这里配置文件的关键部分,感兴趣的同学可以自己学习和测试 – yarn.resourcemanager.hostname – 同理因为使用集群模式,该选项应该关闭 &nbsp; [root@nn01 hadoop]# cd&nbsp;&nbsp;/usr/local/hadoop/etc/hadoop/ 0. &nbsp;[root@nn01 hadoop]# vim hadoop-env.sh&nbsp; export JAVA_HOME=&quot;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre&quot; export HADOOP_CONF_DIR=&quot;/usr/local/hadoop/etc/hadoop&quot; &nbsp; &nbsp; 1. &nbsp;[root@root hadoop]# vim core-site.xml&nbsp; &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;fs.defaultFS&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;hdfs://nsd1902&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/var/hadoop&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;node1:2181,node2:2181,node3:2181&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.proxyuser.nfsuser.groups&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;*&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.proxyuser.nfsuser.hosts&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;*&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; ~ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2. &nbsp;vim hdfs-site.xml&nbsp; &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.nameservices&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#指定hdfs的nameservices名称 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nsd1902&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.namenodes.nsd1902&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp;#指定集群的两个namenode的名称分别为nn1,nn2 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn1,nn2&lt;/value&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #这里不是指主机名 &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.rpc-address.nsd1902.nn1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置nn1,nn2的rpc通信端口 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01:8020&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.rpc-address.nsd1902.nn2&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02:8020&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.http-address.nsd1902.nn1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置nn1,nn2的http通信端口 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01:50070&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.http-address.nsd1902.nn2&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02:50070&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;#指定namenode元数据存储在journalnode中的路径 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;qjournal://node1:8485;node2:8485;node3:8485/nsd1902&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#指定journalnode日志文件存储路径 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/var/hadoop/journal&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.client.failover.proxy.provider.nsd1902&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; #指定HDFS客户端连接Active NameNode的java类 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置隔离机制为ssh &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;sshfence&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; #指定密钥的位置 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #开启自动故障转移 &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.replication&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;2&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; &nbsp; 3. &nbsp;vim mapred-site.xml &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;yarn&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; &nbsp; &nbsp; 4. &nbsp;vim slaves node1 node2 node3 &nbsp; &nbsp; 5. &nbsp; vim yarn-site.xml&nbsp; &lt;configuration&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;rm1,rm2&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;node1:2181,node2:2181,node3:2181&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;yarn-ha&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;!-- Site specific YARN configuration properties --&gt; &nbsp; &nbsp; &lt;property&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &nbsp; &nbsp; &lt;/property&gt; &lt;/configuration&gt; &nbsp; 6. &nbsp; vim &nbsp;exclude &nbsp; 把里面的内容删除 &nbsp; &nbsp; 集群初始化: &nbsp; – ALL: 代表所有机器 – nodeX: 代表node1 &nbsp;node2 &nbsp;node3 &nbsp;(标示方便全部一起操作) &nbsp; – ALL: 同步配置到所有集群机器 for i in node{1..3} nn02; do scp -r /usr/local/hadoop &nbsp;${i}:/usr/local &amp; done – NN1: 初始化ZK集群 # ./bin/hdfs zkfc -formatZK – nodeX: 启动journalnode服务 # ./sbin/hadoop-daemon.sh start journalnode &nbsp; • 初始化 – NN1: 格式化 # ./bin/hdfs namenode -format – NN2: 数据同步到本地/var/hadoop/dfs # rsync -aSH nn01:/var/hadoop/dfs /var/hadoop/ – NN1: 初始化JNS # ./bin/hdfs namenode -initializeSharedEdits – nodeX: 停止journalnode服务 # ./sbin/hadoop-daemon.sh stop journalnode &nbsp; &nbsp; &nbsp; • 启动集群 – NN1: 启动hdfs # ./sbin/start-dfs.sh – NN1: 启动yarn # ./sbin/start-yarn.sh – NN2: 启动热备ResourceManager # ./sbin/yarn-daemon.sh start resourcemanager &nbsp; &nbsp; &nbsp; • 查看集群状态 – 获取NameNode状态 # ./bin/hdfs haadmin -getServiceState nn1 # ./bin/hdfs haadmin -getServiceState nn2 – 获取ResourceManager状态 # ./bin/yarn rmadmin -getServiceState rm1 # ./bin/yarn rmadmin -getServiceState rm2 &nbsp; &nbsp; – 获取节点信息 # ./bin/hdfs dfsadmin -report &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#查看有3台集群节点 # ./bin/yarn node -list – 访问集群文件 # ./bin/hadoop fs -mkdir /input # ./bin/hadoop fs -ls hdfs://mycluster/ – 主从切换Activate # ./sbin/hadoop-daemon.sh stop namenode &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(把Activate的那台namenode停掉服务) &nbsp; ./bin/hadoop fs -put *.txt &nbsp;/input &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;./sbin/hadoop-daemon.sh start&nbsp;namenode ./bin/hadoop fs &nbsp;-ls /input &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/06/23/793974.html","headline":"大数据hadoop","dateModified":"2019-06-23T00:00:00+08:00","datePublished":"2019-06-23T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/06/23/793974.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>大数据hadoop</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>大数据的由来<br> • 大数据<br> – 随着计算机技术的发展,互联网的普及,信息的积累<br> 已经到了一个非常庞大的地步,信息的增长也在不断<br> 的加快,随着互联网、物联网建设的加快,信息更是<br> 爆炸是增长,收集、检索、统计这些信息越发困难,<br> 必须使用新的技术来解决这些问题什么是大数据<br> • 大数据的定义<br> – 大数据指无法在一定时间范围内用常规软件工具进行捕捉、<br> 管理和处理的数据集合,需要新处理模式才能具有更强的<br> 决策力、洞察发现力和流程优化能力的海量、高增长率和<br> 多样化的信息资产<br> – 是指从各种各样类型的数据中,快速获得有价值的信息</p> 
  <p>&nbsp;</p> 
  <p>• 大数据能做什么<br> – 企业组织利用相关数据分析帮助他们降低成本、提高<br> 效率、开发新产品、做出更明智的业务决策等<br> – 把数据集合并后进行分析得出的信息和数据关系性,<br> 用来察觉商业趋势、判定研究质量、避免疾病扩散、<br> 打击犯罪或测定即时交通路况等<br> – 大规模并行处理数据库,数据挖掘电网,分布式文件<br> 系统或数据库,云计算平和可扩展的存储系统等</p> 
  <p>&nbsp;</p> 
  <h3>特性</h3> 
  <ol>
   <li>大体量– (V)olume (大体量)<br> 可从数百TB到数十数百PB、甚至EB的规模</li> 
   <li>速度快– (V)elocity(时效性)<br> 很多大数据需要在一定的时间限度下得到及时处理</li> 
   <li>不限种类– (V)ariety(多样性)<br> 大数据包括各种格式和形态的数据</li> 
   <li>统计分析，预测性– (V)alue(大价值)<br> 大数据包含很多深度的价值,大数据分析挖掘和利用将带来巨大<br> 的商业价值</li> 
   <li>真实性– (V)eracity(准确性)<br> 处理的结果要保证一定的准确性</li> 
  </ol>
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h1>hadoop &nbsp; &nbsp; &nbsp;&nbsp;</h1> 
  <p>开源,基于java开发,提供分布式基础架构, 特点:高可靠性、高扩展性、高效性、高容错性、低成本</p> 
  <p>• 2003年开始Google陆续发表了3篇论文<br> – GFS,MapReduce,BigTable</p> 
  <p><strong>• GFS</strong><br> – GFS是一个可扩展的分布式文件系统,用于大型的、分布式<br> 的、对大量数据进行访问的应用<br> – 可以运行于廉价的普通硬件上,提供容错功能</p> 
  <p><br><strong>• MapReduce</strong><br> – MapReduce是针对分布式并行计算的一套编程模型,由<br> Map和Reduce组成,Map是映射,把指令分发到多个<br> worker上,Reduce是规约,把worker计算出的结果合并</p> 
  <p><strong>• BigTable</strong><br> – BigTable是存储结构化数据<br> – BigTable建立在GFS,Scheduler,Lock Service和<br> MapReduce之上<br> – 每个Table都是一个多维的稀疏图</p> 
  <p>&nbsp;</p> 
  <p><strong>• GFS、MapReduce和BigTable</strong>三大技术被称为<br> Google的三驾马车,虽然没有公布源码,但发布了<br> 这三个产品的详细设计论<br> • Yahoo资助的Hadoop,是按照这三篇论文的开源<br> Java实现的,但在性能上Hadoop比Google要差很多<br> – GFS - - -&gt; HDFS<br> – MapReduce - - -&gt; MapReduce<br> – BigTable - - -&gt; HbaseHadoop组件</p> 
  <p>&nbsp;</p> 
  <h3>组件</h3> 
  <ul>
   <li><span style="color:#f33b45;">HDFS: &nbsp; &nbsp;</span> &nbsp; &nbsp; &nbsp; Hadoop分布式文件系统(核心组件) &nbsp; &nbsp; &nbsp;存储</li> 
   <li><span style="color:#f33b45;">MapReduce: </span>分布式计算框架(核心组件)</li> 
   <li><span style="color:#f33b45;">Yarn: </span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 集群资源管理系统(核心组件)</li> 
   <li>Zookeeper: &nbsp; 分布式协作服务</li> 
   <li>Hbase: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;分布式列存数据库</li> 
   <li>Hive: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 基于Hadoop的数据仓库</li> 
   <li>Sqoop: &nbsp; &nbsp; &nbsp; &nbsp; 数据同步工具</li> 
   <li>Pig: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 基于hadoop的数据流系统</li> 
   <li>Mahout: &nbsp; &nbsp; &nbsp; &nbsp;数据挖掘算法库</li> 
   <li>Flume: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;日志收集工具</li> 
  </ul>
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h3>&nbsp;</h3> 
  <h3><span style="color:#3399ea;">HDFS</span></h3> 
  <p>结构</p> 
  <p><img alt="" class="has" height="405" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190621173846965.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rvbmd6aHVvMTIyMA==,size_16,color_FFFFFF,t_70" width="665"></p> 
  <p>&nbsp;</p> 
  <p>Hadoop体系中数据存储管理的基础,是一个高度容错的系统,用于在低成本的通用硬件上运行</p> 
  <blockquote> 
   <ul>
    <li><strong>角色</strong></li> 
   </ul>
   <p>client</p> 
   <p>namenode</p> 
   <p>secondarynode</p> 
   <p>datanode</p> 
  </blockquote> 
  <p>• <strong>NameNode</strong><br> – Master节点,管理HDFS的名称空间和数据块映射信<br> 息(fsimage),配置副本策略,处理所有客户端请求</p> 
  <p><br><strong>• Secondary NameNode</strong><br> – 定期合并fsimage 和fsedits,推送给NameNode<br> – 紧急情况下,可辅助恢复NameNode</p> 
  <p>fsedits 变更日志(打补丁)<br> • 但Secondary NameNode并非NameNode的热备</p> 
  <p>&nbsp;</p> 
  <p><strong>• DataNode</strong><br> – 数据存储节点,存储实际的数据<br> – 汇报存储信息给NameNode</p> 
  <p><strong>• Client</strong><br> – 切分文件<br> – 访问HDFS<br> – 与NameNode交互,获取文件位置信息<br> – 与DataNode交互,读取和写入数据</p> 
  <p>block 每块缺省128MB大小,每块可以多个副本</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h3><span style="color:#3399ea;">MapReduce</span></h3> 
  <p>&nbsp;</p> 
  <p>结构</p> 
  <p><img alt="" class="has" height="429" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019062117420830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rvbmd6aHVvMTIyMA==,size_16,color_FFFFFF,t_70" width="604"></p> 
  <p>&nbsp;</p> 
  <p>• 源自于Google的MapReduce论文,JAVA实现的分布式计算框架<br><br> • 角色和概念<br><span style="color:#e579b6;">– JobTracker</span></p> 
  <p>&nbsp;Master节点只有一个,管理所有作业/任务的监控、错误处理等<br> 将任务分解成一系列任务,并分派给TaskTracker</p> 
  <p><br><span style="color:#e579b6;">– TaskTracker</span></p> 
  <p>Slave节点,一般是多台,运行Map Task和Reduce Task<br> 并与JobTracker交互,汇报任务状态</p> 
  <p><br><span style="color:#e579b6;">– Map Task</span></p> 
  <p>解析每条数据记录,传递给用户编写的<br> map()并执行,将输出结果写入本地磁盘<br> – 如果为map-only作业,直接写入HDFS</p> 
  <p><br><span style="color:#e579b6;">– Reducer Task</span></p> 
  <p>从Map Task的执行结果中,远程读<br> 取输入数据,对数据进行排序,将数据按照分组传递<br> 给用户编写的reduce函数执行</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h3><span style="color:#3399ea;">Yarn</span></h3> 
  <p>结构</p> 
  <p><img alt="" class="has" height="464" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190621174653219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rvbmd6aHVvMTIyMA==,size_16,color_FFFFFF,t_70" width="711"></p> 
  <p>&nbsp;</p> 
  <p>Yarn是Hadoop的一个通用的资源管理系统</p> 
  <p>角色</p> 
  <p><span style="color:#7c79e5;">– Resourcemanager</span></p> 
  <p>– 处理客户端请求<br> – 启动/监控ApplicationMaster<br> – 监控NodeManager<br> – 资源分配与调度</p> 
  <p><br><span style="color:#7c79e5;">– Nodemanager</span></p> 
  <p>– 单个节点上的资源管理<br> – 处理来自ResourceManager的命令<br> – 处理来自ApplicationMaster的命令</p> 
  <p><br><span style="color:#7c79e5;">– ApplicationMaster</span></p> 
  <p>– 数据切分<br> – 为应用程序申请资源,并分配给内部任务<br> – 任务监控与容错</p> 
  <p><br><span style="color:#7c79e5;">– Container</span></p> 
  <p>– 对任务运行行环境的抽象,封装了CPU 、内存等<br> – 多维资源以及环境变量、启动命令等任务运行相关的信息资源分配与调度</p> 
  <p><br><span style="color:#7c79e5;">– Client</span></p> 
  <p>– 用户与Yarn交互的客户端程序<br> – 提交应用程序、监控应用程序状态,杀死应用程序等</p> 
  <p>&nbsp;</p> 
  <p>• Yarn的核心思想<br> • 将JobTracker和TaskTacker进行分离,它由下面几大构成组件<br> – ResourceManager一个全局的资源管理器<br> – NodeManager每个节点(RM)代理<br> – ApplicationMaster表示每个应用<br> – 每一个ApplicationMaster有多个Container在NodeManager上运行</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>• Hadoop的部署模式有三种<br> – 单机<br> – 伪分布式<br> – 完全分布式</p> 
  <ul>
   <li>单机模式</li> 
  </ul>
  <p>新虚拟机 192.168.5.61 node1</p> 
  <p>安装&nbsp;java-1.8.0-openjdk-devel</p> 
  <p>[root@node1 ~]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#测试是否成功<br> 1446 Jps<br> &nbsp;</p> 
  <p>[root@node1 ~]# tar -xf <span style="color:#f33b45;">hadoop-2.7.7.tar.gz&nbsp;</span><br> [root@node1 ~]# mv hadoop-2.7.7/ &nbsp; &nbsp; &nbsp; /usr/local/hadoop &nbsp; &nbsp;#保证所有者和所属组为root</p> 
  <p>[root@node1 ~]# cd /usr/local/hadoop</p> 
  <p>[root@node1 hadoop]# rpm -ql java-1.8.0-openjdk<br><span style="color:#f33b45;">/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre/ &nbsp;截取前面相同的路径</span></p> 
  <p>[root@node1 hadoop]# ./bin/hadoop verion<br> Error: JAVA_HOME is not set and could not be found. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#找不到java你装哪了,需定义变量指定<br> &nbsp;</p> 
  <p>[root@node1 hadoop]# vim etc/hadoop/hadoop-env.sh&nbsp;</p> 
  <p># The java implementation to use.<br> export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre"</p> 
  <p>export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"<br> [root@node1 hadoop]# ./bin/hadoop version<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[root@node1 ~]# cd /usr/local/hadoop/<br> [root@node1 hadoop]# mkdir input</p> 
  <p>[root@node1 hadoop]# ls<br><span style="color:#3399ea;">bin &nbsp;include &nbsp;lib &nbsp; </span>&nbsp; &nbsp;LICENSE.txt &nbsp;&nbsp; README.txt<span style="color:#3399ea;"> &nbsp;share</span><br><span style="color:#3399ea;">etc &nbsp;input &nbsp; &nbsp;libexec </span>&nbsp;NOTICE.txt &nbsp; <span style="color:#3399ea;">output</span> &nbsp;<span style="color:#3399ea;">sbin</span></p> 
  <p>[root@node1 hadoop]# cp *.txt input/</p> 
  <p>[root@node1 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount input output<br> &nbsp;</p> 
  <p>[root@node1 hadoop]# ls &nbsp;output/<br> part-r-00000 &nbsp;_SUCCESS</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h3>HDFS分布式文件系统</h3> 
  <p>需修改的配置文件</p> 
  <p>– Hadoop-env.sh<br><br> JAVA_HOME<br> HADOOP_CONF_DIR</p> 
  <p>&nbsp;</p> 
  <p>– xml文件配置格式<br> &lt;property&gt;<br> &lt;name&gt;关键字&lt;/name&gt;<br> &lt;value&gt;变量值&lt;/value&gt;<br> &lt;description&gt; 描述 &lt;/description&gt;<br> &lt;/property&gt;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>环境准备</p> 
  <p>192.168.5.60 nn01<br> 192.168.5.61 node1 &nbsp; &nbsp; 前面已配置<br> 192.168.5.62 node2<br> 192.168.5.63 node3</p> 
  <p>&nbsp;</p> 
  <p>修改 &nbsp;/etc/hosts<br> # ::1&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost6 localhost6.localdomain6<br> 127.0.0.1&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost4 localhost4.localdomain4<br> 192.168.5.60 nn01<br> 192.168.5.61 node1<br> 192.168.5.62 node2<br> 192.168.5.63 node3<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>全部装 &nbsp; &nbsp;&nbsp;java-1.8.0-openjdk-devel</p> 
  <p>selinux关闭,禁用firewalld</p> 
  <p>所有机器能被nn01无密码ssh</p> 
  <p>&nbsp;ssh-keygen &nbsp;生成密钥,</p> 
  <p>ssh-copy-id &nbsp; <em>nn01/node1/node2/node3 &nbsp;</em></p> 
  <p>[root@nn01 ~]# vim /etc/ssh/ssh_config&nbsp;<br> Host *<br> &nbsp; &nbsp; &nbsp; <span style="color:#f33b45;">&nbsp; StrictHostKeyChecking no &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;设置第一次ssh时不需要输入yes</span><br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p><a href="https://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-common/core-default.xml" rel="nofollow" data-token="872029e3afb863bde80714a67f1c5568">https://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-common/core-default.xml</a></p> 
  <p>tar -xf hadoop-2.7.7.tar.gz</p> 
  <p>&nbsp;chown -R root:root &nbsp;hadoop-2.7.7/</p> 
  <p>mv hadoop-2.7.7/ &nbsp;/usr/local/hadoop</p> 
  <p>cd &nbsp;/usr/local/hadoop</p> 
  <p>[root@nn01 hadoop]# vim &nbsp;etc/hadoop/<span style="color:#f33b45;">core-site.xml &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;核心配置文件</span><br> &lt;configuration&gt;<br> &lt;property&gt;<br> &lt;name&gt;<span style="color:#f33b45;">fs.defaultFS</span>&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;文件系统配置参数<br> &lt;value&gt;<span style="color:#f33b45;">hdfs://nn01:9000</span>&lt;/value&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<br> &lt;/property&gt;<br> &lt;property&gt;<br> &lt;name&gt;<span style="color:#f33b45;">hadoop.tmp.dir</span>&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;数据根目录配置参数<br> &lt;value&gt;<span style="color:#f33b45;">/var/hadoop</span>&lt;/value&gt;<br> &lt;/property&gt;<br> &lt;/configuration&gt;</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# &nbsp;vim etc/hadoop/<span style="color:#f33b45;">hdfs-site.xml&nbsp;</span><br> &lt;configuration&gt;<br> &lt;property&gt;<br> &lt;name&gt;<span style="color:#f33b45;">dfs.namenode.http-address</span>&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 地址声明(谁是namenode)<br> &lt;value&gt;nn01:50070&lt;/value&gt;<br> &lt;/property&gt;<br> &lt;property&gt;<br> &lt;name&gt;<span style="color:#f33b45;">dfs.namenode.secondary.http-address</span>&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 地址声明(SecondaryNameNode)<br> &lt;value&gt;nn01:50090&lt;/value&gt;<br> &lt;/property&gt;<br> &lt;property&gt;<br> &lt;name&gt;<span style="color:#f33b45;">dfs.replication</span>&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 文件冗余份数()<br> &lt;value&gt;2&lt;/value&gt;<br> &lt;/property&gt;<br> &lt;/configuration&gt;</p> 
  <p>[root@nn01 hadoop]# cd etc/hadoop</p> 
  <p>[root@nn01 hadoop]# vim slaves &nbsp; &nbsp; &nbsp;#只写DataNode节点的主机名称</p> 
  <p>node1<br> node2<br> node3<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# vim etc/hadoop/hadoop-env.sh&nbsp;</p> 
  <p>export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre"<br> export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"</p> 
  <p>Hadoop所有节点的配置参数完全一样,在一台配置好<br> 后,把配置文件同步到其它所有主机上<br> &nbsp;</p> 
  <p>rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node1:/usr/local/</p> 
  <p>rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node2:/usr/local/</p> 
  <p>rsync -aSH --delete /usr/local/hadoop &nbsp; &nbsp;node3:/usr/local/</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# cd /usr/local/hadoop/</p> 
  <p>[root@nn01 hadoop]# mkdir /var/hadoop<br> [root@nn01 hadoop]# ./bin/hdfs namenode -format<br> [root@nn01 hadoop]# ./sbin/start-dfs.sh&nbsp;<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# jps<br> 30871 SecondaryNameNode<br> 30680 NameNode<br> 31017 Jps<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[root@node1 logs]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;其他节点上查看<br> 1093 Jps<br> 998 DataNode<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>核心手段:拆()</p> 
  <p>不同服务合在一台机器上</p> 
  <p>&nbsp;</p> 
  <table border="1" cellpadding="1" cellspacing="1" style="width:500px;">
   <tbody>
    <tr>
     <td style="width:165px;">主机</td> 
     <td style="width:167px;">角色</td> 
     <td>软件</td> 
    </tr>
    <tr>
     <td style="width:165px;">192.168.1.60<br> Master</td> 
     <td style="width:167px;">NameNode<br> SecondaryNameNode<br> ResourceManager</td> 
     <td>HDFS<br> YARN</td> 
    </tr>
    <tr>
     <td style="width:165px;">192.168.1.61<br> node1</td> 
     <td style="width:167px;">DataNode<br> NodeManager</td> 
     <td>HDFS<br> YARN</td> 
    </tr>
    <tr>
     <td style="width:165px;">192.168.1.62<br> node2</td> 
     <td style="width:167px;">DataNode<br> NodeManager</td> 
     <td>HDFS<br> YARN</td> 
    </tr>
    <tr>
     <td style="width:165px;">192.168.1.63<br> node3</td> 
     <td style="width:167px;">DataNode<br> NodeManager</td> 
     <td>HDFS<br> YARN</td> 
    </tr>
   </tbody>
  </table>
  <p>&nbsp;</p> 
  <p><strong>[root@nn01 hadoop]# </strong>cd &nbsp;/usr/local/hadoop/etc/hadoop<br> &nbsp;</p> 
  <p><strong>[root@nn01 hadoop]# </strong>mv mapred-site.xml.template &nbsp;<span style="color:#f33b45;"> mapred-site.xml</span><br><strong>[root@nn01 hadoop]# </strong>vim mapred-site.xml</p> 
  <p>&lt;configuration&gt;<br> &lt;property&gt;<br> &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br> &lt;value&gt;yarn&lt;/value&gt;<br> &lt;/property&gt;<br> &lt;/configuration&gt;<br> &nbsp;</p> 
  <p><strong>[root@nn01 hadoop]#</strong> vim yarn-site.xml&nbsp;</p> 
  <p>&lt;configuration&gt;</p> 
  <p>&lt;!-- Site specific YARN configuration properties --&gt;<br> &lt;property&gt;<br> &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br> &lt;value&gt;nn01&lt;/value&gt;<br> &lt;/property&gt;</p> 
  <p>&lt;property&gt;<br> &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br> &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br> &lt;/property&gt;</p> 
  <p>&lt;/configuration&gt;<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# for i in node1 node2 node3; do rsync -avXSH --delete /usr/local/hadoop &nbsp;${i}:/usr/local; done<br> [root@nn01 hadoop]# ./sbin/start-yarn.sh&nbsp;</p> 
  <p>[root@nn01 hadoop]# ./bin/yarn node -list<br> 19/06/21 09:54:01 INFO client.RMProxy: Connecting to ResourceManager at nn01/192.168.5.60:8032<br> Total Nodes:3<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Node-Id&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; Node-State&nbsp;&nbsp; &nbsp;Node-Http-Address&nbsp;&nbsp; &nbsp;Number-of-Running-Containers<br> &nbsp; &nbsp; &nbsp;node1:38135&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node1:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0<br> &nbsp; &nbsp; &nbsp;node3:39287&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node3:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0<br> &nbsp; &nbsp; &nbsp;node2:33541&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RUNNING&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; node2:8042&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0<br> &nbsp;</p> 
  <p>[root@nn01 hadoop]# jps<br> 1684 NameNode<br> 1876 SecondaryNameNode<br> 2775 Jps<br> 2206 ResourceManager<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[root@node1 ~]# jps &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<br> 914 DataNode<br> 1474 NodeManager<br> 1634 Jps<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p><a href="http://192.168.5.60:8088/cluster" rel="nofollow" data-token="390f98feb008e3cf564fffdf2fbd0de1">http://192.168.5.60:8088/</a>&nbsp; &nbsp; resourcemanager &nbsp;WEB页面(nn01)</p> 
  <p><a href="http://192.168.5.61:8042/" rel="nofollow" data-token="351a258c4d315fc1cb34dc4e04c4124a">http://192.168.5.61:8042/</a>&nbsp; &nbsp; &nbsp; nodemanager WEB页面(node1 node2 node3)</p> 
  <p><a href="http://192.168.5.60:50090" rel="nofollow" data-token="82403166a7e7267bbac2ca35a1210fd3">http://192.168.5.60:50090</a>/&nbsp; &nbsp; &nbsp; &nbsp;secondory &nbsp; namenode WEB页面 &nbsp;</p> 
  <p><a href="http://192.168.5.60:50070/" rel="nofollow" data-token="63f1354fac80ec902935ae6ed4df76af">http://192.168.5.60:50070/</a>&nbsp; &nbsp; &nbsp; &nbsp; namenode &nbsp;WEB页面&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p> 
  <p><a href="http://192.168.5.61:50075/" rel="nofollow" data-token="bd663958272f93b47c04a553157ccfee">http://192.168.5.61:50075/</a>&nbsp; &nbsp; &nbsp; &nbsp;datanode(node1 &nbsp;node2 node3) &nbsp;WEB页面</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# ./bin/hadoop fs -ls / &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p> 
  <p>[root@nn01 hadoop]# ./bin/hadoop fs -mkdir /abc<br> [root@nn01 hadoop]# ./bin/hadoop fs &nbsp;-ls /<br> [root@nn01 hadoop]# ./bin/hadoop fs -touchz /abc/f1 &nbsp; &nbsp; &nbsp;创建<br> [root@nn01 hadoop]# ./bin/hadoop fs -put &nbsp;*.txt &nbsp;/abc &nbsp; &nbsp;&nbsp;上传文件<br> [root@nn01 hadoop]# ./bin/hadoop fs -get /abc/f1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 下载文件<br> [root@nn01 hadoop]# ls<br> f1&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>hdfs安全模式相关命令(不是必要操作)</p> 
  <p><u>hdfs &nbsp; dfsadmin -report &nbsp; &nbsp;查看</u></p> 
  <p><u>hadoop &nbsp;dfsadmin &nbsp;safemode &nbsp;leave &nbsp;强制namenode退出安全模式</u></p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar &nbsp;wordcount &nbsp;/abc/* &nbsp; /bcd &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#提交分析作业</p> 
  <p>[root@nn01 hadoop]# ./bin/hadoop fs -cat /bcd/* &nbsp; &nbsp; &nbsp;#查看结果<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>扩容</p> 
  <p>新增192.168.5.64 &nbsp; &nbsp;newnode</p> 
  <p>– 启动一个新的系统,设置SSH免密码登录</p> 
  <p>[root@nn01 #] ssh-copy-id newnode<br> – 在所有节点修改 /etc/hosts,增加新节点的主机信息</p> 
  <p>vim /etc/hosts</p> 
  <p># ::1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6<br> 127.0.0.1 &nbsp; &nbsp; &nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4<br> 192.168.5.60 nn01<br> 192.168.5.61 node1<br> 192.168.5.62 node2<br> 192.168.5.63 node3<br> 192.168.5.64 newnode<br> &nbsp;</p> 
  <p>]# &nbsp;for i in node{1..3}; do scp /etc/hosts &nbsp; ${i}:/etc/; done</p> 
  <p>&nbsp;]# scp /etc/hosts &nbsp; newnode:/etc/<br> – 安装java运行环境(java-1.8.0-openjdk-devel)<br> – 修改NameNode的slaves文件增加该节点</p> 
  <p>[root@nn01 hadoop]# vim /usr/local/hadoop/etc/hadoop/</p> 
  <p>node1<br> node2<br> node3<br> newnode</p> 
  <p>#] &nbsp;for i in node1 node2 node3 ; do scp ./slaves &nbsp;${i}:/usr/local/hadoop/etc/hadoop; done</p> 
  <p><br> – 拷贝NamNode的/usr/local/hadoop到本机</p> 
  <p>#] &nbsp; scp -r /usr/local/hadoop &nbsp; newnode:/usr/local<br> – 在该节点启动DataNode<br> ./sbin/hadoop-daemon.sh start datanode</p> 
  <p>&nbsp;</p> 
  <p>– 设置同步带宽,并同步数据<br><br> # ./bin/hdfs dfsadmin -setBalancerBandwidth 60000000<br> # ./sbin/start-balancer.sh<br> – 查看集群状态<br> # ./bin/hdfs dfsadmin -report</p> 
  <p>&nbsp;</p> 
  <p>HDFS 修复节点比较简单,与增加节点基本一致</p> 
  <p>--注意:新节点的ip和主机名要与损坏节点的一致</p> 
  <p>--启动服务: &nbsp; /usr/local/hadoop/sbin/hadoop-daemon.sh &nbsp;start datanode</p> 
  <p>--数据恢复是自动的</p> 
  <p>--上线以后会自动恢复数据,如果数据量非常巨大,可能需要一段时间</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# cd &nbsp;/usr/local/hadoop/</p> 
  <p>[root@nn01 hadoop]#&nbsp;&nbsp;./bin/hadoop fs -put /root/CentOS7-1804.iso &nbsp;/bcd</p> 
  <p>[root@nn01 ~]# vim etc/hadoop/hdfs-site.xml&nbsp;</p> 
  <p>&lt;property&gt;<br> &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;<br> &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt;<br> &lt;/property&gt;<br> &nbsp;</p> 
  <p>[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>NFS网关</p> 
  <p>• NFS 网关用途<br> – 用户可以通过操作系统兼容的本地NFSv3客户端来浏<br> 览HDFS文件系统<br> – 用户可以从HDFS文件系统下载文档到本地文件系统<br> – 用户可以通过挂载点直接流化数据,支持文件附加,<br> 但是不支持随机写<br> – NFS网关支持NFSv3和允许HDFS作为客户端文件系统<br> 的一部分被挂载</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>新机器192.168.5.65 &nbsp;nfsgw</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 192.168.5.66 &nbsp;localhost</p> 
  <p>添加/etc/hosts,然后同步给其他集群主机</p> 
  <p>192.168.5.60 nn01<br> 192.168.5.61 node1<br> 192.168.5.62 node2<br> 192.168.5.63 node3<br> 192.168.5.64 newnode<br> 192.168.5.65 nfsgw<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p> 
  <p>安装&nbsp;&nbsp;java-1.8.0-openjdk-devel</p> 
  <p>卸载rpcbind &nbsp;nfs-utils</p> 
  <p>nfsgw服务器和nn01上配置用户</p> 
  <p>]# useradd nfsgw ; useradd nn01<br> ]# groupadd -g 800 nfsuser<br> ]# &nbsp;useradd &nbsp;-u &nbsp;800 -g 800 -r -d /var/hadoop &nbsp;nfsuser</p> 
  <p>&nbsp;</p> 
  <p><strong>[root@nn01 hadoop]# </strong>./sbin/stop-all.sh &nbsp; &nbsp; 停止集群<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# vim etc/hadoop/core-site.xml</p> 
  <p>&lt;property&gt;<br> &lt;name&gt;hadoop.proxyuser.nfsuser.groups&lt;/name&gt;<br> &lt;value&gt;*&lt;/value&gt;<br> &lt;/property&gt;<br> &lt;property&gt;<br> &lt;name&gt;hadoop.proxyuser.nfsuser.hosts&lt;/name&gt;<br> &lt;value&gt;*&lt;/value&gt;<br> &lt;/property&gt;<br> &nbsp;</p> 
  <p>同步/usr/local/hadoop给其他集群主机(node1,node2,node3)</p> 
  <p>启动集群</p> 
  <p>jps &nbsp;</p> 
  <p>hdfs &nbsp;dfsadmin &nbsp;-report</p> 
  <p>&nbsp;</p> 
  <p><span style="color:#f33b45;">[<strong>root@nfsgw ~]#</strong> </span>rsync -aSH --delete &nbsp;nn01:/usr/local/hadoop &nbsp;/usr/local<br> &nbsp;</p> 
  <p>[root@nfsgw ~]# vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml&nbsp;</p> 
  <p>&lt;configuration&gt;</p> 
  <p>&lt;property&gt;<br> &lt;name&gt;<span style="color:#f33b45;">nfs.exports.allowed.hosts</span>&lt;/name&gt;<br> &lt;value&gt;<span style="color:#86ca5e;">* rw</span>&lt;/value&gt;<br> &lt;/property&gt;</p> 
  <p>&lt;property&gt;<br> &lt;name&gt;<span style="color:#f33b45;">nfs.dump.dir</span>&lt;/name&gt;<br> &lt;value&gt;<span style="color:#86ca5e;">/var/nfstmp</span>&lt;/value&gt;<br> &lt;/property&gt;<br> &nbsp;</p> 
  <p>&lt;/configuration&gt;<br> &nbsp;</p> 
  <p>[root@nfsgw ~]# mkdir /var/nfstmp<br> [root@nfsgw ~]# chown 800.800 &nbsp;/var/nfstmp<br> [root@nfsgw ~]# ls -ld /var/nfstmp<br> drwxr-xr-x 2 nfsuser nfsuser 6 6月 &nbsp;21 16:47 /var/nfstmp<br> &nbsp;</p> 
  <p>[root@nfsgw ~]# cd /usr/local/hadoop/<br> [root@nfsgw hadoop]# rm -rf logs/*<br> [root@nfsgw hadoop]# setfacl -m nfsuser:rwx logs/</p> 
  <p>[root@nfsgw hadoop]# ./sbin/hadoop-daemon.sh --script ./bin/hdfs start portmap &nbsp; 使用root启动portmap服务</p> 
  <p>[root@nfsgw hadoop]# sudo -u nfsuser ./sbin/hadoop-daemon.sh --script &nbsp;./bin/hdfs start nfs3 &nbsp; &nbsp;使用代理用户启动nfs3<br> [root@nfsgw hadoop]# jps<br> 23659 Nfs3<br> 23596 Portmap<br> 23710 Jps<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>pormap(root用户先起服务,停的话后停,先停nfs)</p> 
  <p>nfs3(nfsuser用户后起服务,)</p> 
  <p>atime &nbsp;访问的时间</p> 
  <p>ctmie &nbsp;改变的时间</p> 
  <p>mtime &nbsp;修改的时间</p> 
  <p>&nbsp;</p> 
  <p>客户端192.168.5.66测试</p> 
  <p>[root@localhost ~]# yum -y install nfs-utils</p> 
  <p>[root@localhost ~]# mount -t nfs -o vers=3,proto=tcp,noatime,nolock,sync,noacl 192.168.5.65:/ &nbsp;/mnt<br> [root@localhost ~]# ls /mnt<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <h1><span style="color:#86ca5e;">zookeeper</span></h1> 
  <p>开源的分布式应用程序协调服务</p> 
  <p>保证数据在集群间的事务一致性</p> 
  <p>&nbsp;</p> 
  <p><strong>应用场景</strong><br> – 集群分布式锁<br> – 集群统一命名服务<br> – 分布式协调服务</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p><strong>• Zookeeper角色与特性</strong></p> 
  <p><br> – Leader:接受所有Follower的提案请求并统一协调发起<br> 提案的投票,负责与所有的Follower进行内部数据交换<br> – Follower:直接为客户端服务并参与提案的投票,同时<br> 与Leader进行数据交换<br> – Observer:直接为客户端服务但并不参与提案的投票,<br> 同时也与Leader进行数据交换</p> 
  <p>&nbsp;</p> 
  <p>• Zookeeper角色与选举<br> – 服务在启动的时候是没有角色的(LOOKING)<br> – 角色是通过选举产生的<br> – 选举产生一个Leader,剩下的是Follower<br> • 选举Leader原则<br> – 集群中超过半数机器投票选择Leader<br> – 假如集群中拥有n台服务器,那么Leader必须得到<br> n/2+1台服务器的投票</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>• Zookeeper角色与选举<br> – 如果Leader死亡,重新选举Leader<br> – 如果死亡的机器数量达到一半,则集群挂掉<br> – 如果无法得到足够的投票数量,就重新发起投票,如果参与投票的机器不足n/2+1,则集群停止工作<br> – 如果Follower死亡过多,剩余机器不足n/2+1,则集群也会停止工作<br> – Observer不计算在投票总设备数量里面</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>• Zookeeper可伸缩扩展性原理与设计<br> – Leader所有写相关操作<br> – Follower读操作与响应Leader提议<br> – 在Observer出现以前,Zookeeper的伸缩性由Follower<br> 来实现,我们可以通过添加Follower节点的数量来保证<br> Zookeeper服务的读性能,但是随着Follower节点数量<br> 的增加,Zookeeper服务的写性能受到了影响</p> 
  <p>&nbsp;</p> 
  <p>• Zookeeper可伸缩扩展性原理与设计<br><br> – 客户端提交一个请求,若是读请求,则由每台Server的本地<br> 副本数据库直接响应。若是写请求,需要通过一致性协议<br> (Zab)来处理<br> – Zab协议规定:来自Client的所有写请求都要转发给ZK服务<br> 中唯一的Leader,由Leader根据该请求发起一个Proposal。<br> 然后其他的Server对该Proposal进行Vote。之后Leader对<br> Vote进行收集,当Vote数量过半时Leader会向所有的<br> Server发送一个通知消息。最后当Client所连接的Server收<br> 到该消息时,会把该操作更新到内存中并对Client的写请求<br> 做出回应</p> 
  <p>&nbsp;</p> 
  <p><br> – ZooKeeper在上述协议中实际扮演了两个职能。一方面从<br> 客户端接受连接与操作请求,另一方面对操作结果进行投票。<br> 这两个职能在Zookeeper集群扩展的时候彼此制约<br> – 从Zab协议对写请求的处理过程中可以发现,增加Follower<br> 的数量,则增加了协议投票过程的压力。因为Leader节点<br> 必须等待集群中过半Server响应投票,是节点的增加使得部<br> 分计算机运行较慢,从而拖慢整个投票过程的可能性也随之<br> 提高,随着集群变大,写操作也会随之下降</p> 
  <p>&nbsp;</p> 
  <p>– 所以,我们不得不在增加Client数量的期望和我们希望保<br> 持较好吞吐性能的期望间进行权衡。要打破这一耦合关系,<br> 我们引入了不参与投票的服务器Observer。Observer可<br> 以接受客户端的连接,并将写请求转发给Leader节点。但<br> Leader节点不会要求Observer参加投票,仅仅在上述第3<br> 歩那样,和其他服务节点一起得到投票结果</p> 
  <p>&nbsp;</p> 
  <p>– Observer的扩展,给Zookeeper的可伸缩性带来了全<br> 新的景象。加入很多Observer节点,无须担心严重影<br> 响写吞吐量。但并非是无懈可击,因为协议中的通知<br> 阶段,仍然与服务器的数量呈线性关系。但是这里的<br> 串行开销非常低。因此,我们可以认为在通知服务器<br> 阶段的开销不会成为瓶颈<br> – Observer提升读性能的可伸缩性<br> – Observer提供了广域网能力</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;[root@nn01 ~]# tar -xf zookeeper-3.4.13.tar.gz&nbsp;</p> 
  <p>[root@nn01 ~]# mv &nbsp;~/zookeeper-3.4.13 &nbsp; /usr/local/zookeeper</p> 
  <p>[root@nn01 ~]# cd /usr/local/zookeeper/</p> 
  <p>[root@nn01 zookeeper]#cd conf/<br> [root@nn01 conf]# mv zoo_sample.cfg &nbsp;<span style="color:#f33b45;">zoo.cfg</span></p> 
  <p>[root@nn01 conf]# vim zoo.cfg</p> 
  <p>server.1=node1:2888:3888<br> server.2=node2:2888:3888<br> server.3=node3:2888:3888<br> server.4=nn01:2888:3888:observer<br> &nbsp;</p> 
  <p>&nbsp;scp -r /usr/local/zookeeper &nbsp; node1:/usr/local<br> &nbsp;scp -r /usr/local/zookeeper &nbsp; node2:/usr/local<br> &nbsp;scp -r /usr/local/zookeeper &nbsp; node3:/usr/local</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>mkdir /tmp/zookeeper &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #所有集群上创建</p> 
  <p>node1]# echo &nbsp;1 &gt; /tmp/zookeeper/myid &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;请确保每个server的myid文件中id数字不同,server.id中的id与myid中的id必须一致&nbsp; &nbsp;</p> 
  <p>node2]# echo &nbsp;2 &gt; /tmp/zookeeper/myid &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;id的范围是1~255</p> 
  <p>node3]# echo &nbsp;3 &gt; /tmp/zookeeper/myid</p> 
  <p>nn01]# &nbsp; echo 4 &nbsp;&gt; /tmp/zookeeper/myid</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>启动集群,查看验证(在所有集群节点执行)</p> 
  <p>/usr/local/zookeeper/bin/zkServer.sh &nbsp;start</p> 
  <p>/usr/local/zookeeper/bin/zkServer.sh status &nbsp; &nbsp; &nbsp; &nbsp;#刚启动一个查看状态为不运行的,因为还需投票,等全部起完了就运行了</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>Zookeeper管理文档<br> http://zookeeper.apache.org/doc/r3.4.10/zookeeperAdmin.html</p> 
  <table border="1" cellpadding="1" cellspacing="1">
   <tbody>
    <tr>
     <td> <p>man &nbsp; bash &nbsp;---&gt; &nbsp; &nbsp; 搜索/dev/tcp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p> <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/fd/fd<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If fd is a valid integer, file descriptor fd is duplicated.<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stdin<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 0 is duplicated.<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stdout<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 1 is duplicated.<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/stderr<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;File descriptor 2 is duplicated.<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/tcp/host/port<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If &nbsp;host &nbsp;is &nbsp;a valid hostname or Internet address, and port is an integer port number or service name, bash<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;attempts to open a TCP connection to the corresponding socket.<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /dev/udp/host/port<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If host is a valid hostname or Internet address, and port is an integer port number or &nbsp;service &nbsp;name, &nbsp;bash<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;attempts to open a UDP connection to the corresponding socket.</p> </td> 
    </tr>
   </tbody>
  </table>
  <p>&nbsp;</p> 
  <blockquote> 
   <p>[root@nn01 ~]# vim zkstats &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #检测zookeeper状态脚本</p> 
   <p>#!/bin/bash<br> function getzkstat(){<br> &nbsp; &nbsp; exec 2&gt;/dev/null<br> &nbsp; &nbsp; exec 8&lt;&gt;/dev/tcp/$1/2181<br> &nbsp; &nbsp; echo stat &gt;&amp;8<br> &nbsp; &nbsp; Msg=$(cat &lt;&amp;8 |grep -P "^Mode:")<br> &nbsp; &nbsp; echo -e "$1\t${Msg:-Mode: \x1b[31mNULL\x1b[0m}"<br> &nbsp; &nbsp; exec 8&lt;&amp;-<br> }</p> 
   <p>if (( $# == 0 ));then<br> &nbsp; &nbsp; echo "${0##*/} zk1 zk2 zk3 ... ..."<br> else<br> &nbsp; &nbsp; for i in $@;do<br> &nbsp; &nbsp; &nbsp; &nbsp; getzkstat ${i}<br> &nbsp; &nbsp; done<br> fi<br> [nn01 ~]# &nbsp;./zkstats &nbsp;node{1..3}</p> 
  </blockquote> 
  <p>&nbsp;</p> 
  <h2><span style="color:#86ca5e;">kafka</span></h2> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>– Kafka是由LinkedIn开发的一个分布式的消息系统<br> – Kafka是使用Scala编写<br> – Kafka是一种消息中间件<br> • 为什么要使用Kafka<br> – 解耦、冗余、提高扩展性、缓冲<br> – 保证顺序,灵活,削峰填谷<br> – 异步通信</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>• Kafka角色与集群结构<br> – producer:生产者,负责发布消息<br> – consumer:消费者,负责读取处理消息<br> – topic:消息的类别<br> – Parition:每个Topic包含一个或多个Partition<br> – Broker:Kafka集群包含一个或多个服务器</p> 
  <p>• Kafka通过Zookeeper管理集群配置,选举Leader</p> 
  <p>&nbsp;</p> 
  <p>• Kafka集群的安装配置<br> – Kafka集群的安装配置依赖Zookeeper,搭建Kafka集群之前,请先创建好一个可用的Zookeeper集群<br> – 安装OpenJDK运行环境<br> – 同步Kafka拷贝到所有集群主机<br> – 修改配置文件<br> – 启动与验证</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>vim /usr/local/kafka/config/<span style="color:#f33b45;">server.properties&nbsp;</span><br> broker.id=<span style="color:#f33b45;">1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span>&nbsp;#每台服务器的broker.id都不能相同</span><br> zookeeper.connect=<span style="color:#f33b45;">node1:2181,node2:2181,node3:2181 </span><span>&nbsp;#zookeeper集群地址,不用都列出,写一部分即可</span><br> [root@nn01 kafka]# for i in node{1..3}; do scp -r /usr/local/kafka &nbsp; ${i}:/usr/local &nbsp;; done</p> 
  <p>&nbsp;/usr/local/kafka/bin/kafka-server-start.sh -daemon &nbsp;/usr/local/kafka/config/server.properties &nbsp; &nbsp; &nbsp;#所有主机启动服务<br> – jps命令应该能看到Kafka模块<br> – netstat应该能看到9092在监听</p> 
  <p>[root@node1 ~]# /usr/local/kafka/bin/kafka-topics.sh --create --partitions 2 --replication-factor 2 --zookeeper localhost:2181 --topic mymsg &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#创建一个topic<br> Created topic "mymsg".</p> 
  <p>&nbsp;</p> 
  <p>[root@node2 ~]# /usr/local/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mymsg &nbsp; &nbsp; &nbsp; &nbsp; #生产者<br> 发布消息,随便打一些字符串</p> 
  <p>[root@node3 kafka]# /usr/local/kafka/bin/kafka-console-consumer.sh &nbsp;--bootstrap-server localhost:9092 --topic mymsg &nbsp; #消费者<br> 接收消息,上面打的字符串会在这同步显示</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h1>Haddop高可用</h1> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p><strong>为什么需要NameNode</strong><br> • 原因<br> – NameNode是HDFS的核心配置,HDFS又是Hadoop核心组件,NameNode在Hadoop集群中至关重要<br> – NameNode宕机,将导致集群不可用,如果NameNode数据丢失将导致整个集群的数据丢失,而NameNode的数据的更新又比较频繁,实现NameNode高可用势在必行</p> 
  <p>&nbsp;</p> 
  <p><strong>• 官方提供了两种解决方案</strong><br> – HDFS with NFS<br> – HDFS with QJM<br> • 两种方案异同<br><u><strong>NFS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; QJM</strong></u><br><u>NN &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; NN<br> ZK &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; ZK<br> ZKFailoverController &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;ZKFailoverController<br> NFS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp;&nbsp; JournalNode</u></p> 
  <p>&nbsp;</p> 
  <p><strong>• HA方案对比</strong><br> – 都能实现热备<br> – 都是一个Active NN和一个Standby NN<br> – 都使用Zookeeper和ZKFC来实现自动失效恢复<br> – 失效切换都使用Fencin配置的方法来Active NN<br> – NFS数据共享变更方案把数据存储在共享存储里,我们还需要考虑NFS的高可用设计<br> – QJM不需要共享存储,但需要让每一个DN都知道两个NN的位置,并把块信息和心跳包发送给Active和Standby这两个NN</p> 
  <p>&nbsp;</p> 
  <p><strong>• 使用原因(QJM)</strong><br> – 解决NameNode单点故障问题<br> – Hadoop给出了HDFS的高可用HA方案:HDFS通常由两<br> 个NameNode组成,一个处于Active状态,另一个处于<br> Standby状态。Active NameNode对外提供服务,比如<br> 处理来自客户端的RPC请求,而Standby NameNode则<br> 不对外提供服务,仅同步Active NameNode的状态,以<br> 便能够在它失败时进行切换</p> 
  <p><br><strong>• 典型的HA集群</strong><br> – NameNode会被配置在两台独立的机器上,在任何时<br> 候 , 一 个 NameNode 处 于 活 动 状 态 , 而 另 一 个<br> NameNode则处于备份状态<br> – 活动状态的NameNode会响应集群中所有的客户端,<br> 备份状态的NameNode只是作为一个副本,保证在必<br> 要的时候提供一个快速的转移</p> 
  <p>&nbsp;</p> 
  <p><strong>• NameNode高可用架构</strong><br> – 为了让Standby Node与Active Node保持同步,这两个<br> Node 都 与 一 组 称 为 JNS 的 互 相 独 立 的 进 程 保 持 通 信<br> (Journal Nodes)。当Active Node更新了namespace,<br> 它将记录修改日志发送给JNS的多数派。Standby Node将<br> 会从JNS中读取这些edits,并持续关注它们对日志的变更</p> 
  <p><br> – Standby Node将日志变更应用在自己的namespace中,<br> 当Failover发生时,Standby将会在提升自己为Active之前,<br> 确保能够从JNS中读取所有的edits,即在Failover发生之前<br> Standy持有的namespace与Active保持完全同步</p> 
  <p><br> – NameNode更新很频繁,为了保持主备数据的一致性,<br> 为了支持快速Failover,Standby Node持有集群中<br> blocks的最新位置是非常必要的。为了达到这一目的,<br> DataNodes上需要同时配置这两个Namenode的地址,<br> 同时和它们都建立心跳连接,并把block位置发送给它们</p> 
  <p>&nbsp;</p> 
  <p>– 任何时刻,只能有一个Active NameNode,否则会导致<br> 集群操作混乱,两个NameNode将会有两种不同的数据<br> 状态,可能会导致数据丢失或状态异常,这种情况通常<br> 称为"split-brain"(脑裂,三节点通讯阻断,即集群中不<br> 同的DataNode看到了不同的Active NameNodes)</p> 
  <p><br> – 对于JNS而言,任何时候只允许一个NameNode作为<br> writer;在Failover期间,原来的Standby Node将会接<br> 管Active的所有职能,并负责向JNS写入日志记录,这种<br> 机制阻止了其他NameNode处于Active状态的问题</p> 
  <p>&nbsp;</p> 
  <p>系统规划:</p> 
  <p><img alt="" class="has" height="309" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190623142841907.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rvbmd6aHVvMTIyMA==,size_16,color_FFFFFF,t_70" width="572"></p> 
  <p>&nbsp;</p> 
  <p>新准备机器 192.168.5.66 nn02</p> 
  <p>免密码登录</p> 
  <p>[<strong>root@nn02 ~]# </strong>scp -r 192.168.5.60:/root/.ssh &nbsp; &nbsp;/root<br> [root@nn02 ~]# cd ./.ssh<br> [root@nn02 .ssh]# ls<br> authorized_keys &nbsp;id_rsa &nbsp;id_rsa.pub &nbsp;known_hosts<br> [root@nn02 .ssh]# rm &nbsp;-rf known_hosts&nbsp;</p> 
  <p><strong>[root@nn01 ~]#</strong> vim &nbsp;/etc/hosts<br> # ::1&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost6 localhost6.localdomain6<br> 127.0.0.1&nbsp;&nbsp; &nbsp;localhost localhost.localdomain localhost4 localhost4.localdomain4<br> 192.168.5.60 nn01<br> 192.168.5.61 node1<br> 192.168.5.62 node2<br> 192.168.5.63 node3<br> 192.168.5.65 nfsgw<br> 192.168.5.66 nn02<br> ]# &nbsp;for i in 61 62 63 66; do scp &nbsp;/etc/hosts &nbsp; &nbsp;192.168.5.$i:/etc; done</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>关机重启(全部)</p> 
  <p>再开启zookeeper</p> 
  <p>/usr/local/zookeeper/bin/zkServer.sh start &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #node{1..3},nn01都起zookeeper服务</p> 
  <p>]# jps</p> 
  <p>&nbsp;</p> 
  <p>[root@nn01 ~]# rm -rf /var/hadoop/*<br> [root@nn01 ~]# ssh node1 rm -rf /var/hadoop/*<br> [root@nn01 ~]# ssh node2 rm -rf /var/hadoop/*<br> [root@nn01 ~]# ssh node3 rm -rf /var/hadoop/*<br> [root@nn01 ~]# rm -rf /usr/local/hadoop/logs/*<br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>[<strong>root@nn02 ~]# </strong>mkdir /var/hadoop</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <table border="1" cellpadding="1" cellspacing="1">
   <tbody>
    <tr>
     <td>yarn高可用<br> • ResourceManager高可用<br> – RM的高可用原理与NN一样,需要依赖ZK来实现,这里配置文件的关键部分,感兴趣的同学可以自己学习和测试<br> – yarn.resourcemanager.hostname<br> – 同理因为使用集群模式,该选项应该关闭</td> 
    </tr>
   </tbody>
  </table>
  <p>&nbsp;</p> 
  <p>[root@nn01 hadoop]# cd&nbsp;&nbsp;/usr/local/hadoop/etc/hadoop/</p> 
  <p>0. &nbsp;[root@nn01 hadoop]# vim<span style="color:#f33b45;"> hadoop-env.sh&nbsp;</span></p> 
  <blockquote> 
   <p>export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre"</p> 
   <p>export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"<br> &nbsp;</p> 
  </blockquote> 
  <p>&nbsp;</p> 
  <p>1. &nbsp;[root@root hadoop]# vim <span style="color:#f33b45;">core-site.xml&nbsp;</span></p> 
  <blockquote> 
   <p>&lt;configuration&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;fs.defaultFS&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;<span style="color:#f33b45;">hdfs://nsd1902</span>&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/var/hadoop&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;<span style="color:#f33b45;">ha.zookeeper.quorum</span>&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;<span style="color:#f33b45;">node1:2181,node2:2181,node3:2181</span>&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.proxyuser.nfsuser.groups&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;*&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;hadoop.proxyuser.nfsuser.hosts&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;*&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &lt;/configuration&gt;<br> ~ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p> 
  </blockquote> 
  <p>&nbsp;2. &nbsp;vim <span style="color:#f33b45;">hdfs-site.xml&nbsp;</span></p> 
  <blockquote> 
   <p>&lt;configuration&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.nameservices&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#指定hdfs的nameservices名称<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nsd1902&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.namenodes.nsd1902&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp;#指定集群的两个namenode的名称分别为nn1,nn2<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn1,nn2&lt;/value&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #这里不是指主机名<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.rpc-address.nsd1902.nn1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置nn1,nn2的rpc通信端口<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01:8020&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.rpc-address.nsd1902.nn2&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02:8020&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.http-address.nsd1902.nn1&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置nn1,nn2的http通信端口<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01:50070&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.http-address.nsd1902.nn2&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02:50070&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;#指定namenode元数据存储在journalnode中的路径<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;qjournal://node1:8485;node2:8485;node3:8485/nsd1902&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#指定journalnode日志文件存储路径<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/var/hadoop/journal&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.client.failover.proxy.provider.nsd1902&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; #指定HDFS客户端连接Active NameNode的java类<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#配置隔离机制为ssh<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;sshfence&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; #指定密钥的位置<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #开启自动故障转移<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.replication&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;2&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &lt;/configuration&gt;</p> 
  </blockquote> 
  <p>&nbsp;</p> 
  <p>3. &nbsp;vim<span style="color:#f33b45;"> mapred-site.xml</span></p> 
  <blockquote> 
   <p>&lt;configuration&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;yarn&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &lt;/configuration&gt;<br> &nbsp;</p> 
  </blockquote> 
  <p>&nbsp;</p> 
  <p>4. &nbsp;vim <span style="color:#f33b45;">slaves</span></p> 
  <blockquote> 
   <p>node1<br> node2<br> node3<br> &nbsp;</p> 
  </blockquote> 
  <p>&nbsp;</p> 
  <p>5. &nbsp; vim<span style="color:#f33b45;"> yarn-site.xml&nbsp;</span></p> 
  <blockquote> 
   <p>&lt;configuration&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;rm1,rm2&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;true&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;node1:2181,node2:2181,node3:2181&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;yarn-ha&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn01&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;nn02&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &lt;!-- Site specific YARN configuration properties --&gt;<br> &nbsp; &nbsp; &lt;property&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br> &nbsp; &nbsp; &lt;/property&gt;<br> &lt;/configuration&gt;<br> &nbsp;</p> 
  </blockquote> 
  <p>6. &nbsp; vim &nbsp;<span style="color:#f33b45;">exclude</span> &nbsp; 把里面的内容删除</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p><strong>集群初始化:</strong></p> 
  <p>&nbsp;</p> 
  <p>– ALL: 代表所有机器<br> – nodeX: 代表node1 &nbsp;node2 &nbsp;node3 &nbsp;(标示方便全部一起操作)</p> 
  <p>&nbsp;</p> 
  <p><br> – ALL: 同步配置到所有集群机器</p> 
  <p>for i in node{1..3} nn02; do scp -r /usr/local/hadoop &nbsp;${i}:/usr/local &amp; done</p> 
  <p><br> – NN1: 初始化ZK集群<br> # ./bin/hdfs zkfc -formatZK</p> 
  <p><br> – nodeX: 启动journalnode服务<br> # ./sbin/hadoop-daemon.sh start journalnode</p> 
  <p>&nbsp;</p> 
  <p>• 初始化<br> – NN1: 格式化<br> # ./bin/hdfs namenode -format</p> 
  <p><br> – NN2: 数据同步到本地/var/hadoop/dfs<br> # rsync -aSH nn01:/var/hadoop/dfs /var/hadoop/</p> 
  <p><br> – NN1: 初始化JNS<br> # ./bin/hdfs namenode -initializeSharedEdits</p> 
  <p><br> – nodeX: 停止journalnode服务<br> # ./sbin/hadoop-daemon.sh stop journalnode</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>• 启动集群<br> – NN1: 启动hdfs<br> # ./sbin/start-dfs.sh</p> 
  <p><br> – NN1: 启动yarn<br> # ./sbin/start-yarn.sh</p> 
  <p><br> – NN2: 启动热备ResourceManager<br> # ./sbin/yarn-daemon.sh start resourcemanager</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>• 查看集群状态<br> – 获取NameNode状态<br> # ./bin/hdfs haadmin -getServiceState nn1<br> # ./bin/hdfs haadmin -getServiceState nn2</p> 
  <p><br> – 获取ResourceManager状态<br> # ./bin/yarn rmadmin -getServiceState rm1<br> # ./bin/yarn rmadmin -getServiceState rm2</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p><br> – 获取节点信息<br> # ./bin/hdfs dfsadmin -report &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#查看有3台集群节点<br> # ./bin/yarn node -list</p> 
  <p><br> – 访问集群文件<br> # ./bin/hadoop fs -mkdir /input<br> # ./bin/hadoop fs -ls hdfs://mycluster/</p> 
  <p><br> – 主从切换Activate<br> # ./sbin/hadoop-daemon.sh stop namenode &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(把Activate的那台namenode停掉服务)</p> 
  <p>&nbsp;</p> 
  <p>./bin/hadoop fs -put *.txt &nbsp;/input &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<br> &nbsp;</p> 
  <p>&nbsp;./sbin/hadoop-daemon.sh start&nbsp;namenode</p> 
  <p>./bin/hadoop fs &nbsp;-ls /input</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
