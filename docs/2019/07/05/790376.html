<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>基于 PyTorch 的Cifar图像分类器代码实现（线性，全连接，卷积） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="基于 PyTorch 的Cifar图像分类器代码实现（线性，全连接，卷积）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="基于 PyTorch 的Cifar图像分类器代码实现 做实验的时候一共要求做三个实验分别是只使用线性层实现，通过全连接实现，通过卷积实现，代码也是参考了许多前人的代码，有一定程度的修改，然而准确度低的不成样子，望各位海涵。 同时本次的实验是在python 3.7 Jupyter上运行的，Cifar数据在py文件同级目录下的data文件中。由于我的显卡不支持CUDA因此都是直接用cpu搞定的。最长的100个epoch可能要跑一两个小时左右，基本上40个epoch就可以得出大概的结果 仅使用线性层实现图像分类&nbsp; 正确率百分之四十 import torch import torch.nn as nn import torchvision import torchvision.transforms as transforms import torchvision import datetime transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # 训练模型的超参数 input_size = 3072 output_size = 10 num_epochs = 50 batch_size = 64 learning_rate = 0.001 train_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) test_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) #线性层的建立 model = nn.Linear(input_size, output_size) # 损失函数和优化算法 criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # train_loader的大小，也就是含有多少个bach。 total_step = len(train_loader) # 训练模型 # 在整个数据集上迭代的次数 for epoch in range(num_epochs): starttime = datetime.datetime.now() # 每次取一个bach进行训练。 for i, (images, labels) in enumerate(train_loader): # 将数据reshape到模型需要的大小。 images = images.reshape(-1,3*32*32) # 前向传播 outputs = model(images) # 计算模型的loss loss = criterion(outputs, labels) # 后向传播，更新模型参数 optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 782 == 0: print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39; .format(epoch+1, num_epochs, i+1, total_step, loss.item())) with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, 3*32*32) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print(&#39;Accuracy of the model on the 10000 test images: {} %&#39;.format(100 * correct / total)) &nbsp;使用全连接层 正确率百分之五十六 import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms from torch.autograd import Variable import torchvision import datetime # Training settings batch_size = 128 transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) #更换下顺序便于训练多次 train_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) test_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) # Data Loader (Input Pipeline) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) y_loss=[] y_time=[] #在cifar中，为一个3*32*32的图像 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.l0 = nn.Linear(3072,2048) self.l1 = nn.Linear(2048, 1024) self.l2 = nn.Linear(1024, 512) self.l3 = nn.Linear(512, 256) self.l4 = nn.Linear(256, 128) self.l5 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 3072) x = F.relu(self.l0(x)) x = F.relu(self.l1(x)) x = F.relu(self.l2(x)) x = F.relu(self.l3(x)) x = F.relu(self.l4(x)) return F.log_softmax(self.l5(x), dim=1) #return self.l5(x) #如果直接使用self.15(x)会出现NAN的情况，也就是梯度爆炸 model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) def train(epoch): # 每次输入barch_idx个数据 for batch_idx, (data, target) in enumerate(train_loader): data, target = Variable(data), Variable(target) optimizer.zero_grad() output = model(data) # loss loss = F.nll_loss(output, target) loss.backward() # update optimizer.step() if batch_idx % 391 == 0: print(&#39;Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}&#39;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) y_loss.append(loss.item()) def test(): test_loss = 0 correct = 0 # 测试集 for data, target in test_loader: data, target = Variable(data, volatile=True), Variable(target) output = model(data) # sum up batch loss test_loss += F.nll_loss(output, target).item() # get the index of the max pred = output.data.max(1, keepdim=True)[1] correct += pred.eq(target.data.view_as(pred)).cpu().sum() test_loss /= len(test_loader.dataset) print(&#39;\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&#39;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) for epoch in range(1,100): starttime = datetime.datetime.now() train(epoch) endtime = datetime.datetime.now() y_time.append(endtime - starttime) with torch.no_grad(): test() &nbsp;卷积层 正确率百分之六十三 import torch import torchvision import torchvision.transforms as transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=1) testset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=1) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) # 3*32*32 from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import datetime y_loss=[] y_time=[] class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 8, 7)#输入3通道，输出6通道，kernelsize=5 #通过stride可以定义每次卷积移动的步长 self.pool = nn.MaxPool2d(2, 2) #通过池化对信息进行筛选过滤 self.conv2 = nn.Conv2d(8, 24, 7) #self.fc0 = nn.Linear(16 *5*5,10) #也可以直接进行输出，不用全连接层 self.fc1 = nn.Linear(24 * 3 * 3,128) self.fc2 = nn.Linear(128,64) self.fc3 = nn.Linear(64, 10) def forward(self, x): #print(0,x.size()) x = self.pool(F.relu(self.conv1(x))) #print(1,x.size()) x = self.pool(F.relu(self.conv2(x))) #print(2,x.size()) # fully connect x = x.view(-1, 24 * 3 * 3) # x = self.fc0(x) # print(3,x.size()) x = F.relu(self.fc1(x)) # print(4,x.size()) x = F.relu(self.fc2(x)) # print(5,x.size()) x = self.fc3(x) # print(6,x.size()) return x net = Net() import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) if __name__ == &#39;__main__&#39;: print(&quot;Beginning Training&quot;) for epoch in range(80): starttime = datetime.datetime.now() running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if (i+1) % 12500 == 0: print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 3125)) running_loss = 0.0 y_loss.append(loss.item()) endtime = datetime.datetime.now() y_time.append(endtime - starttime) print(&quot;Finished Training&quot;) print(&quot;Beginning Testing&quot;) correct = 0 total = 0 for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % (100 * correct / total)) &nbsp;" />
<meta property="og:description" content="基于 PyTorch 的Cifar图像分类器代码实现 做实验的时候一共要求做三个实验分别是只使用线性层实现，通过全连接实现，通过卷积实现，代码也是参考了许多前人的代码，有一定程度的修改，然而准确度低的不成样子，望各位海涵。 同时本次的实验是在python 3.7 Jupyter上运行的，Cifar数据在py文件同级目录下的data文件中。由于我的显卡不支持CUDA因此都是直接用cpu搞定的。最长的100个epoch可能要跑一两个小时左右，基本上40个epoch就可以得出大概的结果 仅使用线性层实现图像分类&nbsp; 正确率百分之四十 import torch import torch.nn as nn import torchvision import torchvision.transforms as transforms import torchvision import datetime transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # 训练模型的超参数 input_size = 3072 output_size = 10 num_epochs = 50 batch_size = 64 learning_rate = 0.001 train_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) test_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) #线性层的建立 model = nn.Linear(input_size, output_size) # 损失函数和优化算法 criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # train_loader的大小，也就是含有多少个bach。 total_step = len(train_loader) # 训练模型 # 在整个数据集上迭代的次数 for epoch in range(num_epochs): starttime = datetime.datetime.now() # 每次取一个bach进行训练。 for i, (images, labels) in enumerate(train_loader): # 将数据reshape到模型需要的大小。 images = images.reshape(-1,3*32*32) # 前向传播 outputs = model(images) # 计算模型的loss loss = criterion(outputs, labels) # 后向传播，更新模型参数 optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 782 == 0: print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39; .format(epoch+1, num_epochs, i+1, total_step, loss.item())) with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, 3*32*32) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print(&#39;Accuracy of the model on the 10000 test images: {} %&#39;.format(100 * correct / total)) &nbsp;使用全连接层 正确率百分之五十六 import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms from torch.autograd import Variable import torchvision import datetime # Training settings batch_size = 128 transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) #更换下顺序便于训练多次 train_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) test_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) # Data Loader (Input Pipeline) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) y_loss=[] y_time=[] #在cifar中，为一个3*32*32的图像 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.l0 = nn.Linear(3072,2048) self.l1 = nn.Linear(2048, 1024) self.l2 = nn.Linear(1024, 512) self.l3 = nn.Linear(512, 256) self.l4 = nn.Linear(256, 128) self.l5 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 3072) x = F.relu(self.l0(x)) x = F.relu(self.l1(x)) x = F.relu(self.l2(x)) x = F.relu(self.l3(x)) x = F.relu(self.l4(x)) return F.log_softmax(self.l5(x), dim=1) #return self.l5(x) #如果直接使用self.15(x)会出现NAN的情况，也就是梯度爆炸 model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) def train(epoch): # 每次输入barch_idx个数据 for batch_idx, (data, target) in enumerate(train_loader): data, target = Variable(data), Variable(target) optimizer.zero_grad() output = model(data) # loss loss = F.nll_loss(output, target) loss.backward() # update optimizer.step() if batch_idx % 391 == 0: print(&#39;Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}&#39;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) y_loss.append(loss.item()) def test(): test_loss = 0 correct = 0 # 测试集 for data, target in test_loader: data, target = Variable(data, volatile=True), Variable(target) output = model(data) # sum up batch loss test_loss += F.nll_loss(output, target).item() # get the index of the max pred = output.data.max(1, keepdim=True)[1] correct += pred.eq(target.data.view_as(pred)).cpu().sum() test_loss /= len(test_loader.dataset) print(&#39;\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&#39;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) for epoch in range(1,100): starttime = datetime.datetime.now() train(epoch) endtime = datetime.datetime.now() y_time.append(endtime - starttime) with torch.no_grad(): test() &nbsp;卷积层 正确率百分之六十三 import torch import torchvision import torchvision.transforms as transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=1) testset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=1) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) # 3*32*32 from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import datetime y_loss=[] y_time=[] class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 8, 7)#输入3通道，输出6通道，kernelsize=5 #通过stride可以定义每次卷积移动的步长 self.pool = nn.MaxPool2d(2, 2) #通过池化对信息进行筛选过滤 self.conv2 = nn.Conv2d(8, 24, 7) #self.fc0 = nn.Linear(16 *5*5,10) #也可以直接进行输出，不用全连接层 self.fc1 = nn.Linear(24 * 3 * 3,128) self.fc2 = nn.Linear(128,64) self.fc3 = nn.Linear(64, 10) def forward(self, x): #print(0,x.size()) x = self.pool(F.relu(self.conv1(x))) #print(1,x.size()) x = self.pool(F.relu(self.conv2(x))) #print(2,x.size()) # fully connect x = x.view(-1, 24 * 3 * 3) # x = self.fc0(x) # print(3,x.size()) x = F.relu(self.fc1(x)) # print(4,x.size()) x = F.relu(self.fc2(x)) # print(5,x.size()) x = self.fc3(x) # print(6,x.size()) return x net = Net() import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) if __name__ == &#39;__main__&#39;: print(&quot;Beginning Training&quot;) for epoch in range(80): starttime = datetime.datetime.now() running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if (i+1) % 12500 == 0: print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 3125)) running_loss = 0.0 y_loss.append(loss.item()) endtime = datetime.datetime.now() y_time.append(endtime - starttime) print(&quot;Finished Training&quot;) print(&quot;Beginning Testing&quot;) correct = 0 total = 0 for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % (100 * correct / total)) &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/07/05/790376.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/05/790376.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-05T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"基于 PyTorch 的Cifar图像分类器代码实现 做实验的时候一共要求做三个实验分别是只使用线性层实现，通过全连接实现，通过卷积实现，代码也是参考了许多前人的代码，有一定程度的修改，然而准确度低的不成样子，望各位海涵。 同时本次的实验是在python 3.7 Jupyter上运行的，Cifar数据在py文件同级目录下的data文件中。由于我的显卡不支持CUDA因此都是直接用cpu搞定的。最长的100个epoch可能要跑一两个小时左右，基本上40个epoch就可以得出大概的结果 仅使用线性层实现图像分类&nbsp; 正确率百分之四十 import torch import torch.nn as nn import torchvision import torchvision.transforms as transforms import torchvision import datetime transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # 训练模型的超参数 input_size = 3072 output_size = 10 num_epochs = 50 batch_size = 64 learning_rate = 0.001 train_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) test_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) #线性层的建立 model = nn.Linear(input_size, output_size) # 损失函数和优化算法 criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # train_loader的大小，也就是含有多少个bach。 total_step = len(train_loader) # 训练模型 # 在整个数据集上迭代的次数 for epoch in range(num_epochs): starttime = datetime.datetime.now() # 每次取一个bach进行训练。 for i, (images, labels) in enumerate(train_loader): # 将数据reshape到模型需要的大小。 images = images.reshape(-1,3*32*32) # 前向传播 outputs = model(images) # 计算模型的loss loss = criterion(outputs, labels) # 后向传播，更新模型参数 optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 782 == 0: print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39; .format(epoch+1, num_epochs, i+1, total_step, loss.item())) with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, 3*32*32) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print(&#39;Accuracy of the model on the 10000 test images: {} %&#39;.format(100 * correct / total)) &nbsp;使用全连接层 正确率百分之五十六 import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms from torch.autograd import Variable import torchvision import datetime # Training settings batch_size = 128 transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) #更换下顺序便于训练多次 train_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) test_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) # Data Loader (Input Pipeline) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) y_loss=[] y_time=[] #在cifar中，为一个3*32*32的图像 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.l0 = nn.Linear(3072,2048) self.l1 = nn.Linear(2048, 1024) self.l2 = nn.Linear(1024, 512) self.l3 = nn.Linear(512, 256) self.l4 = nn.Linear(256, 128) self.l5 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 3072) x = F.relu(self.l0(x)) x = F.relu(self.l1(x)) x = F.relu(self.l2(x)) x = F.relu(self.l3(x)) x = F.relu(self.l4(x)) return F.log_softmax(self.l5(x), dim=1) #return self.l5(x) #如果直接使用self.15(x)会出现NAN的情况，也就是梯度爆炸 model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) def train(epoch): # 每次输入barch_idx个数据 for batch_idx, (data, target) in enumerate(train_loader): data, target = Variable(data), Variable(target) optimizer.zero_grad() output = model(data) # loss loss = F.nll_loss(output, target) loss.backward() # update optimizer.step() if batch_idx % 391 == 0: print(&#39;Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}&#39;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) y_loss.append(loss.item()) def test(): test_loss = 0 correct = 0 # 测试集 for data, target in test_loader: data, target = Variable(data, volatile=True), Variable(target) output = model(data) # sum up batch loss test_loss += F.nll_loss(output, target).item() # get the index of the max pred = output.data.max(1, keepdim=True)[1] correct += pred.eq(target.data.view_as(pred)).cpu().sum() test_loss /= len(test_loader.dataset) print(&#39;\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n&#39;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) for epoch in range(1,100): starttime = datetime.datetime.now() train(epoch) endtime = datetime.datetime.now() y_time.append(endtime - starttime) with torch.no_grad(): test() &nbsp;卷积层 正确率百分之六十三 import torch import torchvision import torchvision.transforms as transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=1) testset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=1) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) # 3*32*32 from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import datetime y_loss=[] y_time=[] class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 8, 7)#输入3通道，输出6通道，kernelsize=5 #通过stride可以定义每次卷积移动的步长 self.pool = nn.MaxPool2d(2, 2) #通过池化对信息进行筛选过滤 self.conv2 = nn.Conv2d(8, 24, 7) #self.fc0 = nn.Linear(16 *5*5,10) #也可以直接进行输出，不用全连接层 self.fc1 = nn.Linear(24 * 3 * 3,128) self.fc2 = nn.Linear(128,64) self.fc3 = nn.Linear(64, 10) def forward(self, x): #print(0,x.size()) x = self.pool(F.relu(self.conv1(x))) #print(1,x.size()) x = self.pool(F.relu(self.conv2(x))) #print(2,x.size()) # fully connect x = x.view(-1, 24 * 3 * 3) # x = self.fc0(x) # print(3,x.size()) x = F.relu(self.fc1(x)) # print(4,x.size()) x = F.relu(self.fc2(x)) # print(5,x.size()) x = self.fc3(x) # print(6,x.size()) return x net = Net() import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) if __name__ == &#39;__main__&#39;: print(&quot;Beginning Training&quot;) for epoch in range(80): starttime = datetime.datetime.now() running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if (i+1) % 12500 == 0: print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 3125)) running_loss = 0.0 y_loss.append(loss.item()) endtime = datetime.datetime.now() y_time.append(endtime - starttime) print(&quot;Finished Training&quot;) print(&quot;Beginning Testing&quot;) correct = 0 total = 0 for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print(&#39;Accuracy of the network on the 10000 test images: %d %%&#39; % (100 * correct / total)) &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/07/05/790376.html","headline":"基于 PyTorch 的Cifar图像分类器代码实现（线性，全连接，卷积）","dateModified":"2019-07-05T00:00:00+08:00","datePublished":"2019-07-05T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/05/790376.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>基于 PyTorch 的Cifar图像分类器代码实现（线性，全连接，卷积）</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix" data-track-view="{&quot;mod&quot;:&quot;popu_307&quot;,&quot;con&quot;:&quot;,https://blog.csdn.net/weixin_40693859/article/details/94736657,&quot;}" data-track-click="{&quot;mod&quot;:&quot;popu_307&quot;,&quot;con&quot;:&quot;,https://blog.csdn.net/weixin_40693859/article/details/94736657&quot;}"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-cd6c485e8b.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-cd6c485e8b.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1>基于 PyTorch 的Cifar图像分类器代码实现</h1> 
  <p style="text-indent:50px;">做实验的时候一共要求做三个实验分别是只使用线性层实现，通过全连接实现，通过卷积实现，代码也是参考了许多前人的代码，有一定程度的修改，然而准确度低的不成样子，望各位海涵。</p> 
  <p style="text-indent:50px;">同时本次的实验是在python 3.7 Jupyter上运行的，Cifar数据在py文件同级目录下的data文件中。由于我的显卡不支持CUDA因此都是直接用cpu搞定的。最长的100个epoch可能要跑一两个小时左右，基本上40个epoch就可以得出大概的结果</p> 
  <blockquote> 
   <p style="text-indent:50px;">仅使用线性层实现图像分类&nbsp; 正确率百分之四十</p> 
  </blockquote> 
  <pre class="has">
<code class="language-python">import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import torchvision
import datetime

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
 
# 训练模型的超参数
input_size = 3072
output_size = 10
num_epochs = 50
batch_size = 64
learning_rate = 0.001
 

train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)

test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
 
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)
 
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)

#线性层的建立
model = nn.Linear(input_size, output_size)
 
# 损失函数和优化算法
criterion = nn.CrossEntropyLoss()  
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  
 
# train_loader的大小，也就是含有多少个bach。
total_step = len(train_loader)
# 训练模型
# 在整个数据集上迭代的次数
for epoch in range(num_epochs):
    starttime = datetime.datetime.now()
    # 每次取一个bach进行训练。
    for i, (images, labels) in enumerate(train_loader):
        # 将数据reshape到模型需要的大小。
        images = images.reshape(-1,3*32*32)
        # 前向传播
        outputs = model(images)
        # 计算模型的loss
        loss = criterion(outputs, labels)
        # 后向传播，更新模型参数
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if (i+1) % 782 == 0:
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' 
                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_loader:
        images = images.reshape(-1, 3*32*32)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum()   
    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))</code></pre> 
  <blockquote> 
   <p>&nbsp;使用全连接层 正确率百分之五十六</p> 
  </blockquote> 
  <pre class="has">
<code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.autograd import Variable
import torchvision
import datetime
# Training settings
batch_size = 128
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
#更换下顺序便于训练多次
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)

test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)

# Data Loader (Input Pipeline)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)
y_loss=[]
y_time=[]
#在cifar中，为一个3*32*32的图像
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.l0 = nn.Linear(3072,2048)
        self.l1 = nn.Linear(2048, 1024)
        self.l2 = nn.Linear(1024, 512)
        self.l3 = nn.Linear(512, 256)
        self.l4 = nn.Linear(256, 128)
        self.l5 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(-1, 3072)
        x = F.relu(self.l0(x))
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        x = F.relu(self.l3(x))
        x = F.relu(self.l4(x))
        return F.log_softmax(self.l5(x), dim=1)
        #return self.l5(x)
        #如果直接使用self.15(x)会出现NAN的情况，也就是梯度爆炸

model = Net()

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)

def train(epoch):
    # 每次输入barch_idx个数据
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)

        optimizer.zero_grad()
        output = model(data)
        # loss
        loss = F.nll_loss(output, target)
        loss.backward()
        # update
        optimizer.step()
        if batch_idx % 391 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
            y_loss.append(loss.item())

def test():
    test_loss = 0
    correct = 0
    # 测试集
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        # sum up batch loss
        test_loss += F.nll_loss(output, target).item()
        # get the index of the max
        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

for epoch in range(1,100):
    starttime = datetime.datetime.now()
    train(epoch)
    endtime = datetime.datetime.now()
    y_time.append(endtime - starttime)
with torch.no_grad(): 
        test() </code></pre> 
  <blockquote> 
   <p>&nbsp;卷积层 正确率百分之六十三</p> 
  </blockquote> 
  <pre class="has">
<code class="language-python">import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=1)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=1)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

# 3*32*32
from torch.autograd import  Variable
import torch.nn as nn
import torch.nn.functional as F
import datetime
y_loss=[]
y_time=[]
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 8, 7)#输入3通道，输出6通道，kernelsize=5
        #通过stride可以定义每次卷积移动的步长
        self.pool = nn.MaxPool2d(2, 2)
        #通过池化对信息进行筛选过滤
        self.conv2 = nn.Conv2d(8, 24, 7)
        #self.fc0 = nn.Linear(16 *5*5,10)
        
        #也可以直接进行输出，不用全连接层
        self.fc1 = nn.Linear(24 * 3 * 3,128)
        self.fc2 = nn.Linear(128,64)
        self.fc3 = nn.Linear(64, 10)

    def forward(self, x):
        #print(0,x.size())
        x = self.pool(F.relu(self.conv1(x)))
        #print(1,x.size())
        x = self.pool(F.relu(self.conv2(x)))
        #print(2,x.size())
        # fully connect
        x = x.view(-1, 24 * 3 * 3)
       # x = self.fc0(x)
       # print(3,x.size())
        x = F.relu(self.fc1(x))
       # print(4,x.size())
        x = F.relu(self.fc2(x))
       # print(5,x.size())
        x = self.fc3(x)
       # print(6,x.size())
        return x
    
net = Net()

import torch.optim as optim

criterion = nn.CrossEntropyLoss()

optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

if __name__ ==  '__main__':
    print("Beginning Training")
    for epoch in range(80):
        starttime = datetime.datetime.now()
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            if (i+1) % 12500 == 0: 
                print('[%d, %5d] loss: %.3f' %
                      (epoch + 1, i + 1, running_loss / 3125))
                running_loss = 0.0
                y_loss.append(loss.item())
                endtime = datetime.datetime.now()
                y_time.append(endtime - starttime)
    print("Finished Training")

    print("Beginning Testing")
    correct = 0
    total = 0
    for data in testloader:
        images, labels = data
        outputs = net(Variable(images))
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum()

    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))
</code></pre> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
