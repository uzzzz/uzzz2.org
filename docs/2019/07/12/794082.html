<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>从0开始学大数据-Hive基础篇 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="从0开始学大数据-Hive基础篇" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 Hive的特点 Hive体系架构 Hive运行机制 Hive与关系数据库的区别 Hive中的基本数据类型 基本数据类型 复杂数据类型 Hive表 创建表 Hive表类型 内部表 外部表 分区表 桶表 常用HiveQL整理 自定义函数 UDF(user-defined functions) UDAF(user-defined aggregation functions) UDTF(user-defined table-generating functions) Hive起源于Facebook，是基于 Hadoop HDFS 分布式文件系统的分布式 数据仓库 架构。它为数据仓库的管理提供了许多功能：数据ETL（抽取、转换和加载）工具、数据存储管理和大型数据集的查询和分析能力。同时Hive还定义了类SQL的语言（HiveQL）。允许用户进行和SQL相似的操作，它可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能。还允许开发人员方便的使用Mapper和Reducer操作，可以将SQL语句转换为MapReduce任务运行。 Hive的特点 Hive作为Hadoop之上的数据仓库处理工具，它所有的数据都是存储在Hadoop兼容的文件系统中。Hive在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive设定的目录下。因此，Hive不支持对数据的修改和添加，所有的数据都是在加载的时候确定的。 Hive 最大的特点是通过类 SQL 来分析大数据，而避免了写 MapReduce 程序来分析数据，这样使得分析数据更容易。 数据是存储在 HDFS 上的，Hive 本身并不提供数据的存储功能 Hive 是将数据映射成数据库和一张张的表，库和表的元数据信息一般存在关系型数据库上（比如 MySQL）。 数据存储方面：它能够存储很大的数据集，并且对数据完整性、格式要求并不严格。 数据处理方面：不适用于实时计算和响应，使用于离线分析。 Hive体系架构 用户接口：包括CLI，JDBC/ODBC，WebUI等方式。 元数据Metastore：通常存储在关系数据库如mysql，derby中。元数据包括：表名，表所属的数据库，表的列/分区字段，表的属性（是否为外部表等），表的数据所在目录等信息。 Driver：包括解释器、编译器、优化器、执行器。HiveQL查询语句从词法分析。语法分析、编译、优化以及查询计划生成。生成的查询计划存储在HDFS中，并在后面的MapReduce中进行调用执行。 解析器：将SQL转换成抽象的语法树AST。 编译器：将抽象语法树AST编译生成逻辑执行计划。 优化器：将逻辑执行计划进行优化。 执行器：把逻辑执行计划转换成可以运行的物理执行计划。 Hadoop：使用HDFS进行存储，利用MapReduce进行计算。 Hive运行机制 编写HiveQL，并提交； Hive解析查询语句，并生成查询计划； Hive将查询计划转化为MR作业； 运行MR作业，得到最终结果。 Hive与关系数据库的区别 Hive中的基本数据类型 基本数据类型 Hive支持关系数据库中大多数据基本数据类型，同时还支持三种复杂类型。 复杂数据类型 示例： Hive表 创建表 -- 直接建表法 create table t_page_view ( page_id bigint comment &#39;页面ID&#39;, page_name string comment &#39;页面名称&#39;, page_url string comment &#39;页面URL&#39; ) comment &#39;页面视图&#39; partitioned by (ds string comment &#39;当前时间，用于分区字段&#39;) stored as parquet location &#39;/user/hive/warehouse/t_page_view&#39;; -- 查询建表法 create table t_page_view2 as select * from t_page_view; -- like建表法(克隆表) create table t_page_view3 like t_page_view; Hive表类型 内部表 Hive中默认创建的是内部表，内部表的数据由Hive来管理。在 drop表的时候，表的数据和元数据都会被删除。 CREATE TABLE pokes ( foo INT, bar STRING ); 外部表 外部表的数据不由Hive管理。Hive可以创建外部表和 HDFS、HBase 和 Elasticsearch 等进行整合。drop外部表时，只会删除元数据，不会删除真实数据。外部表关键字EXTERNAL。 CREATE EXTERNAL TABLE IF NOT EXISTS tb_station ( station string, lon string, lat string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LOCATION &#39;/user/test/coordinate/&#39;; 内部表与外部表转换 -- 内部表转外部表 alter table tableA set TBLPROPERTIES(&#39;EXTERNAL&#39;=&#39;true&#39;); -- 外部表转内部表 alter table tableA set TBLPROPERTIES(&#39;EXTERNAL&#39;=&#39;false&#39;); 分区表 Hive表的数据是存储在HFDS中对应的目录中，普通表的数据直接存储在这个目录下，而分区表存储时，会再划分子目录来存储，一个分区对应一个子目录。主要作用是来优化查询性能。在Hive表查询的时候，如果指定了分区字段作为筛选条件，那么只需要到对应的分区目录下去检索数据即可，减少了处理的数据量，从而有效的提高了效率。在对分区表进行查询时，尽量使用分区字段作为筛选条件。 CREATE TABLE invites ( foo INT, bar STRING ) PARTITIONED BY (ds STRING); 桶表 分桶是将数据分解成更容易管理的若干部分的技术，桶表示对数据源数据文件本身来拆分数据。桶是更为细粒度的数据范围划分。Hive采用对列进行哈希，然后对桶的个数取模的方式决定该条记录存放在哪个桶当中。 创建桶表时，需要指定桶的个数，分桶的依据字段，Hive就可以自动将数据分桶存储。 查询时，只需要遍历一个桶里的数据，或者遍历部分桶，这样就提高了查询效率。 示例： CREATE TABLE ods.tb_ad_action_d ( op_time STRING, user_id STRING, task_id STRING, event_name STRING ) CLUSTERED BY (user_id) SORTED BY(task_id) INTO 10 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; STORED AS TEXTFILE; CLUSTERED BY：是指根据user_id的值进行哈希后模除分桶个数，根据得到的结果，确定这行数据分入哪个桶中。 SORTED BY：指定桶中的数据以哪个字段进行排序，排序的好处是，在 join 操作时能够获得很高的效率。 INTO 10 BUCKETS：指定一共分多少个桶。 常用HiveQL整理 Database -- 创建database create database IF NOT EXISTS ods; USE database_name; USE DEFAULT; -- 删除database drop database if exists ods; Table -- 创建内部表（默认） create table trade_detail( id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39;; -- 创建分区表 create table td_part( id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by &#39;\t&#39;; -- 创建外部表 create external table td_ext( id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39; location &#39;/data/td_ext&#39;; -- 创建外部分区表 create external table td_part_ext( id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) stored as parquet tblproperties(&quot;parquet.compress&quot;=&quot;SNAPPY&quot;) location &#39;/data/td_part_ext&#39;; drop table if exists td_part_ext; Alter -- 修改表明 ALTER TABLE page_view RENAME to page_view_new; -- 修改字段 ALTER TABLE page_view CHANGE ip ip_address string AFTER refererurl; -- 添加字段 ALTER TABLE page_view ADD COLUMNS (name string comment &#39;view name&#39;); -- 添加分区 ALTER TABLE page_view ADD IF NOT EXISTS PARTITION (dt=&#39;20190705&#39;) LOCATION=&#39;/data/page_view/dt=20190705&#39;; -- 修改location ALTER TABLE page_view PARTITION(dt=&#39;20190706&#39;) SET LOCATION &quot;/data/page_view/dt=20190706&quot;; -- 修改分隔符 ALTER TABLE page_view SET SERDEPROPERTIES (&#39;field.delim&#39; = &#39;,&#39;); -- 删除分区 ALTER TABLE page_view DROP PARTITION (dt=&#39;2008-08-08&#39;, country=&#39;us&#39;); Show -- 查看创建表语句 show create table td_part; -- 查看表分区 show partitions td_part; 修复分区 -- 修复分区 msck repair table page_view; 数据导入 -- 将本地文件导入到hive load data local inpath &#39;/home/hadoop/student&#39; overwrite into table student partition(state=&#39;Sichuan&#39;, city=&#39;Chengdu&#39;); -- 将hdfs上文件导入到hive load data inpath &#39;/user/hadoop/add.txt&#39; into table student partition(state=&#39;Sichuan&#39;, city=&#39;Chengdu&#39;); -- 从别的表中查询出相应的数据并导入到hive表中 insert into table test partition(age=&#39;25&#39;) select id ,name from wyp where age=&#39;25&#39;; -- 在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中 create table tmp as select * from student where age&gt;&#39;18&#39;; 数据导出 -- 导出到本地文件系统 insert overwrite local directory &#39;/home/hadoop/student&#39; select * from student; -- 导出到hdfs insert overwrite directory &#39;/user/hadoop/student&#39; select * from student; -- 将查询结果插入到表中（追加） insert into table student_new select id,name from student where age=&#39;25&#39;; -- 将查询结果插入到表中（覆盖） insert into table student_new select id,name from student where age=&#39;25&#39;; 自定义函数 UDF(user-defined functions) 用户自定义普通函数，用于处理单行数据，并生成单个数据行。用来自定义完成对字段处理的函数。 UDAF(user-defined aggregation functions) 聚合函数，多对一，多行变一行，需和group by联合使用。 UDTF(user-defined table-generating functions) 表生成函数，一对多，一行变多行。" />
<meta property="og:description" content="文章目录 Hive的特点 Hive体系架构 Hive运行机制 Hive与关系数据库的区别 Hive中的基本数据类型 基本数据类型 复杂数据类型 Hive表 创建表 Hive表类型 内部表 外部表 分区表 桶表 常用HiveQL整理 自定义函数 UDF(user-defined functions) UDAF(user-defined aggregation functions) UDTF(user-defined table-generating functions) Hive起源于Facebook，是基于 Hadoop HDFS 分布式文件系统的分布式 数据仓库 架构。它为数据仓库的管理提供了许多功能：数据ETL（抽取、转换和加载）工具、数据存储管理和大型数据集的查询和分析能力。同时Hive还定义了类SQL的语言（HiveQL）。允许用户进行和SQL相似的操作，它可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能。还允许开发人员方便的使用Mapper和Reducer操作，可以将SQL语句转换为MapReduce任务运行。 Hive的特点 Hive作为Hadoop之上的数据仓库处理工具，它所有的数据都是存储在Hadoop兼容的文件系统中。Hive在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive设定的目录下。因此，Hive不支持对数据的修改和添加，所有的数据都是在加载的时候确定的。 Hive 最大的特点是通过类 SQL 来分析大数据，而避免了写 MapReduce 程序来分析数据，这样使得分析数据更容易。 数据是存储在 HDFS 上的，Hive 本身并不提供数据的存储功能 Hive 是将数据映射成数据库和一张张的表，库和表的元数据信息一般存在关系型数据库上（比如 MySQL）。 数据存储方面：它能够存储很大的数据集，并且对数据完整性、格式要求并不严格。 数据处理方面：不适用于实时计算和响应，使用于离线分析。 Hive体系架构 用户接口：包括CLI，JDBC/ODBC，WebUI等方式。 元数据Metastore：通常存储在关系数据库如mysql，derby中。元数据包括：表名，表所属的数据库，表的列/分区字段，表的属性（是否为外部表等），表的数据所在目录等信息。 Driver：包括解释器、编译器、优化器、执行器。HiveQL查询语句从词法分析。语法分析、编译、优化以及查询计划生成。生成的查询计划存储在HDFS中，并在后面的MapReduce中进行调用执行。 解析器：将SQL转换成抽象的语法树AST。 编译器：将抽象语法树AST编译生成逻辑执行计划。 优化器：将逻辑执行计划进行优化。 执行器：把逻辑执行计划转换成可以运行的物理执行计划。 Hadoop：使用HDFS进行存储，利用MapReduce进行计算。 Hive运行机制 编写HiveQL，并提交； Hive解析查询语句，并生成查询计划； Hive将查询计划转化为MR作业； 运行MR作业，得到最终结果。 Hive与关系数据库的区别 Hive中的基本数据类型 基本数据类型 Hive支持关系数据库中大多数据基本数据类型，同时还支持三种复杂类型。 复杂数据类型 示例： Hive表 创建表 -- 直接建表法 create table t_page_view ( page_id bigint comment &#39;页面ID&#39;, page_name string comment &#39;页面名称&#39;, page_url string comment &#39;页面URL&#39; ) comment &#39;页面视图&#39; partitioned by (ds string comment &#39;当前时间，用于分区字段&#39;) stored as parquet location &#39;/user/hive/warehouse/t_page_view&#39;; -- 查询建表法 create table t_page_view2 as select * from t_page_view; -- like建表法(克隆表) create table t_page_view3 like t_page_view; Hive表类型 内部表 Hive中默认创建的是内部表，内部表的数据由Hive来管理。在 drop表的时候，表的数据和元数据都会被删除。 CREATE TABLE pokes ( foo INT, bar STRING ); 外部表 外部表的数据不由Hive管理。Hive可以创建外部表和 HDFS、HBase 和 Elasticsearch 等进行整合。drop外部表时，只会删除元数据，不会删除真实数据。外部表关键字EXTERNAL。 CREATE EXTERNAL TABLE IF NOT EXISTS tb_station ( station string, lon string, lat string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LOCATION &#39;/user/test/coordinate/&#39;; 内部表与外部表转换 -- 内部表转外部表 alter table tableA set TBLPROPERTIES(&#39;EXTERNAL&#39;=&#39;true&#39;); -- 外部表转内部表 alter table tableA set TBLPROPERTIES(&#39;EXTERNAL&#39;=&#39;false&#39;); 分区表 Hive表的数据是存储在HFDS中对应的目录中，普通表的数据直接存储在这个目录下，而分区表存储时，会再划分子目录来存储，一个分区对应一个子目录。主要作用是来优化查询性能。在Hive表查询的时候，如果指定了分区字段作为筛选条件，那么只需要到对应的分区目录下去检索数据即可，减少了处理的数据量，从而有效的提高了效率。在对分区表进行查询时，尽量使用分区字段作为筛选条件。 CREATE TABLE invites ( foo INT, bar STRING ) PARTITIONED BY (ds STRING); 桶表 分桶是将数据分解成更容易管理的若干部分的技术，桶表示对数据源数据文件本身来拆分数据。桶是更为细粒度的数据范围划分。Hive采用对列进行哈希，然后对桶的个数取模的方式决定该条记录存放在哪个桶当中。 创建桶表时，需要指定桶的个数，分桶的依据字段，Hive就可以自动将数据分桶存储。 查询时，只需要遍历一个桶里的数据，或者遍历部分桶，这样就提高了查询效率。 示例： CREATE TABLE ods.tb_ad_action_d ( op_time STRING, user_id STRING, task_id STRING, event_name STRING ) CLUSTERED BY (user_id) SORTED BY(task_id) INTO 10 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; STORED AS TEXTFILE; CLUSTERED BY：是指根据user_id的值进行哈希后模除分桶个数，根据得到的结果，确定这行数据分入哪个桶中。 SORTED BY：指定桶中的数据以哪个字段进行排序，排序的好处是，在 join 操作时能够获得很高的效率。 INTO 10 BUCKETS：指定一共分多少个桶。 常用HiveQL整理 Database -- 创建database create database IF NOT EXISTS ods; USE database_name; USE DEFAULT; -- 删除database drop database if exists ods; Table -- 创建内部表（默认） create table trade_detail( id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39;; -- 创建分区表 create table td_part( id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by &#39;\t&#39;; -- 创建外部表 create external table td_ext( id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39; location &#39;/data/td_ext&#39;; -- 创建外部分区表 create external table td_part_ext( id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) stored as parquet tblproperties(&quot;parquet.compress&quot;=&quot;SNAPPY&quot;) location &#39;/data/td_part_ext&#39;; drop table if exists td_part_ext; Alter -- 修改表明 ALTER TABLE page_view RENAME to page_view_new; -- 修改字段 ALTER TABLE page_view CHANGE ip ip_address string AFTER refererurl; -- 添加字段 ALTER TABLE page_view ADD COLUMNS (name string comment &#39;view name&#39;); -- 添加分区 ALTER TABLE page_view ADD IF NOT EXISTS PARTITION (dt=&#39;20190705&#39;) LOCATION=&#39;/data/page_view/dt=20190705&#39;; -- 修改location ALTER TABLE page_view PARTITION(dt=&#39;20190706&#39;) SET LOCATION &quot;/data/page_view/dt=20190706&quot;; -- 修改分隔符 ALTER TABLE page_view SET SERDEPROPERTIES (&#39;field.delim&#39; = &#39;,&#39;); -- 删除分区 ALTER TABLE page_view DROP PARTITION (dt=&#39;2008-08-08&#39;, country=&#39;us&#39;); Show -- 查看创建表语句 show create table td_part; -- 查看表分区 show partitions td_part; 修复分区 -- 修复分区 msck repair table page_view; 数据导入 -- 将本地文件导入到hive load data local inpath &#39;/home/hadoop/student&#39; overwrite into table student partition(state=&#39;Sichuan&#39;, city=&#39;Chengdu&#39;); -- 将hdfs上文件导入到hive load data inpath &#39;/user/hadoop/add.txt&#39; into table student partition(state=&#39;Sichuan&#39;, city=&#39;Chengdu&#39;); -- 从别的表中查询出相应的数据并导入到hive表中 insert into table test partition(age=&#39;25&#39;) select id ,name from wyp where age=&#39;25&#39;; -- 在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中 create table tmp as select * from student where age&gt;&#39;18&#39;; 数据导出 -- 导出到本地文件系统 insert overwrite local directory &#39;/home/hadoop/student&#39; select * from student; -- 导出到hdfs insert overwrite directory &#39;/user/hadoop/student&#39; select * from student; -- 将查询结果插入到表中（追加） insert into table student_new select id,name from student where age=&#39;25&#39;; -- 将查询结果插入到表中（覆盖） insert into table student_new select id,name from student where age=&#39;25&#39;; 自定义函数 UDF(user-defined functions) 用户自定义普通函数，用于处理单行数据，并生成单个数据行。用来自定义完成对字段处理的函数。 UDAF(user-defined aggregation functions) 聚合函数，多对一，多行变一行，需和group by联合使用。 UDTF(user-defined table-generating functions) 表生成函数，一对多，一行变多行。" />
<link rel="canonical" href="https://uzzz.org/2019/07/12/794082.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/12/794082.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-12T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 Hive的特点 Hive体系架构 Hive运行机制 Hive与关系数据库的区别 Hive中的基本数据类型 基本数据类型 复杂数据类型 Hive表 创建表 Hive表类型 内部表 外部表 分区表 桶表 常用HiveQL整理 自定义函数 UDF(user-defined functions) UDAF(user-defined aggregation functions) UDTF(user-defined table-generating functions) Hive起源于Facebook，是基于 Hadoop HDFS 分布式文件系统的分布式 数据仓库 架构。它为数据仓库的管理提供了许多功能：数据ETL（抽取、转换和加载）工具、数据存储管理和大型数据集的查询和分析能力。同时Hive还定义了类SQL的语言（HiveQL）。允许用户进行和SQL相似的操作，它可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能。还允许开发人员方便的使用Mapper和Reducer操作，可以将SQL语句转换为MapReduce任务运行。 Hive的特点 Hive作为Hadoop之上的数据仓库处理工具，它所有的数据都是存储在Hadoop兼容的文件系统中。Hive在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive设定的目录下。因此，Hive不支持对数据的修改和添加，所有的数据都是在加载的时候确定的。 Hive 最大的特点是通过类 SQL 来分析大数据，而避免了写 MapReduce 程序来分析数据，这样使得分析数据更容易。 数据是存储在 HDFS 上的，Hive 本身并不提供数据的存储功能 Hive 是将数据映射成数据库和一张张的表，库和表的元数据信息一般存在关系型数据库上（比如 MySQL）。 数据存储方面：它能够存储很大的数据集，并且对数据完整性、格式要求并不严格。 数据处理方面：不适用于实时计算和响应，使用于离线分析。 Hive体系架构 用户接口：包括CLI，JDBC/ODBC，WebUI等方式。 元数据Metastore：通常存储在关系数据库如mysql，derby中。元数据包括：表名，表所属的数据库，表的列/分区字段，表的属性（是否为外部表等），表的数据所在目录等信息。 Driver：包括解释器、编译器、优化器、执行器。HiveQL查询语句从词法分析。语法分析、编译、优化以及查询计划生成。生成的查询计划存储在HDFS中，并在后面的MapReduce中进行调用执行。 解析器：将SQL转换成抽象的语法树AST。 编译器：将抽象语法树AST编译生成逻辑执行计划。 优化器：将逻辑执行计划进行优化。 执行器：把逻辑执行计划转换成可以运行的物理执行计划。 Hadoop：使用HDFS进行存储，利用MapReduce进行计算。 Hive运行机制 编写HiveQL，并提交； Hive解析查询语句，并生成查询计划； Hive将查询计划转化为MR作业； 运行MR作业，得到最终结果。 Hive与关系数据库的区别 Hive中的基本数据类型 基本数据类型 Hive支持关系数据库中大多数据基本数据类型，同时还支持三种复杂类型。 复杂数据类型 示例： Hive表 创建表 -- 直接建表法 create table t_page_view ( page_id bigint comment &#39;页面ID&#39;, page_name string comment &#39;页面名称&#39;, page_url string comment &#39;页面URL&#39; ) comment &#39;页面视图&#39; partitioned by (ds string comment &#39;当前时间，用于分区字段&#39;) stored as parquet location &#39;/user/hive/warehouse/t_page_view&#39;; -- 查询建表法 create table t_page_view2 as select * from t_page_view; -- like建表法(克隆表) create table t_page_view3 like t_page_view; Hive表类型 内部表 Hive中默认创建的是内部表，内部表的数据由Hive来管理。在 drop表的时候，表的数据和元数据都会被删除。 CREATE TABLE pokes ( foo INT, bar STRING ); 外部表 外部表的数据不由Hive管理。Hive可以创建外部表和 HDFS、HBase 和 Elasticsearch 等进行整合。drop外部表时，只会删除元数据，不会删除真实数据。外部表关键字EXTERNAL。 CREATE EXTERNAL TABLE IF NOT EXISTS tb_station ( station string, lon string, lat string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LOCATION &#39;/user/test/coordinate/&#39;; 内部表与外部表转换 -- 内部表转外部表 alter table tableA set TBLPROPERTIES(&#39;EXTERNAL&#39;=&#39;true&#39;); -- 外部表转内部表 alter table tableA set TBLPROPERTIES(&#39;EXTERNAL&#39;=&#39;false&#39;); 分区表 Hive表的数据是存储在HFDS中对应的目录中，普通表的数据直接存储在这个目录下，而分区表存储时，会再划分子目录来存储，一个分区对应一个子目录。主要作用是来优化查询性能。在Hive表查询的时候，如果指定了分区字段作为筛选条件，那么只需要到对应的分区目录下去检索数据即可，减少了处理的数据量，从而有效的提高了效率。在对分区表进行查询时，尽量使用分区字段作为筛选条件。 CREATE TABLE invites ( foo INT, bar STRING ) PARTITIONED BY (ds STRING); 桶表 分桶是将数据分解成更容易管理的若干部分的技术，桶表示对数据源数据文件本身来拆分数据。桶是更为细粒度的数据范围划分。Hive采用对列进行哈希，然后对桶的个数取模的方式决定该条记录存放在哪个桶当中。 创建桶表时，需要指定桶的个数，分桶的依据字段，Hive就可以自动将数据分桶存储。 查询时，只需要遍历一个桶里的数据，或者遍历部分桶，这样就提高了查询效率。 示例： CREATE TABLE ods.tb_ad_action_d ( op_time STRING, user_id STRING, task_id STRING, event_name STRING ) CLUSTERED BY (user_id) SORTED BY(task_id) INTO 10 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39; STORED AS TEXTFILE; CLUSTERED BY：是指根据user_id的值进行哈希后模除分桶个数，根据得到的结果，确定这行数据分入哪个桶中。 SORTED BY：指定桶中的数据以哪个字段进行排序，排序的好处是，在 join 操作时能够获得很高的效率。 INTO 10 BUCKETS：指定一共分多少个桶。 常用HiveQL整理 Database -- 创建database create database IF NOT EXISTS ods; USE database_name; USE DEFAULT; -- 删除database drop database if exists ods; Table -- 创建内部表（默认） create table trade_detail( id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\\t&#39;; -- 创建分区表 create table td_part( id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by &#39;\\t&#39;; -- 创建外部表 create external table td_ext( id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\\t&#39; location &#39;/data/td_ext&#39;; -- 创建外部分区表 create external table td_part_ext( id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) stored as parquet tblproperties(&quot;parquet.compress&quot;=&quot;SNAPPY&quot;) location &#39;/data/td_part_ext&#39;; drop table if exists td_part_ext; Alter -- 修改表明 ALTER TABLE page_view RENAME to page_view_new; -- 修改字段 ALTER TABLE page_view CHANGE ip ip_address string AFTER refererurl; -- 添加字段 ALTER TABLE page_view ADD COLUMNS (name string comment &#39;view name&#39;); -- 添加分区 ALTER TABLE page_view ADD IF NOT EXISTS PARTITION (dt=&#39;20190705&#39;) LOCATION=&#39;/data/page_view/dt=20190705&#39;; -- 修改location ALTER TABLE page_view PARTITION(dt=&#39;20190706&#39;) SET LOCATION &quot;/data/page_view/dt=20190706&quot;; -- 修改分隔符 ALTER TABLE page_view SET SERDEPROPERTIES (&#39;field.delim&#39; = &#39;,&#39;); -- 删除分区 ALTER TABLE page_view DROP PARTITION (dt=&#39;2008-08-08&#39;, country=&#39;us&#39;); Show -- 查看创建表语句 show create table td_part; -- 查看表分区 show partitions td_part; 修复分区 -- 修复分区 msck repair table page_view; 数据导入 -- 将本地文件导入到hive load data local inpath &#39;/home/hadoop/student&#39; overwrite into table student partition(state=&#39;Sichuan&#39;, city=&#39;Chengdu&#39;); -- 将hdfs上文件导入到hive load data inpath &#39;/user/hadoop/add.txt&#39; into table student partition(state=&#39;Sichuan&#39;, city=&#39;Chengdu&#39;); -- 从别的表中查询出相应的数据并导入到hive表中 insert into table test partition(age=&#39;25&#39;) select id ,name from wyp where age=&#39;25&#39;; -- 在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中 create table tmp as select * from student where age&gt;&#39;18&#39;; 数据导出 -- 导出到本地文件系统 insert overwrite local directory &#39;/home/hadoop/student&#39; select * from student; -- 导出到hdfs insert overwrite directory &#39;/user/hadoop/student&#39; select * from student; -- 将查询结果插入到表中（追加） insert into table student_new select id,name from student where age=&#39;25&#39;; -- 将查询结果插入到表中（覆盖） insert into table student_new select id,name from student where age=&#39;25&#39;; 自定义函数 UDF(user-defined functions) 用户自定义普通函数，用于处理单行数据，并生成单个数据行。用来自定义完成对字段处理的函数。 UDAF(user-defined aggregation functions) 聚合函数，多对一，多行变一行，需和group by联合使用。 UDTF(user-defined table-generating functions) 表生成函数，一对多，一行变多行。","@type":"BlogPosting","url":"https://uzzz.org/2019/07/12/794082.html","headline":"从0开始学大数据-Hive基础篇","dateModified":"2019-07-12T00:00:00+08:00","datePublished":"2019-07-12T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/12/794082.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>从0开始学大数据-Hive基础篇</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-tomorrow-night-eighties"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <ul>
     <li><a href="#Hive_5" rel="nofollow" data-token="0d5d9a30513912a44dcabd5a063f374f">Hive的特点</a></li>
     <li><a href="#Hive_20" rel="nofollow" data-token="fb1985ae58f44517cb83210f2ea5b910">Hive体系架构</a></li>
     <li><a href="#Hive_32" rel="nofollow" data-token="1eba297d80075c2913fce8fc1500a962">Hive运行机制</a></li>
     <li><a href="#Hive_41" rel="nofollow" data-token="f32c7a7647e144440eb35193a531125b">Hive与关系数据库的区别</a></li>
     <li><a href="#Hive_45" rel="nofollow" data-token="e7b6f0e1e8b72432ba1ee395a4965510">Hive中的基本数据类型</a></li>
     <ul>
      <li><a href="#_47" rel="nofollow" data-token="15c2b2a973449a1bf89faf079a53b678">基本数据类型</a></li>
      <li><a href="#_52" rel="nofollow" data-token="6e961d1f8df88fdb859c31cd0b782c89">复杂数据类型</a></li>
     </ul>
     <li><a href="#Hive_60" rel="nofollow" data-token="4c24b997a7e239260bdb281d0a90f612">Hive表</a></li>
     <ul>
      <li><a href="#_62" rel="nofollow" data-token="1711890e3e4a76548d80eaf05529d310">创建表</a></li>
      <li><a href="#Hive_83" rel="nofollow" data-token="0dbc760e063e51925119f0bd3282750a">Hive表类型</a></li>
      <ul>
       <li><a href="#_86" rel="nofollow" data-token="08cc2d8dc040376cd2ba41c0728d3bda">内部表</a></li>
       <li><a href="#_96" rel="nofollow" data-token="2d1d660f5191031f305d349917fb5779">外部表</a></li>
       <li><a href="#_119" rel="nofollow" data-token="a7b62dd50a6fc62023837b7a5e507247">分区表</a></li>
       <li><a href="#_131" rel="nofollow" data-token="2b4e31e452cf61a9cd23486af68659f2">桶表</a></li>
      </ul>
     </ul>
     <li><a href="#HiveQL_162" rel="nofollow" data-token="6dfe6508d011d98cf29ffbb8b49bea7e">常用HiveQL整理</a></li>
     <li><a href="#_304" rel="nofollow" data-token="67eff42b35ed7b65b3797fd8d6b05d18">自定义函数</a></li>
     <ul>
      <li><a href="#UDFuserdefined_functions_306" rel="nofollow" data-token="f121cbd3a81a9a0e2051a7acba7fa7b9">UDF(user-defined functions)</a></li>
      <li><a href="#UDAFuserdefined_aggregation_functions_312" rel="nofollow" data-token="56fe9fc7613b9a68c4ffae5df4234f42">UDAF(user-defined aggregation functions)</a></li>
      <li><a href="#UDTFuserdefined_tablegenerating_functions_318" rel="nofollow" data-token="67b734e4be4c8489c9ce1fb2f03232da">UDTF(user-defined table-generating functions)</a></li>
     </ul>
    </ul>
   </ul>
  </div>
  <br> 
  <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ub3RlLnlvdWRhby5jb20veXdzL3B1YmxpYy9yZXNvdXJjZS81MDFlOTU3Y2JmZTIyN2NjM2NmM2Q3MTUwYzNlMTJjNC94bWxub3RlL0M1MzI2NzUzODAxNjRCRDdBMjdFN0M0MTk0NjdBNkE1LzIwMTE2" alt="">
  <p></p> 
  <p>Hive起源于Facebook，是基于 Hadoop HDFS 分布式文件系统的分布式 <strong>数据仓库</strong> 架构。它为数据仓库的管理提供了许多功能：数据ETL（抽取、转换和加载）工具、数据存储管理和大型数据集的查询和分析能力。同时Hive还定义了类SQL的语言（HiveQL）。允许用户进行和SQL相似的操作，它可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能。还允许开发人员方便的使用Mapper和Reducer操作，可以将SQL语句转换为MapReduce任务运行。</p> 
  <h2><a id="Hive_5"></a>Hive的特点</h2> 
  <p>Hive作为Hadoop之上的数据仓库处理工具，它所有的数据都是存储在Hadoop兼容的文件系统中。Hive在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive设定的目录下。因此，Hive<strong>不支持</strong>对数据的<strong>修改和添加</strong>，所有的数据都是在加载的时候确定的。</p> 
  <ul> 
   <li> <p>Hive 最大的特点是通过类 SQL 来分析大数据，而避免了写 MapReduce 程序来分析数据，这样使得分析数据更容易。</p> </li> 
   <li> <p>数据是存储在 HDFS 上的，Hive 本身并不提供数据的存储功能</p> </li> 
   <li> <p>Hive 是将数据映射成数据库和一张张的表，库和表的元数据信息一般存在关系型数据库上（比如 MySQL）。</p> </li> 
   <li> <p>数据存储方面：它能够存储很大的数据集，并且对数据完整性、格式要求并不严格。</p> </li> 
   <li> <p>数据处理方面：不适用于实时计算和响应，使用于离线分析。</p> </li> 
  </ul> 
  <h2><a id="Hive_20"></a>Hive体系架构</h2> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ub3RlLnlvdWRhby5jb20veXdzL3B1YmxpYy9yZXNvdXJjZS81MDFlOTU3Y2JmZTIyN2NjM2NmM2Q3MTUwYzNlMTJjNC94bWxub3RlL0MzNzgyNzI2OUQ5OTQ3RDVCMzFCODRDNTQxNEVGMTQ2LzIwMTE0" alt=""></p> 
  <ul> 
   <li><strong>用户接口</strong>：包括CLI，JDBC/ODBC，WebUI等方式。</li> 
   <li><strong>元数据Metastore</strong>：通常存储在关系数据库如mysql，derby中。元数据包括：表名，表所属的数据库，表的列/分区字段，表的属性（是否为外部表等），表的数据所在目录等信息。</li> 
   <li><strong>Driver</strong>：包括解释器、编译器、优化器、执行器。HiveQL查询语句从词法分析。语法分析、编译、优化以及查询计划生成。生成的查询计划存储在HDFS中，并在后面的MapReduce中进行调用执行。 
    <ul> 
     <li>解析器：将SQL转换成抽象的语法树AST。</li> 
     <li>编译器：将抽象语法树AST编译生成逻辑执行计划。</li> 
     <li>优化器：将逻辑执行计划进行优化。</li> 
     <li>执行器：把逻辑执行计划转换成可以运行的物理执行计划。</li> 
    </ul> </li> 
   <li><strong>Hadoop</strong>：使用HDFS进行存储，利用MapReduce进行计算。</li> 
  </ul> 
  <h2><a id="Hive_32"></a>Hive运行机制</h2> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ub3RlLnlvdWRhby5jb20veXdzL3B1YmxpYy9yZXNvdXJjZS81MDFlOTU3Y2JmZTIyN2NjM2NmM2Q3MTUwYzNlMTJjNC94bWxub3RlL0E5RkM4NzhDQUYyMjRGMDc5MDkzOTQ3RTVBNzUwMkRGLzIwMTIz" alt=""></p> 
  <ol> 
   <li>编写HiveQL，并提交；</li> 
   <li>Hive解析查询语句，并生成查询计划；</li> 
   <li>Hive将查询计划转化为MR作业；</li> 
   <li>运行MR作业，得到最终结果。</li> 
  </ol> 
  <h2><a id="Hive_41"></a>Hive与关系数据库的区别</h2> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ub3RlLnlvdWRhby5jb20veXdzL3B1YmxpYy9yZXNvdXJjZS81MDFlOTU3Y2JmZTIyN2NjM2NmM2Q3MTUwYzNlMTJjNC94bWxub3RlLzU0ODRCNTkzM0VGRTRDNDdBNTI5OUVEMUMxOTk3RDkzLzIwMTEy" alt=""></p> 
  <h2><a id="Hive_45"></a>Hive中的基本数据类型</h2> 
  <h3><a id="_47"></a>基本数据类型</h3> 
  <p>Hive支持关系数据库中大多数据基本数据类型，同时还支持三种复杂类型。</p> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ub3RlLnlvdWRhby5jb20veXdzL3B1YmxpYy9yZXNvdXJjZS81MDFlOTU3Y2JmZTIyN2NjM2NmM2Q3MTUwYzNlMTJjNC94bWxub3RlL0JBNDg4MDIxNjE3MTQ3NTk5QkNFODhDMTVGRkIyMDcxLzIwMTE4" alt=""></p> 
  <h3><a id="_52"></a>复杂数据类型</h3> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ub3RlLnlvdWRhby5jb20veXdzL3B1YmxpYy9yZXNvdXJjZS81MDFlOTU3Y2JmZTIyN2NjM2NmM2Q3MTUwYzNlMTJjNC94bWxub3RlLzgwNzQ4RkVFQjY2NzRGOEJBMEQxQzAxMUMxOUMxMTA0LzIwMTIw" alt=""></p> 
  <p>示例：</p> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ub3RlLnlvdWRhby5jb20veXdzL3B1YmxpYy9yZXNvdXJjZS81MDFlOTU3Y2JmZTIyN2NjM2NmM2Q3MTUwYzNlMTJjNC94bWxub3RlLzc3NkY0MjQ1OEUyQzREMzE4ODgwNDA0Nzk1ODQzQTJCLzIwMTI1" alt=""></p> 
  <h2><a id="Hive_60"></a>Hive表</h2> 
  <h3><a id="_62"></a>创建表</h3> 
  <pre><code class="prism language-sql"><span class="token comment">-- 直接建表法</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> t_page_view <span class="token punctuation">(</span>  
  page_id <span class="token keyword">bigint</span> <span class="token keyword">comment</span> <span class="token string">'页面ID'</span><span class="token punctuation">,</span>  
  page_name string <span class="token keyword">comment</span> <span class="token string">'页面名称'</span><span class="token punctuation">,</span>  
  page_url string <span class="token keyword">comment</span> <span class="token string">'页面URL'</span>  
<span class="token punctuation">)</span>  
<span class="token keyword">comment</span> <span class="token string">'页面视图'</span>
partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span>ds string <span class="token keyword">comment</span> <span class="token string">'当前时间，用于分区字段'</span><span class="token punctuation">)</span>   
stored <span class="token keyword">as</span> parquet
location <span class="token string">'/user/hive/warehouse/t_page_view'</span><span class="token punctuation">;</span>

<span class="token comment">-- 查询建表法</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> t_page_view2  <span class="token keyword">as</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> t_page_view<span class="token punctuation">;</span>

<span class="token comment">-- like建表法(克隆表)</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> t_page_view3 <span class="token operator">like</span> t_page_view<span class="token punctuation">;</span>
</code></pre> 
  <h3><a id="Hive_83"></a>Hive表类型</h3> 
  <h4><a id="_86"></a>内部表</h4> 
  <p>Hive中默认创建的是内部表，内部表的数据由Hive来管理。在 <code>drop</code>表的时候，表的数据和元数据都会被删除。</p> 
  <pre><code class="prism language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> pokes <span class="token punctuation">(</span>
  foo <span class="token keyword">INT</span><span class="token punctuation">,</span>
  bar STRING
<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
  <h4><a id="_96"></a>外部表</h4> 
  <p>外部表的数据不由Hive管理。Hive可以创建外部表和 HDFS、HBase 和 Elasticsearch 等进行整合。<code>drop</code>外部表时，只会删除元数据，不会删除真实数据。外部表关键字<code>EXTERNAL</code>。</p> 
  <pre><code class="prism language-mysql">CREATE EXTERNAL TABLE IF NOT EXISTS tb_station (
  station string,
  lon string,
  lat string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/user/test/coordinate/';
</code></pre> 
  <p><strong>内部表与外部表转换</strong></p> 
  <pre><code class="prism language-mysql">-- 内部表转外部表
alter table tableA set TBLPROPERTIES('EXTERNAL'='true');
-- 外部表转内部表
alter table tableA set TBLPROPERTIES('EXTERNAL'='false');
</code></pre> 
  <h4><a id="_119"></a>分区表</h4> 
  <p>Hive表的数据是存储在HFDS中对应的目录中，普通表的数据直接存储在这个目录下，而分区表存储时，会再划分子目录来存储，一个分区对应一个子目录。主要作用是来<strong>优化查询性能</strong>。在Hive表查询的时候，如果指定了分区字段作为筛选条件，那么只需要到对应的分区目录下去检索数据即可，减少了处理的数据量，从而有效的提高了效率。<strong>在对分区表进行查询时，尽量使用分区字段作为筛选条件</strong>。</p> 
  <pre><code class="prism language-mysql">CREATE TABLE invites (
  foo INT,
  bar STRING
)
PARTITIONED BY (ds STRING);
</code></pre> 
  <h4><a id="_131"></a>桶表</h4> 
  <p>分桶是将数据分解成更容易管理的若干部分的技术，桶表示对数据源数据文件本身来拆分数据。桶是更为细粒度的数据范围划分。Hive采用对列进行哈希，然后对桶的个数取模的方式决定该条记录存放在哪个桶当中。</p> 
  <p>创建桶表时，需要指定桶的个数，分桶的依据字段，Hive就可以自动将数据分桶存储。</p> 
  <p>查询时，只需要遍历一个桶里的数据，或者遍历部分桶，这样就提高了查询效率。</p> 
  <p>示例：</p> 
  <pre><code class="prism language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> ods<span class="token punctuation">.</span>tb_ad_action_d <span class="token punctuation">(</span>
  op_time STRING<span class="token punctuation">,</span>
  user_id STRING<span class="token punctuation">,</span>
  task_id STRING<span class="token punctuation">,</span>
  event_name STRING
<span class="token punctuation">)</span>
<span class="token keyword">CLUSTERED</span> <span class="token keyword">BY</span> <span class="token punctuation">(</span>user_id<span class="token punctuation">)</span>
SORTED <span class="token keyword">BY</span><span class="token punctuation">(</span>task_id<span class="token punctuation">)</span>
<span class="token keyword">INTO</span> <span class="token number">10</span> BUCKETS
<span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">'\t'</span>
STORED <span class="token keyword">AS</span> TEXTFILE<span class="token punctuation">;</span>
</code></pre> 
  <ul> 
   <li><strong>CLUSTERED BY</strong>：是指根据<code>user_id</code>的值进行哈希后模除分桶个数，根据得到的结果，确定这行数据分入哪个桶中。</li> 
   <li><strong>SORTED BY</strong>：指定桶中的数据以哪个字段进行排序，排序的好处是，在 join 操作时能够获得很高的效率。</li> 
   <li><strong>INTO 10 BUCKETS</strong>：指定一共分多少个桶。</li> 
  </ul> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ub3RlLnlvdWRhby5jb20veXdzL3B1YmxpYy9yZXNvdXJjZS81MDFlOTU3Y2JmZTIyN2NjM2NmM2Q3MTUwYzNlMTJjNC94bWxub3RlLzNFODhCMEVCNzU3RDRGNDVCQjQxRUI5OUUyOTE4NDJFLzIwMTEw" alt=""></p> 
  <h2><a id="HiveQL_162"></a>常用HiveQL整理</h2> 
  <p><strong>Database</strong></p> 
  <pre><code class="prism language-sql"><span class="token comment">-- 创建database</span>
<span class="token keyword">create</span> <span class="token keyword">database</span> <span class="token keyword">IF</span> <span class="token operator">NOT</span> <span class="token keyword">EXISTS</span> ods<span class="token punctuation">;</span>

<span class="token keyword">USE</span> database_name<span class="token punctuation">;</span>

<span class="token keyword">USE</span> <span class="token keyword">DEFAULT</span><span class="token punctuation">;</span>

<span class="token comment">-- 删除database</span>
<span class="token keyword">drop</span> <span class="token keyword">database</span> <span class="token keyword">if</span> <span class="token keyword">exists</span> ods<span class="token punctuation">;</span>
</code></pre> 
  <p><strong>Table</strong></p> 
  <pre><code class="prism language-mysql">-- 创建内部表（默认）
create table trade_detail(
    id bigint, 
    account string, 
    income double, 
    expenses double, 
    time string) 
row format delimited 
fields terminated by '\t';

-- 创建分区表
create table td_part(
    id bigint, 
    account string, 
    income double, 
    expenses double, 
    time string) 
partitioned by (logdate string) 
row format delimited 
fields terminated by '\t';

-- 创建外部表
create external table td_ext(
    id bigint, 
    account string, 
    income double, 
    expenses double, 
    time string) 
row format delimited 
fields terminated by '\t' 
location '/data/td_ext';

-- 创建外部分区表
create external table td_part_ext(
    id bigint, 
    account string, 
    income double, 
    expenses double, 
    time string) 
partitioned by (logdate string) 
stored as parquet tblproperties("parquet.compress"="SNAPPY")
location '/data/td_part_ext';

drop table if exists td_part_ext;
</code></pre> 
  <p><strong>Alter</strong></p> 
  <pre><code class="prism language-mysql">-- 修改表明
ALTER TABLE page_view RENAME to page_view_new;

-- 修改字段
ALTER TABLE page_view CHANGE ip ip_address string AFTER refererurl;

-- 添加字段
ALTER TABLE page_view ADD COLUMNS (name string comment 'view name');

-- 添加分区
ALTER TABLE page_view ADD IF NOT EXISTS PARTITION (dt='20190705') LOCATION='/data/page_view/dt=20190705';


-- 修改location
ALTER TABLE page_view PARTITION(dt='20190706') SET LOCATION "/data/page_view/dt=20190706";

-- 修改分隔符
ALTER TABLE page_view SET SERDEPROPERTIES ('field.delim' = ',');

-- 删除分区
ALTER TABLE page_view DROP PARTITION (dt='2008-08-08', country='us');
</code></pre> 
  <p><strong>Show</strong></p> 
  <pre><code class="prism language-mysql">-- 查看创建表语句
show create table td_part;

-- 查看表分区
show partitions td_part;
</code></pre> 
  <p><strong>修复分区</strong></p> 
  <pre><code class="prism language-mysql">-- 修复分区
msck repair table page_view;
</code></pre> 
  <p><strong>数据导入</strong></p> 
  <pre><code class="prism language-sql"><span class="token comment">-- 将本地文件导入到hive</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/home/hadoop/student'</span> overwrite <span class="token keyword">into</span> <span class="token keyword">table</span> student <span class="token keyword">partition</span><span class="token punctuation">(</span>state<span class="token operator">=</span><span class="token string">'Sichuan'</span><span class="token punctuation">,</span> city<span class="token operator">=</span><span class="token string">'Chengdu'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">-- 将hdfs上文件导入到hive</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> inpath <span class="token string">'/user/hadoop/add.txt'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> student <span class="token keyword">partition</span><span class="token punctuation">(</span>state<span class="token operator">=</span><span class="token string">'Sichuan'</span><span class="token punctuation">,</span> city<span class="token operator">=</span><span class="token string">'Chengdu'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">-- 从别的表中查询出相应的数据并导入到hive表中</span>
<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> test <span class="token keyword">partition</span><span class="token punctuation">(</span>age<span class="token operator">=</span><span class="token string">'25'</span><span class="token punctuation">)</span> <span class="token keyword">select</span> id <span class="token punctuation">,</span>name <span class="token keyword">from</span> wyp <span class="token keyword">where</span> age<span class="token operator">=</span><span class="token string">'25'</span><span class="token punctuation">;</span>

<span class="token comment">-- 在创建表的时候通过从别的表中查询出相应的记录并插入到所创建的表中</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> tmp <span class="token keyword">as</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> student <span class="token keyword">where</span> age<span class="token operator">&gt;</span><span class="token string">'18'</span><span class="token punctuation">;</span>
</code></pre> 
  <p><strong>数据导出</strong></p> 
  <pre><code class="prism language-sql"><span class="token comment">-- 导出到本地文件系统</span>
<span class="token keyword">insert</span> overwrite <span class="token keyword">local</span> directory <span class="token string">'/home/hadoop/student'</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> student<span class="token punctuation">;</span>

<span class="token comment">-- 导出到hdfs</span>
<span class="token keyword">insert</span> overwrite directory <span class="token string">'/user/hadoop/student'</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> student<span class="token punctuation">;</span>

<span class="token comment">-- 将查询结果插入到表中（追加）</span>
<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> student_new <span class="token keyword">select</span> id<span class="token punctuation">,</span>name <span class="token keyword">from</span> student <span class="token keyword">where</span> age<span class="token operator">=</span><span class="token string">'25'</span><span class="token punctuation">;</span>

<span class="token comment">-- 将查询结果插入到表中（覆盖）</span>
<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> student_new <span class="token keyword">select</span> id<span class="token punctuation">,</span>name <span class="token keyword">from</span> student <span class="token keyword">where</span> age<span class="token operator">=</span><span class="token string">'25'</span><span class="token punctuation">;</span>
</code></pre> 
  <h2><a id="_304"></a>自定义函数</h2> 
  <h3><a id="UDFuserdefined_functions_306"></a>UDF(user-defined functions)</h3> 
  <p>用户自定义普通函数，用于处理单行数据，并生成单个数据行。用来自定义完成对字段处理的函数。</p> 
  <h3><a id="UDAFuserdefined_aggregation_functions_312"></a>UDAF(user-defined aggregation functions)</h3> 
  <p>聚合函数，多对一，多行变一行，需和group by联合使用。</p> 
  <h3><a id="UDTFuserdefined_tablegenerating_functions_318"></a>UDTF(user-defined table-generating functions)</h3> 
  <p>表生成函数，一对多，一行变多行。</p> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9ub3RlLnlvdWRhby5jb20veXdzL3B1YmxpYy9yZXNvdXJjZS81NTI3YWNiMDk2Y2VlNWZjNDYxNjBlNzFlZDI2Y2EwOS94bWxub3RlLzY1NEVFOUYyNTU0NTRDQzJCNDE0MzRFM0RCNjNBNDBCLzE5OTQ4" alt=""></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
