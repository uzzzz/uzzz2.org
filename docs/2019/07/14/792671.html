<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>文本分类（5）-TextCNN实现文本分类 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="文本分类（5）-TextCNN实现文本分类" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="利用TextCNN对IMDB Reviwe文本进行分类，数据集地址：https://pan.baidu.com/s/1EYoqAcW238saKy3uQCfC3w 提取码：ilze import numpy as np import logging from keras import Input from keras.layers import Conv1D, MaxPool1D, Dense, Flatten, concatenate, Embedding from keras.models import Model # from keras.utils import plot_model from keras.utils.vis_utils import plot_model import pandas as pd import warnings import keras import re import matplotlib.pyplot as plt from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, MaxPooling1D, Bidirectional from keras.models import Sequential from keras.utils import np_utils warnings.filterwarnings(&#39;ignore&#39;) # get data df1 = pd.read_csv(&#39;word2vec-nlp-tutorial/labeledTrainData.tsv&#39;, sep=&#39;\t&#39;, error_bad_lines=False) df2 = pd.read_csv(&#39;word2vec-nlp-tutorial/imdb_master.csv&#39;, encoding=&quot;latin-1&quot;) df3 = pd.read_csv(&#39;word2vec-nlp-tutorial/testData.tsv&#39;, sep=&#39;\t&#39;, error_bad_lines=False) df2 = df2.drop([&#39;Unnamed: 0&#39;,&#39;type&#39;,&#39;file&#39;],axis=1) df2.columns = [&quot;review&quot;,&quot;sentiment&quot;] df2 = df2[df2.sentiment != &#39;unsup&#39;] df2[&#39;sentiment&#39;] = df2[&#39;sentiment&#39;].map({&#39;pos&#39;: 1, &#39;neg&#39;: 0}) df = pd.concat([df1, df2]).reset_index(drop=True) train_texts = df.review train_labels = df.sentiment test_texts = df3.review def replace_abbreviations(text): texts = [] for item in text: item = item.lower().replace(&quot;it&#39;s&quot;, &quot;it is&quot;).replace(&quot;i&#39;m&quot;, &quot;i am&quot;).replace(&quot;he&#39;s&quot;, &quot;he is&quot;).replace(&quot;she&#39;s&quot;, &quot;she is&quot;)\ .replace(&quot;we&#39;re&quot;, &quot;we are&quot;).replace(&quot;they&#39;re&quot;, &quot;they are&quot;).replace(&quot;you&#39;re&quot;, &quot;you are&quot;).replace(&quot;that&#39;s&quot;, &quot;that is&quot;)\ .replace(&quot;this&#39;s&quot;, &quot;this is&quot;).replace(&quot;can&#39;t&quot;, &quot;can not&quot;).replace(&quot;don&#39;t&quot;, &quot;do not&quot;).replace(&quot;doesn&#39;t&quot;, &quot;does not&quot;)\ .replace(&quot;we&#39;ve&quot;, &quot;we have&quot;).replace(&quot;i&#39;ve&quot;, &quot; i have&quot;).replace(&quot;isn&#39;t&quot;, &quot;is not&quot;).replace(&quot;won&#39;t&quot;, &quot;will not&quot;)\ .replace(&quot;hasn&#39;t&quot;, &quot;has not&quot;).replace(&quot;wasn&#39;t&quot;, &quot;was not&quot;).replace(&quot;weren&#39;t&quot;, &quot;were not&quot;).replace(&quot;let&#39;s&quot;, &quot;let us&quot;)\ .replace(&quot;didn&#39;t&quot;, &quot;did not&quot;).replace(&quot;hadn&#39;t&quot;, &quot;had not&quot;).replace(&quot;waht&#39;s&quot;, &quot;what is&quot;).replace(&quot;couldn&#39;t&quot;, &quot;could not&quot;)\ .replace(&quot;you&#39;ll&quot;, &quot;you will&quot;).replace(&quot;you&#39;ve&quot;, &quot;you have&quot;) item = item.replace(&quot;&#39;s&quot;, &quot;&quot;) texts.append(item) return texts def clear_review(text): texts = [] for item in text: item = item.replace(&quot;&lt;br /&gt;&lt;br /&gt;&quot;, &quot;&quot;) item = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, item.lower()) texts.append(&quot; &quot;.join(item.split())) return texts def stemed_words(text): stop_words = stopwords.words(&quot;english&quot;) lemma = WordNetLemmatizer() texts = [] for item in text: words = [lemma.lemmatize(w, pos=&#39;v&#39;) for w in item.split() if w not in stop_words] texts.append(&quot; &quot;.join(words)) return texts def preprocess(text): text = replace_abbreviations(text) text = clear_review(text) text = stemed_words(text) return text train_texts = preprocess(train_texts) test_texts = preprocess(test_texts) max_features = 6000 texts = train_texts + test_texts tok = Tokenizer(num_words=max_features) tok.fit_on_texts(texts) list_tok = tok.texts_to_sequences(texts) maxlen = 130 seq_tok = pad_sequences(list_tok, maxlen=maxlen) x_train = seq_tok[:len(train_texts)] y_train = train_labels y_train = np_utils.to_categorical(y_train, num_classes=2) # 绘图 def show_history(trian_model): plt.figure(figsize=(10, 5)) plt.subplot(121) plt.plot(trian_model.history[&#39;acc&#39;], c=&#39;b&#39;, label=&#39;train&#39;) plt.plot(trian_model.history[&#39;val_acc&#39;], c=&#39;g&#39;, label=&#39;validation&#39;) plt.legend() plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.title(&#39;Model accuracy&#39;) plt.subplot(122) plt.plot(trian_model.history[&#39;loss&#39;], c=&#39;b&#39;, label=&#39;train&#39;) plt.plot(trian_model.history[&#39;val_loss&#39;], c=&#39;g&#39;, label=&#39;validation&#39;) plt.legend() plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.title(&#39;Model loss&#39;) plt.show() def test_cnn(y,maxlen,max_features,embedding_dims,filters = 250): #Inputs seq = Input(shape=[maxlen],name=&#39;x_seq&#39;) #Embedding layers emb = Embedding(max_features,embedding_dims)(seq) # conv layers convs = [] filter_sizes = [2,3,4] for fsz in filter_sizes: conv1 = Conv1D(filters,kernel_size=fsz,activation=&#39;tanh&#39;)(emb) pool1 = MaxPool1D(maxlen-fsz+1)(conv1) pool1 = Flatten()(pool1) convs.append(pool1) merge = concatenate(convs,axis=1) out = Dropout(0.5)(merge) output = Dense(32,activation=&#39;relu&#39;)(out) output = Dense(units=y.shape[1],activation=&#39;sigmoid&#39;)(output) model = Model([seq],output) # model.compile(loss=&#39;categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;]) return model def model_train(model, x_train, y_train): keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=0, verbose=0, mode=&#39;auto&#39;) history = model.fit(x_train, y_train, validation_split=0.2, batch_size=100, epochs=20) return history model = test_cnn(y_train, maxlen, max_features, embedding_dims=128, filters=250) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) history = model_train(model, x_train, y_train)" />
<meta property="og:description" content="利用TextCNN对IMDB Reviwe文本进行分类，数据集地址：https://pan.baidu.com/s/1EYoqAcW238saKy3uQCfC3w 提取码：ilze import numpy as np import logging from keras import Input from keras.layers import Conv1D, MaxPool1D, Dense, Flatten, concatenate, Embedding from keras.models import Model # from keras.utils import plot_model from keras.utils.vis_utils import plot_model import pandas as pd import warnings import keras import re import matplotlib.pyplot as plt from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, MaxPooling1D, Bidirectional from keras.models import Sequential from keras.utils import np_utils warnings.filterwarnings(&#39;ignore&#39;) # get data df1 = pd.read_csv(&#39;word2vec-nlp-tutorial/labeledTrainData.tsv&#39;, sep=&#39;\t&#39;, error_bad_lines=False) df2 = pd.read_csv(&#39;word2vec-nlp-tutorial/imdb_master.csv&#39;, encoding=&quot;latin-1&quot;) df3 = pd.read_csv(&#39;word2vec-nlp-tutorial/testData.tsv&#39;, sep=&#39;\t&#39;, error_bad_lines=False) df2 = df2.drop([&#39;Unnamed: 0&#39;,&#39;type&#39;,&#39;file&#39;],axis=1) df2.columns = [&quot;review&quot;,&quot;sentiment&quot;] df2 = df2[df2.sentiment != &#39;unsup&#39;] df2[&#39;sentiment&#39;] = df2[&#39;sentiment&#39;].map({&#39;pos&#39;: 1, &#39;neg&#39;: 0}) df = pd.concat([df1, df2]).reset_index(drop=True) train_texts = df.review train_labels = df.sentiment test_texts = df3.review def replace_abbreviations(text): texts = [] for item in text: item = item.lower().replace(&quot;it&#39;s&quot;, &quot;it is&quot;).replace(&quot;i&#39;m&quot;, &quot;i am&quot;).replace(&quot;he&#39;s&quot;, &quot;he is&quot;).replace(&quot;she&#39;s&quot;, &quot;she is&quot;)\ .replace(&quot;we&#39;re&quot;, &quot;we are&quot;).replace(&quot;they&#39;re&quot;, &quot;they are&quot;).replace(&quot;you&#39;re&quot;, &quot;you are&quot;).replace(&quot;that&#39;s&quot;, &quot;that is&quot;)\ .replace(&quot;this&#39;s&quot;, &quot;this is&quot;).replace(&quot;can&#39;t&quot;, &quot;can not&quot;).replace(&quot;don&#39;t&quot;, &quot;do not&quot;).replace(&quot;doesn&#39;t&quot;, &quot;does not&quot;)\ .replace(&quot;we&#39;ve&quot;, &quot;we have&quot;).replace(&quot;i&#39;ve&quot;, &quot; i have&quot;).replace(&quot;isn&#39;t&quot;, &quot;is not&quot;).replace(&quot;won&#39;t&quot;, &quot;will not&quot;)\ .replace(&quot;hasn&#39;t&quot;, &quot;has not&quot;).replace(&quot;wasn&#39;t&quot;, &quot;was not&quot;).replace(&quot;weren&#39;t&quot;, &quot;were not&quot;).replace(&quot;let&#39;s&quot;, &quot;let us&quot;)\ .replace(&quot;didn&#39;t&quot;, &quot;did not&quot;).replace(&quot;hadn&#39;t&quot;, &quot;had not&quot;).replace(&quot;waht&#39;s&quot;, &quot;what is&quot;).replace(&quot;couldn&#39;t&quot;, &quot;could not&quot;)\ .replace(&quot;you&#39;ll&quot;, &quot;you will&quot;).replace(&quot;you&#39;ve&quot;, &quot;you have&quot;) item = item.replace(&quot;&#39;s&quot;, &quot;&quot;) texts.append(item) return texts def clear_review(text): texts = [] for item in text: item = item.replace(&quot;&lt;br /&gt;&lt;br /&gt;&quot;, &quot;&quot;) item = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, item.lower()) texts.append(&quot; &quot;.join(item.split())) return texts def stemed_words(text): stop_words = stopwords.words(&quot;english&quot;) lemma = WordNetLemmatizer() texts = [] for item in text: words = [lemma.lemmatize(w, pos=&#39;v&#39;) for w in item.split() if w not in stop_words] texts.append(&quot; &quot;.join(words)) return texts def preprocess(text): text = replace_abbreviations(text) text = clear_review(text) text = stemed_words(text) return text train_texts = preprocess(train_texts) test_texts = preprocess(test_texts) max_features = 6000 texts = train_texts + test_texts tok = Tokenizer(num_words=max_features) tok.fit_on_texts(texts) list_tok = tok.texts_to_sequences(texts) maxlen = 130 seq_tok = pad_sequences(list_tok, maxlen=maxlen) x_train = seq_tok[:len(train_texts)] y_train = train_labels y_train = np_utils.to_categorical(y_train, num_classes=2) # 绘图 def show_history(trian_model): plt.figure(figsize=(10, 5)) plt.subplot(121) plt.plot(trian_model.history[&#39;acc&#39;], c=&#39;b&#39;, label=&#39;train&#39;) plt.plot(trian_model.history[&#39;val_acc&#39;], c=&#39;g&#39;, label=&#39;validation&#39;) plt.legend() plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.title(&#39;Model accuracy&#39;) plt.subplot(122) plt.plot(trian_model.history[&#39;loss&#39;], c=&#39;b&#39;, label=&#39;train&#39;) plt.plot(trian_model.history[&#39;val_loss&#39;], c=&#39;g&#39;, label=&#39;validation&#39;) plt.legend() plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.title(&#39;Model loss&#39;) plt.show() def test_cnn(y,maxlen,max_features,embedding_dims,filters = 250): #Inputs seq = Input(shape=[maxlen],name=&#39;x_seq&#39;) #Embedding layers emb = Embedding(max_features,embedding_dims)(seq) # conv layers convs = [] filter_sizes = [2,3,4] for fsz in filter_sizes: conv1 = Conv1D(filters,kernel_size=fsz,activation=&#39;tanh&#39;)(emb) pool1 = MaxPool1D(maxlen-fsz+1)(conv1) pool1 = Flatten()(pool1) convs.append(pool1) merge = concatenate(convs,axis=1) out = Dropout(0.5)(merge) output = Dense(32,activation=&#39;relu&#39;)(out) output = Dense(units=y.shape[1],activation=&#39;sigmoid&#39;)(output) model = Model([seq],output) # model.compile(loss=&#39;categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;]) return model def model_train(model, x_train, y_train): keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=0, verbose=0, mode=&#39;auto&#39;) history = model.fit(x_train, y_train, validation_split=0.2, batch_size=100, epochs=20) return history model = test_cnn(y_train, maxlen, max_features, embedding_dims=128, filters=250) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) history = model_train(model, x_train, y_train)" />
<link rel="canonical" href="https://uzzz.org/2019/07/14/792671.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/14/792671.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-14T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"利用TextCNN对IMDB Reviwe文本进行分类，数据集地址：https://pan.baidu.com/s/1EYoqAcW238saKy3uQCfC3w 提取码：ilze import numpy as np import logging from keras import Input from keras.layers import Conv1D, MaxPool1D, Dense, Flatten, concatenate, Embedding from keras.models import Model # from keras.utils import plot_model from keras.utils.vis_utils import plot_model import pandas as pd import warnings import keras import re import matplotlib.pyplot as plt from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, MaxPooling1D, Bidirectional from keras.models import Sequential from keras.utils import np_utils warnings.filterwarnings(&#39;ignore&#39;) # get data df1 = pd.read_csv(&#39;word2vec-nlp-tutorial/labeledTrainData.tsv&#39;, sep=&#39;\\t&#39;, error_bad_lines=False) df2 = pd.read_csv(&#39;word2vec-nlp-tutorial/imdb_master.csv&#39;, encoding=&quot;latin-1&quot;) df3 = pd.read_csv(&#39;word2vec-nlp-tutorial/testData.tsv&#39;, sep=&#39;\\t&#39;, error_bad_lines=False) df2 = df2.drop([&#39;Unnamed: 0&#39;,&#39;type&#39;,&#39;file&#39;],axis=1) df2.columns = [&quot;review&quot;,&quot;sentiment&quot;] df2 = df2[df2.sentiment != &#39;unsup&#39;] df2[&#39;sentiment&#39;] = df2[&#39;sentiment&#39;].map({&#39;pos&#39;: 1, &#39;neg&#39;: 0}) df = pd.concat([df1, df2]).reset_index(drop=True) train_texts = df.review train_labels = df.sentiment test_texts = df3.review def replace_abbreviations(text): texts = [] for item in text: item = item.lower().replace(&quot;it&#39;s&quot;, &quot;it is&quot;).replace(&quot;i&#39;m&quot;, &quot;i am&quot;).replace(&quot;he&#39;s&quot;, &quot;he is&quot;).replace(&quot;she&#39;s&quot;, &quot;she is&quot;)\\ .replace(&quot;we&#39;re&quot;, &quot;we are&quot;).replace(&quot;they&#39;re&quot;, &quot;they are&quot;).replace(&quot;you&#39;re&quot;, &quot;you are&quot;).replace(&quot;that&#39;s&quot;, &quot;that is&quot;)\\ .replace(&quot;this&#39;s&quot;, &quot;this is&quot;).replace(&quot;can&#39;t&quot;, &quot;can not&quot;).replace(&quot;don&#39;t&quot;, &quot;do not&quot;).replace(&quot;doesn&#39;t&quot;, &quot;does not&quot;)\\ .replace(&quot;we&#39;ve&quot;, &quot;we have&quot;).replace(&quot;i&#39;ve&quot;, &quot; i have&quot;).replace(&quot;isn&#39;t&quot;, &quot;is not&quot;).replace(&quot;won&#39;t&quot;, &quot;will not&quot;)\\ .replace(&quot;hasn&#39;t&quot;, &quot;has not&quot;).replace(&quot;wasn&#39;t&quot;, &quot;was not&quot;).replace(&quot;weren&#39;t&quot;, &quot;were not&quot;).replace(&quot;let&#39;s&quot;, &quot;let us&quot;)\\ .replace(&quot;didn&#39;t&quot;, &quot;did not&quot;).replace(&quot;hadn&#39;t&quot;, &quot;had not&quot;).replace(&quot;waht&#39;s&quot;, &quot;what is&quot;).replace(&quot;couldn&#39;t&quot;, &quot;could not&quot;)\\ .replace(&quot;you&#39;ll&quot;, &quot;you will&quot;).replace(&quot;you&#39;ve&quot;, &quot;you have&quot;) item = item.replace(&quot;&#39;s&quot;, &quot;&quot;) texts.append(item) return texts def clear_review(text): texts = [] for item in text: item = item.replace(&quot;&lt;br /&gt;&lt;br /&gt;&quot;, &quot;&quot;) item = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, item.lower()) texts.append(&quot; &quot;.join(item.split())) return texts def stemed_words(text): stop_words = stopwords.words(&quot;english&quot;) lemma = WordNetLemmatizer() texts = [] for item in text: words = [lemma.lemmatize(w, pos=&#39;v&#39;) for w in item.split() if w not in stop_words] texts.append(&quot; &quot;.join(words)) return texts def preprocess(text): text = replace_abbreviations(text) text = clear_review(text) text = stemed_words(text) return text train_texts = preprocess(train_texts) test_texts = preprocess(test_texts) max_features = 6000 texts = train_texts + test_texts tok = Tokenizer(num_words=max_features) tok.fit_on_texts(texts) list_tok = tok.texts_to_sequences(texts) maxlen = 130 seq_tok = pad_sequences(list_tok, maxlen=maxlen) x_train = seq_tok[:len(train_texts)] y_train = train_labels y_train = np_utils.to_categorical(y_train, num_classes=2) # 绘图 def show_history(trian_model): plt.figure(figsize=(10, 5)) plt.subplot(121) plt.plot(trian_model.history[&#39;acc&#39;], c=&#39;b&#39;, label=&#39;train&#39;) plt.plot(trian_model.history[&#39;val_acc&#39;], c=&#39;g&#39;, label=&#39;validation&#39;) plt.legend() plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.title(&#39;Model accuracy&#39;) plt.subplot(122) plt.plot(trian_model.history[&#39;loss&#39;], c=&#39;b&#39;, label=&#39;train&#39;) plt.plot(trian_model.history[&#39;val_loss&#39;], c=&#39;g&#39;, label=&#39;validation&#39;) plt.legend() plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.title(&#39;Model loss&#39;) plt.show() def test_cnn(y,maxlen,max_features,embedding_dims,filters = 250): #Inputs seq = Input(shape=[maxlen],name=&#39;x_seq&#39;) #Embedding layers emb = Embedding(max_features,embedding_dims)(seq) # conv layers convs = [] filter_sizes = [2,3,4] for fsz in filter_sizes: conv1 = Conv1D(filters,kernel_size=fsz,activation=&#39;tanh&#39;)(emb) pool1 = MaxPool1D(maxlen-fsz+1)(conv1) pool1 = Flatten()(pool1) convs.append(pool1) merge = concatenate(convs,axis=1) out = Dropout(0.5)(merge) output = Dense(32,activation=&#39;relu&#39;)(out) output = Dense(units=y.shape[1],activation=&#39;sigmoid&#39;)(output) model = Model([seq],output) # model.compile(loss=&#39;categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;]) return model def model_train(model, x_train, y_train): keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=0, verbose=0, mode=&#39;auto&#39;) history = model.fit(x_train, y_train, validation_split=0.2, batch_size=100, epochs=20) return history model = test_cnn(y_train, maxlen, max_features, embedding_dims=128, filters=250) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) history = model_train(model, x_train, y_train)","@type":"BlogPosting","url":"https://uzzz.org/2019/07/14/792671.html","headline":"文本分类（5）-TextCNN实现文本分类","dateModified":"2019-07-14T00:00:00+08:00","datePublished":"2019-07-14T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/14/792671.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>文本分类（5）-TextCNN实现文本分类</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>利用TextCNN对IMDB Reviwe文本进行分类，数据集地址：<a href="https://pan.baidu.com/s/1EYoqAcW238saKy3uQCfC3w" rel="nofollow" data-token="5de9e3b52e42ef77d4a81f8369fb26cc">https://pan.baidu.com/s/1EYoqAcW238saKy3uQCfC3w</a><br> 提取码：ilze</p> 
  <pre><code>import numpy as np
import logging

from keras import Input
from keras.layers import Conv1D, MaxPool1D, Dense, Flatten, concatenate, Embedding
from keras.models import Model
# from keras.utils import plot_model
from keras.utils.vis_utils import plot_model
import pandas as pd
import warnings
import keras
import re
import matplotlib.pyplot as plt
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, MaxPooling1D, Bidirectional
from keras.models import Sequential
from keras.utils import np_utils

warnings.filterwarnings('ignore')

# get data
df1 = pd.read_csv('word2vec-nlp-tutorial/labeledTrainData.tsv', sep='\t', error_bad_lines=False)
df2 = pd.read_csv('word2vec-nlp-tutorial/imdb_master.csv', encoding="latin-1")
df3 = pd.read_csv('word2vec-nlp-tutorial/testData.tsv', sep='\t', error_bad_lines=False)

df2 = df2.drop(['Unnamed: 0','type','file'],axis=1)
df2.columns = ["review","sentiment"]
df2 = df2[df2.sentiment != 'unsup']
df2['sentiment'] = df2['sentiment'].map({'pos': 1, 'neg': 0})

df = pd.concat([df1, df2]).reset_index(drop=True)

train_texts = df.review
train_labels = df.sentiment

test_texts = df3.review

def replace_abbreviations(text):
    texts = []
    for item in text:
        item = item.lower().replace("it's", "it is").replace("i'm", "i am").replace("he's", "he is").replace("she's", "she is")\
            .replace("we're", "we are").replace("they're", "they are").replace("you're", "you are").replace("that's", "that is")\
            .replace("this's", "this is").replace("can't", "can not").replace("don't", "do not").replace("doesn't", "does not")\
            .replace("we've", "we have").replace("i've", " i have").replace("isn't", "is not").replace("won't", "will not")\
            .replace("hasn't", "has not").replace("wasn't", "was not").replace("weren't", "were not").replace("let's", "let us")\
            .replace("didn't", "did not").replace("hadn't", "had not").replace("waht's", "what is").replace("couldn't", "could not")\
            .replace("you'll", "you will").replace("you've", "you have")
    
        item = item.replace("'s", "")
        texts.append(item)
    
    return texts


def clear_review(text):
    texts = []
    for item in text:
        item = item.replace("&lt;br /&gt;&lt;br /&gt;", "")
        item = re.sub("[^a-zA-Z]", " ", item.lower())
        texts.append(" ".join(item.split()))
    return texts

def stemed_words(text):
    stop_words = stopwords.words("english")
    lemma = WordNetLemmatizer()
    texts = []
    for item in text:
        words = [lemma.lemmatize(w, pos='v') for w in item.split() if w not in stop_words]
        texts.append(" ".join(words))
    return texts
            
def preprocess(text):
    
    text = replace_abbreviations(text)
    text = clear_review(text)
    text = stemed_words(text)
    
    return text

train_texts = preprocess(train_texts)
test_texts = preprocess(test_texts)

max_features = 6000
texts = train_texts + test_texts
tok = Tokenizer(num_words=max_features)
tok.fit_on_texts(texts)
list_tok = tok.texts_to_sequences(texts)

maxlen = 130

seq_tok = pad_sequences(list_tok, maxlen=maxlen)

x_train = seq_tok[:len(train_texts)]
y_train = train_labels
y_train = np_utils.to_categorical(y_train, num_classes=2)

# 绘图
def show_history(trian_model):
    plt.figure(figsize=(10, 5))

    plt.subplot(121)
    plt.plot(trian_model.history['acc'], c='b', label='train')
    plt.plot(trian_model.history['val_acc'], c='g', label='validation')
    plt.legend()
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.title('Model accuracy')

    plt.subplot(122)
    plt.plot(trian_model.history['loss'], c='b', label='train')
    plt.plot(trian_model.history['val_loss'], c='g', label='validation')
    plt.legend()
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.title('Model loss')

    plt.show()


def test_cnn(y,maxlen,max_features,embedding_dims,filters = 250):
    #Inputs
    seq = Input(shape=[maxlen],name='x_seq')

    #Embedding layers
    emb = Embedding(max_features,embedding_dims)(seq)

    # conv layers
    convs = []
    filter_sizes = [2,3,4]
    for fsz in filter_sizes:
        conv1 = Conv1D(filters,kernel_size=fsz,activation='tanh')(emb)
        pool1 = MaxPool1D(maxlen-fsz+1)(conv1)
        pool1 = Flatten()(pool1)
        convs.append(pool1)
    merge = concatenate(convs,axis=1)

    out = Dropout(0.5)(merge)
    output = Dense(32,activation='relu')(out)

    output = Dense(units=y.shape[1],activation='sigmoid')(output)

    model = Model([seq],output)
#     model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
    return model

def model_train(model, x_train, y_train):
    keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')
    history = model.fit(x_train, y_train, validation_split=0.2, batch_size=100, epochs=20)
    return history

model = test_cnn(y_train, maxlen, max_features, embedding_dims=128, filters=250)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model_train(model, x_train, y_train)
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714112001831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc2NjE3OQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
