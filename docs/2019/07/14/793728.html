<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>NLP：基于nltk和jieba库对文本实现提取文本摘要(两种方法实现：top_n_summary和mean_scored_summary) | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="NLP：基于nltk和jieba库对文本实现提取文本摘要(两种方法实现：top_n_summary和mean_scored_summary)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="NLP：基于nltk和jieba库对文本实现提取文本摘要(两种方法实现：top_n_summary和mean_scored_summary) &nbsp; &nbsp; &nbsp; &nbsp; 目录 输出结果 设计思路 核心代码 &nbsp; &nbsp; &nbsp; &nbsp; 输出结果 1、测试文本 今天一大早，两位男子在故宫抽烟对镜头炫耀的视频在网络上传播，引发网友愤怒。有人感到后怕，600年的故宫真要这两个人给点了，万死莫赎。也有评论称，把无知当成炫耀的资本，丢人！ 视频中两位男子坐在故宫公共休息区的遮阳伞下，面对镜头问出：“谁敢在故宫抽烟？”语气极其嚣张，表情带有挑衅意味。话音刚落，另外一位男子面向镜头吸了一口烟。而视频中两人也表示知道有故宫禁止吸烟的规定。 事实上，2013年5月18日是国际博物馆日，故宫从这一天开始至今一直实行全面禁烟。根据规定，故宫博物院全体员工在院合作单位和个人不管在室内和室外，也不分开放区与工作区，一律禁止吸烟，对违反禁止吸烟规定的人员将进行严格处罚并通报全院。 此外，在2015年6月1日起北京全市也开始了《控制吸烟条例》，规定公共场所工作场所室内环境室外排队等场合禁止吸烟，违者将最高被罚200元，全市统一设立举报电话12320。 视频在网络上传播开来，不少网友担心故宫的安危，称一旦发生火情，后果不堪设想，有网友表示，这样的行为应该被旅游景区拉近黑名单，建议终身禁止进入任何景区和各种场馆。 &nbsp; 设计思路 后期更新…… &nbsp; &nbsp; 核心代码 def sent_tokenizer(texts): start=0 i=0#每个字符的位置 sentences=[] punt_list=&#39;.!?。！？&#39;.encode(&#39;utf-8&#39;) #&#39;,.!?:;~，。！？：；～&#39;.decode(&#39;utf8&#39;) # punt_lists=&#39;.!?。！？&#39;.decode() for text in texts: if text in punt_list and token not in punt_list: #检查标点符号下一个字符是否还是标点 sentences.append(texts[start:i+1])#当前标点符号位置 start=i+1#start标记到下一句的开头 i+=1 else: i+=1#若不是标点符号，则字符位置继续前移 token=list(texts[start:i+2]).pop()#取下一个字符 if start&lt;len(texts): sentences.append(texts[start:])#这是为了处理文本末尾没有标点符号的情况 return sentences def load_stopwordslist(path): print(&#39;load stopwords...&#39;) stoplist=[line.strip() for line in codecs.open(path,&#39;r&#39;,encoding=&#39;utf8&#39;).readlines()] stopwrods={}.fromkeys(stoplist) return stopwrods def summarize(text): stopwords=load_stopwordslist(&#39;stopwords.txt&#39;) sentences=sent_tokenizer(text) words=[w for sentence in sentences for w in jieba.cut(sentence) if w not in stopwords if len(w)&gt;1 and w!=&#39;\t&#39;] wordfre=nltk.FreqDist(words) topn_words=[w[0] for w in sorted(wordfre.items(),key=lambda d:d[1],reverse=True)][:N] scored_sentences=_score_sentences(sentences,topn_words) #approach 1,利用均值和标准差过滤非重要句子 avg=numpy.mean([s[1] for s in scored_sentences])#均值 std=numpy.std([s[1] for s in scored_sentences])#标准差 mean_scored=[(sent_idx,score) for (sent_idx,score) in scored_sentences if score&gt;(avg+0.5*std)] #approach 2，返回top n句子 top_n_scored=sorted(scored_sentences,key=lambda s:s[1])[-TOP_SENTENCES:] top_n_scored=sorted(top_n_scored,key=lambda s:s[0]) return dict(top_n_summary=[sentences[idx] for (idx,score) in top_n_scored],mean_scored_summary=[sentences[idx] for (idx,score) in mean_scored]) def _score_sentences(sentences,topn_words): scores=[] sentence_idx=-1 for s in [list(jieba.cut(s)) for s in sentences]: sentence_idx+=1 word_idx=[] for w in topn_words: try: word_idx.append(s.index(w))#关键词出现在该句子中的索引位置 except ValueError:#w不在句子中 pass word_idx.sort() if len(word_idx)==0: continue #对于两个连续的单词，利用单词位置索引，通过距离阀值计算族 clusters=[] cluster=[word_idx[0]] i=1 while i&lt;len(word_idx): if word_idx[i]-word_idx[i-1]&lt;CLUSTER_THRESHOLD: cluster.append(word_idx[i]) else: clusters.append(cluster[:]) cluster=[word_idx[i]] i+=1 clusters.append(cluster) #对每个族打分，每个族类的最大分数是对句子的打分 max_cluster_score=0 for c in clusters: significant_words_in_cluster=len(c) total_words_in_cluster=c[-1]-c[0]+1 score=1.0*significant_words_in_cluster*significant_words_in_cluster/total_words_in_cluster if score&gt;max_cluster_score: max_cluster_score=score scores.append((sentence_idx,max_cluster_score)) return scores; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="NLP：基于nltk和jieba库对文本实现提取文本摘要(两种方法实现：top_n_summary和mean_scored_summary) &nbsp; &nbsp; &nbsp; &nbsp; 目录 输出结果 设计思路 核心代码 &nbsp; &nbsp; &nbsp; &nbsp; 输出结果 1、测试文本 今天一大早，两位男子在故宫抽烟对镜头炫耀的视频在网络上传播，引发网友愤怒。有人感到后怕，600年的故宫真要这两个人给点了，万死莫赎。也有评论称，把无知当成炫耀的资本，丢人！ 视频中两位男子坐在故宫公共休息区的遮阳伞下，面对镜头问出：“谁敢在故宫抽烟？”语气极其嚣张，表情带有挑衅意味。话音刚落，另外一位男子面向镜头吸了一口烟。而视频中两人也表示知道有故宫禁止吸烟的规定。 事实上，2013年5月18日是国际博物馆日，故宫从这一天开始至今一直实行全面禁烟。根据规定，故宫博物院全体员工在院合作单位和个人不管在室内和室外，也不分开放区与工作区，一律禁止吸烟，对违反禁止吸烟规定的人员将进行严格处罚并通报全院。 此外，在2015年6月1日起北京全市也开始了《控制吸烟条例》，规定公共场所工作场所室内环境室外排队等场合禁止吸烟，违者将最高被罚200元，全市统一设立举报电话12320。 视频在网络上传播开来，不少网友担心故宫的安危，称一旦发生火情，后果不堪设想，有网友表示，这样的行为应该被旅游景区拉近黑名单，建议终身禁止进入任何景区和各种场馆。 &nbsp; 设计思路 后期更新…… &nbsp; &nbsp; 核心代码 def sent_tokenizer(texts): start=0 i=0#每个字符的位置 sentences=[] punt_list=&#39;.!?。！？&#39;.encode(&#39;utf-8&#39;) #&#39;,.!?:;~，。！？：；～&#39;.decode(&#39;utf8&#39;) # punt_lists=&#39;.!?。！？&#39;.decode() for text in texts: if text in punt_list and token not in punt_list: #检查标点符号下一个字符是否还是标点 sentences.append(texts[start:i+1])#当前标点符号位置 start=i+1#start标记到下一句的开头 i+=1 else: i+=1#若不是标点符号，则字符位置继续前移 token=list(texts[start:i+2]).pop()#取下一个字符 if start&lt;len(texts): sentences.append(texts[start:])#这是为了处理文本末尾没有标点符号的情况 return sentences def load_stopwordslist(path): print(&#39;load stopwords...&#39;) stoplist=[line.strip() for line in codecs.open(path,&#39;r&#39;,encoding=&#39;utf8&#39;).readlines()] stopwrods={}.fromkeys(stoplist) return stopwrods def summarize(text): stopwords=load_stopwordslist(&#39;stopwords.txt&#39;) sentences=sent_tokenizer(text) words=[w for sentence in sentences for w in jieba.cut(sentence) if w not in stopwords if len(w)&gt;1 and w!=&#39;\t&#39;] wordfre=nltk.FreqDist(words) topn_words=[w[0] for w in sorted(wordfre.items(),key=lambda d:d[1],reverse=True)][:N] scored_sentences=_score_sentences(sentences,topn_words) #approach 1,利用均值和标准差过滤非重要句子 avg=numpy.mean([s[1] for s in scored_sentences])#均值 std=numpy.std([s[1] for s in scored_sentences])#标准差 mean_scored=[(sent_idx,score) for (sent_idx,score) in scored_sentences if score&gt;(avg+0.5*std)] #approach 2，返回top n句子 top_n_scored=sorted(scored_sentences,key=lambda s:s[1])[-TOP_SENTENCES:] top_n_scored=sorted(top_n_scored,key=lambda s:s[0]) return dict(top_n_summary=[sentences[idx] for (idx,score) in top_n_scored],mean_scored_summary=[sentences[idx] for (idx,score) in mean_scored]) def _score_sentences(sentences,topn_words): scores=[] sentence_idx=-1 for s in [list(jieba.cut(s)) for s in sentences]: sentence_idx+=1 word_idx=[] for w in topn_words: try: word_idx.append(s.index(w))#关键词出现在该句子中的索引位置 except ValueError:#w不在句子中 pass word_idx.sort() if len(word_idx)==0: continue #对于两个连续的单词，利用单词位置索引，通过距离阀值计算族 clusters=[] cluster=[word_idx[0]] i=1 while i&lt;len(word_idx): if word_idx[i]-word_idx[i-1]&lt;CLUSTER_THRESHOLD: cluster.append(word_idx[i]) else: clusters.append(cluster[:]) cluster=[word_idx[i]] i+=1 clusters.append(cluster) #对每个族打分，每个族类的最大分数是对句子的打分 max_cluster_score=0 for c in clusters: significant_words_in_cluster=len(c) total_words_in_cluster=c[-1]-c[0]+1 score=1.0*significant_words_in_cluster*significant_words_in_cluster/total_words_in_cluster if score&gt;max_cluster_score: max_cluster_score=score scores.append((sentence_idx,max_cluster_score)) return scores; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/07/14/793728.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/14/793728.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-14T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"NLP：基于nltk和jieba库对文本实现提取文本摘要(两种方法实现：top_n_summary和mean_scored_summary) &nbsp; &nbsp; &nbsp; &nbsp; 目录 输出结果 设计思路 核心代码 &nbsp; &nbsp; &nbsp; &nbsp; 输出结果 1、测试文本 今天一大早，两位男子在故宫抽烟对镜头炫耀的视频在网络上传播，引发网友愤怒。有人感到后怕，600年的故宫真要这两个人给点了，万死莫赎。也有评论称，把无知当成炫耀的资本，丢人！ 视频中两位男子坐在故宫公共休息区的遮阳伞下，面对镜头问出：“谁敢在故宫抽烟？”语气极其嚣张，表情带有挑衅意味。话音刚落，另外一位男子面向镜头吸了一口烟。而视频中两人也表示知道有故宫禁止吸烟的规定。 事实上，2013年5月18日是国际博物馆日，故宫从这一天开始至今一直实行全面禁烟。根据规定，故宫博物院全体员工在院合作单位和个人不管在室内和室外，也不分开放区与工作区，一律禁止吸烟，对违反禁止吸烟规定的人员将进行严格处罚并通报全院。 此外，在2015年6月1日起北京全市也开始了《控制吸烟条例》，规定公共场所工作场所室内环境室外排队等场合禁止吸烟，违者将最高被罚200元，全市统一设立举报电话12320。 视频在网络上传播开来，不少网友担心故宫的安危，称一旦发生火情，后果不堪设想，有网友表示，这样的行为应该被旅游景区拉近黑名单，建议终身禁止进入任何景区和各种场馆。 &nbsp; 设计思路 后期更新…… &nbsp; &nbsp; 核心代码 def sent_tokenizer(texts): start=0 i=0#每个字符的位置 sentences=[] punt_list=&#39;.!?。！？&#39;.encode(&#39;utf-8&#39;) #&#39;,.!?:;~，。！？：；～&#39;.decode(&#39;utf8&#39;) # punt_lists=&#39;.!?。！？&#39;.decode() for text in texts: if text in punt_list and token not in punt_list: #检查标点符号下一个字符是否还是标点 sentences.append(texts[start:i+1])#当前标点符号位置 start=i+1#start标记到下一句的开头 i+=1 else: i+=1#若不是标点符号，则字符位置继续前移 token=list(texts[start:i+2]).pop()#取下一个字符 if start&lt;len(texts): sentences.append(texts[start:])#这是为了处理文本末尾没有标点符号的情况 return sentences def load_stopwordslist(path): print(&#39;load stopwords...&#39;) stoplist=[line.strip() for line in codecs.open(path,&#39;r&#39;,encoding=&#39;utf8&#39;).readlines()] stopwrods={}.fromkeys(stoplist) return stopwrods def summarize(text): stopwords=load_stopwordslist(&#39;stopwords.txt&#39;) sentences=sent_tokenizer(text) words=[w for sentence in sentences for w in jieba.cut(sentence) if w not in stopwords if len(w)&gt;1 and w!=&#39;\\t&#39;] wordfre=nltk.FreqDist(words) topn_words=[w[0] for w in sorted(wordfre.items(),key=lambda d:d[1],reverse=True)][:N] scored_sentences=_score_sentences(sentences,topn_words) #approach 1,利用均值和标准差过滤非重要句子 avg=numpy.mean([s[1] for s in scored_sentences])#均值 std=numpy.std([s[1] for s in scored_sentences])#标准差 mean_scored=[(sent_idx,score) for (sent_idx,score) in scored_sentences if score&gt;(avg+0.5*std)] #approach 2，返回top n句子 top_n_scored=sorted(scored_sentences,key=lambda s:s[1])[-TOP_SENTENCES:] top_n_scored=sorted(top_n_scored,key=lambda s:s[0]) return dict(top_n_summary=[sentences[idx] for (idx,score) in top_n_scored],mean_scored_summary=[sentences[idx] for (idx,score) in mean_scored]) def _score_sentences(sentences,topn_words): scores=[] sentence_idx=-1 for s in [list(jieba.cut(s)) for s in sentences]: sentence_idx+=1 word_idx=[] for w in topn_words: try: word_idx.append(s.index(w))#关键词出现在该句子中的索引位置 except ValueError:#w不在句子中 pass word_idx.sort() if len(word_idx)==0: continue #对于两个连续的单词，利用单词位置索引，通过距离阀值计算族 clusters=[] cluster=[word_idx[0]] i=1 while i&lt;len(word_idx): if word_idx[i]-word_idx[i-1]&lt;CLUSTER_THRESHOLD: cluster.append(word_idx[i]) else: clusters.append(cluster[:]) cluster=[word_idx[i]] i+=1 clusters.append(cluster) #对每个族打分，每个族类的最大分数是对句子的打分 max_cluster_score=0 for c in clusters: significant_words_in_cluster=len(c) total_words_in_cluster=c[-1]-c[0]+1 score=1.0*significant_words_in_cluster*significant_words_in_cluster/total_words_in_cluster if score&gt;max_cluster_score: max_cluster_score=score scores.append((sentence_idx,max_cluster_score)) return scores; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/07/14/793728.html","headline":"NLP：基于nltk和jieba库对文本实现提取文本摘要(两种方法实现：top_n_summary和mean_scored_summary)","dateModified":"2019-07-14T00:00:00+08:00","datePublished":"2019-07-14T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/14/793728.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>NLP：基于nltk和jieba库对文本实现提取文本摘要(两种方法实现：top_n_summary和mean_scored_summary)</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>NLP：基于nltk和jieba库对文本实现提取文本摘要(两种方法实现：top_n_summary和mean_scored_summary)</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C-toc" style="margin-left:40px;"><a href="#%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C" rel="nofollow" data-token="b04a2c387e3f379aee7f87fbe0b48927">输出结果</a></p> 
  <p id="%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF-toc" style="margin-left:40px;"><a href="#%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF" rel="nofollow" data-token="ce8c9512a2f0ce8054a78423984ad057">设计思路</a></p> 
  <p id="%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81-toc" style="margin-left:40px;"><a href="#%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81" rel="nofollow" data-token="c266fe359294c88e381d7b590feb1e17">核心代码</a></p> 
  <hr id="hr-toc">
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h2 id="%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C">输出结果</h2> 
  <p><img alt="" class="has" height="180" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190719170441230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly95dW55YW5pdS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" width="360"></p> 
  <p>1、测试文本</p> 
  <pre class="has">
<code class="language-python">今天一大早，两位男子在故宫抽烟对镜头炫耀的视频在网络上传播，引发网友愤怒。有人感到后怕，600年的故宫真要这两个人给点了，万死莫赎。也有评论称，把无知当成炫耀的资本，丢人！
视频中两位男子坐在故宫公共休息区的遮阳伞下，面对镜头问出：“谁敢在故宫抽烟？”语气极其嚣张，表情带有挑衅意味。话音刚落，另外一位男子面向镜头吸了一口烟。而视频中两人也表示知道有故宫禁止吸烟的规定。
事实上，2013年5月18日是国际博物馆日，故宫从这一天开始至今一直实行全面禁烟。根据规定，故宫博物院全体员工在院合作单位和个人不管在室内和室外，也不分开放区与工作区，一律禁止吸烟，对违反禁止吸烟规定的人员将进行严格处罚并通报全院。
此外，在2015年6月1日起北京全市也开始了《控制吸烟条例》，规定公共场所工作场所室内环境室外排队等场合禁止吸烟，违者将最高被罚200元，全市统一设立举报电话12320。
视频在网络上传播开来，不少网友担心故宫的安危，称一旦发生火情，后果不堪设想，有网友表示，这样的行为应该被旅游景区拉近黑名单，建议终身禁止进入任何景区和各种场馆。
</code></pre> 
  <p>&nbsp;</p> 
  <h2 id="%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF">设计思路</h2> 
  <p>后期更新……</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h2 id="%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81">核心代码</h2> 
  <pre class="has">
<code class="language-python">def sent_tokenizer(texts):
    start=0
    i=0#每个字符的位置
    sentences=[]
    punt_list='.!?。！？'.encode('utf-8') #',.!?:;~，。！？：；～'.decode('utf8')
    # punt_lists='.!?。！？'.decode()
    for text in texts:
        if text in punt_list and token not in punt_list: #检查标点符号下一个字符是否还是标点
            sentences.append(texts[start:i+1])#当前标点符号位置
            start=i+1#start标记到下一句的开头
            i+=1
        else:
            i+=1#若不是标点符号，则字符位置继续前移
            token=list(texts[start:i+2]).pop()#取下一个字符
    if start&lt;len(texts):
        sentences.append(texts[start:])#这是为了处理文本末尾没有标点符号的情况
    return sentences

def load_stopwordslist(path):
    print('load stopwords...')
    stoplist=[line.strip() for line in codecs.open(path,'r',encoding='utf8').readlines()]
    stopwrods={}.fromkeys(stoplist)
    return stopwrods

def summarize(text):
    stopwords=load_stopwordslist('stopwords.txt')
    sentences=sent_tokenizer(text)
    words=[w for sentence in sentences for w in jieba.cut(sentence) if w not in stopwords if len(w)&gt;1 and w!='\t']
    wordfre=nltk.FreqDist(words)
    topn_words=[w[0] for w in sorted(wordfre.items(),key=lambda d:d[1],reverse=True)][:N]
    scored_sentences=_score_sentences(sentences,topn_words)
    #approach 1,利用均值和标准差过滤非重要句子
    avg=numpy.mean([s[1] for s in scored_sentences])#均值
    std=numpy.std([s[1] for s in scored_sentences])#标准差
    mean_scored=[(sent_idx,score) for (sent_idx,score) in scored_sentences if score&gt;(avg+0.5*std)]
    #approach 2，返回top n句子
    top_n_scored=sorted(scored_sentences,key=lambda s:s[1])[-TOP_SENTENCES:]
    top_n_scored=sorted(top_n_scored,key=lambda s:s[0])
    return dict(top_n_summary=[sentences[idx] for (idx,score) in top_n_scored],mean_scored_summary=[sentences[idx] for (idx,score) in mean_scored])

def _score_sentences(sentences,topn_words):
    scores=[]
    sentence_idx=-1
    for s in [list(jieba.cut(s)) for s in sentences]:
        sentence_idx+=1
        word_idx=[]
        for w in topn_words:
            try:
                word_idx.append(s.index(w))#关键词出现在该句子中的索引位置
            except ValueError:#w不在句子中
                pass
        word_idx.sort()
        if len(word_idx)==0:
            continue
        #对于两个连续的单词，利用单词位置索引，通过距离阀值计算族
        clusters=[]
        cluster=[word_idx[0]]
        i=1
        while i&lt;len(word_idx):
            if word_idx[i]-word_idx[i-1]&lt;CLUSTER_THRESHOLD:
                cluster.append(word_idx[i])
            else:
                clusters.append(cluster[:])
                cluster=[word_idx[i]]
            i+=1
        clusters.append(cluster)
        #对每个族打分，每个族类的最大分数是对句子的打分
        max_cluster_score=0
        for c in clusters:
            significant_words_in_cluster=len(c)
            total_words_in_cluster=c[-1]-c[0]+1
            score=1.0*significant_words_in_cluster*significant_words_in_cluster/total_words_in_cluster
            if score&gt;max_cluster_score:
                max_cluster_score=score
        scores.append((sentence_idx,max_cluster_score))
    return scores;</code></pre> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
