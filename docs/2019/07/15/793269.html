<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>kafka简述与集群配置 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="kafka简述与集群配置" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="一、kafka简述 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 官网：http://kafka.apache.org/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 文档：http://kafka.apache.org/082/documentation.html &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 使用的版本是kafka_2.11-0.8.2.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Kafka® is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kafka是一个分布式、分区的、具有副本的一个提交日志的服务 2、集群介绍 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; （1）Kafka架构是由producer（消息生产者）、consumer（消息消费者）、borker(kafka集群的server，负责处理消息读、写请求，存储消息，在kafka cluster这一层这里，其实里面是有很多个broker)、topic（消息队列/分类相当于队列，里面有生产者和消费者模型）、zookeeper(元数据信息存在zookeeper中，包括：存储消费偏移量，topic话题信息，partition信息) 这些部分组成。 （2）kafka里面的消息是有topic来组织的，简单的我们可以想象为一个队列，一个队列就是一个topic，然后它把每个topic又分为很多个partition，这个是为了做并行的，在每个partition内部消息强有序，相当于有序的队列，其中每个消息都有个序号offset，比如0到12，从前面读往后面写。一个partition对应一个broker，一个broker可以管多个partition，比如说，topic有6个partition，有两个broker，那每个broker就管3个partition。这个partition可以很简单想象为一个文件，当数据发过来的时候它就往这个partition上面append，追加就行，消息不经过内存缓冲，直接写入文件，kafka和很多消息系统不一样，很多消息系统是消费完了我就把它删掉，而kafka是根据时间策略删除，而不是消费完就删除，在kafka里面没有一个消费完这么个概念，只有过期这样一个概念。 （3）producer自己决定往哪个partition里面去写，这里有一些的策略，譬如如果hash，不用多个partition之间去join数据了。consumer自己维护消费到哪个offset，每个consumer都有对应的group，group内是queue消费模型（各个consumer消费不同的partition，因此一个消息在group内只消费一次），group间是publish-subscribe消费模型，各个group各自独立消费，互不影响，因此一个消息在被每个group消费一次。 &nbsp; 安装：kafka &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.前提：jdk,scala,zookeeper &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.kafka的安装（伪分布式） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -1.下载组件 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://archive.apache.org/dist/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -2.上传tar包(服务器) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -3.解压tar包 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tar -zxvf kafka_2.11-0.8.2.1.tgz -C /opt/modules/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -4.配置文件---server.properties &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 备注：每个server.properties文件的内容是不一样的 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; broker.id：表示每个kafka的broker服务的具体的id，要求非负数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; port：表示每个kafka的broker服务监控的端口号，默认是9092 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; host.name：表示每个kafka的broker服务绑定在哪一台主机上。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; listeners：表示指定该broker服务监听的主机名和端口号，默认使用的就是上面的配置 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log.dirs:表示该broker的服务所管理的数据储存的目录 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; zookeeper.connect：表示给定kafka集群元数据管理的zk的地址以及储存元数据的目录 编辑配置server.properties &nbsp; &nbsp; &nbsp; &nbsp;这里重点修改三个参数broker.id标识本机、listeners监听地址、log.dirs是kafka接收消息存放路径、zookeeper.connect指定连接的zookeeper集群地址 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1、broker.id=0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2、listeners=PLAINTEXT://XXXX:9092 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3、log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4、zookeeper.connect=XXXX:2181/topic&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;注：XXXX：是你的hostname 其他参数保持默认即可，也可自己根据情况修改 ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. broker.id=0 # Switch to enable topic deletion or not, default value is false #delete.topic.enable=true ############################# Socket Server Settings ############################# # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 listeners=PLAINTEXT://XXXX:9092 # Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for &quot;listeners&quot; if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). #advertised.listeners=PLAINTEXT://your.host.name:9092 # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL # The number of threads that the server uses for receiving requests from the network and sending responses to the network num.network.threads=3 # The number of threads that the server uses for processing requests, which may include disk I/O num.io.threads=8 # The send buffer (SO_SNDBUF) used by the socket server socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) socket.request.max.bytes=104857600 ############################# Log Basics ############################# # A comma seperated list of directories under which to store log files log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/0 # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. num.partitions=1 # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. num.recovery.threads.per.data.dir=1 ############################# Internal Topic Settings ############################# # The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot; # For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3. offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 1. Durability: Unflushed data may be lost if you are not using replication. # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks. # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # The number of messages to accept before forcing a flush of data to disk #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=168 # A size-based retention policy for logs. Segments are pruned from the log as long as the remaining # segments don&#39;t drop below log.retention.bytes. Functions independently of log.retention.hours. #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;. # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. zookeeper.connect=XXXX:2181/topic # Timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=6000 ############################# Group Coordinator Settings ############################# # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. group.initial.rebalance.delay.ms=0 同样的配置文件copy 4份&nbsp;修改如下不分即可： &nbsp; &nbsp; &nbsp; &nbsp;broker.id分别为0、1、2、3&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;port分别为9092/9093/9094/9095&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/*&nbsp; &nbsp; *分别为0,1,2,3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -5.启动服务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.首先必须先启动zk的服务 bin/zkServer.sh start &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.启动kafka的broker的服务 bin/kafka-server-start.sh bin/kafka-server-start.sh&nbsp; -daemon config/server0.properties bin/kafka-server-start.sh&nbsp; -daemon config/server1.properties bin/kafka-server-start.sh&nbsp; -daemon config/server2.properties bin/kafka-server-start.sh&nbsp; -daemon config/server3.properties &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 启动服务的时候，必须给定server.properties的信息，参数[-daemon]，就是后台进程 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -6.关闭服务 bin/kafka-server-stop.sh 3、kafka基本操作 --1.开启服务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.首先必须先启动zk的服务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.其次启动kafka的broker的服务 --2.topic的使用 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bin/kafka-topics.sh 返回一些参数列表&nbsp; Create, delete, describe, or change a topic. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.1创建topic（Create） bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 2 --replication-factor 2 bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 5 --replication-factor 2 bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 5 --replication-factor 5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 报错：kafka.admin.AdminOperationException: replication factor: 5 larger than available brokers: 4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 原因：副本的个数不可以超过broker集群的个数 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 注意： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1.一般应用中，一个topic的分区的数目为broker的个数的1-2倍 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2.一般应用中，一个分区的副本数不超过3个，不能大于broker的分数 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3.一旦一个topic被创建，一般是不会修改的 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.2列出topic的信息 --list bin/kafka-topics.sh&nbsp; --list&nbsp; --zookeeper&nbsp; XXXX:2181/topic &nbsp; &nbsp; &nbsp; --2.3查看topic详细的信息 --describe bin/kafka-topics.sh --describe --zookeeper XXXX:2181/topic bin/kafka-topics.sh --describe --zookeeper XXXX:2181/topic&nbsp; --topic&nbsp; topicName &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.4修改topic&nbsp; --alter(一般不建议修改，只允许修改topic的配置信息，以及增加分区数)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --config &quot;max.message.bytes=102400&quot; bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --delete-config &quot;max.message.bytes&quot; bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --partitions 10 &nbsp; &nbsp; &nbsp; &nbsp;--2.5删除topic --delete bin/kafka-topics.sh --delete --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Topic topicName&nbsp; is marked for deletion. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Note: This will have no impact if delete.topic.enable is not set to true. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 注意：这个删除，只是表示禁用当前的topic，但是并没有彻底删除 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 如果你想彻底删除： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.在配置文件里面配置delete.topic.enable=true &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.先执行删除语句，然后在zkcli里面删除该topic即可 &nbsp; &nbsp;" />
<meta property="og:description" content="一、kafka简述 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 官网：http://kafka.apache.org/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 文档：http://kafka.apache.org/082/documentation.html &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 使用的版本是kafka_2.11-0.8.2.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Kafka® is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kafka是一个分布式、分区的、具有副本的一个提交日志的服务 2、集群介绍 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; （1）Kafka架构是由producer（消息生产者）、consumer（消息消费者）、borker(kafka集群的server，负责处理消息读、写请求，存储消息，在kafka cluster这一层这里，其实里面是有很多个broker)、topic（消息队列/分类相当于队列，里面有生产者和消费者模型）、zookeeper(元数据信息存在zookeeper中，包括：存储消费偏移量，topic话题信息，partition信息) 这些部分组成。 （2）kafka里面的消息是有topic来组织的，简单的我们可以想象为一个队列，一个队列就是一个topic，然后它把每个topic又分为很多个partition，这个是为了做并行的，在每个partition内部消息强有序，相当于有序的队列，其中每个消息都有个序号offset，比如0到12，从前面读往后面写。一个partition对应一个broker，一个broker可以管多个partition，比如说，topic有6个partition，有两个broker，那每个broker就管3个partition。这个partition可以很简单想象为一个文件，当数据发过来的时候它就往这个partition上面append，追加就行，消息不经过内存缓冲，直接写入文件，kafka和很多消息系统不一样，很多消息系统是消费完了我就把它删掉，而kafka是根据时间策略删除，而不是消费完就删除，在kafka里面没有一个消费完这么个概念，只有过期这样一个概念。 （3）producer自己决定往哪个partition里面去写，这里有一些的策略，譬如如果hash，不用多个partition之间去join数据了。consumer自己维护消费到哪个offset，每个consumer都有对应的group，group内是queue消费模型（各个consumer消费不同的partition，因此一个消息在group内只消费一次），group间是publish-subscribe消费模型，各个group各自独立消费，互不影响，因此一个消息在被每个group消费一次。 &nbsp; 安装：kafka &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.前提：jdk,scala,zookeeper &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.kafka的安装（伪分布式） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -1.下载组件 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://archive.apache.org/dist/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -2.上传tar包(服务器) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -3.解压tar包 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tar -zxvf kafka_2.11-0.8.2.1.tgz -C /opt/modules/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -4.配置文件---server.properties &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 备注：每个server.properties文件的内容是不一样的 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; broker.id：表示每个kafka的broker服务的具体的id，要求非负数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; port：表示每个kafka的broker服务监控的端口号，默认是9092 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; host.name：表示每个kafka的broker服务绑定在哪一台主机上。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; listeners：表示指定该broker服务监听的主机名和端口号，默认使用的就是上面的配置 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log.dirs:表示该broker的服务所管理的数据储存的目录 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; zookeeper.connect：表示给定kafka集群元数据管理的zk的地址以及储存元数据的目录 编辑配置server.properties &nbsp; &nbsp; &nbsp; &nbsp;这里重点修改三个参数broker.id标识本机、listeners监听地址、log.dirs是kafka接收消息存放路径、zookeeper.connect指定连接的zookeeper集群地址 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1、broker.id=0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2、listeners=PLAINTEXT://XXXX:9092 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3、log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4、zookeeper.connect=XXXX:2181/topic&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;注：XXXX：是你的hostname 其他参数保持默认即可，也可自己根据情况修改 ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. broker.id=0 # Switch to enable topic deletion or not, default value is false #delete.topic.enable=true ############################# Socket Server Settings ############################# # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 listeners=PLAINTEXT://XXXX:9092 # Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for &quot;listeners&quot; if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). #advertised.listeners=PLAINTEXT://your.host.name:9092 # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL # The number of threads that the server uses for receiving requests from the network and sending responses to the network num.network.threads=3 # The number of threads that the server uses for processing requests, which may include disk I/O num.io.threads=8 # The send buffer (SO_SNDBUF) used by the socket server socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) socket.request.max.bytes=104857600 ############################# Log Basics ############################# # A comma seperated list of directories under which to store log files log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/0 # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. num.partitions=1 # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. num.recovery.threads.per.data.dir=1 ############################# Internal Topic Settings ############################# # The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot; # For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3. offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 1. Durability: Unflushed data may be lost if you are not using replication. # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks. # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # The number of messages to accept before forcing a flush of data to disk #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=168 # A size-based retention policy for logs. Segments are pruned from the log as long as the remaining # segments don&#39;t drop below log.retention.bytes. Functions independently of log.retention.hours. #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;. # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. zookeeper.connect=XXXX:2181/topic # Timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=6000 ############################# Group Coordinator Settings ############################# # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. group.initial.rebalance.delay.ms=0 同样的配置文件copy 4份&nbsp;修改如下不分即可： &nbsp; &nbsp; &nbsp; &nbsp;broker.id分别为0、1、2、3&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;port分别为9092/9093/9094/9095&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/*&nbsp; &nbsp; *分别为0,1,2,3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -5.启动服务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.首先必须先启动zk的服务 bin/zkServer.sh start &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.启动kafka的broker的服务 bin/kafka-server-start.sh bin/kafka-server-start.sh&nbsp; -daemon config/server0.properties bin/kafka-server-start.sh&nbsp; -daemon config/server1.properties bin/kafka-server-start.sh&nbsp; -daemon config/server2.properties bin/kafka-server-start.sh&nbsp; -daemon config/server3.properties &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 启动服务的时候，必须给定server.properties的信息，参数[-daemon]，就是后台进程 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -6.关闭服务 bin/kafka-server-stop.sh 3、kafka基本操作 --1.开启服务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.首先必须先启动zk的服务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.其次启动kafka的broker的服务 --2.topic的使用 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bin/kafka-topics.sh 返回一些参数列表&nbsp; Create, delete, describe, or change a topic. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.1创建topic（Create） bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 2 --replication-factor 2 bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 5 --replication-factor 2 bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 5 --replication-factor 5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 报错：kafka.admin.AdminOperationException: replication factor: 5 larger than available brokers: 4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 原因：副本的个数不可以超过broker集群的个数 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 注意： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1.一般应用中，一个topic的分区的数目为broker的个数的1-2倍 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2.一般应用中，一个分区的副本数不超过3个，不能大于broker的分数 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3.一旦一个topic被创建，一般是不会修改的 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.2列出topic的信息 --list bin/kafka-topics.sh&nbsp; --list&nbsp; --zookeeper&nbsp; XXXX:2181/topic &nbsp; &nbsp; &nbsp; --2.3查看topic详细的信息 --describe bin/kafka-topics.sh --describe --zookeeper XXXX:2181/topic bin/kafka-topics.sh --describe --zookeeper XXXX:2181/topic&nbsp; --topic&nbsp; topicName &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.4修改topic&nbsp; --alter(一般不建议修改，只允许修改topic的配置信息，以及增加分区数)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --config &quot;max.message.bytes=102400&quot; bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --delete-config &quot;max.message.bytes&quot; bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --partitions 10 &nbsp; &nbsp; &nbsp; &nbsp;--2.5删除topic --delete bin/kafka-topics.sh --delete --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Topic topicName&nbsp; is marked for deletion. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Note: This will have no impact if delete.topic.enable is not set to true. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 注意：这个删除，只是表示禁用当前的topic，但是并没有彻底删除 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 如果你想彻底删除： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.在配置文件里面配置delete.topic.enable=true &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.先执行删除语句，然后在zkcli里面删除该topic即可 &nbsp; &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/07/15/793269.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/15/793269.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-15T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"一、kafka简述 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 官网：http://kafka.apache.org/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 文档：http://kafka.apache.org/082/documentation.html &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 使用的版本是kafka_2.11-0.8.2.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Kafka® is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kafka是一个分布式、分区的、具有副本的一个提交日志的服务 2、集群介绍 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; （1）Kafka架构是由producer（消息生产者）、consumer（消息消费者）、borker(kafka集群的server，负责处理消息读、写请求，存储消息，在kafka cluster这一层这里，其实里面是有很多个broker)、topic（消息队列/分类相当于队列，里面有生产者和消费者模型）、zookeeper(元数据信息存在zookeeper中，包括：存储消费偏移量，topic话题信息，partition信息) 这些部分组成。 （2）kafka里面的消息是有topic来组织的，简单的我们可以想象为一个队列，一个队列就是一个topic，然后它把每个topic又分为很多个partition，这个是为了做并行的，在每个partition内部消息强有序，相当于有序的队列，其中每个消息都有个序号offset，比如0到12，从前面读往后面写。一个partition对应一个broker，一个broker可以管多个partition，比如说，topic有6个partition，有两个broker，那每个broker就管3个partition。这个partition可以很简单想象为一个文件，当数据发过来的时候它就往这个partition上面append，追加就行，消息不经过内存缓冲，直接写入文件，kafka和很多消息系统不一样，很多消息系统是消费完了我就把它删掉，而kafka是根据时间策略删除，而不是消费完就删除，在kafka里面没有一个消费完这么个概念，只有过期这样一个概念。 （3）producer自己决定往哪个partition里面去写，这里有一些的策略，譬如如果hash，不用多个partition之间去join数据了。consumer自己维护消费到哪个offset，每个consumer都有对应的group，group内是queue消费模型（各个consumer消费不同的partition，因此一个消息在group内只消费一次），group间是publish-subscribe消费模型，各个group各自独立消费，互不影响，因此一个消息在被每个group消费一次。 &nbsp; 安装：kafka &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.前提：jdk,scala,zookeeper &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.kafka的安装（伪分布式） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -1.下载组件 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://archive.apache.org/dist/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -2.上传tar包(服务器) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -3.解压tar包 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tar -zxvf kafka_2.11-0.8.2.1.tgz -C /opt/modules/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -4.配置文件---server.properties &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 备注：每个server.properties文件的内容是不一样的 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; broker.id：表示每个kafka的broker服务的具体的id，要求非负数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; port：表示每个kafka的broker服务监控的端口号，默认是9092 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; host.name：表示每个kafka的broker服务绑定在哪一台主机上。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; listeners：表示指定该broker服务监听的主机名和端口号，默认使用的就是上面的配置 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; log.dirs:表示该broker的服务所管理的数据储存的目录 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; zookeeper.connect：表示给定kafka集群元数据管理的zk的地址以及储存元数据的目录 编辑配置server.properties &nbsp; &nbsp; &nbsp; &nbsp;这里重点修改三个参数broker.id标识本机、listeners监听地址、log.dirs是kafka接收消息存放路径、zookeeper.connect指定连接的zookeeper集群地址 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1、broker.id=0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2、listeners=PLAINTEXT://XXXX:9092 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3、log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4、zookeeper.connect=XXXX:2181/topic&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;注：XXXX：是你的hostname 其他参数保持默认即可，也可自己根据情况修改 ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. broker.id=0 # Switch to enable topic deletion or not, default value is false #delete.topic.enable=true ############################# Socket Server Settings ############################# # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 listeners=PLAINTEXT://XXXX:9092 # Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for &quot;listeners&quot; if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). #advertised.listeners=PLAINTEXT://your.host.name:9092 # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL # The number of threads that the server uses for receiving requests from the network and sending responses to the network num.network.threads=3 # The number of threads that the server uses for processing requests, which may include disk I/O num.io.threads=8 # The send buffer (SO_SNDBUF) used by the socket server socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) socket.request.max.bytes=104857600 ############################# Log Basics ############################# # A comma seperated list of directories under which to store log files log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/0 # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. num.partitions=1 # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. num.recovery.threads.per.data.dir=1 ############################# Internal Topic Settings ############################# # The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot; # For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3. offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 1. Durability: Unflushed data may be lost if you are not using replication. # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks. # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # The number of messages to accept before forcing a flush of data to disk #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=168 # A size-based retention policy for logs. Segments are pruned from the log as long as the remaining # segments don&#39;t drop below log.retention.bytes. Functions independently of log.retention.hours. #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;. # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. zookeeper.connect=XXXX:2181/topic # Timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=6000 ############################# Group Coordinator Settings ############################# # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. group.initial.rebalance.delay.ms=0 同样的配置文件copy 4份&nbsp;修改如下不分即可： &nbsp; &nbsp; &nbsp; &nbsp;broker.id分别为0、1、2、3&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;port分别为9092/9093/9094/9095&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/*&nbsp; &nbsp; *分别为0,1,2,3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -5.启动服务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.首先必须先启动zk的服务 bin/zkServer.sh start &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.启动kafka的broker的服务 bin/kafka-server-start.sh bin/kafka-server-start.sh&nbsp; -daemon config/server0.properties bin/kafka-server-start.sh&nbsp; -daemon config/server1.properties bin/kafka-server-start.sh&nbsp; -daemon config/server2.properties bin/kafka-server-start.sh&nbsp; -daemon config/server3.properties &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 启动服务的时候，必须给定server.properties的信息，参数[-daemon]，就是后台进程 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -6.关闭服务 bin/kafka-server-stop.sh 3、kafka基本操作 --1.开启服务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.首先必须先启动zk的服务 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.其次启动kafka的broker的服务 --2.topic的使用 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bin/kafka-topics.sh 返回一些参数列表&nbsp; Create, delete, describe, or change a topic. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.1创建topic（Create） bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 2 --replication-factor 2 bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 5 --replication-factor 2 bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 5 --replication-factor 5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 报错：kafka.admin.AdminOperationException: replication factor: 5 larger than available brokers: 4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 原因：副本的个数不可以超过broker集群的个数 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 注意： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1.一般应用中，一个topic的分区的数目为broker的个数的1-2倍 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2.一般应用中，一个分区的副本数不超过3个，不能大于broker的分数 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3.一旦一个topic被创建，一般是不会修改的 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.2列出topic的信息 --list bin/kafka-topics.sh&nbsp; --list&nbsp; --zookeeper&nbsp; XXXX:2181/topic &nbsp; &nbsp; &nbsp; --2.3查看topic详细的信息 --describe bin/kafka-topics.sh --describe --zookeeper XXXX:2181/topic bin/kafka-topics.sh --describe --zookeeper XXXX:2181/topic&nbsp; --topic&nbsp; topicName &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.4修改topic&nbsp; --alter(一般不建议修改，只允许修改topic的配置信息，以及增加分区数)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --config &quot;max.message.bytes=102400&quot; bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --delete-config &quot;max.message.bytes&quot; bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --partitions 10 &nbsp; &nbsp; &nbsp; &nbsp;--2.5删除topic --delete bin/kafka-topics.sh --delete --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Topic topicName&nbsp; is marked for deletion. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Note: This will have no impact if delete.topic.enable is not set to true. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 注意：这个删除，只是表示禁用当前的topic，但是并没有彻底删除 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 如果你想彻底删除： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.在配置文件里面配置delete.topic.enable=true &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.先执行删除语句，然后在zkcli里面删除该topic即可 &nbsp; &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/07/15/793269.html","headline":"kafka简述与集群配置","dateModified":"2019-07-15T00:00:00+08:00","datePublished":"2019-07-15T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/15/793269.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>kafka简述与集群配置</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h2 style="margin-left:0cm;"><strong>一、kafka简述</strong></h2> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 官网：<a href="http://kafka.apache.org/" rel="nofollow" data-token="11788822e64c59b67df7ae56659c13b1">http://kafka.apache.org/</a></p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 文档：<a href="http://kafka.apache.org/082/documentation.html" rel="nofollow" data-token="7ddb8c6363ce47157006c994c77ebb6e">http://kafka.apache.org/082/documentation.html</a></p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 使用的版本是kafka_2.11-0.8.2.1</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Kafka® is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.</p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kafka是一个分布式、分区的、具有副本的一个提交日志的服务</p> 
  <h2>2、集群介绍</h2> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt="" class="has" height="305" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715214443615.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3loODY5NTg1Nzcx,size_16,color_FFFFFF,t_70" width="503"></p> 
  <p>（1）Kafka架构是由producer（消息生产者）、consumer（消息消费者）、borker(kafka集群的server，负责处理消息读、写请求，存储消息，在kafka cluster这一层这里，其实里面是有很多个broker)、topic（消息队列/分类相当于队列，里面有生产者和消费者模型）、zookeeper(元数据信息存在zookeeper中，包括：存储消费偏移量，topic话题信息，partition信息) 这些部分组成。</p> 
  <p>（2）kafka里面的消息是有topic来组织的，简单的我们可以想象为一个队列，一个队列就是一个topic，然后它把每个topic又分为很多个partition，这个是为了做并行的，在每个partition内部消息强有序，相当于有序的队列，其中每个消息都有个序号offset，比如0到12，从前面读往后面写。一个partition对应一个broker，一个broker可以管多个partition，比如说，topic有6个partition，有两个broker，那每个broker就管3个partition。这个partition可以很简单想象为一个文件，当数据发过来的时候它就往这个partition上面append，追加就行，消息不经过内存缓冲，直接写入文件，kafka和很多消息系统不一样，很多消息系统是消费完了我就把它删掉，而kafka是根据时间策略删除，而不是消费完就删除，在kafka里面没有一个消费完这么个概念，只有过期这样一个概念。</p> 
  <p>（3）producer自己决定往哪个partition里面去写，这里有一些的策略，譬如如果hash，不用多个partition之间去join数据了。consumer自己维护消费到哪个offset，每个consumer都有对应的group，group内是queue消费模型（各个consumer消费不同的partition，因此一个消息在group内只消费一次），group间是publish-subscribe消费模型，各个group各自独立消费，互不影响，因此一个消息在被每个group消费一次。<br> &nbsp;</p> 
  <p style="margin-left:0cm;">安装：kafka</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.前提：jdk,scala,zookeeper</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.kafka的安装（伪分布式）</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -1.下载组件</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://archive.apache.org/dist/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz" rel="nofollow" data-token="b0edbdb3b22a34c6c11fd970d75643ab">https://archive.apache.org/dist/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz</a></p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -2.上传tar包(服务器)</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -3.解压tar包</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tar -zxvf kafka_2.11-0.8.2.1.tgz -C /opt/modules/</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -4.配置文件---server.properties</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 备注：每个server.properties文件的内容是不一样的</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color:#f33b45;">broker.id</span>：表示每个kafka的broker服务的具体的id，要求非负数</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color:#f33b45;">port</span>：表示每个kafka的broker服务监控的端口号，默认是9092</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color:#f33b45;">host.name</span>：表示每个kafka的broker服务绑定在哪一台主机上。</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color:#f33b45;">listeners</span>：表示指定该broker服务监听的主机名和端口号，默认使用的就是上面的配置</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color:#f33b45;">log.dirs</span>:表示该broker的服务所管理的数据储存的目录</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color:#f33b45;">zookeeper.connect</span>：表示给定kafka集群元数据管理的zk的地址以及储存元数据的目录</p> 
  <p style="text-indent:50px;"><span style="color:#f33b45;">编辑配置</span>server.properties</p> 
  <p style="text-indent:50px;">&nbsp; &nbsp; &nbsp; &nbsp;这里重点修改三个参数broker.id标识本机、listeners监听地址、log.dirs是kafka接收消息存放路径、zookeeper.connect指定连接的zookeeper集群地址</p> 
  <p style="text-indent:50px;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1、broker.id=0</p> 
  <p style="text-indent:50px;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2、listeners=PLAINTEXT://<span style="color:#f33b45;">XXXX</span>:9092</p> 
  <p style="text-indent:50px;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3、log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/0</p> 
  <p style="text-indent:50px;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4、zookeeper.connect=<span style="color:#f33b45;">XXXX</span>:2181/topic&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p> 
  <p style="text-indent:50px;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;注：<span style="color:#f33b45;">XXXX</span>：是你的hostname</p> 
  <p style="text-indent:50px;">其他参数保持默认即可，也可自己根据情况修改</p> 
  <pre class="has">
<code class="language-html">############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=0

# Switch to enable topic deletion or not, default value is false
#delete.topic.enable=true

############################# Socket Server Settings #############################

# The address the socket server listens on. It will get the value returned from 
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
listeners=PLAINTEXT://XXXX:9092

# Hostname and port the broker will advertise to producers and consumers. If not set, 
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
#advertised.listeners=PLAINTEXT://your.host.name:9092

# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# A comma seperated list of directories under which to store log files
log.dirs=/opt/cdh/kafka_2.11-0.11.0.1/data/0

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
# segments don't drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=XXXX:2181/topic

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=6000


############################# Group Coordinator Settings #############################

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0</code></pre> 
  <p>同样的配置文件copy 4份&nbsp;修改如下不分即可：</p> 
  <p><span style="color:#f33b45;">&nbsp; &nbsp; &nbsp; &nbsp;broker.id</span>分别为0、1、2、3&nbsp; &nbsp;</p> 
  <p><span style="color:#f33b45;">&nbsp; &nbsp; &nbsp; &nbsp;port</span>分别为9092/9093/9094/9095&nbsp;</p> 
  <p><span style="color:#f33b45;">&nbsp; &nbsp; &nbsp; &nbsp;log.dirs</span>=/opt/cdh/kafka_2.11-0.11.0.1/data/*&nbsp; &nbsp; *分别为0,1,2,3</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -5.启动服务</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.首先必须先启动zk的服务</p> 
  <pre class="has">
<code>        bin/zkServer.sh start</code></pre> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.启动kafka的broker的服务</p> 
  <pre class="has">
<code>        bin/kafka-server-start.sh
        
        bin/kafka-server-start.sh&nbsp; -daemon config/server0.properties

        bin/kafka-server-start.sh&nbsp; -daemon config/server1.properties

        bin/kafka-server-start.sh&nbsp; -daemon config/server2.properties

        bin/kafka-server-start.sh&nbsp; -daemon config/server3.properties</code></pre> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 启动服务的时候，必须给定server.properties的信息，参数[-daemon]，就是后台进程</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -6.关闭服务</p> 
  <pre class="has">
<code>        bin/kafka-server-stop.sh</code></pre> 
  <h2>3、kafka基本操作</h2> 
  <p style="margin-left:0cm;">--1.开启服务</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.首先必须先启动zk的服务</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.其次启动kafka的broker的服务</p> 
  <p style="margin-left:0cm;">--2.topic的使用</p> 
  <p style="margin-left:0cm;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bin/kafka-topics.sh 返回一些参数列表&nbsp; Create, delete, describe, or change a topic.</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.1创建topic（Create）</p> 
  <pre class="has">
<code>        bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 2 --replication-factor 2

        bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 5 --replication-factor 2

        bin/kafka-topics.sh --create --zookeeper XXXX:2181/topic&nbsp; --topic topicName&nbsp; --partitions&nbsp; 5 --replication-factor 5</code></pre> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color:#f33b45;">报错</span>：kafka.admin.AdminOperationException: replication factor: 5 larger than available brokers: 4</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color:#f33b45;">原因</span>：副本的个数不可以超过broker集群的个数</p> 
  <p style="margin-left:0cm;"><span style="color:#f33b45;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 注意</span>：</p> 
  <p style="margin-left:0cm;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1.一般应用中，一个topic的分区的数目为broker的个数的1-2倍</p> 
  <p style="margin-left:0cm;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2.一般应用中，一个分区的副本数不超过3个，不能大于broker的分数</p> 
  <p style="margin-left:0cm;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3.一旦一个topic被创建，一般是不会修改的</p> 
  <p style="margin-left:0cm;">&nbsp;</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.2列出topic的信息 --list</p> 
  <pre class="has">
<code>        bin/kafka-topics.sh&nbsp; --list&nbsp; --zookeeper&nbsp; XXXX:2181/topic</code></pre> 
  <p style="margin-left:0cm;">&nbsp; &nbsp; &nbsp; --2.3查看topic详细的信息 --describe</p> 
  <pre class="has">
<code>        bin/kafka-topics.sh --describe --zookeeper XXXX:2181/topic

        bin/kafka-topics.sh --describe --zookeeper XXXX:2181/topic&nbsp; --topic&nbsp; topicName</code></pre> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --2.4修改topic&nbsp; --alter(一般不建议修改，只允许修改topic的配置信息，以及增加分区数)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p> 
  <pre class="has">
<code>        bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --config "max.message.bytes=102400"

        bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --delete-config "max.message.bytes"

        bin/kafka-topics.sh --alter --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName&nbsp; --partitions 10</code></pre> 
  <p style="margin-left:0cm;">&nbsp; &nbsp; &nbsp; &nbsp;--2.5删除topic --delete</p> 
  <pre class="has">
<code>        bin/kafka-topics.sh --delete --zookeeper&nbsp; XXXX:2181/topic&nbsp; --topic&nbsp; topicName</code></pre> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Topic topicName&nbsp; is marked for deletion.</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Note: This will have no impact if delete.topic.enable is not set to true.</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 注意：这个删除，只是表示禁用当前的topic，但是并没有彻底删除</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 如果你想彻底删除：</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.在配置文件里面配置delete.topic.enable=true</p> 
  <p style="margin-left:0cm;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.先执行删除语句，然后在zkcli里面删除该topic即可</p> 
  <p style="margin-left:0cm;">&nbsp;</p> 
  <p style="margin-left:0cm;">&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
