<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>快乐的强化学习2——DQN及其实现方法 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="快乐的强化学习2——DQN及其实现方法" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="快乐的强化学习2——DQN及其实现方法 学习前言 简介 DQN算法的实现 具体实现代码 学习前言 刚刚从大学毕业，近来闲来无事，开始了机器学习的旅程，深度学习是机器学习的重要一环，其可以使得机器自我尝试，并通过结果进行学习。 在机器学习的过程中，我自网上了解到大神morvanzhou，一个从土木工程转向了计算机的“聪明绝顶”的、英语特好的男人。本篇文章便是按照他的深度学习教程中的Qlearning部分撰写的。 morvanzhou的python个人主页，请有兴趣的同学关注大神morvanzhou的python教程。 简介 DQN是一种结合了深度学习与强化学习的一种学习方法，它以强化学习Q-Learning为基础，结合tensorflow的神经网络。强化学习Q-Learning是一种非常优秀的算法，但是由于现在许多问题的环境非常复杂，如果把每一种环境都单独列出来，整个Q表会非常大，且更新的时候检索时间较长。所以如果可以不建立Q表，只通过每个环境的特点就可以得出整个环境的Q-Value，那么Q表庞大冗杂的问题就迎刃而解了。 其与常规的强化学习Q-Learning最大的不同就是，DQN在初始化的时候不再生成一个完整的Q-Table，每一个观测环境的Q值都是通过神经网络生成的，即通过输入当前环境的特征Features来得到当前环境每个动作的Q-Value，并且以这个Q-Value基准进行动作选择。 DQN一共具有两个神经网络，一个是用于计算q_predict，一个是用于计算q_next的。用于计算q_predict的神经网络，我们简称其为Net_Pre；用于计算q_next我们简称其为Net_Next。 DQN的更新准则与Q-Learning的更新准则类似，都是通过所处的当前环境对各个动作的预测得分，下一步的环境的实际情况进行得分更新的，但是DQN更新的不再是Q表，而是通过所处的当前环境对各个动作的预测得分和下一步的环境的实际情况二者的误差更新Net_Pre和Net_Next的参数。 如果大家对Q-Learning还有疑惑，请大家关注我的另一篇博文https://blog.csdn.net/weixin_44791964/article/details/95410737 DQN算法的实现 接下来我将以小男孩取得玩具为例子，讲述DQN算法的执行过程。 在一开始的时候假设小男孩不知道玩具在哪里，Net_Pre和Net_Next都是随机生成的，在进行第一步动作前，小男孩通过Net_Pre获得其对于当前环境每一个动作的得分的预测值，并在其中优先选择得分最高的一个动作作为下一步的行为Action，并且通过该Action得到下一步的环境。 # 刷新环境 env.render() # DQN 根据观测值选择行为 action = RL.choose_action(observation) # 环境根据行为给出下一个 state, reward, 是否终止 observation_, reward, done = env.step(action) 在获得所处的当前环境对各个动作的预测得分和。下一步的环境的实际情况的时候，常规的Q-Learning便可以通过： self.q_table.loc[observation_now, action] += self.lr * (q_target - q_predict) 进行Q-Table更新，但是DQN不一样，DQN会将这一步按照一定的格式存入记忆，当形成一定规模的时候再进行Net_Pre的更新。在python中，其表示为： # 控制学习起始时间和频率 (先累积一些记忆再开始学习) if (step &gt; 200) and (step % 5 == 0): RL.learn() 可能同学们会很奇怪，为什么是进行Net_Pre的更新而不是进行Net_Pre和Net_Next的更新，其实是因为通过Net_Pre得到的Q_predict的是人物对当前环境的认知，而通过Net_Next通过一定处理之后获得的Q_target是下个环境的实际得分，我们只能通过下个环境的实际得分来更新人物对当前环境的认知。 这是否意味着Net_Next不需要更新呢？答案是否定的，因为当前的Net_Next都是随机生成的，其并没有考虑地图上的一个重要信息，就是每个环境的reward。当小男孩掉下悬崖的时候，他的得分是-1；当小男孩拿到玩具的时候，他的得分是1。因此，每当进行一定次数的Net_Pre的更新后，就要将最新的Net_Pre的参数赋值给Net_Next。 在python中，其表示为： if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\ntarget_params_replaced\n&#39;) 那么Net_Pre的参数又是如何更新的呢，通过Net_Pre得到的Q_predict的是人物对当前环境的认知，而通过Net_Next通过一定处理之后获得的Q_target是下个环境的实际得分，那么Q_target-Q_predict得到便是实际与预测之间的差距，我们将其作为cost传入tensorflow，便可以使用一定的优化器缩小cost，实现Net_Pre神经网络的训练。接下来让我们看看学习过程是如何实现的。 def learn(self): # 确认是否到达了需要进行两个神经网络的参数赋值的代数，是则赋值 if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\ntarget_params_replaced\n&#39;) # 根据当前memory当中的size进行提取，在没有到达memory_size时，根据当前的memory里的数量进行提取，提取的数量都是batchsize if self.memory_counter &gt; self.memory_size: sample_index = np.random.choice(self.memory_size, size=self.batch_size) else: sample_index = np.random.choice(self.memory_counter, size=self.batch_size) #根据sample_index提取出batch batch_memory = self.memory[sample_index, :] #通过神经网络获得q_next和q_eval。 q_next, q_eval = self.sess.run( [self.q_next, self.q_eval], feed_dict={ self.s_: batch_memory[:, -self.n_features:], self.s: batch_memory[:, :self.n_features], }) #该步主要是为了获得q_next, q_eval的格式 q_target = q_eval.copy() batch_index = np.arange(self.batch_size, dtype=np.int32) #取出eval的每一个行为 eval_act_index = batch_memory[:, self.n_features].astype(int) #取出eval的每一个得分 reward = batch_memory[:, self.n_features + 1] #取出每一行的最大值、取出每个环境对应的最大得分 q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1) #利用q_target和q_predict训练 _, self.cost = self.sess.run([self._train_op, self.loss], feed_dict={self.s: batch_memory[:, :self.n_features], self.q_target: q_target}) self.cost_his.append(self.cost) #如果存在epsilon_increment则改变epsilon的值 self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max self.learn_step_counter += 1 可能大家看完这个还是一头雾水，现在，我将以伪代码的方式，将整个DQN执行一遍给大家看一下。 初始化测试环境对象 初始化DQN的大脑对象 step = 0 for episode in range(TIMES): 初始化环境 while(1): #刷新环境 env.render() #根据观测值选择行为 action = RL.choose_action(observation) #获得环境下一步行为，判断下一步环境是否终止, 获得下一步环境的得分。 observation_, reward, done = env.step(action) #调用存储函数存储记忆 RL.store_transition(observation, action, reward, observation_) #先累积一些记忆再开始学习 if (step &gt; 200) and (step % 5 == 0): RL.learn() #将下一个 state_ 变为 下次循环的 state #如果终止, 就跳出循环 if done: break step += 1 # 总步数 在整个函数的执行过程中DQN的大脑包括以下部分，其对应的功能为 模块名称 作用/功能 初始化 初始化学习率、可执行动作、全局学习步数、记忆库大小、每次训练的batch大小、两个神经网络等参数 动作选择 根据当前所处的环境特征和Net_Pre获得当前环境各个动作的Q_Value，并进行动作选择 学习 通过Net_Next获得q_next，再通过一定运算得到q_target，根据q_predict和q_target进行网络更新 记忆存储 按照一定格式存储 当前环境特点、下一步环境特点、得分reward、当前环境的动作。 具体实现代码 具体的实现代码分为三个部分，这是第一部分，主函数： from maze_env import Maze from RL_brain import DeepQNetwork def run_maze(): step = 0 # 用来控制什么时候学习 for episode in range(300): # 初始化环境 observation = env.reset() while True: # 刷新环境 env.render() # DQN 根据观测值选择行为 action = RL.choose_action(observation) # 环境根据行为给出下一个 state, reward, 是否终止 observation_, reward, done = env.step(action) # DQN 存储记忆 RL.store_transition(observation, action, reward, observation_) # 控制学习起始时间和频率 (先累积一些记忆再开始学习) if (step &gt; 200) and (step % 5 == 0): RL.learn() # 将下一个 state_ 变为 下次循环的 state observation = observation_ # 如果终止, 就跳出循环 if done: break step += 1 # 总步数 # end of game print(&#39;game over&#39;) env.destroy() if __name__ == &quot;__main__&quot;: env = Maze() RL = DeepQNetwork(env.n_actions, env.n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=200, # 每 200 步替换一次 target_net 的参数 memory_size=2000, # 记忆上限 # output_graph=True # 是否输出 tensorboard 文件 ) env.after(100, run_maze) env.mainloop() RL.plot_cost() # 观看神经网络的误差曲线 第二部分是DQN的大脑： import numpy as np import pandas as pd import tensorflow as tf np.random.seed(1) tf.set_random_seed(1) # Deep Q Network off-policy class DeepQNetwork: def __init__( self, n_actions, n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=300, memory_size=500, batch_size=32, e_greedy_increment=None, output_graph=False, ): self.n_actions = n_actions #动作数量 self.n_features = n_features #观测环境的特征数量 self.lr = learning_rate #学习率 self.gamma = reward_decay self.epsilon_max = e_greedy #处于epsilon范围内时，选择value值最大的动作 self.replace_target_iter = replace_target_iter #进行参数替换的轮替代数 self.memory_size = memory_size #每个memory的size self.batch_size = batch_size #训练batch的size self.epsilon_increment = e_greedy_increment self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max self.learn_step_counter = 0 # 初始化全局学习步数 # 初始化用于存储memory的地方 [s, a, r, s_] self.memory = np.zeros((self.memory_size, n_features * 2 + 2)) # 利用get_collection获得eval和target两个神经网络的参数 self._build_net() t_params = tf.get_collection(&#39;target_net_params&#39;) e_params = tf.get_collection(&#39;eval_net_params&#39;) # 将e的参数赋予t self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)] self.sess = tf.Session() #如果output_graph==True则在tensorboard上输出结构 if output_graph: tf.summary.FileWriter(&quot;logs/&quot;, self.sess.graph) self.sess.run(tf.global_variables_initializer()) #用于存储历史cost值 self.cost_his = [] def _build_net(self): # ------------------ build evaluate_net ------------------ self.s = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s&#39;) #输入值，列的内容为观测到的环境的特点，一共有n个，用于计算对当前环境的估计 self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name=&#39;Q_target&#39;) #该层神经网络用于计算q_eval with tf.variable_scope(&#39;eval_net&#39;): # 神经网络的参数 c_names =[&#39;eval_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES] #collection的名字 n_l1 = 10 #隐含层神经元的数量 w_initializer = tf.random_normal_initializer(mean = 0., stddev = 0.3) #初始化正态分布生成器 b_initializer = tf.constant_initializer(0.1) #常数生成器 # q_eval神经网络的第一层 with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1) # q_eval神经网络的第二层 with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_eval = tf.matmul(l1, w2) + b2 with tf.variable_scope(&#39;loss&#39;): # q_eval神经网络的损失值，其将q_eval的损失值和q_target对比 self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval)) with tf.variable_scope(&#39;train&#39;): # 利用RMSPropOptimizer进行训练 self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss) # ------------------ build target_net ------------------ self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s_&#39;) # input #该层神经网络用于计算q_target with tf.variable_scope(&#39;target_net&#39;): #神经网络的参数 c_names = [&#39;target_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES] #q_target第一层 with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1) #q_target第二层 with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_next = tf.matmul(l1, w2) + b2 def store_transition(self, s, a, r, s_): #判断是否有memory_counter属性。没有则加入。该部分用于存储当前状态，action，得分，预测状态。 if not hasattr(self, &#39;memory_counter&#39;): self.memory_counter = 0 #将当前状态，action，得分，预测状态进行水平堆叠 transition = np.hstack((s, [a, r], s_)) #index保证在memory_size以内 index = self.memory_counter % self.memory_size #self.memory中加入一行 self.memory[index, :] = transition #数量加1 self.memory_counter += 1 def choose_action(self, observation): #变成可以输入给神经网络的行向量 observation = observation[np.newaxis, :] if np.random.uniform() &lt; self.epsilon: #如果在epsilon内，通过运算得出value最大的action，否则随机生成action actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation}) action = np.argmax(actions_value) else: action = np.random.randint(0, self.n_actions) return action def learn(self): # 确认是否到达了需要进行两个神经网络的参数赋值的代数，是则赋值 if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\ntarget_params_replaced\n&#39;) # 根据当前memory当中的size进行提取，在没有到达memory_size时，根据当前的memory里的数量进行提取，提取的数量都是batchsize if self.memory_counter &gt; self.memory_size: sample_index = np.random.choice(self.memory_size, size=self.batch_size) else: sample_index = np.random.choice(self.memory_counter, size=self.batch_size) #根据sample_index提取出batch batch_memory = self.memory[sample_index, :] q_next, q_eval = self.sess.run( [self.q_next, self.q_eval], feed_dict={ self.s_: batch_memory[:, -self.n_features:], self.s: batch_memory[:, :self.n_features], }) q_target = q_eval.copy() batch_index = np.arange(self.batch_size, dtype=np.int32) #取出eval的每一个行为 eval_act_index = batch_memory[:, self.n_features].astype(int) #取出eval的每一个得分 reward = batch_memory[:, self.n_features + 1] #取出每一行的最大值 q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1) #训练 _, self.cost = self.sess.run([self._train_op, self.loss], feed_dict={self.s: batch_memory[:, :self.n_features], self.q_target: q_target}) self.cost_his.append(self.cost) #如果存在epsilon_increment则改变epsilon的值 self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max self.learn_step_counter += 1 def plot_cost(self): import matplotlib.pyplot as plt plt.plot(np.arange(len(self.cost_his)), self.cost_his) plt.ylabel(&#39;Cost&#39;) plt.xlabel(&#39;training steps&#39;) plt.show() 第三部分为运行环境： &quot;&quot;&quot; Reinforcement learning maze example. Red rectangle: explorer. Black rectangles: hells [reward = -1]. Yellow bin circle: paradise [reward = +1]. All other states: ground [reward = 0]. This script is the environment part of this example. The RL is in RL_brain.py. View more on my tutorial page: https://morvanzhou.github.io/tutorials/ &quot;&quot;&quot; import numpy as np import time import sys if sys.version_info.major == 2: import Tkinter as tk else: import tkinter as tk UNIT = 40 # pixels MAZE_H = 4 # grid height MAZE_W = 4 # grid width class Maze(tk.Tk, object): def __init__(self): super(Maze, self).__init__() self.action_space = [&#39;u&#39;, &#39;d&#39;, &#39;l&#39;, &#39;r&#39;] self.n_actions = len(self.action_space) self.n_features = 2 self.title(&#39;maze&#39;) self.geometry(&#39;{0}x{1}&#39;.format(MAZE_H * UNIT, MAZE_H * UNIT)) self._build_maze() def _build_maze(self): self.canvas = tk.Canvas(self, bg=&#39;white&#39;, height=MAZE_H * UNIT, width=MAZE_W * UNIT) # create grids for c in range(0, MAZE_W * UNIT, UNIT): x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT self.canvas.create_line(x0, y0, x1, y1) for r in range(0, MAZE_H * UNIT, UNIT): x0, y0, x1, y1 = 0, r, MAZE_W * UNIT, r self.canvas.create_line(x0, y0, x1, y1) # create origin origin = np.array([20, 20]) # hell hell1_center = origin + np.array([UNIT * 2, UNIT]) self.hell1 = self.canvas.create_rectangle( hell1_center[0] - 15, hell1_center[1] - 15, hell1_center[0] + 15, hell1_center[1] + 15, fill=&#39;black&#39;) # hell # hell2_center = origin + np.array([UNIT, UNIT * 2]) # self.hell2 = self.canvas.create_rectangle( # hell2_center[0] - 15, hell2_center[1] - 15, # hell2_center[0] + 15, hell2_center[1] + 15, # fill=&#39;black&#39;) # create oval oval_center = origin + UNIT * 2 self.oval = self.canvas.create_oval( oval_center[0] - 15, oval_center[1] - 15, oval_center[0] + 15, oval_center[1] + 15, fill=&#39;yellow&#39;) # create red rect self.rect = self.canvas.create_rectangle( origin[0] - 15, origin[1] - 15, origin[0] + 15, origin[1] + 15, fill=&#39;red&#39;) # pack all self.canvas.pack() def reset(self): self.update() time.sleep(0.1) self.canvas.delete(self.rect) origin = np.array([20, 20]) self.rect = self.canvas.create_rectangle( origin[0] - 15, origin[1] - 15, origin[0] + 15, origin[1] + 15, fill=&#39;red&#39;) # return observation return (np.array(self.canvas.coords(self.rect)[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT) def step(self, action): s = self.canvas.coords(self.rect) base_action = np.array([0, 0]) if action == 0: # up if s[1] &gt; UNIT: base_action[1] -= UNIT elif action == 1: # down if s[1] &lt; (MAZE_H - 1) * UNIT: base_action[1] += UNIT elif action == 2: # right if s[0] &lt; (MAZE_W - 1) * UNIT: base_action[0] += UNIT elif action == 3: # left if s[0] &gt; UNIT: base_action[0] -= UNIT self.canvas.move(self.rect, base_action[0], base_action[1]) # move agent next_coords = self.canvas.coords(self.rect) # next state # reward function if next_coords == self.canvas.coords(self.oval): reward = 1 done = True elif next_coords in [self.canvas.coords(self.hell1)]: reward = -1 done = True else: reward = 0 done = False s_ = (np.array(next_coords[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT) return s_, reward, done def render(self): # time.sleep(0.01) self.update() 除了环境部分外，主函数部分和DQN大脑部分我都进行了详细的备注，由于涉及到模块间的调用，需要设置文件名称，三个文件的名称分别为：run_this.py，RL_brain.py，maze_env.py。 由于代码并不是自己写的，所以就不上传github了，不过还是欢迎大家关注我和我的github https://github.com/bubbliiiing/ 希望得到朋友们的喜欢。 有不懂的朋友可以评论询问噢。" />
<meta property="og:description" content="快乐的强化学习2——DQN及其实现方法 学习前言 简介 DQN算法的实现 具体实现代码 学习前言 刚刚从大学毕业，近来闲来无事，开始了机器学习的旅程，深度学习是机器学习的重要一环，其可以使得机器自我尝试，并通过结果进行学习。 在机器学习的过程中，我自网上了解到大神morvanzhou，一个从土木工程转向了计算机的“聪明绝顶”的、英语特好的男人。本篇文章便是按照他的深度学习教程中的Qlearning部分撰写的。 morvanzhou的python个人主页，请有兴趣的同学关注大神morvanzhou的python教程。 简介 DQN是一种结合了深度学习与强化学习的一种学习方法，它以强化学习Q-Learning为基础，结合tensorflow的神经网络。强化学习Q-Learning是一种非常优秀的算法，但是由于现在许多问题的环境非常复杂，如果把每一种环境都单独列出来，整个Q表会非常大，且更新的时候检索时间较长。所以如果可以不建立Q表，只通过每个环境的特点就可以得出整个环境的Q-Value，那么Q表庞大冗杂的问题就迎刃而解了。 其与常规的强化学习Q-Learning最大的不同就是，DQN在初始化的时候不再生成一个完整的Q-Table，每一个观测环境的Q值都是通过神经网络生成的，即通过输入当前环境的特征Features来得到当前环境每个动作的Q-Value，并且以这个Q-Value基准进行动作选择。 DQN一共具有两个神经网络，一个是用于计算q_predict，一个是用于计算q_next的。用于计算q_predict的神经网络，我们简称其为Net_Pre；用于计算q_next我们简称其为Net_Next。 DQN的更新准则与Q-Learning的更新准则类似，都是通过所处的当前环境对各个动作的预测得分，下一步的环境的实际情况进行得分更新的，但是DQN更新的不再是Q表，而是通过所处的当前环境对各个动作的预测得分和下一步的环境的实际情况二者的误差更新Net_Pre和Net_Next的参数。 如果大家对Q-Learning还有疑惑，请大家关注我的另一篇博文https://blog.csdn.net/weixin_44791964/article/details/95410737 DQN算法的实现 接下来我将以小男孩取得玩具为例子，讲述DQN算法的执行过程。 在一开始的时候假设小男孩不知道玩具在哪里，Net_Pre和Net_Next都是随机生成的，在进行第一步动作前，小男孩通过Net_Pre获得其对于当前环境每一个动作的得分的预测值，并在其中优先选择得分最高的一个动作作为下一步的行为Action，并且通过该Action得到下一步的环境。 # 刷新环境 env.render() # DQN 根据观测值选择行为 action = RL.choose_action(observation) # 环境根据行为给出下一个 state, reward, 是否终止 observation_, reward, done = env.step(action) 在获得所处的当前环境对各个动作的预测得分和。下一步的环境的实际情况的时候，常规的Q-Learning便可以通过： self.q_table.loc[observation_now, action] += self.lr * (q_target - q_predict) 进行Q-Table更新，但是DQN不一样，DQN会将这一步按照一定的格式存入记忆，当形成一定规模的时候再进行Net_Pre的更新。在python中，其表示为： # 控制学习起始时间和频率 (先累积一些记忆再开始学习) if (step &gt; 200) and (step % 5 == 0): RL.learn() 可能同学们会很奇怪，为什么是进行Net_Pre的更新而不是进行Net_Pre和Net_Next的更新，其实是因为通过Net_Pre得到的Q_predict的是人物对当前环境的认知，而通过Net_Next通过一定处理之后获得的Q_target是下个环境的实际得分，我们只能通过下个环境的实际得分来更新人物对当前环境的认知。 这是否意味着Net_Next不需要更新呢？答案是否定的，因为当前的Net_Next都是随机生成的，其并没有考虑地图上的一个重要信息，就是每个环境的reward。当小男孩掉下悬崖的时候，他的得分是-1；当小男孩拿到玩具的时候，他的得分是1。因此，每当进行一定次数的Net_Pre的更新后，就要将最新的Net_Pre的参数赋值给Net_Next。 在python中，其表示为： if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\ntarget_params_replaced\n&#39;) 那么Net_Pre的参数又是如何更新的呢，通过Net_Pre得到的Q_predict的是人物对当前环境的认知，而通过Net_Next通过一定处理之后获得的Q_target是下个环境的实际得分，那么Q_target-Q_predict得到便是实际与预测之间的差距，我们将其作为cost传入tensorflow，便可以使用一定的优化器缩小cost，实现Net_Pre神经网络的训练。接下来让我们看看学习过程是如何实现的。 def learn(self): # 确认是否到达了需要进行两个神经网络的参数赋值的代数，是则赋值 if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\ntarget_params_replaced\n&#39;) # 根据当前memory当中的size进行提取，在没有到达memory_size时，根据当前的memory里的数量进行提取，提取的数量都是batchsize if self.memory_counter &gt; self.memory_size: sample_index = np.random.choice(self.memory_size, size=self.batch_size) else: sample_index = np.random.choice(self.memory_counter, size=self.batch_size) #根据sample_index提取出batch batch_memory = self.memory[sample_index, :] #通过神经网络获得q_next和q_eval。 q_next, q_eval = self.sess.run( [self.q_next, self.q_eval], feed_dict={ self.s_: batch_memory[:, -self.n_features:], self.s: batch_memory[:, :self.n_features], }) #该步主要是为了获得q_next, q_eval的格式 q_target = q_eval.copy() batch_index = np.arange(self.batch_size, dtype=np.int32) #取出eval的每一个行为 eval_act_index = batch_memory[:, self.n_features].astype(int) #取出eval的每一个得分 reward = batch_memory[:, self.n_features + 1] #取出每一行的最大值、取出每个环境对应的最大得分 q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1) #利用q_target和q_predict训练 _, self.cost = self.sess.run([self._train_op, self.loss], feed_dict={self.s: batch_memory[:, :self.n_features], self.q_target: q_target}) self.cost_his.append(self.cost) #如果存在epsilon_increment则改变epsilon的值 self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max self.learn_step_counter += 1 可能大家看完这个还是一头雾水，现在，我将以伪代码的方式，将整个DQN执行一遍给大家看一下。 初始化测试环境对象 初始化DQN的大脑对象 step = 0 for episode in range(TIMES): 初始化环境 while(1): #刷新环境 env.render() #根据观测值选择行为 action = RL.choose_action(observation) #获得环境下一步行为，判断下一步环境是否终止, 获得下一步环境的得分。 observation_, reward, done = env.step(action) #调用存储函数存储记忆 RL.store_transition(observation, action, reward, observation_) #先累积一些记忆再开始学习 if (step &gt; 200) and (step % 5 == 0): RL.learn() #将下一个 state_ 变为 下次循环的 state #如果终止, 就跳出循环 if done: break step += 1 # 总步数 在整个函数的执行过程中DQN的大脑包括以下部分，其对应的功能为 模块名称 作用/功能 初始化 初始化学习率、可执行动作、全局学习步数、记忆库大小、每次训练的batch大小、两个神经网络等参数 动作选择 根据当前所处的环境特征和Net_Pre获得当前环境各个动作的Q_Value，并进行动作选择 学习 通过Net_Next获得q_next，再通过一定运算得到q_target，根据q_predict和q_target进行网络更新 记忆存储 按照一定格式存储 当前环境特点、下一步环境特点、得分reward、当前环境的动作。 具体实现代码 具体的实现代码分为三个部分，这是第一部分，主函数： from maze_env import Maze from RL_brain import DeepQNetwork def run_maze(): step = 0 # 用来控制什么时候学习 for episode in range(300): # 初始化环境 observation = env.reset() while True: # 刷新环境 env.render() # DQN 根据观测值选择行为 action = RL.choose_action(observation) # 环境根据行为给出下一个 state, reward, 是否终止 observation_, reward, done = env.step(action) # DQN 存储记忆 RL.store_transition(observation, action, reward, observation_) # 控制学习起始时间和频率 (先累积一些记忆再开始学习) if (step &gt; 200) and (step % 5 == 0): RL.learn() # 将下一个 state_ 变为 下次循环的 state observation = observation_ # 如果终止, 就跳出循环 if done: break step += 1 # 总步数 # end of game print(&#39;game over&#39;) env.destroy() if __name__ == &quot;__main__&quot;: env = Maze() RL = DeepQNetwork(env.n_actions, env.n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=200, # 每 200 步替换一次 target_net 的参数 memory_size=2000, # 记忆上限 # output_graph=True # 是否输出 tensorboard 文件 ) env.after(100, run_maze) env.mainloop() RL.plot_cost() # 观看神经网络的误差曲线 第二部分是DQN的大脑： import numpy as np import pandas as pd import tensorflow as tf np.random.seed(1) tf.set_random_seed(1) # Deep Q Network off-policy class DeepQNetwork: def __init__( self, n_actions, n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=300, memory_size=500, batch_size=32, e_greedy_increment=None, output_graph=False, ): self.n_actions = n_actions #动作数量 self.n_features = n_features #观测环境的特征数量 self.lr = learning_rate #学习率 self.gamma = reward_decay self.epsilon_max = e_greedy #处于epsilon范围内时，选择value值最大的动作 self.replace_target_iter = replace_target_iter #进行参数替换的轮替代数 self.memory_size = memory_size #每个memory的size self.batch_size = batch_size #训练batch的size self.epsilon_increment = e_greedy_increment self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max self.learn_step_counter = 0 # 初始化全局学习步数 # 初始化用于存储memory的地方 [s, a, r, s_] self.memory = np.zeros((self.memory_size, n_features * 2 + 2)) # 利用get_collection获得eval和target两个神经网络的参数 self._build_net() t_params = tf.get_collection(&#39;target_net_params&#39;) e_params = tf.get_collection(&#39;eval_net_params&#39;) # 将e的参数赋予t self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)] self.sess = tf.Session() #如果output_graph==True则在tensorboard上输出结构 if output_graph: tf.summary.FileWriter(&quot;logs/&quot;, self.sess.graph) self.sess.run(tf.global_variables_initializer()) #用于存储历史cost值 self.cost_his = [] def _build_net(self): # ------------------ build evaluate_net ------------------ self.s = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s&#39;) #输入值，列的内容为观测到的环境的特点，一共有n个，用于计算对当前环境的估计 self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name=&#39;Q_target&#39;) #该层神经网络用于计算q_eval with tf.variable_scope(&#39;eval_net&#39;): # 神经网络的参数 c_names =[&#39;eval_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES] #collection的名字 n_l1 = 10 #隐含层神经元的数量 w_initializer = tf.random_normal_initializer(mean = 0., stddev = 0.3) #初始化正态分布生成器 b_initializer = tf.constant_initializer(0.1) #常数生成器 # q_eval神经网络的第一层 with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1) # q_eval神经网络的第二层 with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_eval = tf.matmul(l1, w2) + b2 with tf.variable_scope(&#39;loss&#39;): # q_eval神经网络的损失值，其将q_eval的损失值和q_target对比 self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval)) with tf.variable_scope(&#39;train&#39;): # 利用RMSPropOptimizer进行训练 self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss) # ------------------ build target_net ------------------ self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s_&#39;) # input #该层神经网络用于计算q_target with tf.variable_scope(&#39;target_net&#39;): #神经网络的参数 c_names = [&#39;target_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES] #q_target第一层 with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1) #q_target第二层 with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_next = tf.matmul(l1, w2) + b2 def store_transition(self, s, a, r, s_): #判断是否有memory_counter属性。没有则加入。该部分用于存储当前状态，action，得分，预测状态。 if not hasattr(self, &#39;memory_counter&#39;): self.memory_counter = 0 #将当前状态，action，得分，预测状态进行水平堆叠 transition = np.hstack((s, [a, r], s_)) #index保证在memory_size以内 index = self.memory_counter % self.memory_size #self.memory中加入一行 self.memory[index, :] = transition #数量加1 self.memory_counter += 1 def choose_action(self, observation): #变成可以输入给神经网络的行向量 observation = observation[np.newaxis, :] if np.random.uniform() &lt; self.epsilon: #如果在epsilon内，通过运算得出value最大的action，否则随机生成action actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation}) action = np.argmax(actions_value) else: action = np.random.randint(0, self.n_actions) return action def learn(self): # 确认是否到达了需要进行两个神经网络的参数赋值的代数，是则赋值 if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\ntarget_params_replaced\n&#39;) # 根据当前memory当中的size进行提取，在没有到达memory_size时，根据当前的memory里的数量进行提取，提取的数量都是batchsize if self.memory_counter &gt; self.memory_size: sample_index = np.random.choice(self.memory_size, size=self.batch_size) else: sample_index = np.random.choice(self.memory_counter, size=self.batch_size) #根据sample_index提取出batch batch_memory = self.memory[sample_index, :] q_next, q_eval = self.sess.run( [self.q_next, self.q_eval], feed_dict={ self.s_: batch_memory[:, -self.n_features:], self.s: batch_memory[:, :self.n_features], }) q_target = q_eval.copy() batch_index = np.arange(self.batch_size, dtype=np.int32) #取出eval的每一个行为 eval_act_index = batch_memory[:, self.n_features].astype(int) #取出eval的每一个得分 reward = batch_memory[:, self.n_features + 1] #取出每一行的最大值 q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1) #训练 _, self.cost = self.sess.run([self._train_op, self.loss], feed_dict={self.s: batch_memory[:, :self.n_features], self.q_target: q_target}) self.cost_his.append(self.cost) #如果存在epsilon_increment则改变epsilon的值 self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max self.learn_step_counter += 1 def plot_cost(self): import matplotlib.pyplot as plt plt.plot(np.arange(len(self.cost_his)), self.cost_his) plt.ylabel(&#39;Cost&#39;) plt.xlabel(&#39;training steps&#39;) plt.show() 第三部分为运行环境： &quot;&quot;&quot; Reinforcement learning maze example. Red rectangle: explorer. Black rectangles: hells [reward = -1]. Yellow bin circle: paradise [reward = +1]. All other states: ground [reward = 0]. This script is the environment part of this example. The RL is in RL_brain.py. View more on my tutorial page: https://morvanzhou.github.io/tutorials/ &quot;&quot;&quot; import numpy as np import time import sys if sys.version_info.major == 2: import Tkinter as tk else: import tkinter as tk UNIT = 40 # pixels MAZE_H = 4 # grid height MAZE_W = 4 # grid width class Maze(tk.Tk, object): def __init__(self): super(Maze, self).__init__() self.action_space = [&#39;u&#39;, &#39;d&#39;, &#39;l&#39;, &#39;r&#39;] self.n_actions = len(self.action_space) self.n_features = 2 self.title(&#39;maze&#39;) self.geometry(&#39;{0}x{1}&#39;.format(MAZE_H * UNIT, MAZE_H * UNIT)) self._build_maze() def _build_maze(self): self.canvas = tk.Canvas(self, bg=&#39;white&#39;, height=MAZE_H * UNIT, width=MAZE_W * UNIT) # create grids for c in range(0, MAZE_W * UNIT, UNIT): x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT self.canvas.create_line(x0, y0, x1, y1) for r in range(0, MAZE_H * UNIT, UNIT): x0, y0, x1, y1 = 0, r, MAZE_W * UNIT, r self.canvas.create_line(x0, y0, x1, y1) # create origin origin = np.array([20, 20]) # hell hell1_center = origin + np.array([UNIT * 2, UNIT]) self.hell1 = self.canvas.create_rectangle( hell1_center[0] - 15, hell1_center[1] - 15, hell1_center[0] + 15, hell1_center[1] + 15, fill=&#39;black&#39;) # hell # hell2_center = origin + np.array([UNIT, UNIT * 2]) # self.hell2 = self.canvas.create_rectangle( # hell2_center[0] - 15, hell2_center[1] - 15, # hell2_center[0] + 15, hell2_center[1] + 15, # fill=&#39;black&#39;) # create oval oval_center = origin + UNIT * 2 self.oval = self.canvas.create_oval( oval_center[0] - 15, oval_center[1] - 15, oval_center[0] + 15, oval_center[1] + 15, fill=&#39;yellow&#39;) # create red rect self.rect = self.canvas.create_rectangle( origin[0] - 15, origin[1] - 15, origin[0] + 15, origin[1] + 15, fill=&#39;red&#39;) # pack all self.canvas.pack() def reset(self): self.update() time.sleep(0.1) self.canvas.delete(self.rect) origin = np.array([20, 20]) self.rect = self.canvas.create_rectangle( origin[0] - 15, origin[1] - 15, origin[0] + 15, origin[1] + 15, fill=&#39;red&#39;) # return observation return (np.array(self.canvas.coords(self.rect)[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT) def step(self, action): s = self.canvas.coords(self.rect) base_action = np.array([0, 0]) if action == 0: # up if s[1] &gt; UNIT: base_action[1] -= UNIT elif action == 1: # down if s[1] &lt; (MAZE_H - 1) * UNIT: base_action[1] += UNIT elif action == 2: # right if s[0] &lt; (MAZE_W - 1) * UNIT: base_action[0] += UNIT elif action == 3: # left if s[0] &gt; UNIT: base_action[0] -= UNIT self.canvas.move(self.rect, base_action[0], base_action[1]) # move agent next_coords = self.canvas.coords(self.rect) # next state # reward function if next_coords == self.canvas.coords(self.oval): reward = 1 done = True elif next_coords in [self.canvas.coords(self.hell1)]: reward = -1 done = True else: reward = 0 done = False s_ = (np.array(next_coords[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT) return s_, reward, done def render(self): # time.sleep(0.01) self.update() 除了环境部分外，主函数部分和DQN大脑部分我都进行了详细的备注，由于涉及到模块间的调用，需要设置文件名称，三个文件的名称分别为：run_this.py，RL_brain.py，maze_env.py。 由于代码并不是自己写的，所以就不上传github了，不过还是欢迎大家关注我和我的github https://github.com/bubbliiiing/ 希望得到朋友们的喜欢。 有不懂的朋友可以评论询问噢。" />
<link rel="canonical" href="https://uzzz.org/2019/07/18/793805.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/18/793805.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-18T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"快乐的强化学习2——DQN及其实现方法 学习前言 简介 DQN算法的实现 具体实现代码 学习前言 刚刚从大学毕业，近来闲来无事，开始了机器学习的旅程，深度学习是机器学习的重要一环，其可以使得机器自我尝试，并通过结果进行学习。 在机器学习的过程中，我自网上了解到大神morvanzhou，一个从土木工程转向了计算机的“聪明绝顶”的、英语特好的男人。本篇文章便是按照他的深度学习教程中的Qlearning部分撰写的。 morvanzhou的python个人主页，请有兴趣的同学关注大神morvanzhou的python教程。 简介 DQN是一种结合了深度学习与强化学习的一种学习方法，它以强化学习Q-Learning为基础，结合tensorflow的神经网络。强化学习Q-Learning是一种非常优秀的算法，但是由于现在许多问题的环境非常复杂，如果把每一种环境都单独列出来，整个Q表会非常大，且更新的时候检索时间较长。所以如果可以不建立Q表，只通过每个环境的特点就可以得出整个环境的Q-Value，那么Q表庞大冗杂的问题就迎刃而解了。 其与常规的强化学习Q-Learning最大的不同就是，DQN在初始化的时候不再生成一个完整的Q-Table，每一个观测环境的Q值都是通过神经网络生成的，即通过输入当前环境的特征Features来得到当前环境每个动作的Q-Value，并且以这个Q-Value基准进行动作选择。 DQN一共具有两个神经网络，一个是用于计算q_predict，一个是用于计算q_next的。用于计算q_predict的神经网络，我们简称其为Net_Pre；用于计算q_next我们简称其为Net_Next。 DQN的更新准则与Q-Learning的更新准则类似，都是通过所处的当前环境对各个动作的预测得分，下一步的环境的实际情况进行得分更新的，但是DQN更新的不再是Q表，而是通过所处的当前环境对各个动作的预测得分和下一步的环境的实际情况二者的误差更新Net_Pre和Net_Next的参数。 如果大家对Q-Learning还有疑惑，请大家关注我的另一篇博文https://blog.csdn.net/weixin_44791964/article/details/95410737 DQN算法的实现 接下来我将以小男孩取得玩具为例子，讲述DQN算法的执行过程。 在一开始的时候假设小男孩不知道玩具在哪里，Net_Pre和Net_Next都是随机生成的，在进行第一步动作前，小男孩通过Net_Pre获得其对于当前环境每一个动作的得分的预测值，并在其中优先选择得分最高的一个动作作为下一步的行为Action，并且通过该Action得到下一步的环境。 # 刷新环境 env.render() # DQN 根据观测值选择行为 action = RL.choose_action(observation) # 环境根据行为给出下一个 state, reward, 是否终止 observation_, reward, done = env.step(action) 在获得所处的当前环境对各个动作的预测得分和。下一步的环境的实际情况的时候，常规的Q-Learning便可以通过： self.q_table.loc[observation_now, action] += self.lr * (q_target - q_predict) 进行Q-Table更新，但是DQN不一样，DQN会将这一步按照一定的格式存入记忆，当形成一定规模的时候再进行Net_Pre的更新。在python中，其表示为： # 控制学习起始时间和频率 (先累积一些记忆再开始学习) if (step &gt; 200) and (step % 5 == 0): RL.learn() 可能同学们会很奇怪，为什么是进行Net_Pre的更新而不是进行Net_Pre和Net_Next的更新，其实是因为通过Net_Pre得到的Q_predict的是人物对当前环境的认知，而通过Net_Next通过一定处理之后获得的Q_target是下个环境的实际得分，我们只能通过下个环境的实际得分来更新人物对当前环境的认知。 这是否意味着Net_Next不需要更新呢？答案是否定的，因为当前的Net_Next都是随机生成的，其并没有考虑地图上的一个重要信息，就是每个环境的reward。当小男孩掉下悬崖的时候，他的得分是-1；当小男孩拿到玩具的时候，他的得分是1。因此，每当进行一定次数的Net_Pre的更新后，就要将最新的Net_Pre的参数赋值给Net_Next。 在python中，其表示为： if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\\ntarget_params_replaced\\n&#39;) 那么Net_Pre的参数又是如何更新的呢，通过Net_Pre得到的Q_predict的是人物对当前环境的认知，而通过Net_Next通过一定处理之后获得的Q_target是下个环境的实际得分，那么Q_target-Q_predict得到便是实际与预测之间的差距，我们将其作为cost传入tensorflow，便可以使用一定的优化器缩小cost，实现Net_Pre神经网络的训练。接下来让我们看看学习过程是如何实现的。 def learn(self): # 确认是否到达了需要进行两个神经网络的参数赋值的代数，是则赋值 if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\\ntarget_params_replaced\\n&#39;) # 根据当前memory当中的size进行提取，在没有到达memory_size时，根据当前的memory里的数量进行提取，提取的数量都是batchsize if self.memory_counter &gt; self.memory_size: sample_index = np.random.choice(self.memory_size, size=self.batch_size) else: sample_index = np.random.choice(self.memory_counter, size=self.batch_size) #根据sample_index提取出batch batch_memory = self.memory[sample_index, :] #通过神经网络获得q_next和q_eval。 q_next, q_eval = self.sess.run( [self.q_next, self.q_eval], feed_dict={ self.s_: batch_memory[:, -self.n_features:], self.s: batch_memory[:, :self.n_features], }) #该步主要是为了获得q_next, q_eval的格式 q_target = q_eval.copy() batch_index = np.arange(self.batch_size, dtype=np.int32) #取出eval的每一个行为 eval_act_index = batch_memory[:, self.n_features].astype(int) #取出eval的每一个得分 reward = batch_memory[:, self.n_features + 1] #取出每一行的最大值、取出每个环境对应的最大得分 q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1) #利用q_target和q_predict训练 _, self.cost = self.sess.run([self._train_op, self.loss], feed_dict={self.s: batch_memory[:, :self.n_features], self.q_target: q_target}) self.cost_his.append(self.cost) #如果存在epsilon_increment则改变epsilon的值 self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max self.learn_step_counter += 1 可能大家看完这个还是一头雾水，现在，我将以伪代码的方式，将整个DQN执行一遍给大家看一下。 初始化测试环境对象 初始化DQN的大脑对象 step = 0 for episode in range(TIMES): 初始化环境 while(1): #刷新环境 env.render() #根据观测值选择行为 action = RL.choose_action(observation) #获得环境下一步行为，判断下一步环境是否终止, 获得下一步环境的得分。 observation_, reward, done = env.step(action) #调用存储函数存储记忆 RL.store_transition(observation, action, reward, observation_) #先累积一些记忆再开始学习 if (step &gt; 200) and (step % 5 == 0): RL.learn() #将下一个 state_ 变为 下次循环的 state #如果终止, 就跳出循环 if done: break step += 1 # 总步数 在整个函数的执行过程中DQN的大脑包括以下部分，其对应的功能为 模块名称 作用/功能 初始化 初始化学习率、可执行动作、全局学习步数、记忆库大小、每次训练的batch大小、两个神经网络等参数 动作选择 根据当前所处的环境特征和Net_Pre获得当前环境各个动作的Q_Value，并进行动作选择 学习 通过Net_Next获得q_next，再通过一定运算得到q_target，根据q_predict和q_target进行网络更新 记忆存储 按照一定格式存储 当前环境特点、下一步环境特点、得分reward、当前环境的动作。 具体实现代码 具体的实现代码分为三个部分，这是第一部分，主函数： from maze_env import Maze from RL_brain import DeepQNetwork def run_maze(): step = 0 # 用来控制什么时候学习 for episode in range(300): # 初始化环境 observation = env.reset() while True: # 刷新环境 env.render() # DQN 根据观测值选择行为 action = RL.choose_action(observation) # 环境根据行为给出下一个 state, reward, 是否终止 observation_, reward, done = env.step(action) # DQN 存储记忆 RL.store_transition(observation, action, reward, observation_) # 控制学习起始时间和频率 (先累积一些记忆再开始学习) if (step &gt; 200) and (step % 5 == 0): RL.learn() # 将下一个 state_ 变为 下次循环的 state observation = observation_ # 如果终止, 就跳出循环 if done: break step += 1 # 总步数 # end of game print(&#39;game over&#39;) env.destroy() if __name__ == &quot;__main__&quot;: env = Maze() RL = DeepQNetwork(env.n_actions, env.n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=200, # 每 200 步替换一次 target_net 的参数 memory_size=2000, # 记忆上限 # output_graph=True # 是否输出 tensorboard 文件 ) env.after(100, run_maze) env.mainloop() RL.plot_cost() # 观看神经网络的误差曲线 第二部分是DQN的大脑： import numpy as np import pandas as pd import tensorflow as tf np.random.seed(1) tf.set_random_seed(1) # Deep Q Network off-policy class DeepQNetwork: def __init__( self, n_actions, n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=300, memory_size=500, batch_size=32, e_greedy_increment=None, output_graph=False, ): self.n_actions = n_actions #动作数量 self.n_features = n_features #观测环境的特征数量 self.lr = learning_rate #学习率 self.gamma = reward_decay self.epsilon_max = e_greedy #处于epsilon范围内时，选择value值最大的动作 self.replace_target_iter = replace_target_iter #进行参数替换的轮替代数 self.memory_size = memory_size #每个memory的size self.batch_size = batch_size #训练batch的size self.epsilon_increment = e_greedy_increment self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max self.learn_step_counter = 0 # 初始化全局学习步数 # 初始化用于存储memory的地方 [s, a, r, s_] self.memory = np.zeros((self.memory_size, n_features * 2 + 2)) # 利用get_collection获得eval和target两个神经网络的参数 self._build_net() t_params = tf.get_collection(&#39;target_net_params&#39;) e_params = tf.get_collection(&#39;eval_net_params&#39;) # 将e的参数赋予t self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)] self.sess = tf.Session() #如果output_graph==True则在tensorboard上输出结构 if output_graph: tf.summary.FileWriter(&quot;logs/&quot;, self.sess.graph) self.sess.run(tf.global_variables_initializer()) #用于存储历史cost值 self.cost_his = [] def _build_net(self): # ------------------ build evaluate_net ------------------ self.s = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s&#39;) #输入值，列的内容为观测到的环境的特点，一共有n个，用于计算对当前环境的估计 self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name=&#39;Q_target&#39;) #该层神经网络用于计算q_eval with tf.variable_scope(&#39;eval_net&#39;): # 神经网络的参数 c_names =[&#39;eval_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES] #collection的名字 n_l1 = 10 #隐含层神经元的数量 w_initializer = tf.random_normal_initializer(mean = 0., stddev = 0.3) #初始化正态分布生成器 b_initializer = tf.constant_initializer(0.1) #常数生成器 # q_eval神经网络的第一层 with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1) # q_eval神经网络的第二层 with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_eval = tf.matmul(l1, w2) + b2 with tf.variable_scope(&#39;loss&#39;): # q_eval神经网络的损失值，其将q_eval的损失值和q_target对比 self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval)) with tf.variable_scope(&#39;train&#39;): # 利用RMSPropOptimizer进行训练 self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss) # ------------------ build target_net ------------------ self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s_&#39;) # input #该层神经网络用于计算q_target with tf.variable_scope(&#39;target_net&#39;): #神经网络的参数 c_names = [&#39;target_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES] #q_target第一层 with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1) #q_target第二层 with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_next = tf.matmul(l1, w2) + b2 def store_transition(self, s, a, r, s_): #判断是否有memory_counter属性。没有则加入。该部分用于存储当前状态，action，得分，预测状态。 if not hasattr(self, &#39;memory_counter&#39;): self.memory_counter = 0 #将当前状态，action，得分，预测状态进行水平堆叠 transition = np.hstack((s, [a, r], s_)) #index保证在memory_size以内 index = self.memory_counter % self.memory_size #self.memory中加入一行 self.memory[index, :] = transition #数量加1 self.memory_counter += 1 def choose_action(self, observation): #变成可以输入给神经网络的行向量 observation = observation[np.newaxis, :] if np.random.uniform() &lt; self.epsilon: #如果在epsilon内，通过运算得出value最大的action，否则随机生成action actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation}) action = np.argmax(actions_value) else: action = np.random.randint(0, self.n_actions) return action def learn(self): # 确认是否到达了需要进行两个神经网络的参数赋值的代数，是则赋值 if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\\ntarget_params_replaced\\n&#39;) # 根据当前memory当中的size进行提取，在没有到达memory_size时，根据当前的memory里的数量进行提取，提取的数量都是batchsize if self.memory_counter &gt; self.memory_size: sample_index = np.random.choice(self.memory_size, size=self.batch_size) else: sample_index = np.random.choice(self.memory_counter, size=self.batch_size) #根据sample_index提取出batch batch_memory = self.memory[sample_index, :] q_next, q_eval = self.sess.run( [self.q_next, self.q_eval], feed_dict={ self.s_: batch_memory[:, -self.n_features:], self.s: batch_memory[:, :self.n_features], }) q_target = q_eval.copy() batch_index = np.arange(self.batch_size, dtype=np.int32) #取出eval的每一个行为 eval_act_index = batch_memory[:, self.n_features].astype(int) #取出eval的每一个得分 reward = batch_memory[:, self.n_features + 1] #取出每一行的最大值 q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1) #训练 _, self.cost = self.sess.run([self._train_op, self.loss], feed_dict={self.s: batch_memory[:, :self.n_features], self.q_target: q_target}) self.cost_his.append(self.cost) #如果存在epsilon_increment则改变epsilon的值 self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max self.learn_step_counter += 1 def plot_cost(self): import matplotlib.pyplot as plt plt.plot(np.arange(len(self.cost_his)), self.cost_his) plt.ylabel(&#39;Cost&#39;) plt.xlabel(&#39;training steps&#39;) plt.show() 第三部分为运行环境： &quot;&quot;&quot; Reinforcement learning maze example. Red rectangle: explorer. Black rectangles: hells [reward = -1]. Yellow bin circle: paradise [reward = +1]. All other states: ground [reward = 0]. This script is the environment part of this example. The RL is in RL_brain.py. View more on my tutorial page: https://morvanzhou.github.io/tutorials/ &quot;&quot;&quot; import numpy as np import time import sys if sys.version_info.major == 2: import Tkinter as tk else: import tkinter as tk UNIT = 40 # pixels MAZE_H = 4 # grid height MAZE_W = 4 # grid width class Maze(tk.Tk, object): def __init__(self): super(Maze, self).__init__() self.action_space = [&#39;u&#39;, &#39;d&#39;, &#39;l&#39;, &#39;r&#39;] self.n_actions = len(self.action_space) self.n_features = 2 self.title(&#39;maze&#39;) self.geometry(&#39;{0}x{1}&#39;.format(MAZE_H * UNIT, MAZE_H * UNIT)) self._build_maze() def _build_maze(self): self.canvas = tk.Canvas(self, bg=&#39;white&#39;, height=MAZE_H * UNIT, width=MAZE_W * UNIT) # create grids for c in range(0, MAZE_W * UNIT, UNIT): x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT self.canvas.create_line(x0, y0, x1, y1) for r in range(0, MAZE_H * UNIT, UNIT): x0, y0, x1, y1 = 0, r, MAZE_W * UNIT, r self.canvas.create_line(x0, y0, x1, y1) # create origin origin = np.array([20, 20]) # hell hell1_center = origin + np.array([UNIT * 2, UNIT]) self.hell1 = self.canvas.create_rectangle( hell1_center[0] - 15, hell1_center[1] - 15, hell1_center[0] + 15, hell1_center[1] + 15, fill=&#39;black&#39;) # hell # hell2_center = origin + np.array([UNIT, UNIT * 2]) # self.hell2 = self.canvas.create_rectangle( # hell2_center[0] - 15, hell2_center[1] - 15, # hell2_center[0] + 15, hell2_center[1] + 15, # fill=&#39;black&#39;) # create oval oval_center = origin + UNIT * 2 self.oval = self.canvas.create_oval( oval_center[0] - 15, oval_center[1] - 15, oval_center[0] + 15, oval_center[1] + 15, fill=&#39;yellow&#39;) # create red rect self.rect = self.canvas.create_rectangle( origin[0] - 15, origin[1] - 15, origin[0] + 15, origin[1] + 15, fill=&#39;red&#39;) # pack all self.canvas.pack() def reset(self): self.update() time.sleep(0.1) self.canvas.delete(self.rect) origin = np.array([20, 20]) self.rect = self.canvas.create_rectangle( origin[0] - 15, origin[1] - 15, origin[0] + 15, origin[1] + 15, fill=&#39;red&#39;) # return observation return (np.array(self.canvas.coords(self.rect)[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT) def step(self, action): s = self.canvas.coords(self.rect) base_action = np.array([0, 0]) if action == 0: # up if s[1] &gt; UNIT: base_action[1] -= UNIT elif action == 1: # down if s[1] &lt; (MAZE_H - 1) * UNIT: base_action[1] += UNIT elif action == 2: # right if s[0] &lt; (MAZE_W - 1) * UNIT: base_action[0] += UNIT elif action == 3: # left if s[0] &gt; UNIT: base_action[0] -= UNIT self.canvas.move(self.rect, base_action[0], base_action[1]) # move agent next_coords = self.canvas.coords(self.rect) # next state # reward function if next_coords == self.canvas.coords(self.oval): reward = 1 done = True elif next_coords in [self.canvas.coords(self.hell1)]: reward = -1 done = True else: reward = 0 done = False s_ = (np.array(next_coords[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT) return s_, reward, done def render(self): # time.sleep(0.01) self.update() 除了环境部分外，主函数部分和DQN大脑部分我都进行了详细的备注，由于涉及到模块间的调用，需要设置文件名称，三个文件的名称分别为：run_this.py，RL_brain.py，maze_env.py。 由于代码并不是自己写的，所以就不上传github了，不过还是欢迎大家关注我和我的github https://github.com/bubbliiiing/ 希望得到朋友们的喜欢。 有不懂的朋友可以评论询问噢。","@type":"BlogPosting","url":"https://uzzz.org/2019/07/18/793805.html","headline":"快乐的强化学习2——DQN及其实现方法","dateModified":"2019-07-18T00:00:00+08:00","datePublished":"2019-07-18T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/18/793805.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>快乐的强化学习2——DQN及其实现方法</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-tomorrow-night-eighties"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>快乐的强化学习2——DQN及其实现方法</h3>
   <ul>
    <li><a href="#_3" rel="nofollow" data-token="7dd3319ba2e7cd2a473917a328bff8f2">学习前言</a></li>
    <ul>
     <li><a href="#_10" rel="nofollow" data-token="0b55d8b03094c4658c48197d60e69875">简介</a></li>
     <li><a href="#DQN_17" rel="nofollow" data-token="afeebe2067c576f5b1f42f8f7bdf07c4">DQN算法的实现</a></li>
     <li><a href="#_127" rel="nofollow" data-token="bf029c9a6caca3d397e6ee01feb5959a">具体实现代码</a></li>
    </ul>
   </ul>
  </div>
  <p></p> 
  <h1><a id="_3"></a>学习前言</h1> 
  <p>刚刚从大学毕业，近来闲来无事，开始了机器学习的旅程，深度学习是机器学习的重要一环，其可以使得机器自我尝试，并通过结果进行学习。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190723165901974.jpg#pic_center" alt="在这里插入图片描述"><br> 在机器学习的过程中，我自网上了解到大神morvanzhou，一个从土木工程转向了计算机的“聪明绝顶”的、英语特好的男人。本篇文章便是按照他的深度学习教程中的Qlearning部分撰写的。<br> <a href="https://morvanzhou.github.io/" rel="nofollow">morvanzhou的python个人主页</a>，请有兴趣的同学关注大神morvanzhou的python教程。</p> 
  <h2><a id="_10"></a>简介</h2> 
  <p>DQN是一种结合了深度学习与强化学习的一种学习方法，它以强化学习Q-Learning为基础，结合tensorflow的神经网络。强化学习Q-Learning是一种非常优秀的算法，但是由于现在许多问题的环境非常复杂，如果把每一种环境都单独列出来，整个Q表会非常大，且更新的时候检索时间较长。<strong>所以如果可以不建立Q表，只通过每个环境的特点就可以得出整个环境的Q-Value，那么Q表庞大冗杂的问题就迎刃而解了。</strong><br> 其与常规的强化学习Q-Learning最大的不同就是，DQN在初始化的时候不再生成一个完整的Q-Table，<strong>每一个观测环境的Q值都是通过神经网络生成的，即通过输入当前环境的特征Features来得到当前环境每个动作的Q-Value，并且以这个Q-Value基准进行动作选择。</strong><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190718095932487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> DQN一共具有两个神经网络，<strong>一个是用于计算q_predict，一个是用于计算q_next的。用于计算q_predict的神经网络，我们简称其为Net_Pre；用于计算q_next我们简称其为Net_Next。</strong><br> DQN的更新准则与Q-Learning的更新准则类似，都是通过<strong>所处的当前环境对各个动作的预测得分</strong>，<strong>下一步的环境的实际情况</strong>进行得分更新的，但是DQN更新的不再是Q表，而是通过<strong>所处的当前环境对各个动作的预测得分</strong>和<strong>下一步的环境的实际情况</strong>二者的误差更新<strong>Net_Pre和Net_Next的参数</strong>。<br> 如果大家对Q-Learning还有疑惑，请大家关注我的另一篇博文https://blog.csdn.net/weixin_44791964/article/details/95410737</p> 
  <h2><a id="DQN_17"></a>DQN算法的实现</h2> 
  <p>接下来我将以小男孩取得玩具为例子，讲述DQN算法的执行过程。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190713104623618.png" alt="小男孩取得玩具"><br> 在一开始的时候假设小男孩不知道玩具在哪里，Net_Pre和Net_Next都是随机生成的，在进行第一步动作前，小男孩<strong>通过Net_Pre获得其对于当前环境每一个动作的得分的预测值</strong>，并在其中优先选择得分最高的一个动作作为下一步的行为Action，并且通过该Action得到下一步的环境。</p> 
  <pre><code class="prism language-python"><span class="token comment"># 刷新环境</span>
env<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># DQN 根据观测值选择行为</span>
action <span class="token operator">=</span> RL<span class="token punctuation">.</span>choose_action<span class="token punctuation">(</span>observation<span class="token punctuation">)</span>
<span class="token comment"># 环境根据行为给出下一个 state, reward, 是否终止</span>
observation_<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
</code></pre> 
  <p>在获得<strong>所处的当前环境对各个动作的预测得分</strong>和。<strong>下一步的环境的实际情况</strong>的时候，常规的Q-Learning便可以通过：</p> 
  <pre><code class="prism language-python">self<span class="token punctuation">.</span>q_table<span class="token punctuation">.</span>loc<span class="token punctuation">[</span>observation_now<span class="token punctuation">,</span> action<span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> <span class="token punctuation">(</span>q_target <span class="token operator">-</span> q_predict<span class="token punctuation">)</span>  
</code></pre> 
  <p>进行Q-Table更新，但是DQN不一样，DQN会将这一步按照一定的格式存入记忆，当形成一定规模的时候再进行Net_Pre的更新。在python中，其表示为：</p> 
  <pre><code class="prism language-python"><span class="token comment"># 控制学习起始时间和频率 (先累积一些记忆再开始学习)</span>
<span class="token keyword">if</span> <span class="token punctuation">(</span>step <span class="token operator">&gt;</span> <span class="token number">200</span><span class="token punctuation">)</span> <span class="token operator">and</span> <span class="token punctuation">(</span>step <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            RL<span class="token punctuation">.</span>learn<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p>可能同学们会很奇怪，<strong>为什么是进行Net_Pre的更新而不是进行Net_Pre和Net_Next的更新</strong>，其实是因为通过Net_Pre得到的Q_predict的是<strong>人物对当前环境的认知</strong>，而通过Net_Next通过一定处理之后获得的Q_target是<strong>下个环境的实际得分</strong>，我们只能通过<strong>下个环境的实际得分</strong>来更新<strong>人物对当前环境的认知</strong>。<br> 这是否意味着<strong>Net_Next</strong>不需要更新呢？答案是否定的，因为当前的<strong>Net_Next</strong>都是随机生成的，其并没有考虑地图上的一个重要信息，就是每个环境的reward。当小男孩掉下悬崖的时候，他的得分是-1；当小男孩拿到玩具的时候，他的得分是1。因此，每当进行一定次数的Net_Pre的更新后，就要将<strong>最新的Net_Pre的参数赋值给Net_Next</strong>。<br> 在python中，其表示为：</p> 
  <pre><code class="prism language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>learn_step_counter <span class="token operator">%</span> self<span class="token punctuation">.</span>replace_target_iter <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>self<span class="token punctuation">.</span>replace_target_op<span class="token punctuation">)</span>
      <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\ntarget_params_replaced\n'</span><span class="token punctuation">)</span>
</code></pre> 
  <p>那么Net_Pre的参数又是如何更新的呢，通过Net_Pre得到的Q_predict的是<strong>人物对当前环境的认知</strong>，而通过Net_Next通过一定处理之后获得的Q_target是<strong>下个环境的实际得分</strong>，那么Q_target-Q_predict得到便是<strong>实际</strong>与<strong>预测</strong>之间的差距，我们将其作为cost传入tensorflow，便可以使用一定的优化器缩小cost，实现Net_Pre神经网络的训练。接下来让我们看看学习过程是如何实现的。</p> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">learn</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 确认是否到达了需要进行两个神经网络的参数赋值的代数，是则赋值</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>learn_step_counter <span class="token operator">%</span> self<span class="token punctuation">.</span>replace_target_iter <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
    	self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>self<span class="token punctuation">.</span>replace_target_op<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\ntarget_params_replaced\n'</span><span class="token punctuation">)</span>
            
    <span class="token comment"># 根据当前memory当中的size进行提取，在没有到达memory_size时，根据当前的memory里的数量进行提取，提取的数量都是batchsize</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>memory_counter <span class="token operator">&gt;</span> self<span class="token punctuation">.</span>memory_size<span class="token punctuation">:</span>
        sample_index <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>self<span class="token punctuation">.</span>memory_size<span class="token punctuation">,</span> size<span class="token operator">=</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>                                           
        sample_index <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>self<span class="token punctuation">.</span>memory_counter<span class="token punctuation">,</span> size<span class="token operator">=</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>
    
    <span class="token comment">#根据sample_index提取出batch</span>
    batch_memory <span class="token operator">=</span> self<span class="token punctuation">.</span>memory<span class="token punctuation">[</span>sample_index<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
	
	<span class="token comment">#通过神经网络获得q_next和q_eval。</span>
    q_next<span class="token punctuation">,</span> q_eval <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>self<span class="token punctuation">.</span>q_next<span class="token punctuation">,</span> self<span class="token punctuation">.</span>q_eval<span class="token punctuation">]</span><span class="token punctuation">,</span>
        feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>
            self<span class="token punctuation">.</span>s_<span class="token punctuation">:</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>n_features<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
            self<span class="token punctuation">.</span>s<span class="token punctuation">:</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>n_features<span class="token punctuation">]</span><span class="token punctuation">,</span> 
        <span class="token punctuation">}</span><span class="token punctuation">)</span>

    <span class="token comment">#该步主要是为了获得q_next, q_eval的格式</span>
    q_target <span class="token operator">=</span> q_eval<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>

    batch_index <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
    
    <span class="token comment">#取出eval的每一个行为</span>
    eval_act_index <span class="token operator">=</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_features<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>
    <span class="token comment">#取出eval的每一个得分</span>
    reward <span class="token operator">=</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_features <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token comment">#取出每一行的最大值、取出每个环境对应的最大得分</span>
    q_target<span class="token punctuation">[</span>batch_index<span class="token punctuation">,</span> eval_act_index<span class="token punctuation">]</span> <span class="token operator">=</span> reward <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>q_next<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token comment">#利用q_target和q_predict训练</span>
    _<span class="token punctuation">,</span> self<span class="token punctuation">.</span>cost <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>_train_op<span class="token punctuation">,</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">]</span><span class="token punctuation">,</span>
                                  feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>self<span class="token punctuation">.</span>s<span class="token punctuation">:</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>n_features<span class="token punctuation">]</span><span class="token punctuation">,</span>
                                  self<span class="token punctuation">.</span>q_target<span class="token punctuation">:</span> q_target<span class="token punctuation">}</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>cost_his<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cost<span class="token punctuation">)</span>

    <span class="token comment">#如果存在epsilon_increment则改变epsilon的值</span>
    self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> self<span class="token punctuation">.</span>epsilon <span class="token operator">+</span> self<span class="token punctuation">.</span>epsilon_increment <span class="token keyword">if</span> self<span class="token punctuation">.</span>epsilon <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>epsilon_max <span class="token keyword">else</span> self<span class="token punctuation">.</span>epsilon_max
    self<span class="token punctuation">.</span>learn_step_counter <span class="token operator">+=</span> <span class="token number">1</span>
</code></pre> 
  <p>可能大家看完这个还是一头雾水，现在，我将以伪代码的方式，将整个DQN执行一遍给大家看一下。</p> 
  <pre><code class="prism language-python">初始化测试环境对象
初始化DQN的大脑对象
step <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>TIMES<span class="token punctuation">)</span><span class="token punctuation">:</span>
    初始化环境
    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    	<span class="token comment">#刷新环境</span>
    	env<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token punctuation">)</span>
		<span class="token comment">#根据观测值选择行为</span>
		action <span class="token operator">=</span> RL<span class="token punctuation">.</span>choose_action<span class="token punctuation">(</span>observation<span class="token punctuation">)</span>
    	<span class="token comment">#获得环境下一步行为，判断下一步环境是否终止, 获得下一步环境的得分。</span>
    	observation_<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
    	<span class="token comment">#调用存储函数存储记忆</span>
    	RL<span class="token punctuation">.</span>store_transition<span class="token punctuation">(</span>observation<span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> observation_<span class="token punctuation">)</span>
    	<span class="token comment">#先累积一些记忆再开始学习</span>
    	<span class="token keyword">if</span> <span class="token punctuation">(</span>step <span class="token operator">&gt;</span> <span class="token number">200</span><span class="token punctuation">)</span> <span class="token operator">and</span> <span class="token punctuation">(</span>step <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        	RL<span class="token punctuation">.</span>learn<span class="token punctuation">(</span><span class="token punctuation">)</span>
    	<span class="token comment">#将下一个 state_ 变为 下次循环的 state</span>
        <span class="token comment">#如果终止, 就跳出循环</span>
        <span class="token keyword">if</span> done<span class="token punctuation">:</span>
        	<span class="token keyword">break</span>
        step <span class="token operator">+=</span> <span class="token number">1</span>   <span class="token comment"># 总步数</span>
</code></pre> 
  <p>在整个函数的执行过程中DQN的大脑包括以下部分，其对应的功能为</p> 
  <table> 
   <thead> 
    <tr> 
     <th align="center">模块名称</th> 
     <th align="center">作用/功能</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td align="center">初始化</td> 
     <td align="center">初始化学习率、可执行动作、全局学习步数、记忆库大小、每次训练的batch大小、两个神经网络等参数</td> 
    </tr> 
    <tr> 
     <td align="center">动作选择</td> 
     <td align="center">根据当前所处的环境特征和Net_Pre获得当前环境各个动作的Q_Value，并进行动作选择</td> 
    </tr> 
    <tr> 
     <td align="center">学习</td> 
     <td align="center">通过Net_Next获得q_next，再通过一定运算得到q_target，根据q_predict和q_target进行网络更新</td> 
    </tr> 
    <tr> 
     <td align="center">记忆存储</td> 
     <td align="center">按照一定格式存储 当前环境特点、下一步环境特点、得分reward、当前环境的动作。</td> 
    </tr> 
   </tbody> 
  </table>
  <h2><a id="_127"></a>具体实现代码</h2> 
  <p>具体的实现代码分为三个部分，这是第一部分，主函数：</p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> maze_env <span class="token keyword">import</span> Maze
<span class="token keyword">from</span> RL_brain <span class="token keyword">import</span> DeepQNetwork
<span class="token keyword">def</span> <span class="token function">run_maze</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    step <span class="token operator">=</span> <span class="token number">0</span>    <span class="token comment"># 用来控制什么时候学习</span>
    <span class="token keyword">for</span> episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 初始化环境</span>
        observation <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            <span class="token comment"># 刷新环境</span>
            env<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># DQN 根据观测值选择行为</span>
            action <span class="token operator">=</span> RL<span class="token punctuation">.</span>choose_action<span class="token punctuation">(</span>observation<span class="token punctuation">)</span>

            <span class="token comment"># 环境根据行为给出下一个 state, reward, 是否终止</span>
            observation_<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>

            <span class="token comment"># DQN 存储记忆</span>
            RL<span class="token punctuation">.</span>store_transition<span class="token punctuation">(</span>observation<span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> observation_<span class="token punctuation">)</span>

            <span class="token comment"># 控制学习起始时间和频率 (先累积一些记忆再开始学习)</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>step <span class="token operator">&gt;</span> <span class="token number">200</span><span class="token punctuation">)</span> <span class="token operator">and</span> <span class="token punctuation">(</span>step <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                RL<span class="token punctuation">.</span>learn<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># 将下一个 state_ 变为 下次循环的 state</span>
            observation <span class="token operator">=</span> observation_

            <span class="token comment"># 如果终止, 就跳出循环</span>
            <span class="token keyword">if</span> done<span class="token punctuation">:</span>
                <span class="token keyword">break</span>
            step <span class="token operator">+=</span> <span class="token number">1</span>   <span class="token comment"># 总步数</span>

    <span class="token comment"># end of game</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'game over'</span><span class="token punctuation">)</span>
    env<span class="token punctuation">.</span>destroy<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    env <span class="token operator">=</span> Maze<span class="token punctuation">(</span><span class="token punctuation">)</span>
    RL <span class="token operator">=</span> DeepQNetwork<span class="token punctuation">(</span>env<span class="token punctuation">.</span>n_actions<span class="token punctuation">,</span> env<span class="token punctuation">.</span>n_features<span class="token punctuation">,</span>
                      learning_rate<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
                      reward_decay<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span>
                      e_greedy<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span>
                      replace_target_iter<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>  <span class="token comment"># 每 200 步替换一次 target_net 的参数</span>
                      memory_size<span class="token operator">=</span><span class="token number">2000</span><span class="token punctuation">,</span> <span class="token comment"># 记忆上限</span>
                      <span class="token comment"># output_graph=True # 是否输出 tensorboard 文件</span>
                      <span class="token punctuation">)</span>
    env<span class="token punctuation">.</span>after<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> run_maze<span class="token punctuation">)</span>
    env<span class="token punctuation">.</span>mainloop<span class="token punctuation">(</span><span class="token punctuation">)</span>
    RL<span class="token punctuation">.</span>plot_cost<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 观看神经网络的误差曲线</span>
</code></pre> 
  <p>第二部分是DQN的大脑：</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
tf<span class="token punctuation">.</span>set_random_seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>


<span class="token comment"># Deep Q Network off-policy</span>
<span class="token keyword">class</span> <span class="token class-name">DeepQNetwork</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span>
            n_actions<span class="token punctuation">,</span>
            n_features<span class="token punctuation">,</span>
            learning_rate<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
            reward_decay<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span>
            e_greedy<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span>
            replace_target_iter<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span>
            memory_size<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>
            batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>
            e_greedy_increment<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            output_graph<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>n_actions <span class="token operator">=</span> n_actions      <span class="token comment">#动作数量</span>
        self<span class="token punctuation">.</span>n_features <span class="token operator">=</span> n_features    <span class="token comment">#观测环境的特征数量</span>
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> learning_rate         <span class="token comment">#学习率</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> reward_decay       
        self<span class="token punctuation">.</span>epsilon_max <span class="token operator">=</span> e_greedy     <span class="token comment">#处于epsilon范围内时，选择value值最大的动作</span>
        self<span class="token punctuation">.</span>replace_target_iter <span class="token operator">=</span> replace_target_iter  <span class="token comment">#进行参数替换的轮替代数</span>
        self<span class="token punctuation">.</span>memory_size <span class="token operator">=</span> memory_size  <span class="token comment">#每个memory的size</span>
        self<span class="token punctuation">.</span>batch_size <span class="token operator">=</span> batch_size    <span class="token comment">#训练batch的size</span>
        self<span class="token punctuation">.</span>epsilon_increment <span class="token operator">=</span> e_greedy_increment
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> <span class="token number">0</span> <span class="token keyword">if</span> e_greedy_increment <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>epsilon_max
        
        self<span class="token punctuation">.</span>learn_step_counter <span class="token operator">=</span> <span class="token number">0</span>     <span class="token comment"># 初始化全局学习步数</span>

        <span class="token comment"># 初始化用于存储memory的地方 [s, a, r, s_]</span>
        self<span class="token punctuation">.</span>memory <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>memory_size<span class="token punctuation">,</span> n_features <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 利用get_collection获得eval和target两个神经网络的参数</span>
        self<span class="token punctuation">.</span>_build_net<span class="token punctuation">(</span><span class="token punctuation">)</span>
        t_params <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span><span class="token string">'target_net_params'</span><span class="token punctuation">)</span>
        e_params <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span><span class="token string">'eval_net_params'</span><span class="token punctuation">)</span>
        <span class="token comment"># 将e的参数赋予t</span>
        self<span class="token punctuation">.</span>replace_target_op <span class="token operator">=</span> <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>t<span class="token punctuation">,</span> e<span class="token punctuation">)</span> <span class="token keyword">for</span> t<span class="token punctuation">,</span> e <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>t_params<span class="token punctuation">,</span> e_params<span class="token punctuation">)</span><span class="token punctuation">]</span>

        self<span class="token punctuation">.</span>sess <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment">#如果output_graph==True则在tensorboard上输出结构</span>
        <span class="token keyword">if</span> output_graph<span class="token punctuation">:</span> 
            tf<span class="token punctuation">.</span>summary<span class="token punctuation">.</span>FileWriter<span class="token punctuation">(</span><span class="token string">"logs/"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>graph<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#用于存储历史cost值</span>
        self<span class="token punctuation">.</span>cost_his <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">_build_net</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># ------------------ build evaluate_net ------------------</span>
        self<span class="token punctuation">.</span>s <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_features<span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'s'</span><span class="token punctuation">)</span>  
        <span class="token comment">#输入值，列的内容为观测到的环境的特点，一共有n个，用于计算对当前环境的估计</span>
        self<span class="token punctuation">.</span>q_target <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_actions<span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'Q_target'</span><span class="token punctuation">)</span>
        <span class="token comment">#该层神经网络用于计算q_eval</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'eval_net'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 神经网络的参数</span>
            c_names <span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'eval_net_params'</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>GLOBAL_VARIABLES<span class="token punctuation">]</span> <span class="token comment">#collection的名字</span>
            n_l1 <span class="token operator">=</span> <span class="token number">10</span>   <span class="token comment">#隐含层神经元的数量</span>
            w_initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal_initializer<span class="token punctuation">(</span>mean <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> stddev <span class="token operator">=</span> <span class="token number">0.3</span><span class="token punctuation">)</span>   <span class="token comment">#初始化正态分布生成器</span>
            b_initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span>    <span class="token comment">#常数生成器</span>

            <span class="token comment"># q_eval神经网络的第一层</span>
            <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'l1'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                w1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'w1'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>n_features<span class="token punctuation">,</span> n_l1<span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>w_initializer<span class="token punctuation">,</span> collections<span class="token operator">=</span>c_names<span class="token punctuation">)</span>
                b1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'b1'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> n_l1<span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>b_initializer<span class="token punctuation">,</span> collections<span class="token operator">=</span>c_names<span class="token punctuation">)</span>
                l1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>s<span class="token punctuation">,</span> w1<span class="token punctuation">)</span> <span class="token operator">+</span> b1<span class="token punctuation">)</span>

            <span class="token comment"># q_eval神经网络的第二层</span>
            <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'l2'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                w2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'w2'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>n_l1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_actions<span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>w_initializer<span class="token punctuation">,</span> collections<span class="token operator">=</span>c_names<span class="token punctuation">)</span>
                b2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'b2'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_actions<span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>b_initializer<span class="token punctuation">,</span> collections<span class="token operator">=</span>c_names<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>q_eval <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>l1<span class="token punctuation">,</span> w2<span class="token punctuation">)</span> <span class="token operator">+</span> b2

        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'loss'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment"># q_eval神经网络的损失值，其将q_eval的损失值和q_target对比</span>
            self<span class="token punctuation">.</span>loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>squared_difference<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_target<span class="token punctuation">,</span> self<span class="token punctuation">.</span>q_eval<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 利用RMSPropOptimizer进行训练</span>
            self<span class="token punctuation">.</span>_train_op <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>RMSPropOptimizer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lr<span class="token punctuation">)</span><span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>self<span class="token punctuation">.</span>loss<span class="token punctuation">)</span>

        <span class="token comment"># ------------------ build target_net ------------------</span>
        self<span class="token punctuation">.</span>s_ <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_features<span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'s_'</span><span class="token punctuation">)</span>    <span class="token comment"># input</span>
        <span class="token comment">#该层神经网络用于计算q_target</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'target_net'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment">#神经网络的参数</span>
            c_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'target_net_params'</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>GLOBAL_VARIABLES<span class="token punctuation">]</span>

            <span class="token comment">#q_target第一层</span>
            <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'l1'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                w1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'w1'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>n_features<span class="token punctuation">,</span> n_l1<span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>w_initializer<span class="token punctuation">,</span> collections<span class="token operator">=</span>c_names<span class="token punctuation">)</span>
                b1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'b1'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> n_l1<span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>b_initializer<span class="token punctuation">,</span> collections<span class="token operator">=</span>c_names<span class="token punctuation">)</span>
                l1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>s_<span class="token punctuation">,</span> w1<span class="token punctuation">)</span> <span class="token operator">+</span> b1<span class="token punctuation">)</span>

            <span class="token comment">#q_target第二层</span>
            <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">'l2'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                w2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'w2'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>n_l1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_actions<span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>w_initializer<span class="token punctuation">,</span> collections<span class="token operator">=</span>c_names<span class="token punctuation">)</span>
                b2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'b2'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_actions<span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>b_initializer<span class="token punctuation">,</span> collections<span class="token operator">=</span>c_names<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>q_next <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>l1<span class="token punctuation">,</span> w2<span class="token punctuation">)</span> <span class="token operator">+</span> b2

    <span class="token keyword">def</span> <span class="token function">store_transition</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> s<span class="token punctuation">,</span> a<span class="token punctuation">,</span> r<span class="token punctuation">,</span> s_<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#判断是否有memory_counter属性。没有则加入。该部分用于存储当前状态，action，得分，预测状态。</span>
        <span class="token keyword">if</span> <span class="token operator">not</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">'memory_counter'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>memory_counter <span class="token operator">=</span> <span class="token number">0</span>               
        <span class="token comment">#将当前状态，action，得分，预测状态进行水平堆叠</span>
        transition <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> <span class="token punctuation">[</span>a<span class="token punctuation">,</span> r<span class="token punctuation">]</span><span class="token punctuation">,</span> s_<span class="token punctuation">)</span><span class="token punctuation">)</span>   
        <span class="token comment">#index保证在memory_size以内</span>
        index <span class="token operator">=</span> self<span class="token punctuation">.</span>memory_counter <span class="token operator">%</span> self<span class="token punctuation">.</span>memory_size 
        <span class="token comment">#self.memory中加入一行</span>
        self<span class="token punctuation">.</span>memory<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> transition      
        <span class="token comment">#数量加1</span>
        self<span class="token punctuation">.</span>memory_counter <span class="token operator">+=</span> <span class="token number">1</span>                

    <span class="token keyword">def</span> <span class="token function">choose_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> observation<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#变成可以输入给神经网络的行向量</span>
        observation <span class="token operator">=</span> observation<span class="token punctuation">[</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>    

        <span class="token keyword">if</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>epsilon<span class="token punctuation">:</span>      
        <span class="token comment">#如果在epsilon内，通过运算得出value最大的action，否则随机生成action</span>
            actions_value <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_eval<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>self<span class="token punctuation">.</span>s<span class="token punctuation">:</span> observation<span class="token punctuation">}</span><span class="token punctuation">)</span>
            action <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>actions_value<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            action <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_actions<span class="token punctuation">)</span>   
        <span class="token keyword">return</span> action

    <span class="token keyword">def</span> <span class="token function">learn</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 确认是否到达了需要进行两个神经网络的参数赋值的代数，是则赋值</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>learn_step_counter <span class="token operator">%</span> self<span class="token punctuation">.</span>replace_target_iter <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>self<span class="token punctuation">.</span>replace_target_op<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\ntarget_params_replaced\n'</span><span class="token punctuation">)</span>

        <span class="token comment"># 根据当前memory当中的size进行提取，在没有到达memory_size时，根据当前的memory里的数量进行提取，提取的数量都是batchsize</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>memory_counter <span class="token operator">&gt;</span> self<span class="token punctuation">.</span>memory_size<span class="token punctuation">:</span>
            sample_index <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>self<span class="token punctuation">.</span>memory_size<span class="token punctuation">,</span> size<span class="token operator">=</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>                                           
            sample_index <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>self<span class="token punctuation">.</span>memory_counter<span class="token punctuation">,</span> size<span class="token operator">=</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>
        <span class="token comment">#根据sample_index提取出batch</span>
        batch_memory <span class="token operator">=</span> self<span class="token punctuation">.</span>memory<span class="token punctuation">[</span>sample_index<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

        q_next<span class="token punctuation">,</span> q_eval <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>self<span class="token punctuation">.</span>q_next<span class="token punctuation">,</span> self<span class="token punctuation">.</span>q_eval<span class="token punctuation">]</span><span class="token punctuation">,</span>
            feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>
                self<span class="token punctuation">.</span>s_<span class="token punctuation">:</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>n_features<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                self<span class="token punctuation">.</span>s<span class="token punctuation">:</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>n_features<span class="token punctuation">]</span><span class="token punctuation">,</span> 
            <span class="token punctuation">}</span><span class="token punctuation">)</span>

        
        q_target <span class="token operator">=</span> q_eval<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>

        batch_index <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>


        <span class="token comment">#取出eval的每一个行为</span>
        eval_act_index <span class="token operator">=</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_features<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>
        <span class="token comment">#取出eval的每一个得分</span>
        reward <span class="token operator">=</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_features <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span>

        <span class="token comment">#取出每一行的最大值</span>
        q_target<span class="token punctuation">[</span>batch_index<span class="token punctuation">,</span> eval_act_index<span class="token punctuation">]</span> <span class="token operator">=</span> reward <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>q_next<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment">#训练</span>
        _<span class="token punctuation">,</span> self<span class="token punctuation">.</span>cost <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>_train_op<span class="token punctuation">,</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">]</span><span class="token punctuation">,</span>
                                        feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>self<span class="token punctuation">.</span>s<span class="token punctuation">:</span> batch_memory<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>n_features<span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                self<span class="token punctuation">.</span>q_target<span class="token punctuation">:</span> q_target<span class="token punctuation">}</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>cost_his<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cost<span class="token punctuation">)</span>

        <span class="token comment">#如果存在epsilon_increment则改变epsilon的值</span>
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> self<span class="token punctuation">.</span>epsilon <span class="token operator">+</span> self<span class="token punctuation">.</span>epsilon_increment <span class="token keyword">if</span> self<span class="token punctuation">.</span>epsilon <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>epsilon_max <span class="token keyword">else</span> self<span class="token punctuation">.</span>epsilon_max
        self<span class="token punctuation">.</span>learn_step_counter <span class="token operator">+=</span> <span class="token number">1</span>

    <span class="token keyword">def</span> <span class="token function">plot_cost</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
        plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>cost_his<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>cost_his<span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Cost'</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'training steps'</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p>第三部分为运行环境：</p> 
  <pre><code class="prism language-python"><span class="token triple-quoted-string string">""" Reinforcement learning maze example. Red rectangle: explorer. Black rectangles: hells [reward = -1]. Yellow bin circle: paradise [reward = +1]. All other states: ground [reward = 0]. This script is the environment part of this example. The RL is in RL_brain.py. View more on my tutorial page: https://morvanzhou.github.io/tutorials/ """</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> time
<span class="token keyword">import</span> sys
<span class="token keyword">if</span> sys<span class="token punctuation">.</span>version_info<span class="token punctuation">.</span>major <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
    <span class="token keyword">import</span> Tkinter <span class="token keyword">as</span> tk
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">import</span> tkinter <span class="token keyword">as</span> tk

UNIT <span class="token operator">=</span> <span class="token number">40</span>   <span class="token comment"># pixels</span>
MAZE_H <span class="token operator">=</span> <span class="token number">4</span>  <span class="token comment"># grid height</span>
MAZE_W <span class="token operator">=</span> <span class="token number">4</span>  <span class="token comment"># grid width</span>


<span class="token keyword">class</span> <span class="token class-name">Maze</span><span class="token punctuation">(</span>tk<span class="token punctuation">.</span>Tk<span class="token punctuation">,</span> <span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Maze<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>action_space <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'u'</span><span class="token punctuation">,</span> <span class="token string">'d'</span><span class="token punctuation">,</span> <span class="token string">'l'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>n_actions <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>action_space<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>n_features <span class="token operator">=</span> <span class="token number">2</span>
        self<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'maze'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>geometry<span class="token punctuation">(</span><span class="token string">'{0}x{1}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>MAZE_H <span class="token operator">*</span> UNIT<span class="token punctuation">,</span> MAZE_H <span class="token operator">*</span> UNIT<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_build_maze<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_build_maze</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>canvas <span class="token operator">=</span> tk<span class="token punctuation">.</span>Canvas<span class="token punctuation">(</span>self<span class="token punctuation">,</span> bg<span class="token operator">=</span><span class="token string">'white'</span><span class="token punctuation">,</span>
                           height<span class="token operator">=</span>MAZE_H <span class="token operator">*</span> UNIT<span class="token punctuation">,</span>
                           width<span class="token operator">=</span>MAZE_W <span class="token operator">*</span> UNIT<span class="token punctuation">)</span>

        <span class="token comment"># create grids</span>
        <span class="token keyword">for</span> c <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> MAZE_W <span class="token operator">*</span> UNIT<span class="token punctuation">,</span> UNIT<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x0<span class="token punctuation">,</span> y0<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> y1 <span class="token operator">=</span> c<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> c<span class="token punctuation">,</span> MAZE_H <span class="token operator">*</span> UNIT
            self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>create_line<span class="token punctuation">(</span>x0<span class="token punctuation">,</span> y0<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> y1<span class="token punctuation">)</span>
        <span class="token keyword">for</span> r <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> MAZE_H <span class="token operator">*</span> UNIT<span class="token punctuation">,</span> UNIT<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x0<span class="token punctuation">,</span> y0<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> y1 <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> r<span class="token punctuation">,</span> MAZE_W <span class="token operator">*</span> UNIT<span class="token punctuation">,</span> r
            self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>create_line<span class="token punctuation">(</span>x0<span class="token punctuation">,</span> y0<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> y1<span class="token punctuation">)</span>

        <span class="token comment"># create origin</span>
        origin <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># hell</span>
        hell1_center <span class="token operator">=</span> origin <span class="token operator">+</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>UNIT <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> UNIT<span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hell1 <span class="token operator">=</span> self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>create_rectangle<span class="token punctuation">(</span>
            hell1_center<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">15</span><span class="token punctuation">,</span> hell1_center<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">15</span><span class="token punctuation">,</span>
            hell1_center<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">15</span><span class="token punctuation">,</span> hell1_center<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">15</span><span class="token punctuation">,</span>
            fill<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span>
        <span class="token comment"># hell</span>
        <span class="token comment"># hell2_center = origin + np.array([UNIT, UNIT * 2])</span>
        <span class="token comment"># self.hell2 = self.canvas.create_rectangle(</span>
        <span class="token comment"># hell2_center[0] - 15, hell2_center[1] - 15,</span>
        <span class="token comment"># hell2_center[0] + 15, hell2_center[1] + 15,</span>
        <span class="token comment"># fill='black')</span>

        <span class="token comment"># create oval</span>
        oval_center <span class="token operator">=</span> origin <span class="token operator">+</span> UNIT <span class="token operator">*</span> <span class="token number">2</span>
        self<span class="token punctuation">.</span>oval <span class="token operator">=</span> self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>create_oval<span class="token punctuation">(</span>
            oval_center<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">15</span><span class="token punctuation">,</span> oval_center<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">15</span><span class="token punctuation">,</span>
            oval_center<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">15</span><span class="token punctuation">,</span> oval_center<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">15</span><span class="token punctuation">,</span>
            fill<span class="token operator">=</span><span class="token string">'yellow'</span><span class="token punctuation">)</span>

        <span class="token comment"># create red rect</span>
        self<span class="token punctuation">.</span>rect <span class="token operator">=</span> self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>create_rectangle<span class="token punctuation">(</span>
            origin<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">15</span><span class="token punctuation">,</span> origin<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">15</span><span class="token punctuation">,</span>
            origin<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">15</span><span class="token punctuation">,</span> origin<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">15</span><span class="token punctuation">,</span>
            fill<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">)</span>

        <span class="token comment"># pack all</span>
        self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>pack<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">reset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span>
        time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>delete<span class="token punctuation">(</span>self<span class="token punctuation">.</span>rect<span class="token punctuation">)</span>
        origin <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rect <span class="token operator">=</span> self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>create_rectangle<span class="token punctuation">(</span>
            origin<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">15</span><span class="token punctuation">,</span> origin<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">15</span><span class="token punctuation">,</span>
            origin<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">15</span><span class="token punctuation">,</span> origin<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">15</span><span class="token punctuation">,</span>
            fill<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">)</span>
        <span class="token comment"># return observation</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>coords<span class="token punctuation">(</span>self<span class="token punctuation">.</span>rect<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>coords<span class="token punctuation">(</span>self<span class="token punctuation">.</span>oval<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span>MAZE_H<span class="token operator">*</span>UNIT<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">:</span>
        s <span class="token operator">=</span> self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>coords<span class="token punctuation">(</span>self<span class="token punctuation">.</span>rect<span class="token punctuation">)</span>
        base_action <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> action <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>   <span class="token comment"># up</span>
            <span class="token keyword">if</span> s<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&gt;</span> UNIT<span class="token punctuation">:</span>
                base_action<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-=</span> UNIT
        <span class="token keyword">elif</span> action <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>   <span class="token comment"># down</span>
            <span class="token keyword">if</span> s<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token punctuation">(</span>MAZE_H <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> UNIT<span class="token punctuation">:</span>
                base_action<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> UNIT
        <span class="token keyword">elif</span> action <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>   <span class="token comment"># right</span>
            <span class="token keyword">if</span> s<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token punctuation">(</span>MAZE_W <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> UNIT<span class="token punctuation">:</span>
                base_action<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> UNIT
        <span class="token keyword">elif</span> action <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">:</span>   <span class="token comment"># left</span>
            <span class="token keyword">if</span> s<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&gt;</span> UNIT<span class="token punctuation">:</span>
                base_action<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-=</span> UNIT

        self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>move<span class="token punctuation">(</span>self<span class="token punctuation">.</span>rect<span class="token punctuation">,</span> base_action<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> base_action<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># move agent</span>

        next_coords <span class="token operator">=</span> self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>coords<span class="token punctuation">(</span>self<span class="token punctuation">.</span>rect<span class="token punctuation">)</span>  <span class="token comment"># next state</span>

        <span class="token comment"># reward function</span>
        <span class="token keyword">if</span> next_coords <span class="token operator">==</span> self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>coords<span class="token punctuation">(</span>self<span class="token punctuation">.</span>oval<span class="token punctuation">)</span><span class="token punctuation">:</span>
            reward <span class="token operator">=</span> <span class="token number">1</span>
            done <span class="token operator">=</span> <span class="token boolean">True</span>
        <span class="token keyword">elif</span> next_coords <span class="token keyword">in</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>coords<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hell1<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            reward <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>
            done <span class="token operator">=</span> <span class="token boolean">True</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            reward <span class="token operator">=</span> <span class="token number">0</span>
            done <span class="token operator">=</span> <span class="token boolean">False</span>
        s_ <span class="token operator">=</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>next_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>self<span class="token punctuation">.</span>canvas<span class="token punctuation">.</span>coords<span class="token punctuation">(</span>self<span class="token punctuation">.</span>oval<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span>MAZE_H<span class="token operator">*</span>UNIT<span class="token punctuation">)</span>
        <span class="token keyword">return</span> s_<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done

    <span class="token keyword">def</span> <span class="token function">render</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># time.sleep(0.01)</span>
        self<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p>除了环境部分外，主函数部分和DQN大脑部分我都进行了详细的备注，由于涉及到模块间的调用，需要设置文件名称，三个文件的名称分别为：run_this.py，RL_brain.py，maze_env.py。<br> 由于代码并不是自己写的，所以就不上传github了，不过还是欢迎大家关注我和我的github<br> <a href="https://github.com/bubbliiiing/" rel="nofollow">https://github.com/bubbliiiing/</a><br> 希望得到朋友们的喜欢。</p> 
  <p><strong>有不懂的朋友可以评论询问噢。</strong></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
