<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Spark Streaming整合kafka实战简单 一看就会 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Spark Streaming整合kafka实战简单 一看就会" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="首先新建一个由maven管理的scala的项目 在pom文件中添加以下依赖 &lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;hadoop.version&gt;2.7.4&lt;/hadoop.version&gt; &lt;spark.version&gt;2.0.2&lt;/spark.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;${scala.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--sparkSql 需要引入的包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--HiveContext需要引入的包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 以上依赖可能有的不需要，但是为了程序能够顺利，就加上了 （4）启动zookeeper集群 zkServer.sh start （5）启动kafka集群（根据自己集群的kafka命令启动为准） kafka-server-start.sh /export/servers/kafka/config/server.properties 创建topic（主题） kafka-topics.sh --create --zookeeper 主机名:2181 --replication-factor 1 --partitions 3 --topic 主题名 创建生产者 /opt/software/kafka/bin/kafka-console-producer.sh --broker-list 主机名--topic 主题名 （前面根据kafka安装的目录更改） 创建消费者 /opt/software/kafka/bin/kafka-console-consumer.sh --bootstrap-server 主机名 --topic 主题名 --from-beginning （前面根据kafka安装的目录更改） 从生产者发送消息看消费者是否接受成功 （8）编写Spark Streaming应用程序 package cn.bw.kafka import org.apache.spark.streaming.dstream.DStream import org.apache.spark.streaming.kafka.KafkaUtils import org.apache.spark.streaming.{Seconds, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} import scala.collection.immutable //todo:利用sparkStreaming接受kafka中的数据实现单词计数----采用receivers object SparkStreamingKafka_Receiver_checkpoint { def updateFunc(a:Seq[Int], b:Option[Int]) :Option[Int] ={ Some(a.sum+b.getOrElse(0)) } def main(args: Array[String]): Unit = { val checkpointPath = &quot;./kafka-receiver&quot; val ssc = StreamingContext.getOrCreate(checkpointPath, () =&gt; { createFunc(checkpointPath) }) ssc.start() ssc.awaitTermination() } def createFunc(checkpointPath:String): StreamingContext = { //todo:1、创建sparkConf val sparkConf: SparkConf = new SparkConf() .setAppName(&quot;SparkStreamingKafka_Receiver_checkpoint&quot;) .setMaster(&quot;local[4]&quot;) //todo:开启wal预写日志 .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;) //todo:2、创建sparkContext val sc = new SparkContext(sparkConf) sc.setLogLevel(&quot;WARN&quot;) //todo:3、创建StreamingContext val ssc = new StreamingContext(sc,Seconds(5)) ssc.checkpoint(checkpointPath) //todo:4、指定zkServer val zkServer=&quot;node1:2181,node2:2181,node3:2181&quot; //todo:5、指定groupId val groupId=&quot;spark-kafka-receiver01&quot; //todo:6、指定topics 这个可以利用一个消费者组来消费多个topic, //(topic_name -&gt; numPartitions) 指定topic消费的线程数 val topics=Map(&quot;kafka_spark&quot;-&gt;1) //todo:7、并行运行更多的接收器读取kafak topic中的数据，这里设置3个 val resultDStream: immutable.IndexedSeq[DStream[String]] = (1 to 3).map(x =&gt; { //todo:8、通过使用KafkaUtils的createStream接受kafka topic中的数据，生成DStream val kafkaDataDStream: DStream[String] = KafkaUtils.createStream(ssc, zkServer, groupId, topics).map(x =&gt; x._2) kafkaDataDStream } ) //todo:利用StreamContext将所有的DStream组合在一起 val kafkaDStream: DStream[String] = ssc.union(resultDStream) //todo:8、获取kafka中topic的内容 //todo:9、切分每一行。每个单词记为1 val wordAndOne: DStream[(String, Int)] = kafkaDStream.flatMap(_.split(&quot; &quot;)).map((_,1)) //todo:10、相同单词出现的次数累加 val result: DStream[(String, Int)] = wordAndOne.updateStateByKey(updateFunc) //todo:打印 result.print() ssc } } （9）运行代码,查看控制台结果数据 出现结果就成功啦！" />
<meta property="og:description" content="首先新建一个由maven管理的scala的项目 在pom文件中添加以下依赖 &lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;hadoop.version&gt;2.7.4&lt;/hadoop.version&gt; &lt;spark.version&gt;2.0.2&lt;/spark.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;${scala.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--sparkSql 需要引入的包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--HiveContext需要引入的包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 以上依赖可能有的不需要，但是为了程序能够顺利，就加上了 （4）启动zookeeper集群 zkServer.sh start （5）启动kafka集群（根据自己集群的kafka命令启动为准） kafka-server-start.sh /export/servers/kafka/config/server.properties 创建topic（主题） kafka-topics.sh --create --zookeeper 主机名:2181 --replication-factor 1 --partitions 3 --topic 主题名 创建生产者 /opt/software/kafka/bin/kafka-console-producer.sh --broker-list 主机名--topic 主题名 （前面根据kafka安装的目录更改） 创建消费者 /opt/software/kafka/bin/kafka-console-consumer.sh --bootstrap-server 主机名 --topic 主题名 --from-beginning （前面根据kafka安装的目录更改） 从生产者发送消息看消费者是否接受成功 （8）编写Spark Streaming应用程序 package cn.bw.kafka import org.apache.spark.streaming.dstream.DStream import org.apache.spark.streaming.kafka.KafkaUtils import org.apache.spark.streaming.{Seconds, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} import scala.collection.immutable //todo:利用sparkStreaming接受kafka中的数据实现单词计数----采用receivers object SparkStreamingKafka_Receiver_checkpoint { def updateFunc(a:Seq[Int], b:Option[Int]) :Option[Int] ={ Some(a.sum+b.getOrElse(0)) } def main(args: Array[String]): Unit = { val checkpointPath = &quot;./kafka-receiver&quot; val ssc = StreamingContext.getOrCreate(checkpointPath, () =&gt; { createFunc(checkpointPath) }) ssc.start() ssc.awaitTermination() } def createFunc(checkpointPath:String): StreamingContext = { //todo:1、创建sparkConf val sparkConf: SparkConf = new SparkConf() .setAppName(&quot;SparkStreamingKafka_Receiver_checkpoint&quot;) .setMaster(&quot;local[4]&quot;) //todo:开启wal预写日志 .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;) //todo:2、创建sparkContext val sc = new SparkContext(sparkConf) sc.setLogLevel(&quot;WARN&quot;) //todo:3、创建StreamingContext val ssc = new StreamingContext(sc,Seconds(5)) ssc.checkpoint(checkpointPath) //todo:4、指定zkServer val zkServer=&quot;node1:2181,node2:2181,node3:2181&quot; //todo:5、指定groupId val groupId=&quot;spark-kafka-receiver01&quot; //todo:6、指定topics 这个可以利用一个消费者组来消费多个topic, //(topic_name -&gt; numPartitions) 指定topic消费的线程数 val topics=Map(&quot;kafka_spark&quot;-&gt;1) //todo:7、并行运行更多的接收器读取kafak topic中的数据，这里设置3个 val resultDStream: immutable.IndexedSeq[DStream[String]] = (1 to 3).map(x =&gt; { //todo:8、通过使用KafkaUtils的createStream接受kafka topic中的数据，生成DStream val kafkaDataDStream: DStream[String] = KafkaUtils.createStream(ssc, zkServer, groupId, topics).map(x =&gt; x._2) kafkaDataDStream } ) //todo:利用StreamContext将所有的DStream组合在一起 val kafkaDStream: DStream[String] = ssc.union(resultDStream) //todo:8、获取kafka中topic的内容 //todo:9、切分每一行。每个单词记为1 val wordAndOne: DStream[(String, Int)] = kafkaDStream.flatMap(_.split(&quot; &quot;)).map((_,1)) //todo:10、相同单词出现的次数累加 val result: DStream[(String, Int)] = wordAndOne.updateStateByKey(updateFunc) //todo:打印 result.print() ssc } } （9）运行代码,查看控制台结果数据 出现结果就成功啦！" />
<link rel="canonical" href="https://uzzz.org/2019/07/19/794270.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/19/794270.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-19T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"首先新建一个由maven管理的scala的项目 在pom文件中添加以下依赖 &lt;properties&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;hadoop.version&gt;2.7.4&lt;/hadoop.version&gt; &lt;spark.version&gt;2.0.2&lt;/spark.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;${scala.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--sparkSql 需要引入的包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--HiveContext需要引入的包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 以上依赖可能有的不需要，但是为了程序能够顺利，就加上了 （4）启动zookeeper集群 zkServer.sh start （5）启动kafka集群（根据自己集群的kafka命令启动为准） kafka-server-start.sh /export/servers/kafka/config/server.properties 创建topic（主题） kafka-topics.sh --create --zookeeper 主机名:2181 --replication-factor 1 --partitions 3 --topic 主题名 创建生产者 /opt/software/kafka/bin/kafka-console-producer.sh --broker-list 主机名--topic 主题名 （前面根据kafka安装的目录更改） 创建消费者 /opt/software/kafka/bin/kafka-console-consumer.sh --bootstrap-server 主机名 --topic 主题名 --from-beginning （前面根据kafka安装的目录更改） 从生产者发送消息看消费者是否接受成功 （8）编写Spark Streaming应用程序 package cn.bw.kafka import org.apache.spark.streaming.dstream.DStream import org.apache.spark.streaming.kafka.KafkaUtils import org.apache.spark.streaming.{Seconds, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} import scala.collection.immutable //todo:利用sparkStreaming接受kafka中的数据实现单词计数----采用receivers object SparkStreamingKafka_Receiver_checkpoint { def updateFunc(a:Seq[Int], b:Option[Int]) :Option[Int] ={ Some(a.sum+b.getOrElse(0)) } def main(args: Array[String]): Unit = { val checkpointPath = &quot;./kafka-receiver&quot; val ssc = StreamingContext.getOrCreate(checkpointPath, () =&gt; { createFunc(checkpointPath) }) ssc.start() ssc.awaitTermination() } def createFunc(checkpointPath:String): StreamingContext = { //todo:1、创建sparkConf val sparkConf: SparkConf = new SparkConf() .setAppName(&quot;SparkStreamingKafka_Receiver_checkpoint&quot;) .setMaster(&quot;local[4]&quot;) //todo:开启wal预写日志 .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;) //todo:2、创建sparkContext val sc = new SparkContext(sparkConf) sc.setLogLevel(&quot;WARN&quot;) //todo:3、创建StreamingContext val ssc = new StreamingContext(sc,Seconds(5)) ssc.checkpoint(checkpointPath) //todo:4、指定zkServer val zkServer=&quot;node1:2181,node2:2181,node3:2181&quot; //todo:5、指定groupId val groupId=&quot;spark-kafka-receiver01&quot; //todo:6、指定topics 这个可以利用一个消费者组来消费多个topic, //(topic_name -&gt; numPartitions) 指定topic消费的线程数 val topics=Map(&quot;kafka_spark&quot;-&gt;1) //todo:7、并行运行更多的接收器读取kafak topic中的数据，这里设置3个 val resultDStream: immutable.IndexedSeq[DStream[String]] = (1 to 3).map(x =&gt; { //todo:8、通过使用KafkaUtils的createStream接受kafka topic中的数据，生成DStream val kafkaDataDStream: DStream[String] = KafkaUtils.createStream(ssc, zkServer, groupId, topics).map(x =&gt; x._2) kafkaDataDStream } ) //todo:利用StreamContext将所有的DStream组合在一起 val kafkaDStream: DStream[String] = ssc.union(resultDStream) //todo:8、获取kafka中topic的内容 //todo:9、切分每一行。每个单词记为1 val wordAndOne: DStream[(String, Int)] = kafkaDStream.flatMap(_.split(&quot; &quot;)).map((_,1)) //todo:10、相同单词出现的次数累加 val result: DStream[(String, Int)] = wordAndOne.updateStateByKey(updateFunc) //todo:打印 result.print() ssc } } （9）运行代码,查看控制台结果数据 出现结果就成功啦！","@type":"BlogPosting","url":"https://uzzz.org/2019/07/19/794270.html","headline":"Spark Streaming整合kafka实战简单 一看就会","dateModified":"2019-07-19T00:00:00+08:00","datePublished":"2019-07-19T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/19/794270.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Spark Streaming整合kafka实战简单 一看就会</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>首先新建一个由maven管理的scala的项目<br> 在pom文件中添加以下依赖</p> 
  <pre><code>  &lt;properties&gt;
        &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;
        &lt;hadoop.version&gt;2.7.4&lt;/hadoop.version&gt;
        &lt;spark.version&gt;2.0.2&lt;/spark.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
            &lt;version&gt;${scala.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!--sparkSql 需要引入的包--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!--HiveContext需要引入的包--&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
        &lt;/dependency&gt;


        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.1.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt;
            &lt;artifactId&gt;zkclient&lt;/artifactId&gt;
            &lt;version&gt;0.1&lt;/version&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;
    
</code></pre> 
  <p>以上依赖可能有的不需要，但是为了程序能够顺利，就加上了<br> （4）启动zookeeper集群<br> <a href="http://zkServer.sh" rel="nofollow" data-token="9c9640d24cfc31d10d740c2f0e602c3b">zkServer.sh</a> start</p> 
  <p>（5）启动kafka集群（根据自己集群的kafka命令启动为准）</p> 
  <pre><code>kafka-server-start.sh  /export/servers/kafka/config/server.properties
</code></pre> 
  <p>创建topic（主题）</p> 
  <pre><code>kafka-topics.sh --create --zookeeper 主机名:2181 --replication-factor 1 --partitions 3 --topic 主题名
</code></pre> 
  <p>创建生产者</p> 
  <p><code>/opt/software/kafka/bin/kafka-console-producer.sh --broker-list 主机名--topic 主题名</code><br> （前面根据kafka安装的目录更改）</p> 
  <p>创建消费者<br> <code>/opt/software/kafka/bin/kafka-console-consumer.sh --bootstrap-server 主机名 --topic 主题名 --from-beginning</code><br> （前面根据kafka安装的目录更改）<br> 从生产者发送消息看消费者是否接受成功<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190719143704888.png" alt="在这里插入图片描述"><br> （8）编写Spark Streaming应用程序</p> 
  <pre><code>package cn.bw.kafka

import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.immutable

//todo:利用sparkStreaming接受kafka中的数据实现单词计数----采用receivers
object SparkStreamingKafka_Receiver_checkpoint {

  def updateFunc(a:Seq[Int], b:Option[Int]) :Option[Int] ={
    Some(a.sum+b.getOrElse(0))
  }
  def main(args: Array[String]): Unit = {
    val checkpointPath = "./kafka-receiver"

    val ssc = StreamingContext.getOrCreate(checkpointPath, () =&gt; {
      createFunc(checkpointPath)
    })
    ssc.start()
    ssc.awaitTermination()
  }
  def createFunc(checkpointPath:String): StreamingContext = {

    //todo:1、创建sparkConf
     val sparkConf: SparkConf = new SparkConf()
                                .setAppName("SparkStreamingKafka_Receiver_checkpoint")
                                .setMaster("local[4]")
                                //todo:开启wal预写日志
                                .set("spark.streaming.receiver.writeAheadLog.enable","true")
    //todo:2、创建sparkContext
    val sc = new SparkContext(sparkConf)

    sc.setLogLevel("WARN")

    //todo:3、创建StreamingContext
    val ssc = new StreamingContext(sc,Seconds(5))
    ssc.checkpoint(checkpointPath)
    //todo:4、指定zkServer
    val zkServer="node1:2181,node2:2181,node3:2181"

    //todo:5、指定groupId
    val groupId="spark-kafka-receiver01"

    //todo:6、指定topics 这个可以利用一个消费者组来消费多个topic,
    //(topic_name -&gt; numPartitions)  指定topic消费的线程数
    val topics=Map("kafka_spark"-&gt;1)

    //todo:7、并行运行更多的接收器读取kafak topic中的数据，这里设置3个
    val resultDStream: immutable.IndexedSeq[DStream[String]] = (1 to 3).map(x =&gt; {
      //todo:8、通过使用KafkaUtils的createStream接受kafka topic中的数据，生成DStream
      val kafkaDataDStream: DStream[String] = KafkaUtils.createStream(ssc, zkServer, groupId, topics).map(x =&gt; x._2)
      kafkaDataDStream
    }
    )
    //todo:利用StreamContext将所有的DStream组合在一起
    val kafkaDStream: DStream[String] = ssc.union(resultDStream)

    //todo:8、获取kafka中topic的内容

    //todo:9、切分每一行。每个单词记为1
    val wordAndOne: DStream[(String, Int)] = kafkaDStream.flatMap(_.split(" ")).map((_,1))

    //todo:10、相同单词出现的次数累加
    val result: DStream[(String, Int)] = wordAndOne.updateStateByKey(updateFunc)

    //todo:打印
    result.print()
    ssc

  }

}

</code></pre> 
  <p>（9）运行代码,查看控制台结果数据<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190719143819250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1NxcnNDYnJPbmx5MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">出现结果就成功啦！</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
