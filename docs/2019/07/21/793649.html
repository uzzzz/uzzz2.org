<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>快乐的强化学习3——环境模块gym的调用 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="快乐的强化学习3——环境模块gym的调用" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="快乐的强化学习3——环境模块gym的调用 学习前言 gym模块的安装 gym模块中环境的常用函数 gym的初始化 gym的各个参数的获取 刷新环境 刷新环境 应用实例 终于要安装gym模块了，一想到我可以去测试其它的强化学习环境，突然有点小兴奋。 但是听说windows不能安装OpenAI gym，我的想法是这样的。 不过我还是怀着好奇宝宝的内心决定实验一下……好的还是成功了! 学习前言 在通常的情况下，手动编写强化学习环境是一件非常耗时间的事情，所以如果能够调用其它人已经编写好的环境，我们就可以节约很多时间，将更多的时间用于关注算法本身。这个时候我们就要用到OpenAI的gym模块。这个模块为我们提供了许多现成的环境用于测试我们的RL算法。 gym模块的安装 现在在Windows下也基本支持了gym模块的使用。我们只要打开cmd命令行窗口。 在命令行中输入： pip install gym 即可。 在安装的时候可能会遇到一些错误，我遇到的错误是： cannot uninstall a distutils installed project 解决方法可以参照我转载的博文：https://blog.csdn.net/weixin_44791964/article/details/96761819 gym模块中环境的常用函数 gym的初始化 env = gym.make(&#39;CartPole-v0&#39;) # 定义使用gym库中的某一个环境，&#39;CartPole-v0&#39;可以改为其它环境 env = env.unwrapped # 据说不做这个动作会有很多限制，unwrapped是打开限制的意思 gym的各个参数的获取 env.action_space # 查看这个环境中可用的action有多少个，返回Discrete()格式 env.observation_space # 查看这个环境中observation的特征，返回Box()格式 n_actions=env.action_space.n # 查看这个环境中可用的action有多少个，返回int n_features=env.observation_space.shape[0] # 查看这个环境中observation的特征有多少个，返回int 刷新环境 env.reset() # 用于一个世代（done）后环境的重启，获取回合的第一个observation env.render() # 用于每一步后刷新环境状态 刷新环境 observation_, reward, done, info = env.step(action) # 获取下一步的环境、得分、检测是否完成。 应用实例 这是一个板子支撑杆子的例子，支撑的越久得分越高，同时杆子不能移出屏幕外，不能倾斜过大。 实际上像这个样子： 测试代码为： import gym import math from RL_brain import DeepQNetwork env = gym.make(&#39;CartPole-v0&#39;) # 定义使用gym库中的某一个环境，&#39;CartPole-v0&#39;可以改为其它环境 env = env.unwrapped # 据说不做这个动作会有很多限制，unwrapped是打开限制的意思 print(env.action_space) # 查看这个环境中可用的action有多少个，返回Discrete()格式 print(env.observation_space) # 查看这个环境中observation的特征，返回Box()格式 # 定义使用 DQN 的算法， # n_actions=env.action_space.n用于查看这个环境中可用的action有多少个，返回int # n_features=env.observation_space.shape[0]用于查看这个环境中observation的特征有多少个，返回int # replace_target_iter=100代表每学习一百次将Net_Pre的参数赋予Net_Next RL = DeepQNetwork(n_actions=env.action_space.n, n_features=env.observation_space.shape[0], learning_rate=0.01, e_greedy=0.9, replace_target_iter=100, memory_size=2000, e_greedy_increment=0.0008,) # 记录步数 total_steps = 0 for i in range(100): # 获取回合 i 第一个 observation observation = env.reset() #每一轮得分的合集 ep_r = 0 while True: # 用于每一步后刷新环境状态 env.render() # 根据当前心目中预测的状态选行为 action = RL.choose_action(observation) # 获取下一步的环境、得分、检测是否完成。 observation_, reward, done, info = env.step(action) # 通过查找源代码发现observation_由以下四个特征构成。 # x 是车的水平位移 # theta 是棒子离垂直的角度 x, x_dot, theta, theta_dot = observation_ # r1代表车的 x水平位移 与 x最大边距 的距离差的得分 r1 = math.exp((env.x_threshold - abs(x))/env.x_threshold) - math.exp(1)/2 # r2代表棒子的 theta离垂直的角度 与 theta最大角度 的差的得分 r2 = math.exp((env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians) - math.exp(1)/2 # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度。 reward = r1 + r2 # 保存这一组记忆 RL.store_transition(observation, action, reward, observation_) # 在数据具有一定规模后进行学习 if total_steps &gt; 1000: RL.learn() #记录总分 ep_r += reward if done: print(&#39;世代: &#39;, i,&#39;。总分: &#39;, round(ep_r, 2)) break #环境更新 observation = observation_ total_steps += 1 # 最后输出 cost 曲线 RL.plot_cost() RL_brain的代码可以从我的另一篇博文中得到： https://blog.csdn.net/weixin_44791964/article/details/96422796 细心的同学可能观察到我并没有直接使用 observation_, reward, done, info = env.step(action) 返回的reward值，而是重新定义了。因为我试了直接用原来reward值效果并不好。我定义的reward值由两部分组成，分别是： r1 = math.exp((env.x_threshold - abs(x))/env.x_threshold) - math.exp(1)/2 # r1代表车的 x水平位移 与 x最大边距 的距离差的得分 r2 = math.exp((env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians) - math.exp(1)/2 # r2代表棒子的 theta离垂直的角度 与 theta最大角度 的差的得分 这些参数需要去查询CartPole的源文件才可以得到。 我是通过在C:\Anaconda3\envs\tensorflow-gpu\Lib里搜索得到的。 这个环境里有详细的描述，需要大家好好看看噢！ 希望得到朋友们的喜欢。 有不懂的朋友可以评论询问噢。" />
<meta property="og:description" content="快乐的强化学习3——环境模块gym的调用 学习前言 gym模块的安装 gym模块中环境的常用函数 gym的初始化 gym的各个参数的获取 刷新环境 刷新环境 应用实例 终于要安装gym模块了，一想到我可以去测试其它的强化学习环境，突然有点小兴奋。 但是听说windows不能安装OpenAI gym，我的想法是这样的。 不过我还是怀着好奇宝宝的内心决定实验一下……好的还是成功了! 学习前言 在通常的情况下，手动编写强化学习环境是一件非常耗时间的事情，所以如果能够调用其它人已经编写好的环境，我们就可以节约很多时间，将更多的时间用于关注算法本身。这个时候我们就要用到OpenAI的gym模块。这个模块为我们提供了许多现成的环境用于测试我们的RL算法。 gym模块的安装 现在在Windows下也基本支持了gym模块的使用。我们只要打开cmd命令行窗口。 在命令行中输入： pip install gym 即可。 在安装的时候可能会遇到一些错误，我遇到的错误是： cannot uninstall a distutils installed project 解决方法可以参照我转载的博文：https://blog.csdn.net/weixin_44791964/article/details/96761819 gym模块中环境的常用函数 gym的初始化 env = gym.make(&#39;CartPole-v0&#39;) # 定义使用gym库中的某一个环境，&#39;CartPole-v0&#39;可以改为其它环境 env = env.unwrapped # 据说不做这个动作会有很多限制，unwrapped是打开限制的意思 gym的各个参数的获取 env.action_space # 查看这个环境中可用的action有多少个，返回Discrete()格式 env.observation_space # 查看这个环境中observation的特征，返回Box()格式 n_actions=env.action_space.n # 查看这个环境中可用的action有多少个，返回int n_features=env.observation_space.shape[0] # 查看这个环境中observation的特征有多少个，返回int 刷新环境 env.reset() # 用于一个世代（done）后环境的重启，获取回合的第一个observation env.render() # 用于每一步后刷新环境状态 刷新环境 observation_, reward, done, info = env.step(action) # 获取下一步的环境、得分、检测是否完成。 应用实例 这是一个板子支撑杆子的例子，支撑的越久得分越高，同时杆子不能移出屏幕外，不能倾斜过大。 实际上像这个样子： 测试代码为： import gym import math from RL_brain import DeepQNetwork env = gym.make(&#39;CartPole-v0&#39;) # 定义使用gym库中的某一个环境，&#39;CartPole-v0&#39;可以改为其它环境 env = env.unwrapped # 据说不做这个动作会有很多限制，unwrapped是打开限制的意思 print(env.action_space) # 查看这个环境中可用的action有多少个，返回Discrete()格式 print(env.observation_space) # 查看这个环境中observation的特征，返回Box()格式 # 定义使用 DQN 的算法， # n_actions=env.action_space.n用于查看这个环境中可用的action有多少个，返回int # n_features=env.observation_space.shape[0]用于查看这个环境中observation的特征有多少个，返回int # replace_target_iter=100代表每学习一百次将Net_Pre的参数赋予Net_Next RL = DeepQNetwork(n_actions=env.action_space.n, n_features=env.observation_space.shape[0], learning_rate=0.01, e_greedy=0.9, replace_target_iter=100, memory_size=2000, e_greedy_increment=0.0008,) # 记录步数 total_steps = 0 for i in range(100): # 获取回合 i 第一个 observation observation = env.reset() #每一轮得分的合集 ep_r = 0 while True: # 用于每一步后刷新环境状态 env.render() # 根据当前心目中预测的状态选行为 action = RL.choose_action(observation) # 获取下一步的环境、得分、检测是否完成。 observation_, reward, done, info = env.step(action) # 通过查找源代码发现observation_由以下四个特征构成。 # x 是车的水平位移 # theta 是棒子离垂直的角度 x, x_dot, theta, theta_dot = observation_ # r1代表车的 x水平位移 与 x最大边距 的距离差的得分 r1 = math.exp((env.x_threshold - abs(x))/env.x_threshold) - math.exp(1)/2 # r2代表棒子的 theta离垂直的角度 与 theta最大角度 的差的得分 r2 = math.exp((env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians) - math.exp(1)/2 # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度。 reward = r1 + r2 # 保存这一组记忆 RL.store_transition(observation, action, reward, observation_) # 在数据具有一定规模后进行学习 if total_steps &gt; 1000: RL.learn() #记录总分 ep_r += reward if done: print(&#39;世代: &#39;, i,&#39;。总分: &#39;, round(ep_r, 2)) break #环境更新 observation = observation_ total_steps += 1 # 最后输出 cost 曲线 RL.plot_cost() RL_brain的代码可以从我的另一篇博文中得到： https://blog.csdn.net/weixin_44791964/article/details/96422796 细心的同学可能观察到我并没有直接使用 observation_, reward, done, info = env.step(action) 返回的reward值，而是重新定义了。因为我试了直接用原来reward值效果并不好。我定义的reward值由两部分组成，分别是： r1 = math.exp((env.x_threshold - abs(x))/env.x_threshold) - math.exp(1)/2 # r1代表车的 x水平位移 与 x最大边距 的距离差的得分 r2 = math.exp((env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians) - math.exp(1)/2 # r2代表棒子的 theta离垂直的角度 与 theta最大角度 的差的得分 这些参数需要去查询CartPole的源文件才可以得到。 我是通过在C:\Anaconda3\envs\tensorflow-gpu\Lib里搜索得到的。 这个环境里有详细的描述，需要大家好好看看噢！ 希望得到朋友们的喜欢。 有不懂的朋友可以评论询问噢。" />
<link rel="canonical" href="https://uzzz.org/2019/07/21/793649.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/21/793649.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-21T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"快乐的强化学习3——环境模块gym的调用 学习前言 gym模块的安装 gym模块中环境的常用函数 gym的初始化 gym的各个参数的获取 刷新环境 刷新环境 应用实例 终于要安装gym模块了，一想到我可以去测试其它的强化学习环境，突然有点小兴奋。 但是听说windows不能安装OpenAI gym，我的想法是这样的。 不过我还是怀着好奇宝宝的内心决定实验一下……好的还是成功了! 学习前言 在通常的情况下，手动编写强化学习环境是一件非常耗时间的事情，所以如果能够调用其它人已经编写好的环境，我们就可以节约很多时间，将更多的时间用于关注算法本身。这个时候我们就要用到OpenAI的gym模块。这个模块为我们提供了许多现成的环境用于测试我们的RL算法。 gym模块的安装 现在在Windows下也基本支持了gym模块的使用。我们只要打开cmd命令行窗口。 在命令行中输入： pip install gym 即可。 在安装的时候可能会遇到一些错误，我遇到的错误是： cannot uninstall a distutils installed project 解决方法可以参照我转载的博文：https://blog.csdn.net/weixin_44791964/article/details/96761819 gym模块中环境的常用函数 gym的初始化 env = gym.make(&#39;CartPole-v0&#39;) # 定义使用gym库中的某一个环境，&#39;CartPole-v0&#39;可以改为其它环境 env = env.unwrapped # 据说不做这个动作会有很多限制，unwrapped是打开限制的意思 gym的各个参数的获取 env.action_space # 查看这个环境中可用的action有多少个，返回Discrete()格式 env.observation_space # 查看这个环境中observation的特征，返回Box()格式 n_actions=env.action_space.n # 查看这个环境中可用的action有多少个，返回int n_features=env.observation_space.shape[0] # 查看这个环境中observation的特征有多少个，返回int 刷新环境 env.reset() # 用于一个世代（done）后环境的重启，获取回合的第一个observation env.render() # 用于每一步后刷新环境状态 刷新环境 observation_, reward, done, info = env.step(action) # 获取下一步的环境、得分、检测是否完成。 应用实例 这是一个板子支撑杆子的例子，支撑的越久得分越高，同时杆子不能移出屏幕外，不能倾斜过大。 实际上像这个样子： 测试代码为： import gym import math from RL_brain import DeepQNetwork env = gym.make(&#39;CartPole-v0&#39;) # 定义使用gym库中的某一个环境，&#39;CartPole-v0&#39;可以改为其它环境 env = env.unwrapped # 据说不做这个动作会有很多限制，unwrapped是打开限制的意思 print(env.action_space) # 查看这个环境中可用的action有多少个，返回Discrete()格式 print(env.observation_space) # 查看这个环境中observation的特征，返回Box()格式 # 定义使用 DQN 的算法， # n_actions=env.action_space.n用于查看这个环境中可用的action有多少个，返回int # n_features=env.observation_space.shape[0]用于查看这个环境中observation的特征有多少个，返回int # replace_target_iter=100代表每学习一百次将Net_Pre的参数赋予Net_Next RL = DeepQNetwork(n_actions=env.action_space.n, n_features=env.observation_space.shape[0], learning_rate=0.01, e_greedy=0.9, replace_target_iter=100, memory_size=2000, e_greedy_increment=0.0008,) # 记录步数 total_steps = 0 for i in range(100): # 获取回合 i 第一个 observation observation = env.reset() #每一轮得分的合集 ep_r = 0 while True: # 用于每一步后刷新环境状态 env.render() # 根据当前心目中预测的状态选行为 action = RL.choose_action(observation) # 获取下一步的环境、得分、检测是否完成。 observation_, reward, done, info = env.step(action) # 通过查找源代码发现observation_由以下四个特征构成。 # x 是车的水平位移 # theta 是棒子离垂直的角度 x, x_dot, theta, theta_dot = observation_ # r1代表车的 x水平位移 与 x最大边距 的距离差的得分 r1 = math.exp((env.x_threshold - abs(x))/env.x_threshold) - math.exp(1)/2 # r2代表棒子的 theta离垂直的角度 与 theta最大角度 的差的得分 r2 = math.exp((env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians) - math.exp(1)/2 # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度。 reward = r1 + r2 # 保存这一组记忆 RL.store_transition(observation, action, reward, observation_) # 在数据具有一定规模后进行学习 if total_steps &gt; 1000: RL.learn() #记录总分 ep_r += reward if done: print(&#39;世代: &#39;, i,&#39;。总分: &#39;, round(ep_r, 2)) break #环境更新 observation = observation_ total_steps += 1 # 最后输出 cost 曲线 RL.plot_cost() RL_brain的代码可以从我的另一篇博文中得到： https://blog.csdn.net/weixin_44791964/article/details/96422796 细心的同学可能观察到我并没有直接使用 observation_, reward, done, info = env.step(action) 返回的reward值，而是重新定义了。因为我试了直接用原来reward值效果并不好。我定义的reward值由两部分组成，分别是： r1 = math.exp((env.x_threshold - abs(x))/env.x_threshold) - math.exp(1)/2 # r1代表车的 x水平位移 与 x最大边距 的距离差的得分 r2 = math.exp((env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians) - math.exp(1)/2 # r2代表棒子的 theta离垂直的角度 与 theta最大角度 的差的得分 这些参数需要去查询CartPole的源文件才可以得到。 我是通过在C:\\Anaconda3\\envs\\tensorflow-gpu\\Lib里搜索得到的。 这个环境里有详细的描述，需要大家好好看看噢！ 希望得到朋友们的喜欢。 有不懂的朋友可以评论询问噢。","@type":"BlogPosting","url":"https://uzzz.org/2019/07/21/793649.html","headline":"快乐的强化学习3——环境模块gym的调用","dateModified":"2019-07-21T00:00:00+08:00","datePublished":"2019-07-21T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/21/793649.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>快乐的强化学习3——环境模块gym的调用</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-tomorrow-night-eighties"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>快乐的强化学习3——环境模块gym的调用</h3>
   <ul>
    <li><a href="#_7" rel="nofollow" data-token="81c09b77a949135270e7a42016fa31c0">学习前言</a></li>
    <ul>
     <li><a href="#gym_9" rel="nofollow" data-token="f1e412157eed679bdd4e816ac4f128d3">gym模块的安装</a></li>
     <li><a href="#gym_22" rel="nofollow" data-token="b4e3e06cc4be06df6de78cd295acee31">gym模块中环境的常用函数</a></li>
     <ul>
      <li><a href="#gym_23" rel="nofollow" data-token="154912c079ac77736839ee4cf7ef1863">gym的初始化</a></li>
      <li><a href="#gym_30" rel="nofollow" data-token="f1ad3b58e6f673ea8f9588fdf5b7d13e">gym的各个参数的获取</a></li>
      <li><a href="#_41" rel="nofollow" data-token="de5e0c460c325f8816c12f094bf8520a">刷新环境</a></li>
      <li><a href="#_48" rel="nofollow" data-token="596e9993c4663e2ee5056f7ce6452828">刷新环境</a></li>
     </ul>
     <li><a href="#_53" rel="nofollow" data-token="d3f00bb44dfd550e69b0a31635785fbc">应用实例</a></li>
    </ul>
   </ul>
  </div>
  <p></p> 
  <p>终于要安装gym模块了，一想到我可以去测试其它的强化学习环境，突然有点小兴奋。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190723132555539.jpg#pic_center" alt="在这里插入图片描述"><br> 但是听说windows不能安装OpenAI gym，我的想法是这样的。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190723132656695.jpg#pic_center" alt="在这里插入图片描述"><br> 不过我还是怀着好奇宝宝的内心决定实验一下……好的还是成功了!</p> 
  <h1><a id="_7"></a>学习前言</h1> 
  <p>在通常的情况下，手动编写强化学习环境是一件非常耗时间的事情，所以如果能够调用其它人已经编写好的环境，我们就可以节约很多时间，将更多的时间用于关注算法本身。这个时候我们就要用到OpenAI的gym模块。这个模块为我们提供了许多现成的环境用于测试我们的RL算法。</p> 
  <h2><a id="gym_9"></a>gym模块的安装</h2> 
  <p>现在在Windows下也基本支持了gym模块的使用。我们只要打开cmd命令行窗口。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721221151697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center" alt="打开命令行cmd"><br> 在命令行中输入：</p> 
  <pre><code class="prism language-cmd">pip install gym
</code></pre> 
  <p>即可。<br> 在安装的时候可能会遇到一些错误，我遇到的错误是：</p> 
  <pre><code class="prism language-cmd">cannot uninstall a distutils installed project
</code></pre> 
  <p>解决方法可以参照我转载的博文：<a href="https://blog.csdn.net/weixin_44791964/article/details/96761819" rel="nofollow" data-token="65c73042724ae653fb0ed460a27ffeb0">https://blog.csdn.net/weixin_44791964/article/details/96761819</a></p> 
  <h2><a id="gym_22"></a>gym模块中环境的常用函数</h2> 
  <h3><a id="gym_23"></a>gym的初始化</h3> 
  <pre><code class="prism language-python">env <span class="token operator">=</span> gym<span class="token punctuation">.</span>make<span class="token punctuation">(</span><span class="token string">'CartPole-v0'</span><span class="token punctuation">)</span>   
<span class="token comment"># 定义使用gym库中的某一个环境，'CartPole-v0'可以改为其它环境</span>
env <span class="token operator">=</span> env<span class="token punctuation">.</span>unwrapped             
<span class="token comment"># 据说不做这个动作会有很多限制，unwrapped是打开限制的意思</span>
</code></pre> 
  <h3><a id="gym_30"></a>gym的各个参数的获取</h3> 
  <pre><code class="prism language-python">env<span class="token punctuation">.</span>action_space   		
<span class="token comment"># 查看这个环境中可用的action有多少个，返回Discrete()格式</span>
env<span class="token punctuation">.</span>observation_space   
<span class="token comment"># 查看这个环境中observation的特征，返回Box()格式</span>
n_actions<span class="token operator">=</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">.</span>n 
<span class="token comment"># 查看这个环境中可用的action有多少个，返回int</span>
n_features<span class="token operator">=</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> 
<span class="token comment"># 查看这个环境中observation的特征有多少个，返回int</span>
</code></pre> 
  <h3><a id="_41"></a>刷新环境</h3> 
  <pre><code class="prism language-python">env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 用于一个世代（done）后环境的重启，获取回合的第一个observation</span>
env<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token punctuation">)</span>	
<span class="token comment"># 用于每一步后刷新环境状态</span>
</code></pre> 
  <h3><a id="_48"></a>刷新环境</h3> 
  <pre><code class="prism language-python">observation_<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
<span class="token comment"># 获取下一步的环境、得分、检测是否完成。</span>
</code></pre> 
  <h2><a id="_53"></a>应用实例</h2> 
  <p>这是一个板子支撑杆子的例子，支撑的越久得分越高，同时杆子不能移出屏幕外，不能倾斜过大。<br> 实际上像这个样子：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019072122342147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br> 测试代码为：</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> gym
<span class="token keyword">import</span> math
<span class="token keyword">from</span> RL_brain <span class="token keyword">import</span> DeepQNetwork

env <span class="token operator">=</span> gym<span class="token punctuation">.</span>make<span class="token punctuation">(</span><span class="token string">'CartPole-v0'</span><span class="token punctuation">)</span>   <span class="token comment"># 定义使用gym库中的某一个环境，'CartPole-v0'可以改为其它环境</span>
env <span class="token operator">=</span> env<span class="token punctuation">.</span>unwrapped             <span class="token comment"># 据说不做这个动作会有很多限制，unwrapped是打开限制的意思</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">)</span>             <span class="token comment"># 查看这个环境中可用的action有多少个，返回Discrete()格式</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">)</span>        <span class="token comment"># 查看这个环境中observation的特征，返回Box()格式</span>

<span class="token comment"># 定义使用 DQN 的算法，</span>
<span class="token comment"># n_actions=env.action_space.n用于查看这个环境中可用的action有多少个，返回int</span>
<span class="token comment"># n_features=env.observation_space.shape[0]用于查看这个环境中observation的特征有多少个，返回int</span>
<span class="token comment"># replace_target_iter=100代表每学习一百次将Net_Pre的参数赋予Net_Next</span>
RL <span class="token operator">=</span> DeepQNetwork<span class="token punctuation">(</span>n_actions<span class="token operator">=</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">.</span>n<span class="token punctuation">,</span>
                  n_features<span class="token operator">=</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  learning_rate<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> e_greedy<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span>
                  replace_target_iter<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> memory_size<span class="token operator">=</span><span class="token number">2000</span><span class="token punctuation">,</span>
                  e_greedy_increment<span class="token operator">=</span><span class="token number">0.0008</span><span class="token punctuation">,</span><span class="token punctuation">)</span>

<span class="token comment"># 记录步数</span>
total_steps <span class="token operator">=</span> <span class="token number">0</span> 

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 获取回合 i 第一个 observation</span>
    observation <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment">#每一轮得分的合集</span>
    ep_r <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        <span class="token comment"># 用于每一步后刷新环境状态</span>
        env<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token punctuation">)</span>    
        
        <span class="token comment"># 根据当前心目中预测的状态选行为</span>
        action <span class="token operator">=</span> RL<span class="token punctuation">.</span>choose_action<span class="token punctuation">(</span>observation<span class="token punctuation">)</span>  

        <span class="token comment"># 获取下一步的环境、得分、检测是否完成。</span>
        observation_<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span> 

        <span class="token comment"># 通过查找源代码发现observation_由以下四个特征构成。</span>
        <span class="token comment"># x 是车的水平位移</span>
        <span class="token comment"># theta 是棒子离垂直的角度</span>
        x<span class="token punctuation">,</span> x_dot<span class="token punctuation">,</span> theta<span class="token punctuation">,</span> theta_dot <span class="token operator">=</span> observation_   

        <span class="token comment"># r1代表车的 x水平位移 与 x最大边距 的距离差的得分</span>
        r1 <span class="token operator">=</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>x_threshold <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span>env<span class="token punctuation">.</span>x_threshold<span class="token punctuation">)</span> <span class="token operator">-</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span>
        <span class="token comment"># r2代表棒子的 theta离垂直的角度 与 theta最大角度 的差的得分</span>
        r2 <span class="token operator">=</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>theta_threshold_radians <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>theta<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span>env<span class="token punctuation">.</span>theta_threshold_radians<span class="token punctuation">)</span> <span class="token operator">-</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span>
        <span class="token comment"># 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度。</span>
        reward <span class="token operator">=</span> r1 <span class="token operator">+</span> r2   

        <span class="token comment"># 保存这一组记忆</span>
        RL<span class="token punctuation">.</span>store_transition<span class="token punctuation">(</span>observation<span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> observation_<span class="token punctuation">)</span>
        

        <span class="token comment"># 在数据具有一定规模后进行学习</span>
        <span class="token keyword">if</span> total_steps <span class="token operator">&gt;</span> <span class="token number">1000</span><span class="token punctuation">:</span>
            RL<span class="token punctuation">.</span>learn<span class="token punctuation">(</span><span class="token punctuation">)</span> 

        <span class="token comment">#记录总分</span>
        ep_r <span class="token operator">+=</span> reward
        <span class="token keyword">if</span> done<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'世代: '</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span><span class="token string">'。总分: '</span><span class="token punctuation">,</span> <span class="token builtin">round</span><span class="token punctuation">(</span>ep_r<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">break</span>
        
        <span class="token comment">#环境更新</span>
        observation <span class="token operator">=</span> observation_
        total_steps <span class="token operator">+=</span> <span class="token number">1</span>
        
<span class="token comment"># 最后输出 cost 曲线</span>
RL<span class="token punctuation">.</span>plot_cost<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p>RL_brain的代码可以从我的另一篇博文中得到：<br> <a href="https://blog.csdn.net/weixin_44791964/article/details/96422796" rel="nofollow" data-token="0d3f461fda28276df55a7978dcc8f986">https://blog.csdn.net/weixin_44791964/article/details/96422796</a><br> 细心的同学可能观察到我并没有直接使用</p> 
  <pre><code class="prism language-python">observation_<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span> 
</code></pre> 
  <p>返回的reward值，而是重新定义了。因为我试了直接用原来reward值效果并不好。我定义的reward值由两部分组成，分别是：</p> 
  <pre><code class="prism language-python">r1 <span class="token operator">=</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>x_threshold <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span>env<span class="token punctuation">.</span>x_threshold<span class="token punctuation">)</span> <span class="token operator">-</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span>
<span class="token comment"># r1代表车的 x水平位移 与 x最大边距 的距离差的得分</span>
r2 <span class="token operator">=</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>theta_threshold_radians <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>theta<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span>env<span class="token punctuation">.</span>theta_threshold_radians<span class="token punctuation">)</span> <span class="token operator">-</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span>
<span class="token comment"># r2代表棒子的 theta离垂直的角度 与 theta最大角度 的差的得分</span>
</code></pre> 
  <p>这些参数需要去查询CartPole的源文件才可以得到。<br> 我是通过在C:\Anaconda3\envs\tensorflow-gpu\Lib里搜索得到的。<br> 这个环境里有详细的描述，需要大家好好看看噢！<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721231204614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70" alt="CartPole的源文件"><br> 希望得到朋友们的喜欢。<br> <strong>有不懂的朋友可以评论询问噢。</strong></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
