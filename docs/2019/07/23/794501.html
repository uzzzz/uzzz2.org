<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Python网络爬虫实战：磁力种子搜索站爬虫（二） BT种子 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Python网络爬虫实战：磁力种子搜索站爬虫（二） BT种子" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="由于平时看个电影，各种找资源太费劲了，而且好多磁力资源网站的广告那叫一个多啊，到处都是坑。一个不小心就跳转到什么 澳门金冠赌场，什么 性感荷官在线发牌的网站，整的搜个资源跟玩扫雷一下，真实令人头大。 所以我找了一些比较靠谱的磁力种子网站《【资源搜集】最新的磁力种子搜索网站汇总（亲测可用）》，用爬虫技术调用他们的搜索接口，直接提取他们的搜索结果，省的花时间花精力跟它们网页上的各种广告各种链接斗智斗勇。 我找了 17 个磁力种子网站，算下来应该是 17 篇爬虫教程，这个应该算是磁力种子爬虫系列吧，哈！如果有对这个系列爬虫感兴趣的朋友，请关注我的博客哦。 今天要爬的网站是 BT种子（ https://btzzii.me/&nbsp;）。 &nbsp; &nbsp; 一、目标网站分析 按照惯例，先分析一波网页，伸手党如果懒得看，可以跳过直接去后面看代码实现部分。 有了上一篇《Python网络爬虫实战：磁力种子搜索站爬虫（一） BT蚂蚁》的分析，这个网站的爬取就要容易一些了，因为两个网站的页面结构，包含的数据，爬取的方式等，基本一致。 所以我们这个分析的部分比较简略的讲一下，详细的分析过程可以参考 BT蚂蚁爬虫 教程文章。 1. 页面中包含哪些数据以及如何获取 通过上面的截图我们可以得知，搜索结果列表页面中，我们可以获取到磁力种子的以下五个信息： 电影名称 时间，包括创建时间和最近下载时间 下载热度 文件大小 下载链接，包括磁力链接和迅雷链接 通过 F12 开发者工具，我们可以很快的定位到这些数据在网页中的存放位置，方便后续提取。 &nbsp;由截图可知， 每条磁力种子记录存放在一个 &lt;div class=&quot;cili-item&quot;&gt;&nbsp;&nbsp;下。其中， 电影名称存放在 &lt;div class=&quot;item-title&quot;&gt; / &lt;h3&gt; / &lt;a&gt; 中， 创建时间存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;span&gt;（第2个 span ）&nbsp;/ &lt;b&gt;&nbsp;中， 文件大小存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;span&gt;（第3个 span ）&nbsp;/ &lt;b&gt;&nbsp;中， 下载热度存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;span&gt;（第4个 span ）&nbsp;/ &lt;b&gt;&nbsp;中， 磁力链接存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;a&gt;（第1个 a ）中。 &nbsp; 2. URL 构造规则分析 搜索结果每页仅展示 10 条，要想获取所有的磁力种子，就必须搞清楚，它是怎么翻页的。 跟 BT蚂蚁 一样，BT种子 网站也是通过 URL 来实现翻页，以及搜索结果排序的。 URL 示例：&nbsp;https://btzzii.me/bt/战狼2/rela-1.html 构成规则 ：https://btzzii.me/bt/&lt;搜索关键字&gt;/&lt;排序方式&gt;-&lt;页码&gt;.html &nbsp;其中，搜索关键字，指的是你要搜索的电影名，如战狼2。 排序方式有四种，热门下载（默认，default），最新收录（time），文件大小（size），相关下载（rela）。 页码从1开始计数，直到最大页码。 &nbsp; 二、编码实现环节 依我的习惯，我将这个爬虫根据功能模块，拆分为了 5 个函数。 fetchURL 函数，用来发起网络请求， getPageNum 函数，用来获取搜索结果的总页码，从而限定翻页参数的范围 parseHTML 函数，用来解析网页，提取网页中的关键数据 saveData 函数，用来将提取出的数据存储至本地文件中 Main 函数，用来作为爬虫调度器。 相比于前面的 BT蚂蚁爬虫，这个爬虫主要改动的地方在于 getPageNum 函数，parseHTML 函数，和 main 函数。 毕竟不同的网站页面的具体解析方法会有些许的差异。 而其他部分，发起网络请求，保存数据的操作都是一样的，可以通用。 def parseHTML(html): &#39;&#39;&#39; 功能：解析网页，提取需要的数据 参数：目标网页的 html 源码 返回：需要提取的数据 &#39;&#39;&#39; #提取的数据有 title，date，hot，size，link bsobj = bs4.BeautifulSoup(html,&quot;html.parser&quot;) movieList = bsobj.find_all(&quot;div&quot;, attrs = {&quot;class&quot; :&quot;cili-item&quot;}) movieInfo = [] for item in movieList: movieItem = [] title = item.find(&quot;div&quot;, attrs = {&quot;class&quot;: &quot;item-title&quot;}).h3.a.text bar = item.find(&quot;div&quot;, attrs = {&quot;class&quot;: &quot;item-bar&quot;}).find_all(&quot;span&quot;) date = bar[1].b.text hot = bar[3].b.text size = bar[2].b.text link = item.find(&quot;div&quot;, attrs = {&quot;class&quot;: &quot;item-bar&quot;}).find_all(&quot;a&quot;)[0][&quot;href&quot;] movieItem.append(title) movieItem.append(date) movieItem.append(hot) movieItem.append(size) movieItem.append(link) movieInfo.append(movieItem) return movieInfo 获取总页码数这里我多说一句，因为 BT蚂蚁 网站是有 “最后一页” 这个选项的，所以我们可以直接从页面中获取它的最后一页链接，然后提取总页码数。 而 BT种子 网站没有尾页这个按钮，只有上一页和下一页，也就是说，我们没有办法直接得知最后一页是第几页。&nbsp; 不过机智的我很快找到了解决方法，在搜索结果顶部，有这么一条数据，显示一共搜索到多少条磁力链接，而我们之前有分析知道，每一页展示 10 条结果，所以。。。直接总条数除以10，向上取整即可得到总页数。 def getPageNum(html): &#39;&#39;&#39; 功能：获取总页码数 &#39;&#39;&#39; bsobj = bs4.BeautifulSoup(html,&quot;html.parser&quot;) pageText = bsobj.find(&quot;div&quot;, attrs = {&quot;id&quot; : &quot;wall&quot;}).find_all(&quot;div&quot;)[0].span.text itemNum = int(pageText.split(&quot; &quot;)[1]) page = int(itemNum / 10) + 1 return page 主函数这里控制整个爬虫的进程，由用户输入要搜索的电影名，然后根据规则构造初始 URL，获取搜索结果的总页数，然后循环爬取每一页的数据，将数据保存至本地 csv 文件中，直到爬取完成。 if __name__ == &#39;__main__&#39;: name = input(&quot;请输入要查找的电影名或番号：&quot;) url = &#39;https://btzzii.me/bt/&#39;+ name + &#39;/default-1.html&#39; html = fetchURL(url) page = getPageNum(html) print(page) path = &quot;Data/BT种子/&quot; filename = name + &quot;.csv&quot; for index in range(0,page): index += 1 print(&quot;-----------第&quot; + str(index) + &quot;页----------&quot;) url = &#39;https://btzzii.me/bt/&#39;+ name + &#39;/default-&#39; + str(index) + &#39;.html&#39; html = fetchURL(url) data = parseHTML(html) save_data(data,path,filename) 还是那句话，因为两个爬虫实在是太像了，唯一有点区别的地方也就是获取总页数的那儿。 所以我这儿没有赘述分析过程，也没有贴全部完整的代码（完整代码在 BT蚂蚁 那篇博客中已经有了，有需要的话自行把代码整合一下用），如果都这样了你还不会。。。。。。那在留言区评论一下，我手把手教你。 &nbsp; 爬取结果展示 &nbsp; &nbsp; &nbsp;三、交流答疑区 2019年8月9日更新 bt 种子的这个域名&nbsp;https://btzzii.me/&nbsp;好像变了，网站上不去了，导致爬虫也没有办法直接使用了。 新的域名我暂时还没有找到，如果读者朋友们，有知道 BT种子的最新网址的话，请评论区里留言，特别感谢。 上面的爬虫程序也并非完全不能用了，只需要在知道了 BT种子最新网址之后，替换代码中的网址&nbsp;https://btzzii.me/&nbsp;即可。 &nbsp; &nbsp; 如果文章中有哪里没有讲明白，或者讲解有误的地方，欢迎在评论区批评指正，或者扫描下面的二维码，加我微信，大家一起学习交流，共同进步。" />
<meta property="og:description" content="由于平时看个电影，各种找资源太费劲了，而且好多磁力资源网站的广告那叫一个多啊，到处都是坑。一个不小心就跳转到什么 澳门金冠赌场，什么 性感荷官在线发牌的网站，整的搜个资源跟玩扫雷一下，真实令人头大。 所以我找了一些比较靠谱的磁力种子网站《【资源搜集】最新的磁力种子搜索网站汇总（亲测可用）》，用爬虫技术调用他们的搜索接口，直接提取他们的搜索结果，省的花时间花精力跟它们网页上的各种广告各种链接斗智斗勇。 我找了 17 个磁力种子网站，算下来应该是 17 篇爬虫教程，这个应该算是磁力种子爬虫系列吧，哈！如果有对这个系列爬虫感兴趣的朋友，请关注我的博客哦。 今天要爬的网站是 BT种子（ https://btzzii.me/&nbsp;）。 &nbsp; &nbsp; 一、目标网站分析 按照惯例，先分析一波网页，伸手党如果懒得看，可以跳过直接去后面看代码实现部分。 有了上一篇《Python网络爬虫实战：磁力种子搜索站爬虫（一） BT蚂蚁》的分析，这个网站的爬取就要容易一些了，因为两个网站的页面结构，包含的数据，爬取的方式等，基本一致。 所以我们这个分析的部分比较简略的讲一下，详细的分析过程可以参考 BT蚂蚁爬虫 教程文章。 1. 页面中包含哪些数据以及如何获取 通过上面的截图我们可以得知，搜索结果列表页面中，我们可以获取到磁力种子的以下五个信息： 电影名称 时间，包括创建时间和最近下载时间 下载热度 文件大小 下载链接，包括磁力链接和迅雷链接 通过 F12 开发者工具，我们可以很快的定位到这些数据在网页中的存放位置，方便后续提取。 &nbsp;由截图可知， 每条磁力种子记录存放在一个 &lt;div class=&quot;cili-item&quot;&gt;&nbsp;&nbsp;下。其中， 电影名称存放在 &lt;div class=&quot;item-title&quot;&gt; / &lt;h3&gt; / &lt;a&gt; 中， 创建时间存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;span&gt;（第2个 span ）&nbsp;/ &lt;b&gt;&nbsp;中， 文件大小存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;span&gt;（第3个 span ）&nbsp;/ &lt;b&gt;&nbsp;中， 下载热度存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;span&gt;（第4个 span ）&nbsp;/ &lt;b&gt;&nbsp;中， 磁力链接存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;a&gt;（第1个 a ）中。 &nbsp; 2. URL 构造规则分析 搜索结果每页仅展示 10 条，要想获取所有的磁力种子，就必须搞清楚，它是怎么翻页的。 跟 BT蚂蚁 一样，BT种子 网站也是通过 URL 来实现翻页，以及搜索结果排序的。 URL 示例：&nbsp;https://btzzii.me/bt/战狼2/rela-1.html 构成规则 ：https://btzzii.me/bt/&lt;搜索关键字&gt;/&lt;排序方式&gt;-&lt;页码&gt;.html &nbsp;其中，搜索关键字，指的是你要搜索的电影名，如战狼2。 排序方式有四种，热门下载（默认，default），最新收录（time），文件大小（size），相关下载（rela）。 页码从1开始计数，直到最大页码。 &nbsp; 二、编码实现环节 依我的习惯，我将这个爬虫根据功能模块，拆分为了 5 个函数。 fetchURL 函数，用来发起网络请求， getPageNum 函数，用来获取搜索结果的总页码，从而限定翻页参数的范围 parseHTML 函数，用来解析网页，提取网页中的关键数据 saveData 函数，用来将提取出的数据存储至本地文件中 Main 函数，用来作为爬虫调度器。 相比于前面的 BT蚂蚁爬虫，这个爬虫主要改动的地方在于 getPageNum 函数，parseHTML 函数，和 main 函数。 毕竟不同的网站页面的具体解析方法会有些许的差异。 而其他部分，发起网络请求，保存数据的操作都是一样的，可以通用。 def parseHTML(html): &#39;&#39;&#39; 功能：解析网页，提取需要的数据 参数：目标网页的 html 源码 返回：需要提取的数据 &#39;&#39;&#39; #提取的数据有 title，date，hot，size，link bsobj = bs4.BeautifulSoup(html,&quot;html.parser&quot;) movieList = bsobj.find_all(&quot;div&quot;, attrs = {&quot;class&quot; :&quot;cili-item&quot;}) movieInfo = [] for item in movieList: movieItem = [] title = item.find(&quot;div&quot;, attrs = {&quot;class&quot;: &quot;item-title&quot;}).h3.a.text bar = item.find(&quot;div&quot;, attrs = {&quot;class&quot;: &quot;item-bar&quot;}).find_all(&quot;span&quot;) date = bar[1].b.text hot = bar[3].b.text size = bar[2].b.text link = item.find(&quot;div&quot;, attrs = {&quot;class&quot;: &quot;item-bar&quot;}).find_all(&quot;a&quot;)[0][&quot;href&quot;] movieItem.append(title) movieItem.append(date) movieItem.append(hot) movieItem.append(size) movieItem.append(link) movieInfo.append(movieItem) return movieInfo 获取总页码数这里我多说一句，因为 BT蚂蚁 网站是有 “最后一页” 这个选项的，所以我们可以直接从页面中获取它的最后一页链接，然后提取总页码数。 而 BT种子 网站没有尾页这个按钮，只有上一页和下一页，也就是说，我们没有办法直接得知最后一页是第几页。&nbsp; 不过机智的我很快找到了解决方法，在搜索结果顶部，有这么一条数据，显示一共搜索到多少条磁力链接，而我们之前有分析知道，每一页展示 10 条结果，所以。。。直接总条数除以10，向上取整即可得到总页数。 def getPageNum(html): &#39;&#39;&#39; 功能：获取总页码数 &#39;&#39;&#39; bsobj = bs4.BeautifulSoup(html,&quot;html.parser&quot;) pageText = bsobj.find(&quot;div&quot;, attrs = {&quot;id&quot; : &quot;wall&quot;}).find_all(&quot;div&quot;)[0].span.text itemNum = int(pageText.split(&quot; &quot;)[1]) page = int(itemNum / 10) + 1 return page 主函数这里控制整个爬虫的进程，由用户输入要搜索的电影名，然后根据规则构造初始 URL，获取搜索结果的总页数，然后循环爬取每一页的数据，将数据保存至本地 csv 文件中，直到爬取完成。 if __name__ == &#39;__main__&#39;: name = input(&quot;请输入要查找的电影名或番号：&quot;) url = &#39;https://btzzii.me/bt/&#39;+ name + &#39;/default-1.html&#39; html = fetchURL(url) page = getPageNum(html) print(page) path = &quot;Data/BT种子/&quot; filename = name + &quot;.csv&quot; for index in range(0,page): index += 1 print(&quot;-----------第&quot; + str(index) + &quot;页----------&quot;) url = &#39;https://btzzii.me/bt/&#39;+ name + &#39;/default-&#39; + str(index) + &#39;.html&#39; html = fetchURL(url) data = parseHTML(html) save_data(data,path,filename) 还是那句话，因为两个爬虫实在是太像了，唯一有点区别的地方也就是获取总页数的那儿。 所以我这儿没有赘述分析过程，也没有贴全部完整的代码（完整代码在 BT蚂蚁 那篇博客中已经有了，有需要的话自行把代码整合一下用），如果都这样了你还不会。。。。。。那在留言区评论一下，我手把手教你。 &nbsp; 爬取结果展示 &nbsp; &nbsp; &nbsp;三、交流答疑区 2019年8月9日更新 bt 种子的这个域名&nbsp;https://btzzii.me/&nbsp;好像变了，网站上不去了，导致爬虫也没有办法直接使用了。 新的域名我暂时还没有找到，如果读者朋友们，有知道 BT种子的最新网址的话，请评论区里留言，特别感谢。 上面的爬虫程序也并非完全不能用了，只需要在知道了 BT种子最新网址之后，替换代码中的网址&nbsp;https://btzzii.me/&nbsp;即可。 &nbsp; &nbsp; 如果文章中有哪里没有讲明白，或者讲解有误的地方，欢迎在评论区批评指正，或者扫描下面的二维码，加我微信，大家一起学习交流，共同进步。" />
<link rel="canonical" href="https://uzzz.org/2019/07/23/794501.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/23/794501.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-23T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"由于平时看个电影，各种找资源太费劲了，而且好多磁力资源网站的广告那叫一个多啊，到处都是坑。一个不小心就跳转到什么 澳门金冠赌场，什么 性感荷官在线发牌的网站，整的搜个资源跟玩扫雷一下，真实令人头大。 所以我找了一些比较靠谱的磁力种子网站《【资源搜集】最新的磁力种子搜索网站汇总（亲测可用）》，用爬虫技术调用他们的搜索接口，直接提取他们的搜索结果，省的花时间花精力跟它们网页上的各种广告各种链接斗智斗勇。 我找了 17 个磁力种子网站，算下来应该是 17 篇爬虫教程，这个应该算是磁力种子爬虫系列吧，哈！如果有对这个系列爬虫感兴趣的朋友，请关注我的博客哦。 今天要爬的网站是 BT种子（ https://btzzii.me/&nbsp;）。 &nbsp; &nbsp; 一、目标网站分析 按照惯例，先分析一波网页，伸手党如果懒得看，可以跳过直接去后面看代码实现部分。 有了上一篇《Python网络爬虫实战：磁力种子搜索站爬虫（一） BT蚂蚁》的分析，这个网站的爬取就要容易一些了，因为两个网站的页面结构，包含的数据，爬取的方式等，基本一致。 所以我们这个分析的部分比较简略的讲一下，详细的分析过程可以参考 BT蚂蚁爬虫 教程文章。 1. 页面中包含哪些数据以及如何获取 通过上面的截图我们可以得知，搜索结果列表页面中，我们可以获取到磁力种子的以下五个信息： 电影名称 时间，包括创建时间和最近下载时间 下载热度 文件大小 下载链接，包括磁力链接和迅雷链接 通过 F12 开发者工具，我们可以很快的定位到这些数据在网页中的存放位置，方便后续提取。 &nbsp;由截图可知， 每条磁力种子记录存放在一个 &lt;div class=&quot;cili-item&quot;&gt;&nbsp;&nbsp;下。其中， 电影名称存放在 &lt;div class=&quot;item-title&quot;&gt; / &lt;h3&gt; / &lt;a&gt; 中， 创建时间存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;span&gt;（第2个 span ）&nbsp;/ &lt;b&gt;&nbsp;中， 文件大小存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;span&gt;（第3个 span ）&nbsp;/ &lt;b&gt;&nbsp;中， 下载热度存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;span&gt;（第4个 span ）&nbsp;/ &lt;b&gt;&nbsp;中， 磁力链接存放在&nbsp;&lt;div class=&quot;item-bar&quot;&gt; / &lt;a&gt;（第1个 a ）中。 &nbsp; 2. URL 构造规则分析 搜索结果每页仅展示 10 条，要想获取所有的磁力种子，就必须搞清楚，它是怎么翻页的。 跟 BT蚂蚁 一样，BT种子 网站也是通过 URL 来实现翻页，以及搜索结果排序的。 URL 示例：&nbsp;https://btzzii.me/bt/战狼2/rela-1.html 构成规则 ：https://btzzii.me/bt/&lt;搜索关键字&gt;/&lt;排序方式&gt;-&lt;页码&gt;.html &nbsp;其中，搜索关键字，指的是你要搜索的电影名，如战狼2。 排序方式有四种，热门下载（默认，default），最新收录（time），文件大小（size），相关下载（rela）。 页码从1开始计数，直到最大页码。 &nbsp; 二、编码实现环节 依我的习惯，我将这个爬虫根据功能模块，拆分为了 5 个函数。 fetchURL 函数，用来发起网络请求， getPageNum 函数，用来获取搜索结果的总页码，从而限定翻页参数的范围 parseHTML 函数，用来解析网页，提取网页中的关键数据 saveData 函数，用来将提取出的数据存储至本地文件中 Main 函数，用来作为爬虫调度器。 相比于前面的 BT蚂蚁爬虫，这个爬虫主要改动的地方在于 getPageNum 函数，parseHTML 函数，和 main 函数。 毕竟不同的网站页面的具体解析方法会有些许的差异。 而其他部分，发起网络请求，保存数据的操作都是一样的，可以通用。 def parseHTML(html): &#39;&#39;&#39; 功能：解析网页，提取需要的数据 参数：目标网页的 html 源码 返回：需要提取的数据 &#39;&#39;&#39; #提取的数据有 title，date，hot，size，link bsobj = bs4.BeautifulSoup(html,&quot;html.parser&quot;) movieList = bsobj.find_all(&quot;div&quot;, attrs = {&quot;class&quot; :&quot;cili-item&quot;}) movieInfo = [] for item in movieList: movieItem = [] title = item.find(&quot;div&quot;, attrs = {&quot;class&quot;: &quot;item-title&quot;}).h3.a.text bar = item.find(&quot;div&quot;, attrs = {&quot;class&quot;: &quot;item-bar&quot;}).find_all(&quot;span&quot;) date = bar[1].b.text hot = bar[3].b.text size = bar[2].b.text link = item.find(&quot;div&quot;, attrs = {&quot;class&quot;: &quot;item-bar&quot;}).find_all(&quot;a&quot;)[0][&quot;href&quot;] movieItem.append(title) movieItem.append(date) movieItem.append(hot) movieItem.append(size) movieItem.append(link) movieInfo.append(movieItem) return movieInfo 获取总页码数这里我多说一句，因为 BT蚂蚁 网站是有 “最后一页” 这个选项的，所以我们可以直接从页面中获取它的最后一页链接，然后提取总页码数。 而 BT种子 网站没有尾页这个按钮，只有上一页和下一页，也就是说，我们没有办法直接得知最后一页是第几页。&nbsp; 不过机智的我很快找到了解决方法，在搜索结果顶部，有这么一条数据，显示一共搜索到多少条磁力链接，而我们之前有分析知道，每一页展示 10 条结果，所以。。。直接总条数除以10，向上取整即可得到总页数。 def getPageNum(html): &#39;&#39;&#39; 功能：获取总页码数 &#39;&#39;&#39; bsobj = bs4.BeautifulSoup(html,&quot;html.parser&quot;) pageText = bsobj.find(&quot;div&quot;, attrs = {&quot;id&quot; : &quot;wall&quot;}).find_all(&quot;div&quot;)[0].span.text itemNum = int(pageText.split(&quot; &quot;)[1]) page = int(itemNum / 10) + 1 return page 主函数这里控制整个爬虫的进程，由用户输入要搜索的电影名，然后根据规则构造初始 URL，获取搜索结果的总页数，然后循环爬取每一页的数据，将数据保存至本地 csv 文件中，直到爬取完成。 if __name__ == &#39;__main__&#39;: name = input(&quot;请输入要查找的电影名或番号：&quot;) url = &#39;https://btzzii.me/bt/&#39;+ name + &#39;/default-1.html&#39; html = fetchURL(url) page = getPageNum(html) print(page) path = &quot;Data/BT种子/&quot; filename = name + &quot;.csv&quot; for index in range(0,page): index += 1 print(&quot;-----------第&quot; + str(index) + &quot;页----------&quot;) url = &#39;https://btzzii.me/bt/&#39;+ name + &#39;/default-&#39; + str(index) + &#39;.html&#39; html = fetchURL(url) data = parseHTML(html) save_data(data,path,filename) 还是那句话，因为两个爬虫实在是太像了，唯一有点区别的地方也就是获取总页数的那儿。 所以我这儿没有赘述分析过程，也没有贴全部完整的代码（完整代码在 BT蚂蚁 那篇博客中已经有了，有需要的话自行把代码整合一下用），如果都这样了你还不会。。。。。。那在留言区评论一下，我手把手教你。 &nbsp; 爬取结果展示 &nbsp; &nbsp; &nbsp;三、交流答疑区 2019年8月9日更新 bt 种子的这个域名&nbsp;https://btzzii.me/&nbsp;好像变了，网站上不去了，导致爬虫也没有办法直接使用了。 新的域名我暂时还没有找到，如果读者朋友们，有知道 BT种子的最新网址的话，请评论区里留言，特别感谢。 上面的爬虫程序也并非完全不能用了，只需要在知道了 BT种子最新网址之后，替换代码中的网址&nbsp;https://btzzii.me/&nbsp;即可。 &nbsp; &nbsp; 如果文章中有哪里没有讲明白，或者讲解有误的地方，欢迎在评论区批评指正，或者扫描下面的二维码，加我微信，大家一起学习交流，共同进步。","@type":"BlogPosting","url":"https://uzzz.org/2019/07/23/794501.html","headline":"Python网络爬虫实战：磁力种子搜索站爬虫（二） BT种子","dateModified":"2019-07-23T00:00:00+08:00","datePublished":"2019-07-23T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/23/794501.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Python网络爬虫实战：磁力种子搜索站爬虫（二） BT种子</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>由于平时看个电影，各种找资源太费劲了，而且好多磁力资源网站的广告那叫一个多啊，到处都是坑。一个不小心就跳转到什么 澳门金冠赌场，什么 性感荷官在线发牌的网站，整的搜个资源跟玩扫雷一下，真实令人头大。</p> 
  <p>所以我找了一些比较靠谱的磁力种子网站《<a href="https://blog.csdn.net/wenxuhonghe/article/details/95492491" rel="nofollow" data-token="ea250a48494ac136c147d67e10eae4fa">【资源搜集】最新的磁力种子搜索网站汇总（亲测可用）</a>》，用爬虫技术调用他们的搜索接口，直接提取他们的搜索结果，省的花时间花精力跟它们网页上的各种广告各种链接斗智斗勇。</p> 
  <p>我找了 17 个磁力种子网站，算下来应该是 17 篇爬虫教程，这个应该算是磁力种子爬虫系列吧，哈！如果有对这个系列爬虫感兴趣的朋友，请关注我的博客哦。</p> 
  <hr>
  <p>今天要爬的网站是 BT种子（ <a href="https://btzzii.me/" rel="nofollow" data-token="c2554e2e33827b632ab58a7a098b3c8f">https://btzzii.me/</a>&nbsp;）。</p> 
  <p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019071915513649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlbnh1aG9uZ2hl,size_16,color_FFFFFF,t_70"></p> 
  <p style="text-align:center;"><img alt="" class="has" height="422" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190719155232230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlbnh1aG9uZ2hl,size_16,color_FFFFFF,t_70" width="886"></p> 
  <p>&nbsp;</p> 
  <hr>
  <h1>&nbsp;</h1> 
  <h1>一、目标网站分析</h1> 
  <p>按照惯例，先分析一波网页，伸手党如果懒得看，可以跳过直接去后面看代码实现部分。</p> 
  <p>有了上一篇《<a href="https://blog.csdn.net/wenxuhonghe/article/details/95638266" rel="nofollow" data-token="fd0521b89e72b2751bbd73149a252bf3">Python网络爬虫实战：磁力种子搜索站爬虫（一） BT蚂蚁</a>》的分析，这个网站的爬取就要容易一些了，因为两个网站的页面结构，包含的数据，爬取的方式等，基本一致。</p> 
  <p>所以我们这个分析的部分比较简略的讲一下，详细的分析过程可以参考 <a href="https://blog.csdn.net/wenxuhonghe/article/details/95638266" rel="nofollow" data-token="fd0521b89e72b2751bbd73149a252bf3">BT蚂蚁爬虫</a> 教程文章。</p> 
  <h3>1. 页面中包含哪些数据以及如何获取</h3> 
  <p>通过上面的截图我们可以得知，搜索结果列表页面中，我们可以获取到磁力种子的以下五个信息：</p> 
  <ul>
   <li>电影名称</li> 
   <li>时间，包括创建时间和最近下载时间</li> 
   <li>下载热度</li> 
   <li>文件大小</li> 
   <li>下载链接，包括磁力链接和迅雷链接</li> 
  </ul>
  <p>通过 F12 开发者工具，我们可以很快的定位到这些数据在网页中的存放位置，方便后续提取。</p> 
  <p style="text-align:center;"><img alt="" class="has" height="329" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019072310033617.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlbnh1aG9uZ2hl,size_16,color_FFFFFF,t_70" width="892"></p> 
  <p>&nbsp;由截图可知，</p> 
  <p>每条磁力种子记录存放在一个 <em><span style="color:#3399ea;">&lt;div class="cili-item"&gt;&nbsp;&nbsp;</span></em>下。其中，</p> 
  <p>电影名称存放在 <span style="color:#3399ea;"><em>&lt;div class="item-title"&gt; </em></span><em>/</em><span style="color:#3399ea;"><em> &lt;h3&gt; </em></span><em>/ </em><span style="color:#3399ea;"><em>&lt;a&gt;</em></span> 中，</p> 
  <p>创建时间存放在&nbsp;<span style="color:#3399ea;"><em>&lt;div class="item-bar"&gt; </em></span><em>/</em><span style="color:#3399ea;"><em> &lt;span&gt;</em>（第2个 span ）<em>&nbsp;</em></span><em>/ </em><span style="color:#3399ea;"><em>&lt;b&gt;</em></span>&nbsp;中，</p> 
  <p>文件大小存放在&nbsp;<span style="color:#3399ea;"><em>&lt;div class="item-bar"&gt; </em></span><em>/</em><span style="color:#3399ea;"><em> &lt;span&gt;</em>（第3个 span ）<em>&nbsp;</em></span><em>/ </em><span style="color:#3399ea;"><em>&lt;b&gt;</em></span>&nbsp;中，</p> 
  <p>下载热度存放在&nbsp;<span style="color:#3399ea;"><em>&lt;div class="item-bar"&gt; </em></span><em>/</em><span style="color:#3399ea;"><em> &lt;span&gt;</em>（第4个 span ）<em>&nbsp;</em></span><em>/ </em><span style="color:#3399ea;"><em>&lt;b&gt;</em></span>&nbsp;中，</p> 
  <p>磁力链接存放在&nbsp;<span style="color:#3399ea;"><em>&lt;div class="item-bar"&gt; </em></span><em>/</em><span style="color:#3399ea;"><em> &lt;a&gt;</em>（第1个 a ）</span>中。</p> 
  <p>&nbsp;</p> 
  <h3>2. URL 构造规则分析</h3> 
  <p>搜索结果每页仅展示 10 条，要想获取所有的磁力种子，就必须搞清楚，它是怎么翻页的。</p> 
  <p>跟 BT蚂蚁 一样，BT种子 网站也是通过 URL 来实现翻页，以及搜索结果排序的。</p> 
  <blockquote> 
   <p>URL 示例：&nbsp;<a href="https://btzzii.me/bt/%E6%88%98%E7%8B%BC2/rela-1.html" rel="nofollow" data-token="3c5cdb5df58c14984b8e18b520a1bb5f">https://btzzii.me/bt/战狼2/rela-1.html</a></p> 
   <p>构成规则 ：https://btzzii.me/bt/&lt;<span style="color:#3399ea;">搜索关键字</span>&gt;/&lt;<span style="color:#3399ea;">排序方式</span>&gt;-&lt;<span style="color:#3399ea;">页码</span>&gt;.html</p> 
  </blockquote> 
  <p>&nbsp;其中，搜索关键字，指的是你要搜索的电影名，如战狼2。</p> 
  <p>排序方式有四种，热门下载（默认，default），最新收录（time），文件大小（size），相关下载（rela）。</p> 
  <p>页码从1开始计数，直到最大页码。</p> 
  <p>&nbsp;</p> 
  <h1>二、编码实现环节</h1> 
  <p>依我的习惯，我将这个爬虫根据功能模块，拆分为了 5 个函数。</p> 
  <ul>
   <li>fetchURL 函数，用来发起网络请求，</li> 
   <li>getPageNum 函数，用来获取搜索结果的总页码，从而限定翻页参数的范围</li> 
   <li>parseHTML 函数，用来解析网页，提取网页中的关键数据</li> 
   <li>saveData 函数，用来将提取出的数据存储至本地文件中</li> 
   <li>Main 函数，用来作为爬虫调度器。</li> 
  </ul>
  <p>相比于前面的 BT蚂蚁爬虫，这个爬虫主要改动的地方在于 getPageNum 函数，parseHTML 函数，和 main 函数。</p> 
  <p>毕竟不同的网站页面的具体解析方法会有些许的差异。</p> 
  <p>而其他部分，发起网络请求，保存数据的操作都是一样的，可以通用。</p> 
  <pre class="has">
<code class="language-python">def parseHTML(html):
    '''
    功能：解析网页，提取需要的数据
    参数：目标网页的 html 源码
    返回：需要提取的数据
    '''
    #提取的数据有 title，date，hot，size，link
    
    bsobj = bs4.BeautifulSoup(html,"html.parser")
    movieList = bsobj.find_all("div", attrs = {"class" :"cili-item"})
    
    movieInfo = []

    for item in movieList:      
        movieItem = []
        
        title = item.find("div", attrs = {"class": "item-title"}).h3.a.text
        bar = item.find("div", attrs = {"class": "item-bar"}).find_all("span")
        date = bar[1].b.text
        hot = bar[3].b.text
        size = bar[2].b.text        
        link = item.find("div", attrs = {"class": "item-bar"}).find_all("a")[0]["href"]
        
        movieItem.append(title)
        movieItem.append(date)
        movieItem.append(hot)
        movieItem.append(size)
        movieItem.append(link)
        movieInfo.append(movieItem)

    return movieInfo</code></pre> 
  <p>获取总页码数这里我多说一句，因为 BT蚂蚁 网站是有 “最后一页” 这个选项的，所以我们可以直接从页面中获取它的最后一页链接，然后提取总页码数。</p> 
  <p>而 BT种子 网站没有尾页这个按钮，只有上一页和下一页，也就是说，我们没有办法直接得知最后一页是第几页。&nbsp;</p> 
  <p>不过机智的我很快找到了解决方法，在搜索结果顶部，有这么一条数据，显示一共搜索到多少条磁力链接，而我们之前有分析知道，每一页展示 10 条结果，所以。。。直接总条数除以10，向上取整即可得到总页数。</p> 
  <p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190723104425251.png"></p> 
  <pre class="has">
<code class="language-python">def getPageNum(html):
    '''
    功能：获取总页码数
    '''    
    bsobj = bs4.BeautifulSoup(html,"html.parser")
    pageText = bsobj.find("div", attrs = {"id" : "wall"}).find_all("div")[0].span.text
    itemNum = int(pageText.split(" ")[1])
    page = int(itemNum / 10) + 1
    return page</code></pre> 
  <p>主函数这里控制整个爬虫的进程，由用户输入要搜索的电影名，然后根据规则构造初始 URL，获取搜索结果的总页数，然后循环爬取每一页的数据，将数据保存至本地 csv 文件中，直到爬取完成。</p> 
  <pre class="has">
<code class="language-python">if __name__ == '__main__':
    
    name = input("请输入要查找的电影名或番号：")
    url = 'https://btzzii.me/bt/'+ name + '/default-1.html'
    html = fetchURL(url)
    page = getPageNum(html)
    print(page)
    
    path = "Data/BT种子/"
    filename = name + ".csv"
    
    for index in range(0,page):
        index += 1
        print("-----------第" + str(index) + "页----------")
        url = 'https://btzzii.me/bt/'+ name + '/default-' + str(index) + '.html'
        html = fetchURL(url)
        data = parseHTML(html)
        save_data(data,path,filename)</code></pre> 
  <p>还是那句话，因为两个爬虫实在是太像了，唯一有点区别的地方也就是获取总页数的那儿。</p> 
  <p>所以我这儿没有赘述分析过程，也没有贴全部完整的代码（完整代码在 BT蚂蚁 那篇博客中已经有了，有需要的话自行把代码整合一下用），如果都这样了你还不会。。。。。。那在留言区评论一下，我手把手教你。</p> 
  <p>&nbsp;</p> 
  <h3>爬取结果展示</h3> 
  <p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190723105236660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlbnh1aG9uZ2hl,size_16,color_FFFFFF,t_70"></p> 
  <p>&nbsp;</p> 
  <p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190723105343480.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlbnh1aG9uZ2hl,size_16,color_FFFFFF,t_70"></p> 
  <p>&nbsp;</p> 
  <h1>&nbsp;三、交流答疑区</h1> 
  <hr>
  <p><strong>2019年8月9日更新</strong></p> 
  <p>bt 种子的这个域名&nbsp;<a href="https://btzzii.me/" rel="nofollow" data-token="c2554e2e33827b632ab58a7a098b3c8f">https://btzzii.me/</a>&nbsp;好像变了，网站上不去了，导致爬虫也没有办法直接使用了。</p> 
  <p>新的域名我暂时还没有找到，如果读者朋友们，有知道 BT种子的最新网址的话，请评论区里留言，特别感谢。</p> 
  <p>上面的爬虫程序也并非完全不能用了，只需要在知道了 BT种子最新网址之后，替换代码中的网址&nbsp;<a href="https://btzzii.me/" rel="nofollow" data-token="c2554e2e33827b632ab58a7a098b3c8f">https://btzzii.me/</a>&nbsp;即可。</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>如果文章中有哪里没有讲明白，或者讲解有误的地方，欢迎在评论区批评指正，或者扫描下面的二维码，加我微信，大家一起学习交流，共同进步。</p> 
  <p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181218160157952.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlbnh1aG9uZ2hl,size_16,color_FFFFFF,t_70"></p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
