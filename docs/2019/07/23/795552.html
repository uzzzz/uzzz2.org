<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>ICCV 2019 ICCV 2019 论文接收列表 ICCV 2019一共接收1077篇 共4303篇投稿 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="ICCV 2019 ICCV 2019 论文接收列表 ICCV 2019一共接收1077篇 共4303篇投稿" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Github持续更新（8月29日更新200篇）： （论文下载百度云见文末） https://github.com/Sophia-11/Awesome-ICCV2019&nbsp; 2019/07/26 * - 更新28篇IIAI录用论文 2019/07/28 * - 更新11篇旷视ICCV2019 2019/08/28 * - 更新31篇Oral 2019/08/29 * - 增加116篇ICCV2019文章 2019/08/29 * - 增加35篇包含开源代码的ICCV2019 Table of Contents ICCV简介 ICCV录用编号 起源人工智能研究院28篇 旷视11篇 2019ICCVOral31篇 增加116篇ICCV2019文章 增加35篇包含开源代码的ICCV2019 ICCV 简介 ICCV 的全称是 IEEE International Conference on Computer Vision，即国际计算机视觉大会，由IEEE主办，与计算机视觉模式识别会议（CVPR）和欧洲计算机视觉会议（ECCV）并称计算机视觉方向的三大顶级会议，被澳大利亚ICT学术会议排名和中国计算机学会等机构评为最高级别学术会议，在业内具有极高的评价。 不同于在美国每年召开一次的CVPR和只在欧洲召开的ECCV，ICCV在世界范围内每两年召开一次。ICCV论文录用率非常低，是三大会议中公认级别最高的 。上一届提交的论文中，其中621篇被接收，录用比例达 28.9%；其中 poster、spotlight、oral 的比例分别为 24.61%、2.61% 以及 2.09%。 ICCV主席 今年有一名大会主席是来自香港中文大学的信息工程系系主任汤晓鸥，他同时还是中国科学院深圳先进技术研究院的副院长兼商汤科技创始人。其他三名大会主席则分别是首尔大学的 Kyoung Mu Lee 教授、伊利诺伊大学厄巴纳-香槟分校的 David Forsyth 教授以及苏黎世联邦理工学院的 Marc Pollefeys 教授。 召开地点 本届大会最终的递交补充材料的截止日期为 3 月 29 日。大会召开时间为2019年10月27日至11月2日，举行地点是韩国首尔的 COEX 会议中心。 刚刚，计算机视觉三大顶会之一ICCV2019终于公布了它的最终论文接收结果，一共有1077篇论文被接收，接收率为25.02% 论文接收序号： 24 17 25 30 31 33 37 41 45 49 59 60 69 84 91 93 102 105 110 126 141 153 157 159 171 175 176 178 184 187 213 226 229 238 242 245 247 251 258 281 285 294 302 320 330 350 351 354 356 361 367 375 376 380 382 383 385 397 405 406 407 409 421 426 428 445 446 450 456 464 466 479 490 491 496 502 504 507 508 520 531 539 568 579 582 585 596 609 620 622 630 634 636 668 669 673 678 680 691 701 706 711 715 719 720 722 727 732 733 735 737 742 751 754 756 759 767 768 770 774 778 782 791 797 800 806 809 811 813 818 827 832 836 838 839 845 855 862 868 876 877 879 888 890 892 899 900 902 904 905 909 912 917 920 921 940 943 959 964 965 976 981 989 1001 1005 1006 1011 1017 1020 1023 1031 1032 1039 1040 1042 1045 1046 1057 1062 1067 1077 1083 1092 1093 1096 1097 1098 1104 1105 1111 1112 1113 1119 1135 1139 1142 1148 1160 1163 1165 1166 1168 1174 1180 1182 1197 1200 1205 1206 1211 1215 1223 1233 1245 1249 1252 1272 1277 1285 1288 1291 1323 1330 1334 1335 1342 1343 1356 1370 1378 1381 1384 1390 1394 1395 1403 1404 1406 1411 1412 1417 1422 1426 1428 1434 1439 1442 1452 1455 1457 1463 1477 1479 1485 1488 1501 1517 1527 1535 1538 1542 1550 1551 1552 1562 1565 1570 1574 1581 1583 1585 1586 1590 1592 1596 1597 1616 1621 1624 1630 1638 1639 1642 1643 1647 1648 1650 1652 1656 1657 1667 1672 1675 1681 1693 1700 1705 1706 1714 1743 1746 1768 1772 1773 1774 1779 1785 1788 1805 1811 1819 1820 1823 1826 1827 1829 1844 1850 1854 1855 1859 1860 1861 1863 1865 1866 1870 1874 1879 1881 1882 1911 1917 1919 1924 1926 1933 1942 1943 1959 1960 1963 1967 1970 1971 1972 1982 1983 1984 1990 2005 2010 2012 2017 2024 2029 2032 2037 2040 2043 2055 2065 2070 2077 2097 2101 2115 2126 2127 2132 2134 2140 2148 2149 2155 2157 2160 2163 2169 2177 2179 2205 2206 2209 2214 2223 2230 2235 2240 2245 2246 2247 2248 2259 2266 2267 2272 2275 2277 2282 2284 2286 2288 2289 2290 2291 2303 2304 2312 2322 2323 2336 2337 2338 2339 2344 2353 2355 2359 2385 2390 2391 2392 2397 2402 2406 2413 2419 2420 2421 2436 2437 2441 2448 2450 2454 2458 2470 2473 2478 2481 2490 2495 2498 2501 2511 2517 2521 2525 2531 2545 2547 2548 2551 2553 2555 2556 2557 2561 2563 2564 2571 2578 2580 2595 2601 2603 2607 2608 2609 2610 2613 2615 2619 2622 2633 2634 2637 2638 2642 2660 2661 2679 2683 2684 2690 2717 2725 2732 2739 2740 2768 2790 2792 2795 2796 2798 2799 2814 2820 2830 2833 2836 2838 2840 2842 2850 2855 2857 2862 2865 2872 2886 2899 2908 2912 2919 2927 2928 2939 2944 2957 2958 2962 2963 2964 2968 2979 2980 3001 3016 3034 3035 3036 3051 3058 3059 3060 3068 3072 3080 3095 3102 3104 3107 3110 3114 3116 3120 3123 3127 3128 3133 3136 3137 3139 3140 3141 3145 3151 3154 3164 3166 3172 3180 3185 3193 3197 3198 3203 3215 3220 3222 3233 3239 3242 3243 3246 3260 3272 3273 3280 3281 3286 3290 3293 3300 3315 3321 3326 3327 3339 3345 3346 3352 3359 3361 3372 3375 3378 3379 3380 3382 3391 3394 3398 3402 3403 3410 3419 3430 3435 3436 3438 3439 3443 3458 3462 3463 3464 3468 3476 3489 3492 3494 3496 3502 3505 3508 3510 3514 3518 3521 3523 3540 3544 3547 3548 3552 3554 3555 3556 3559 3571 3572 3589 3592 3593 3596 3609 3611 3618 3620 3622 3627 3632 3636 3638 3646 3652 3655 3658 3662 3665 3667 3670 3674 3676 3682 3693 3695 3700 3717 3718 3723 3729 3734 3735 3739 3740 3743 3749 3750 3758 3761 3762 3767 3768 3772 3786 3787 3788 3795 3807 3808 3813 3818 3821 3824 3832 3834 3838 3857 3860 3867 3869 3879 3882 3897 3919 3921 3923 3926 3932 3933 3937 3941 3942 3949 3964 3971 3987 3988 3992 3998 4006 4007 4009 4019 4021 4022 4024 4032 4033 4034 4042 4047 4057 4067 4075 4079 4085 4088 4090 4092 4093 4094 4097 4102 4105 4112 4113 4118 4121 4122 4124 4125 4130 4144 4151 4154 4159 4162 4164 4167 4168 4171 4176 4192 4194 4199 4211 4212 4217 4237 4245 4246 4248 4249 4253 4267 4275 4285 4289 4293 4305 4309 4311 4330 4341 4342 4343 4346 4365 4366 4367 4370 4374 4406 4410 4414 4428 4430 4434 4446 4449 4453 4481 4485 4500 4506 4509 4526 4530 4533 4534 4541 4549 4560 4562 4563 4576 4585 4599 4600 4602 4614 4618 4634 4647 4649 4660 4666 4672 4690 4697 4701 4702 4712 4721 4737 4757 4765 4766 4768 4785 4787 4794 4798 4811 4825 4835 4846 4848 4851 4856 4861 4865 4870 4874 4881 4890 4901 4903 4910 4925 4928 4943 4946 4971 4996 5005 5008 5011 5016 5018 5023 5029 5051 5052 5053 5062 5073 5088 5099 5103 5105 5112 5114 5116 5127 5128 5129 5131 5135 5136 5148 5158 5161 5162 5164 5171 5172 5174 5180 5183 5184 5195 5196 5201 5215 5223 5235 5264 5269 5274 5280 5290 5292 5296 5301 5302 5314 5321 5323 5338 5344 5348 5370 5378 5384 5393 5412 5413 5417 5423 5437 5444 5454 5455 5457 5465 5519 5532 5540 5548 5576 5582 5594 5601 5626 5649 5651 5657 5662 5672 5683 5684 5696 5698 5700 5704 5705 5725 5728 5742 5752 5797 5801 5810 5819 5823 5827 5844 5845 5853 5863 5869 5880 5892 5903 5925 5927 5935 5948 5950 5952 5957 5961 5968 6009 6021 6026 6034 6035 6036 6072 6083 6105 6132 6174 6175 6178 6191 6204 6209 6215 6221 6232 6250 6258 6267 6284 6287 6289 6294 6296 6302 6328 6329 6352 6367 6372 6379 6385 6398 6400 6403 6404 6405 6410 6414 6423 6428 6430 6433 6467 6471 6480 6483 6496 6506 6512 6519 6521 6529 6532 6534 6554 6563 6568 6578 6579 6597 6602 6608 6622 6625 6640 6668 6691 6696 6700 6740 6744 6752 6780 6783 6829 6886 6887 6929 6944 6968 6978 6981 起源人工智能研究院 - Inception Institute of Artificial Intelligence (IIAI) 28篇论文 IIAI主页：www.inceptioniai.org/ Unsupervised Video Object Segmentation via Attentive Graph Neural Networks DUAL-GLOWs: Conditional Flow-Based Generative Models for Inter-Modality Transfer in Brain Imaging Unsupervised Graph Association for Person Re-identification Relational Attention Network for Crowd Counting Attentional Neural Fields for Crowd Counting Learning Compositional Neural Information Fusion for Human Parsing RANet: Ranking Attention Network for Fast Video Object Segmentation Learning to Mask Visible Regions for Occluded Pedestrian Detection Boosted Feature Guided Refinement Network for Single-Shot Detection Deep Contextual Attention for Human-Object Interaction Detection Learning the Model Update for Siamese Trackers 3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization Learning Rich Features at High-Speed for Single-Shot Object Detection Transductive learning for zero-shot object detection Ground-to-aerial Image Geo-localization with a Hard Exemplar Reweighting Triplet Loss Towards Bridging Semantic Gap to Improve Semantic Segmentation Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks Motion Deblurring via Human-Aware Attention Network Gaussian Affinity for Max-margin Class Imbalanced Learning A Deep Step Pattern Representation for Multimodal Retinal Image Registration SegEQA: Video Segmentation based Visual Attention for Embodied Question Answering Reciprocal Multi-Layer Subspace Learning for Multi-View Clustering Scoot: A Perceptual Metric for Facial Sketches EGNet: Edge Guidance Network for Salient Object Detection PointAE: Point Auto-encoder for 3D Statistical Shape and Texture Modelling Understanding Human Gaze Communication by Spatio-temporal Graph Reasoning Optimizing the F-measure for Threshold-free Salient Object Detection SynDeMo: Synergistic Deep Feature Alignment for Joint Learning of Depth and Ego-Motion 旷视研究院 11 篇论文入选 ICCV 2019 1、Objects365: A Large-scale, High-quality Dataset for Object Detection 2、ThunderNet: Towards Real-time Generic Object Detection 3、Efficient and Accurate Arbitrary-Shaped Text Detection with PixelAggregation Network 4、Semi-supervised Skin Detection by Network with Mutual Guidance 5、Semi-Supervised Video Salient Object Detection Using Pseudo-Labels 6、Disentangled Image Matting 7、Re-ID Driven Localization Refinement for Person Search 8、Vehicle Re-identification with Viewpoint-aware Metric Learning 9、MetaPruning: Meta Learning for Automatic Neural Network ChannelPruning 10、Symmetry-constrained Rectification Network for Scene Text Recognition 11、Learning to Paint with Model-based Deep Reinforcement Learning 2019 ICCV Oral https://arxiv.org/abs/1908.00382 Interpolated Convolutional Networks for 3D Point Cloud Understanding https://arxiv.org/abs/1908.04512 Memory-Based Neighbourhood Embedding for Visual Recognition https://arxiv.org/abs/1908.04992 Learning Trajectory Dependencies for Human Motion Prediction https://arxiv.org/abs/1908.05436 Domain Adaptation for Structured Output via Discriminative Patch Representations https://arxiv.org/abs/1901.05427 Deep Non-Rigid Structure from Motion https://arxiv.org/abs/1908.00052 Scalable Place Recognition Under Appearance Change for Autonomous Driving https://arxiv.org/abs/1908.00178 Restoration of Non-rigidly Distorted Underwater Images using a Combination of Compressive Sensing and Local Polynomial Image Representations https://arxiv.org/abs/1908.01940 Consensus Maximization Tree Search Revisited https://arxiv.org/abs/1908.02021 Weakly Supervised Energy-Based Learning for Action Segmentation Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification https://arxiv.org/abs/1811.10144 Controllable Artistic Text Style Transfer via Shape-Matching GAN https://arxiv.org/abs/1905.01354 Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition https://arxiv.org/abs/1907.13369 Expectation-Maximization Attention Networks for Semantic Segmentation https://arxiv.org/abs/1907.13426 VideoBERT: A Joint Model for Video and Language Representation Learning https://arxiv.org/abs/1904.01766 CARAFE: Content-Aware ReAssembly of FEatures https://arxiv.org/pdf/1905.02188.pdf Habitat: A Platform for Embodied AI Research https://arxiv.org/abs/1904.01201 Equivariant Multi-View Networks https://arxiv.org/abs/1904.00993 PointFlow : 3D Point Cloud Generation with Continuous Normalizing Flows https://arxiv.org/abs/1906.12320 Learnable Triangulation of Human Pose https://arxiv.org/abs/1905.05754 Learning Implicit Generative Models by Matching Perceptual Features https://arxiv.org/abs/1904.02762v1 COCO-GAN: Generation by Parts via Conditional Coordinating https://arxiv.org/abs/1904.00284 SlowFast Networks for Video Recognition https://arxiv.org/abs/1812.03982 Exploring Randomly Wired Neural Networks for Image Recognition https://arxiv.org/abs/1904.01569 Can GCNs Go as Deep as CNNs? https://arxiv.org/abs/1904.03751 Deep SR-ITM: Joint Learning of Super-resolution and Inverse Tone-Mapping for 4K UHD HDR Applications https://arxiv.org/abs/1904.11176 Meta-Sim Learning to Generate Synthetic Datasets https://arxiv.org/abs/1904.11621 Deep HoughVoting for 3D Object Detection in Point Clouds https://arxiv.org/abs/1904.09664 Variational Adversarial Active Learning https://arxiv.org/abs/1904.00370 Towards Unconstrained End-to-End Text Spotting https://arxiv.org/abs/1908.09231 Non-local Recurrent Neural Memory for Supervised Sequence Modeling https://arxiv.org/abs/1908.09535 Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels https://arxiv.org/abs/1908.09597 &nbsp; 增加116篇ICCV2019文章 Similarity-Preserving Knowledge Distillation https://arxiv.org/abs/1907.09682 GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition https://arxiv.org/abs/1907.09653 Tell, Draw, and Repeat: Generating and modifying images based on continual linguistic instruction https://arxiv.org/pdf/1811.09845.pdf Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers https://arxiv.org/abs/1904.08489 nocaps: novel object captioning at scale https://arxiv.org/abs/1812.08658 ThunderNet: Towards Real-time Generic Object Detection https://arxiv.org/abs/1903.11752 Scene GraphPrediction with Limited Labels https://arxiv.org/abs/1904.11622 Ego-Pose Estimation and Forecasting as Real-Time PD Control https://arxiv.org/abs/1906.03173 The Trajectron: Probabilistic Multi-Agent Trajectory Modeling withDynamic Spatiotemporal Graphs https://arxiv.org/abs/1810.05993 End-to-End Learning of Representations for Asynchronous Event-BasedData https://arxiv.org/abs/1904.08245 Efficient Learning on Point Clouds with Basis Point Sets https://arxiv.org/abs/1908.09186 Dynamic Kernel Distillation for Efficient Pose Estimation in Videos https://arxiv.org/abs/1908.09216 Single-Stage Multi-Person Pose Machines https://arxiv.org/abs/1908.09220 Towards Unsupervised Image Captioning with Shared Multimodal Embeddings https://arxiv.org/abs/1908.09317 advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns https://arxiv.org/abs/1908.09327 Shape-Aware Human Pose and Shape Reconstruction Using Multi-View Images https://arxiv.org/abs/1908.09464 Relation Distillation Networks for Video Object Detection https://arxiv.org/abs/1908.09511 Object-Driven Multi-Layer Scene Decomposition From a Single Image https://arxiv.org/abs/1908.09521 Embarrassingly Simple Binary Representation Learning https://arxiv.org/abs/1908.09573 Moulding Humans: Non-parametric 3D Human Shape Estimation from Single Images https://arxiv.org/abs/1908.00439 Learning the Model Update for Siamese Trackers https://arxiv.org/abs/1908.00855 Distilling Knowledge From a Deep Pose Regressor Network https://arxiv.org/abs/1908.00858 Permutation-invariant Feature Restructuring for Correlation-aware Image Set-based Recognition https://arxiv.org/abs/1908.01174 ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal https://arxiv.org/abs/1908.01323 Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation https://arxiv.org/abs/1908.01491 View N-gram Network for 3D Object Retrieval https://arxiv.org/abs/1908.01958 Semi-supervised Skin Detection by Network with Mutual Guidance https://arxiv.org/abs/1908.01977 Deep Self-Learning From Noisy Labels https://arxiv.org/abs/1908.02160 Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking https://arxiv.org/abs/1908.02231 Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning https://arxiv.org/abs/1908.02441 Expert Sample Consensus Applied to Camera Re-Localization https://arxiv.org/abs/1908.02484 SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition https://arxiv.org/abs/1908.02660 GP2C: Geometric Projection Parameter Consensus for Joint 3D Pose and Focal Length Estimation in the Wild https://arxiv.org/abs/1908.02809 SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences https://arxiv.org/abs/1904.01416 Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction https://arxiv.org/abs/1907.12704 Orientation-aware Semantic Segmentation on Icosahedron Spheres https://arxiv.org/abs/1907.12849 EMPNet: Neural Localisation and Mapping using Embedded Memory Points https://arxiv.org/abs/1907.13268 SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation https://arxiv.org/abs/1907.11308 On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method https://arxiv.org/abs/1907.11684 Goal-Driven Sequential Data Abstraction https://arxiv.org/abs/1907.12336 Recursive Cascaded Networks for Unsupervised Medical Image Registration https://arxiv.org/abs/1907.12353 Learn to Scale: Generating Multipolar Normalized Density Map for Crowd Counting https://arxiv.org/abs/1907.12428 HoloGAN: Unsupervised learning of 3D representations from natural images https://arxiv.org/abs/1904.01326 MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning https://arxiv.org/abs/1903.10258 FrameNet: Learning Local Canonical Frames of 3D Surfaces from a Single RGB Image https://arxiv.org/pdf/1903.12305.pdf Face De-occlusion using 3D Morphable Model and Generative Adversarialhttp://image.inha.ac.kr/paper/ICCV2019_Xaiowei.pdf Deep Meta Learning for Real-Time Target-Aware Visual Tracking https://arxiv.org/pdf/1712.09153.pdf Switchable Whitening for Deep Representation Learning https://arxiv.org/abs/1904.09739 Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution https://arxiv.org/abs/1904.05049 Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction https://arxiv.org/abs/1902.06729 Task2Vec: Task Embedding for Meta-Learning https://arxiv.org/abs/1902.03545 ACE: Adapting to Changing Environments for Semantic Segmentation https://arxiv.org/pdf/1904.06268.pdf Few-shot Object Detection via Feature Reweighting https://arxiv.org/pdf/1812.01866.pdf Disentangling Propagation and Generation for Video Prediction https://arxiv.org/pdf/1812.00452.pdf An Empirical Study of Spatial Attention Mechanisms in Deep Networks https://arxiv.org/pdf/1904.05873.pdf Fashion++: Minimal Edits for Outfit Improvement https://arxiv.org/pdf/1904.09261.pdf Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment https://arxiv.org/pdf/1903.11649.pdf Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded https://arxiv.org/pdf/1902.03751.pdf SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation https://arxiv.org/pdf/1905.07512.pdf EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association https://arxiv.org/abs/1904.11781 Texture Fields: Learning Texture Representations in Function Space https://arxiv.org/abs/1905.07259 AMASS: Archive of Motion Capture as Surface Shapes https://arxiv.org/abs/1904.03278 End-to-end Learning for Graph Decomposition https://arxiv.org/pdf/1812.09737.pdf Towards Multi-pose Guided Virtual Try-on Network https://arxiv.org/abs/1902.11026 Learning to Reconstruct 3D Manhattan Wireframes from a Single Image https://arxiv.org/abs/1905.07482 Coherent Semantic Attention for Image Inpainting https://arxiv.org/abs/1905.12384 LayoutVAE: Stochastic Scene Layout Generation from a Label Set https://arxiv.org/abs/1907.10719 Co-Evolutionary Compression for Unpaired Image Translation https://arxiv.org/abs/1907.10804 Enhancing Adversarial Example Transferability with an Intermediate Level Attack https://arxiv.org/abs/1907.10823 Simultaneous multi-view instance detection with learned geometric soft-constraints https://arxiv.org/abs/1907.10892 Gated2Depth: Real-time Dense Lidar from Gated Images https://www.cs.princeton.edu/~fheide/papers/Gated2Depth_preprint.pdf Moment Matching for Multi-Source Domain Adaptation https://arxiv.org/abs/1812.01754 Learning Compositional Representations for Few-Shot Recognition https://sites.google.com/view/comprepr/home Digging Into Self-Supervised Monocular Depth Estimation https://arxiv.org/pdf/1806.01260.pdf Deep Interpretable Non-Rigid Structure from Motion https://arxiv.org/pdf/1902.10840.pdf PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings https://arxiv.org/pdf/1905.01296.pdf Lifelong GAN: Continual Learning for Conditional Image Generation https://arxiv.org/abs/1907.10107 Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection https://arxiv.org/abs/1907.10164 Towards Adversarially Robust Object Detection https://arxiv.org/abs/1907.10310 6-DOF GraspNet: Variational Grasp Generation for Object Manipulation https://arxiv.org/abs/1905.10520 Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction https://arxiv.org/abs/1907.10178 DAFL: Data-Free Learning of Student Networks https://arxiv.org/abs/1904.01186 Multi-adversarial Faster-RCNN for Unrestricted Object Detection https://arxiv.org/abs/1907.10343 Boosting Few-Shot Visual Learning with Self-Supervision https://arxiv.org/abs/1906.05186 A Quaternion-based Certifiably Optimal Solution to the Wahba Problem with Outliers https://arxiv.org/abs/1905.12536 Embodied Visual Recognition Rethinking ImageNet Pre-training https://arxiv.org/abs/1811.08883 TensorMask: A Foundation for Dense Object Segmentation https://arxiv.org/abs/1903.12174 3D Point Cloud Learning for Large-scale Environment Analysis and Place Recognition https://arxiv.org/abs/1812.07050 Selectivity or Invariance: Boundary-aware Salient Object Detection https://arxiv.org/pdf/1812.10066.pdf Creativity Inspired Zero-Shot Learning https://arxiv.org/abs/1904.01109 HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips https://arxiv.org/abs/1906.03327 Correlation Congruence for Knowledge Distillation https://arxiv.org/abs/1904.018029 VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research https://arxiv.org/abs/1904.03493 Episodic Training for Domain Generalization https://arxiv.org/abs/1902.00113 GarNet: A Two-stream Network for Fast and Accurate 3D Cloth Draping https://arxiv.org/abs/1811.10983v2 Semi-supervised Domain Adaptation via Minimax Entropy https://arxiv.org/abs/1904.06487 xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera https://arxiv.org/abs/1907.10045 Canonical Surface Mapping via Geometric Cycle Consistency https://arxiv.org/abs/1907.10043 Incremental Class Discovery for Semantic Segmentation with RGBD Sensing https://arxiv.org/abs/1907.10008 U4D: Unsupervised 4D Dynamic Scene Understanding https://arxiv.org/abs/1907.09905 BMN: Boundary-Matching Network for Temporal Action Proposal Generation https://arxiv.org/abs/1907.09702 SPGNet: Semantic Prediction Guidance for Scene Parsing https://arxiv.org/abs/1908.09798 Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation https://arxiv.org/abs/1811.07456 DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense https://arxiv.org/abs/1812.11017 Closed-Form Optimal Two-View Triangulation Based on Angular Errors https://arxiv.org/abs/1903.09115 Learning Combinatorial Embedding Networks for Deep Graph Matching https://arxiv.org/abs/1904.00597 A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification https://arxiv.org/abs/1904.03425 Remote Heart Rate Measurement from Highly Compressed Facial Videos: an End-to-end Deep Learning Solution with Video Enhancement https://arxiv.org/abs/1907.11921 Symmetry-constrained Rectification Network for Scene Text Recognition https://arxiv.org/abs/1908.01957 STM: SpatioTemporal and Motion Encoding for Action Recognition https://arxiv.org/abs/1908.02486 Explicit Shape Encoding for Real-Time Instance Segmentation https://arxiv.org/abs/1908.04067 Few-Shot Learning with Global Class Representations https://arxiv.org/abs/1908.05257 Symmetric Cross Entropy for Robust Learning with Noisy Labels https://arxiv.org/abs/1908.06112 Human Mesh Recovery from Monocular Images via a Skeleton-disentangled Representation https://arxiv.org/abs/1908.07172 DADA: Depth-Aware Domain Adaptation in Semantic Segmentation https://arxiv.org/abs/1904.01886 &nbsp; 增加35篇包含开源代码的ICCV2019 Bidirectional One-Shot Unsupervised Domain Mapping https://github.com/tomercohen11/BiOST Joint Monocular 3D Detection and Tracking https://arxiv.org/abs/1811.10742 https://github.com/ucbdrive/3d-vehicle-tracking MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation https://arxiv.org/abs/1906.06059 https://github.com/vita-epfl/monoloco Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data https://github.com/xw-hu/Mask-ShadowGAN Towards High-Resolution Salient Object Detection https://arxiv.org/abs/1908.07274 https://github.com/yi94code/HRSOD Confidence Regularized Self-Training https://arxiv.org/abs/1908.09822 https://github.com/yzou2/CRST Optimizing the F-measure for Threshold-free Salient Object Detectionhttp://data.kaizhao.net/publications/iccv2019fmeasure.pdf https://github.com/zeakey/iccv2019-fmeasure Perspective-Guided Convolution Networks for Crowd Counting https://github.com/Zhaoyi-Yan/PGCNet End-to-End Wireframe Parsing https://arxiv.org/abs/1905.03246 https://github.com/zhou13/lcnn Temporal Attentive Alignment for Large-Scale Video Domain Adaptation https://arxiv.org/abs/1907.12743http://github.com/cmhungsteve/TA3N From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer https://arxiv.org/abs/1908.06473 https://github. com/xhp-hust-2018-2011/S-DCNet Free-form Video Inpainting with 3D Gated Convolution and Temporal PatchGAN https://arxiv.org/abs/1904.10247 https://github.com/amjltc295/Free-Form-Video-Inpainting What Would You Expect? Anticipating Egocentric Actions with Rolling-Unrolling LSTMs and Modality Attention https://arxiv.org/pdf/1905.09035.pdf https://github.com/antoninofurnari/rulstm CompenNet++: End-to-end Full Projector Compensation https://github.com/BingyaoHuang/CompenNet-plusplus Pose-aware Dynamic Attention for Human Object Interaction Detection https://github.com/bobwan1995/Pose-aware-Dynamic-Attention-for-Human-Object-Interaction-Detection Temporally-Aggregating Spatial Encoder-Decoder for Video Saliency Detection https://github.com/kylemin/TASED-Net PU-GAN: a Point Cloud Upsampling Adversarial Network https://arxiv.org/abs/1907.10844 https://github.com/liruihui/PU-GAN A Closed-form Solution to Universal Style Transfer https://arxiv.org/abs/1906.00668 https://github.com/lu-m13/OptimalStyleTransfer Video Face Clustering with Unknown Number of Clusters https://github.com/makarandtapaswi/BallClustering_ICCV2019 TSM: Temporal Shift Module for Efficient Video Understanding https://arxiv.org/abs/1811.08383 https://github.com/mit-han-lab/temporal-shift-module Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image https://arxiv.org/abs/1907.11346 https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE 3D-RelNet: Joint Object and Relational Network for 3D Prediction https://arxiv.org/pdf/1906.02729.pdf https://github.com/nileshkulkarni/relative3d Few-shot Unsupervised Image-to-Image Translation https://arxiv.org/abs/1905.01723 https://github.com/nvlabs/FUNIT/ Metric Learning with HORDE: High-Order Regularizer for Deep Embeddings https://arxiv.org/abs/1908.02735 https://github.com/pierre-jacob/ICCV2019-Horde Model Vulnerability to Distributional Shifts over Image Transformation Sets https://arxiv.org/abs/1903.11900 https://github.com/ricvolpi/domain-shift-robustness Language-Conditioned Graph Networks for Relational Reasoning https://arxiv.org/abs/1905.04405 https://github.com/ronghanghu/lcgn Domain Intersection and Domain Difference https://github.com/sagiebenaim/DomainIntersectionDifference Probabilistic Face Embeddings https://arxiv.org/abs/1904.09658 https://github.com/seasonSH/Probabilistic-Face-Embeddings Counting with Focus for Free https://arxiv.org/abs/1903.12206 https://github.com/shizenglin/Counting-with-Focus-for-Free CCNet: Criss-Cross Attention for Semantic Segmentation https://arxiv.org/abs/1811.11721 https://github.com/speedinghzl/CCNet ABD-Net: Attentive but Diverse Person Re-Identification https://arxiv.org/abs/1908.01114 https://github.com/TAMU-VITA/ABD-Net AutoGAN: Neural Architecture Search for Generative Adversarial Networks https://github.com/TAMU-VITA/AutoGAN SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning https://github.com/TerenceCYJ/SO-HandNet Tex2Shape: Detailed Full Human Body Geometry from a Single Image https://arxiv.org/abs/1904.08645 https://github.com/thmoa/tex2shape FCOS: Fully Convolutional One-Stage Object Detectio https://arxiv.org/abs/1904.01355 https://github.com/tianzhi0549/FCOS/ &nbsp; 百度云链接：链接：https://pan.baidu.com/s/1JnbZFmQwtqk1mQMN-ZpAOw&nbsp; 提取码：mv56&nbsp; 如果百度云失效了，去公众号【计算机视觉联盟】后台回复关键词： &nbsp; &nbsp; &nbsp;ICCV2019 或在本条下方评论，我看到就会回复了" />
<meta property="og:description" content="Github持续更新（8月29日更新200篇）： （论文下载百度云见文末） https://github.com/Sophia-11/Awesome-ICCV2019&nbsp; 2019/07/26 * - 更新28篇IIAI录用论文 2019/07/28 * - 更新11篇旷视ICCV2019 2019/08/28 * - 更新31篇Oral 2019/08/29 * - 增加116篇ICCV2019文章 2019/08/29 * - 增加35篇包含开源代码的ICCV2019 Table of Contents ICCV简介 ICCV录用编号 起源人工智能研究院28篇 旷视11篇 2019ICCVOral31篇 增加116篇ICCV2019文章 增加35篇包含开源代码的ICCV2019 ICCV 简介 ICCV 的全称是 IEEE International Conference on Computer Vision，即国际计算机视觉大会，由IEEE主办，与计算机视觉模式识别会议（CVPR）和欧洲计算机视觉会议（ECCV）并称计算机视觉方向的三大顶级会议，被澳大利亚ICT学术会议排名和中国计算机学会等机构评为最高级别学术会议，在业内具有极高的评价。 不同于在美国每年召开一次的CVPR和只在欧洲召开的ECCV，ICCV在世界范围内每两年召开一次。ICCV论文录用率非常低，是三大会议中公认级别最高的 。上一届提交的论文中，其中621篇被接收，录用比例达 28.9%；其中 poster、spotlight、oral 的比例分别为 24.61%、2.61% 以及 2.09%。 ICCV主席 今年有一名大会主席是来自香港中文大学的信息工程系系主任汤晓鸥，他同时还是中国科学院深圳先进技术研究院的副院长兼商汤科技创始人。其他三名大会主席则分别是首尔大学的 Kyoung Mu Lee 教授、伊利诺伊大学厄巴纳-香槟分校的 David Forsyth 教授以及苏黎世联邦理工学院的 Marc Pollefeys 教授。 召开地点 本届大会最终的递交补充材料的截止日期为 3 月 29 日。大会召开时间为2019年10月27日至11月2日，举行地点是韩国首尔的 COEX 会议中心。 刚刚，计算机视觉三大顶会之一ICCV2019终于公布了它的最终论文接收结果，一共有1077篇论文被接收，接收率为25.02% 论文接收序号： 24 17 25 30 31 33 37 41 45 49 59 60 69 84 91 93 102 105 110 126 141 153 157 159 171 175 176 178 184 187 213 226 229 238 242 245 247 251 258 281 285 294 302 320 330 350 351 354 356 361 367 375 376 380 382 383 385 397 405 406 407 409 421 426 428 445 446 450 456 464 466 479 490 491 496 502 504 507 508 520 531 539 568 579 582 585 596 609 620 622 630 634 636 668 669 673 678 680 691 701 706 711 715 719 720 722 727 732 733 735 737 742 751 754 756 759 767 768 770 774 778 782 791 797 800 806 809 811 813 818 827 832 836 838 839 845 855 862 868 876 877 879 888 890 892 899 900 902 904 905 909 912 917 920 921 940 943 959 964 965 976 981 989 1001 1005 1006 1011 1017 1020 1023 1031 1032 1039 1040 1042 1045 1046 1057 1062 1067 1077 1083 1092 1093 1096 1097 1098 1104 1105 1111 1112 1113 1119 1135 1139 1142 1148 1160 1163 1165 1166 1168 1174 1180 1182 1197 1200 1205 1206 1211 1215 1223 1233 1245 1249 1252 1272 1277 1285 1288 1291 1323 1330 1334 1335 1342 1343 1356 1370 1378 1381 1384 1390 1394 1395 1403 1404 1406 1411 1412 1417 1422 1426 1428 1434 1439 1442 1452 1455 1457 1463 1477 1479 1485 1488 1501 1517 1527 1535 1538 1542 1550 1551 1552 1562 1565 1570 1574 1581 1583 1585 1586 1590 1592 1596 1597 1616 1621 1624 1630 1638 1639 1642 1643 1647 1648 1650 1652 1656 1657 1667 1672 1675 1681 1693 1700 1705 1706 1714 1743 1746 1768 1772 1773 1774 1779 1785 1788 1805 1811 1819 1820 1823 1826 1827 1829 1844 1850 1854 1855 1859 1860 1861 1863 1865 1866 1870 1874 1879 1881 1882 1911 1917 1919 1924 1926 1933 1942 1943 1959 1960 1963 1967 1970 1971 1972 1982 1983 1984 1990 2005 2010 2012 2017 2024 2029 2032 2037 2040 2043 2055 2065 2070 2077 2097 2101 2115 2126 2127 2132 2134 2140 2148 2149 2155 2157 2160 2163 2169 2177 2179 2205 2206 2209 2214 2223 2230 2235 2240 2245 2246 2247 2248 2259 2266 2267 2272 2275 2277 2282 2284 2286 2288 2289 2290 2291 2303 2304 2312 2322 2323 2336 2337 2338 2339 2344 2353 2355 2359 2385 2390 2391 2392 2397 2402 2406 2413 2419 2420 2421 2436 2437 2441 2448 2450 2454 2458 2470 2473 2478 2481 2490 2495 2498 2501 2511 2517 2521 2525 2531 2545 2547 2548 2551 2553 2555 2556 2557 2561 2563 2564 2571 2578 2580 2595 2601 2603 2607 2608 2609 2610 2613 2615 2619 2622 2633 2634 2637 2638 2642 2660 2661 2679 2683 2684 2690 2717 2725 2732 2739 2740 2768 2790 2792 2795 2796 2798 2799 2814 2820 2830 2833 2836 2838 2840 2842 2850 2855 2857 2862 2865 2872 2886 2899 2908 2912 2919 2927 2928 2939 2944 2957 2958 2962 2963 2964 2968 2979 2980 3001 3016 3034 3035 3036 3051 3058 3059 3060 3068 3072 3080 3095 3102 3104 3107 3110 3114 3116 3120 3123 3127 3128 3133 3136 3137 3139 3140 3141 3145 3151 3154 3164 3166 3172 3180 3185 3193 3197 3198 3203 3215 3220 3222 3233 3239 3242 3243 3246 3260 3272 3273 3280 3281 3286 3290 3293 3300 3315 3321 3326 3327 3339 3345 3346 3352 3359 3361 3372 3375 3378 3379 3380 3382 3391 3394 3398 3402 3403 3410 3419 3430 3435 3436 3438 3439 3443 3458 3462 3463 3464 3468 3476 3489 3492 3494 3496 3502 3505 3508 3510 3514 3518 3521 3523 3540 3544 3547 3548 3552 3554 3555 3556 3559 3571 3572 3589 3592 3593 3596 3609 3611 3618 3620 3622 3627 3632 3636 3638 3646 3652 3655 3658 3662 3665 3667 3670 3674 3676 3682 3693 3695 3700 3717 3718 3723 3729 3734 3735 3739 3740 3743 3749 3750 3758 3761 3762 3767 3768 3772 3786 3787 3788 3795 3807 3808 3813 3818 3821 3824 3832 3834 3838 3857 3860 3867 3869 3879 3882 3897 3919 3921 3923 3926 3932 3933 3937 3941 3942 3949 3964 3971 3987 3988 3992 3998 4006 4007 4009 4019 4021 4022 4024 4032 4033 4034 4042 4047 4057 4067 4075 4079 4085 4088 4090 4092 4093 4094 4097 4102 4105 4112 4113 4118 4121 4122 4124 4125 4130 4144 4151 4154 4159 4162 4164 4167 4168 4171 4176 4192 4194 4199 4211 4212 4217 4237 4245 4246 4248 4249 4253 4267 4275 4285 4289 4293 4305 4309 4311 4330 4341 4342 4343 4346 4365 4366 4367 4370 4374 4406 4410 4414 4428 4430 4434 4446 4449 4453 4481 4485 4500 4506 4509 4526 4530 4533 4534 4541 4549 4560 4562 4563 4576 4585 4599 4600 4602 4614 4618 4634 4647 4649 4660 4666 4672 4690 4697 4701 4702 4712 4721 4737 4757 4765 4766 4768 4785 4787 4794 4798 4811 4825 4835 4846 4848 4851 4856 4861 4865 4870 4874 4881 4890 4901 4903 4910 4925 4928 4943 4946 4971 4996 5005 5008 5011 5016 5018 5023 5029 5051 5052 5053 5062 5073 5088 5099 5103 5105 5112 5114 5116 5127 5128 5129 5131 5135 5136 5148 5158 5161 5162 5164 5171 5172 5174 5180 5183 5184 5195 5196 5201 5215 5223 5235 5264 5269 5274 5280 5290 5292 5296 5301 5302 5314 5321 5323 5338 5344 5348 5370 5378 5384 5393 5412 5413 5417 5423 5437 5444 5454 5455 5457 5465 5519 5532 5540 5548 5576 5582 5594 5601 5626 5649 5651 5657 5662 5672 5683 5684 5696 5698 5700 5704 5705 5725 5728 5742 5752 5797 5801 5810 5819 5823 5827 5844 5845 5853 5863 5869 5880 5892 5903 5925 5927 5935 5948 5950 5952 5957 5961 5968 6009 6021 6026 6034 6035 6036 6072 6083 6105 6132 6174 6175 6178 6191 6204 6209 6215 6221 6232 6250 6258 6267 6284 6287 6289 6294 6296 6302 6328 6329 6352 6367 6372 6379 6385 6398 6400 6403 6404 6405 6410 6414 6423 6428 6430 6433 6467 6471 6480 6483 6496 6506 6512 6519 6521 6529 6532 6534 6554 6563 6568 6578 6579 6597 6602 6608 6622 6625 6640 6668 6691 6696 6700 6740 6744 6752 6780 6783 6829 6886 6887 6929 6944 6968 6978 6981 起源人工智能研究院 - Inception Institute of Artificial Intelligence (IIAI) 28篇论文 IIAI主页：www.inceptioniai.org/ Unsupervised Video Object Segmentation via Attentive Graph Neural Networks DUAL-GLOWs: Conditional Flow-Based Generative Models for Inter-Modality Transfer in Brain Imaging Unsupervised Graph Association for Person Re-identification Relational Attention Network for Crowd Counting Attentional Neural Fields for Crowd Counting Learning Compositional Neural Information Fusion for Human Parsing RANet: Ranking Attention Network for Fast Video Object Segmentation Learning to Mask Visible Regions for Occluded Pedestrian Detection Boosted Feature Guided Refinement Network for Single-Shot Detection Deep Contextual Attention for Human-Object Interaction Detection Learning the Model Update for Siamese Trackers 3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization Learning Rich Features at High-Speed for Single-Shot Object Detection Transductive learning for zero-shot object detection Ground-to-aerial Image Geo-localization with a Hard Exemplar Reweighting Triplet Loss Towards Bridging Semantic Gap to Improve Semantic Segmentation Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks Motion Deblurring via Human-Aware Attention Network Gaussian Affinity for Max-margin Class Imbalanced Learning A Deep Step Pattern Representation for Multimodal Retinal Image Registration SegEQA: Video Segmentation based Visual Attention for Embodied Question Answering Reciprocal Multi-Layer Subspace Learning for Multi-View Clustering Scoot: A Perceptual Metric for Facial Sketches EGNet: Edge Guidance Network for Salient Object Detection PointAE: Point Auto-encoder for 3D Statistical Shape and Texture Modelling Understanding Human Gaze Communication by Spatio-temporal Graph Reasoning Optimizing the F-measure for Threshold-free Salient Object Detection SynDeMo: Synergistic Deep Feature Alignment for Joint Learning of Depth and Ego-Motion 旷视研究院 11 篇论文入选 ICCV 2019 1、Objects365: A Large-scale, High-quality Dataset for Object Detection 2、ThunderNet: Towards Real-time Generic Object Detection 3、Efficient and Accurate Arbitrary-Shaped Text Detection with PixelAggregation Network 4、Semi-supervised Skin Detection by Network with Mutual Guidance 5、Semi-Supervised Video Salient Object Detection Using Pseudo-Labels 6、Disentangled Image Matting 7、Re-ID Driven Localization Refinement for Person Search 8、Vehicle Re-identification with Viewpoint-aware Metric Learning 9、MetaPruning: Meta Learning for Automatic Neural Network ChannelPruning 10、Symmetry-constrained Rectification Network for Scene Text Recognition 11、Learning to Paint with Model-based Deep Reinforcement Learning 2019 ICCV Oral https://arxiv.org/abs/1908.00382 Interpolated Convolutional Networks for 3D Point Cloud Understanding https://arxiv.org/abs/1908.04512 Memory-Based Neighbourhood Embedding for Visual Recognition https://arxiv.org/abs/1908.04992 Learning Trajectory Dependencies for Human Motion Prediction https://arxiv.org/abs/1908.05436 Domain Adaptation for Structured Output via Discriminative Patch Representations https://arxiv.org/abs/1901.05427 Deep Non-Rigid Structure from Motion https://arxiv.org/abs/1908.00052 Scalable Place Recognition Under Appearance Change for Autonomous Driving https://arxiv.org/abs/1908.00178 Restoration of Non-rigidly Distorted Underwater Images using a Combination of Compressive Sensing and Local Polynomial Image Representations https://arxiv.org/abs/1908.01940 Consensus Maximization Tree Search Revisited https://arxiv.org/abs/1908.02021 Weakly Supervised Energy-Based Learning for Action Segmentation Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification https://arxiv.org/abs/1811.10144 Controllable Artistic Text Style Transfer via Shape-Matching GAN https://arxiv.org/abs/1905.01354 Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition https://arxiv.org/abs/1907.13369 Expectation-Maximization Attention Networks for Semantic Segmentation https://arxiv.org/abs/1907.13426 VideoBERT: A Joint Model for Video and Language Representation Learning https://arxiv.org/abs/1904.01766 CARAFE: Content-Aware ReAssembly of FEatures https://arxiv.org/pdf/1905.02188.pdf Habitat: A Platform for Embodied AI Research https://arxiv.org/abs/1904.01201 Equivariant Multi-View Networks https://arxiv.org/abs/1904.00993 PointFlow : 3D Point Cloud Generation with Continuous Normalizing Flows https://arxiv.org/abs/1906.12320 Learnable Triangulation of Human Pose https://arxiv.org/abs/1905.05754 Learning Implicit Generative Models by Matching Perceptual Features https://arxiv.org/abs/1904.02762v1 COCO-GAN: Generation by Parts via Conditional Coordinating https://arxiv.org/abs/1904.00284 SlowFast Networks for Video Recognition https://arxiv.org/abs/1812.03982 Exploring Randomly Wired Neural Networks for Image Recognition https://arxiv.org/abs/1904.01569 Can GCNs Go as Deep as CNNs? https://arxiv.org/abs/1904.03751 Deep SR-ITM: Joint Learning of Super-resolution and Inverse Tone-Mapping for 4K UHD HDR Applications https://arxiv.org/abs/1904.11176 Meta-Sim Learning to Generate Synthetic Datasets https://arxiv.org/abs/1904.11621 Deep HoughVoting for 3D Object Detection in Point Clouds https://arxiv.org/abs/1904.09664 Variational Adversarial Active Learning https://arxiv.org/abs/1904.00370 Towards Unconstrained End-to-End Text Spotting https://arxiv.org/abs/1908.09231 Non-local Recurrent Neural Memory for Supervised Sequence Modeling https://arxiv.org/abs/1908.09535 Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels https://arxiv.org/abs/1908.09597 &nbsp; 增加116篇ICCV2019文章 Similarity-Preserving Knowledge Distillation https://arxiv.org/abs/1907.09682 GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition https://arxiv.org/abs/1907.09653 Tell, Draw, and Repeat: Generating and modifying images based on continual linguistic instruction https://arxiv.org/pdf/1811.09845.pdf Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers https://arxiv.org/abs/1904.08489 nocaps: novel object captioning at scale https://arxiv.org/abs/1812.08658 ThunderNet: Towards Real-time Generic Object Detection https://arxiv.org/abs/1903.11752 Scene GraphPrediction with Limited Labels https://arxiv.org/abs/1904.11622 Ego-Pose Estimation and Forecasting as Real-Time PD Control https://arxiv.org/abs/1906.03173 The Trajectron: Probabilistic Multi-Agent Trajectory Modeling withDynamic Spatiotemporal Graphs https://arxiv.org/abs/1810.05993 End-to-End Learning of Representations for Asynchronous Event-BasedData https://arxiv.org/abs/1904.08245 Efficient Learning on Point Clouds with Basis Point Sets https://arxiv.org/abs/1908.09186 Dynamic Kernel Distillation for Efficient Pose Estimation in Videos https://arxiv.org/abs/1908.09216 Single-Stage Multi-Person Pose Machines https://arxiv.org/abs/1908.09220 Towards Unsupervised Image Captioning with Shared Multimodal Embeddings https://arxiv.org/abs/1908.09317 advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns https://arxiv.org/abs/1908.09327 Shape-Aware Human Pose and Shape Reconstruction Using Multi-View Images https://arxiv.org/abs/1908.09464 Relation Distillation Networks for Video Object Detection https://arxiv.org/abs/1908.09511 Object-Driven Multi-Layer Scene Decomposition From a Single Image https://arxiv.org/abs/1908.09521 Embarrassingly Simple Binary Representation Learning https://arxiv.org/abs/1908.09573 Moulding Humans: Non-parametric 3D Human Shape Estimation from Single Images https://arxiv.org/abs/1908.00439 Learning the Model Update for Siamese Trackers https://arxiv.org/abs/1908.00855 Distilling Knowledge From a Deep Pose Regressor Network https://arxiv.org/abs/1908.00858 Permutation-invariant Feature Restructuring for Correlation-aware Image Set-based Recognition https://arxiv.org/abs/1908.01174 ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal https://arxiv.org/abs/1908.01323 Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation https://arxiv.org/abs/1908.01491 View N-gram Network for 3D Object Retrieval https://arxiv.org/abs/1908.01958 Semi-supervised Skin Detection by Network with Mutual Guidance https://arxiv.org/abs/1908.01977 Deep Self-Learning From Noisy Labels https://arxiv.org/abs/1908.02160 Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking https://arxiv.org/abs/1908.02231 Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning https://arxiv.org/abs/1908.02441 Expert Sample Consensus Applied to Camera Re-Localization https://arxiv.org/abs/1908.02484 SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition https://arxiv.org/abs/1908.02660 GP2C: Geometric Projection Parameter Consensus for Joint 3D Pose and Focal Length Estimation in the Wild https://arxiv.org/abs/1908.02809 SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences https://arxiv.org/abs/1904.01416 Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction https://arxiv.org/abs/1907.12704 Orientation-aware Semantic Segmentation on Icosahedron Spheres https://arxiv.org/abs/1907.12849 EMPNet: Neural Localisation and Mapping using Embedded Memory Points https://arxiv.org/abs/1907.13268 SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation https://arxiv.org/abs/1907.11308 On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method https://arxiv.org/abs/1907.11684 Goal-Driven Sequential Data Abstraction https://arxiv.org/abs/1907.12336 Recursive Cascaded Networks for Unsupervised Medical Image Registration https://arxiv.org/abs/1907.12353 Learn to Scale: Generating Multipolar Normalized Density Map for Crowd Counting https://arxiv.org/abs/1907.12428 HoloGAN: Unsupervised learning of 3D representations from natural images https://arxiv.org/abs/1904.01326 MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning https://arxiv.org/abs/1903.10258 FrameNet: Learning Local Canonical Frames of 3D Surfaces from a Single RGB Image https://arxiv.org/pdf/1903.12305.pdf Face De-occlusion using 3D Morphable Model and Generative Adversarialhttp://image.inha.ac.kr/paper/ICCV2019_Xaiowei.pdf Deep Meta Learning for Real-Time Target-Aware Visual Tracking https://arxiv.org/pdf/1712.09153.pdf Switchable Whitening for Deep Representation Learning https://arxiv.org/abs/1904.09739 Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution https://arxiv.org/abs/1904.05049 Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction https://arxiv.org/abs/1902.06729 Task2Vec: Task Embedding for Meta-Learning https://arxiv.org/abs/1902.03545 ACE: Adapting to Changing Environments for Semantic Segmentation https://arxiv.org/pdf/1904.06268.pdf Few-shot Object Detection via Feature Reweighting https://arxiv.org/pdf/1812.01866.pdf Disentangling Propagation and Generation for Video Prediction https://arxiv.org/pdf/1812.00452.pdf An Empirical Study of Spatial Attention Mechanisms in Deep Networks https://arxiv.org/pdf/1904.05873.pdf Fashion++: Minimal Edits for Outfit Improvement https://arxiv.org/pdf/1904.09261.pdf Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment https://arxiv.org/pdf/1903.11649.pdf Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded https://arxiv.org/pdf/1902.03751.pdf SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation https://arxiv.org/pdf/1905.07512.pdf EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association https://arxiv.org/abs/1904.11781 Texture Fields: Learning Texture Representations in Function Space https://arxiv.org/abs/1905.07259 AMASS: Archive of Motion Capture as Surface Shapes https://arxiv.org/abs/1904.03278 End-to-end Learning for Graph Decomposition https://arxiv.org/pdf/1812.09737.pdf Towards Multi-pose Guided Virtual Try-on Network https://arxiv.org/abs/1902.11026 Learning to Reconstruct 3D Manhattan Wireframes from a Single Image https://arxiv.org/abs/1905.07482 Coherent Semantic Attention for Image Inpainting https://arxiv.org/abs/1905.12384 LayoutVAE: Stochastic Scene Layout Generation from a Label Set https://arxiv.org/abs/1907.10719 Co-Evolutionary Compression for Unpaired Image Translation https://arxiv.org/abs/1907.10804 Enhancing Adversarial Example Transferability with an Intermediate Level Attack https://arxiv.org/abs/1907.10823 Simultaneous multi-view instance detection with learned geometric soft-constraints https://arxiv.org/abs/1907.10892 Gated2Depth: Real-time Dense Lidar from Gated Images https://www.cs.princeton.edu/~fheide/papers/Gated2Depth_preprint.pdf Moment Matching for Multi-Source Domain Adaptation https://arxiv.org/abs/1812.01754 Learning Compositional Representations for Few-Shot Recognition https://sites.google.com/view/comprepr/home Digging Into Self-Supervised Monocular Depth Estimation https://arxiv.org/pdf/1806.01260.pdf Deep Interpretable Non-Rigid Structure from Motion https://arxiv.org/pdf/1902.10840.pdf PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings https://arxiv.org/pdf/1905.01296.pdf Lifelong GAN: Continual Learning for Conditional Image Generation https://arxiv.org/abs/1907.10107 Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection https://arxiv.org/abs/1907.10164 Towards Adversarially Robust Object Detection https://arxiv.org/abs/1907.10310 6-DOF GraspNet: Variational Grasp Generation for Object Manipulation https://arxiv.org/abs/1905.10520 Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction https://arxiv.org/abs/1907.10178 DAFL: Data-Free Learning of Student Networks https://arxiv.org/abs/1904.01186 Multi-adversarial Faster-RCNN for Unrestricted Object Detection https://arxiv.org/abs/1907.10343 Boosting Few-Shot Visual Learning with Self-Supervision https://arxiv.org/abs/1906.05186 A Quaternion-based Certifiably Optimal Solution to the Wahba Problem with Outliers https://arxiv.org/abs/1905.12536 Embodied Visual Recognition Rethinking ImageNet Pre-training https://arxiv.org/abs/1811.08883 TensorMask: A Foundation for Dense Object Segmentation https://arxiv.org/abs/1903.12174 3D Point Cloud Learning for Large-scale Environment Analysis and Place Recognition https://arxiv.org/abs/1812.07050 Selectivity or Invariance: Boundary-aware Salient Object Detection https://arxiv.org/pdf/1812.10066.pdf Creativity Inspired Zero-Shot Learning https://arxiv.org/abs/1904.01109 HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips https://arxiv.org/abs/1906.03327 Correlation Congruence for Knowledge Distillation https://arxiv.org/abs/1904.018029 VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research https://arxiv.org/abs/1904.03493 Episodic Training for Domain Generalization https://arxiv.org/abs/1902.00113 GarNet: A Two-stream Network for Fast and Accurate 3D Cloth Draping https://arxiv.org/abs/1811.10983v2 Semi-supervised Domain Adaptation via Minimax Entropy https://arxiv.org/abs/1904.06487 xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera https://arxiv.org/abs/1907.10045 Canonical Surface Mapping via Geometric Cycle Consistency https://arxiv.org/abs/1907.10043 Incremental Class Discovery for Semantic Segmentation with RGBD Sensing https://arxiv.org/abs/1907.10008 U4D: Unsupervised 4D Dynamic Scene Understanding https://arxiv.org/abs/1907.09905 BMN: Boundary-Matching Network for Temporal Action Proposal Generation https://arxiv.org/abs/1907.09702 SPGNet: Semantic Prediction Guidance for Scene Parsing https://arxiv.org/abs/1908.09798 Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation https://arxiv.org/abs/1811.07456 DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense https://arxiv.org/abs/1812.11017 Closed-Form Optimal Two-View Triangulation Based on Angular Errors https://arxiv.org/abs/1903.09115 Learning Combinatorial Embedding Networks for Deep Graph Matching https://arxiv.org/abs/1904.00597 A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification https://arxiv.org/abs/1904.03425 Remote Heart Rate Measurement from Highly Compressed Facial Videos: an End-to-end Deep Learning Solution with Video Enhancement https://arxiv.org/abs/1907.11921 Symmetry-constrained Rectification Network for Scene Text Recognition https://arxiv.org/abs/1908.01957 STM: SpatioTemporal and Motion Encoding for Action Recognition https://arxiv.org/abs/1908.02486 Explicit Shape Encoding for Real-Time Instance Segmentation https://arxiv.org/abs/1908.04067 Few-Shot Learning with Global Class Representations https://arxiv.org/abs/1908.05257 Symmetric Cross Entropy for Robust Learning with Noisy Labels https://arxiv.org/abs/1908.06112 Human Mesh Recovery from Monocular Images via a Skeleton-disentangled Representation https://arxiv.org/abs/1908.07172 DADA: Depth-Aware Domain Adaptation in Semantic Segmentation https://arxiv.org/abs/1904.01886 &nbsp; 增加35篇包含开源代码的ICCV2019 Bidirectional One-Shot Unsupervised Domain Mapping https://github.com/tomercohen11/BiOST Joint Monocular 3D Detection and Tracking https://arxiv.org/abs/1811.10742 https://github.com/ucbdrive/3d-vehicle-tracking MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation https://arxiv.org/abs/1906.06059 https://github.com/vita-epfl/monoloco Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data https://github.com/xw-hu/Mask-ShadowGAN Towards High-Resolution Salient Object Detection https://arxiv.org/abs/1908.07274 https://github.com/yi94code/HRSOD Confidence Regularized Self-Training https://arxiv.org/abs/1908.09822 https://github.com/yzou2/CRST Optimizing the F-measure for Threshold-free Salient Object Detectionhttp://data.kaizhao.net/publications/iccv2019fmeasure.pdf https://github.com/zeakey/iccv2019-fmeasure Perspective-Guided Convolution Networks for Crowd Counting https://github.com/Zhaoyi-Yan/PGCNet End-to-End Wireframe Parsing https://arxiv.org/abs/1905.03246 https://github.com/zhou13/lcnn Temporal Attentive Alignment for Large-Scale Video Domain Adaptation https://arxiv.org/abs/1907.12743http://github.com/cmhungsteve/TA3N From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer https://arxiv.org/abs/1908.06473 https://github. com/xhp-hust-2018-2011/S-DCNet Free-form Video Inpainting with 3D Gated Convolution and Temporal PatchGAN https://arxiv.org/abs/1904.10247 https://github.com/amjltc295/Free-Form-Video-Inpainting What Would You Expect? Anticipating Egocentric Actions with Rolling-Unrolling LSTMs and Modality Attention https://arxiv.org/pdf/1905.09035.pdf https://github.com/antoninofurnari/rulstm CompenNet++: End-to-end Full Projector Compensation https://github.com/BingyaoHuang/CompenNet-plusplus Pose-aware Dynamic Attention for Human Object Interaction Detection https://github.com/bobwan1995/Pose-aware-Dynamic-Attention-for-Human-Object-Interaction-Detection Temporally-Aggregating Spatial Encoder-Decoder for Video Saliency Detection https://github.com/kylemin/TASED-Net PU-GAN: a Point Cloud Upsampling Adversarial Network https://arxiv.org/abs/1907.10844 https://github.com/liruihui/PU-GAN A Closed-form Solution to Universal Style Transfer https://arxiv.org/abs/1906.00668 https://github.com/lu-m13/OptimalStyleTransfer Video Face Clustering with Unknown Number of Clusters https://github.com/makarandtapaswi/BallClustering_ICCV2019 TSM: Temporal Shift Module for Efficient Video Understanding https://arxiv.org/abs/1811.08383 https://github.com/mit-han-lab/temporal-shift-module Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image https://arxiv.org/abs/1907.11346 https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE 3D-RelNet: Joint Object and Relational Network for 3D Prediction https://arxiv.org/pdf/1906.02729.pdf https://github.com/nileshkulkarni/relative3d Few-shot Unsupervised Image-to-Image Translation https://arxiv.org/abs/1905.01723 https://github.com/nvlabs/FUNIT/ Metric Learning with HORDE: High-Order Regularizer for Deep Embeddings https://arxiv.org/abs/1908.02735 https://github.com/pierre-jacob/ICCV2019-Horde Model Vulnerability to Distributional Shifts over Image Transformation Sets https://arxiv.org/abs/1903.11900 https://github.com/ricvolpi/domain-shift-robustness Language-Conditioned Graph Networks for Relational Reasoning https://arxiv.org/abs/1905.04405 https://github.com/ronghanghu/lcgn Domain Intersection and Domain Difference https://github.com/sagiebenaim/DomainIntersectionDifference Probabilistic Face Embeddings https://arxiv.org/abs/1904.09658 https://github.com/seasonSH/Probabilistic-Face-Embeddings Counting with Focus for Free https://arxiv.org/abs/1903.12206 https://github.com/shizenglin/Counting-with-Focus-for-Free CCNet: Criss-Cross Attention for Semantic Segmentation https://arxiv.org/abs/1811.11721 https://github.com/speedinghzl/CCNet ABD-Net: Attentive but Diverse Person Re-Identification https://arxiv.org/abs/1908.01114 https://github.com/TAMU-VITA/ABD-Net AutoGAN: Neural Architecture Search for Generative Adversarial Networks https://github.com/TAMU-VITA/AutoGAN SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning https://github.com/TerenceCYJ/SO-HandNet Tex2Shape: Detailed Full Human Body Geometry from a Single Image https://arxiv.org/abs/1904.08645 https://github.com/thmoa/tex2shape FCOS: Fully Convolutional One-Stage Object Detectio https://arxiv.org/abs/1904.01355 https://github.com/tianzhi0549/FCOS/ &nbsp; 百度云链接：链接：https://pan.baidu.com/s/1JnbZFmQwtqk1mQMN-ZpAOw&nbsp; 提取码：mv56&nbsp; 如果百度云失效了，去公众号【计算机视觉联盟】后台回复关键词： &nbsp; &nbsp; &nbsp;ICCV2019 或在本条下方评论，我看到就会回复了" />
<link rel="canonical" href="https://uzzz.org/2019/07/23/795552.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/23/795552.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-23T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Github持续更新（8月29日更新200篇）： （论文下载百度云见文末） https://github.com/Sophia-11/Awesome-ICCV2019&nbsp; 2019/07/26 * - 更新28篇IIAI录用论文 2019/07/28 * - 更新11篇旷视ICCV2019 2019/08/28 * - 更新31篇Oral 2019/08/29 * - 增加116篇ICCV2019文章 2019/08/29 * - 增加35篇包含开源代码的ICCV2019 Table of Contents ICCV简介 ICCV录用编号 起源人工智能研究院28篇 旷视11篇 2019ICCVOral31篇 增加116篇ICCV2019文章 增加35篇包含开源代码的ICCV2019 ICCV 简介 ICCV 的全称是 IEEE International Conference on Computer Vision，即国际计算机视觉大会，由IEEE主办，与计算机视觉模式识别会议（CVPR）和欧洲计算机视觉会议（ECCV）并称计算机视觉方向的三大顶级会议，被澳大利亚ICT学术会议排名和中国计算机学会等机构评为最高级别学术会议，在业内具有极高的评价。 不同于在美国每年召开一次的CVPR和只在欧洲召开的ECCV，ICCV在世界范围内每两年召开一次。ICCV论文录用率非常低，是三大会议中公认级别最高的 。上一届提交的论文中，其中621篇被接收，录用比例达 28.9%；其中 poster、spotlight、oral 的比例分别为 24.61%、2.61% 以及 2.09%。 ICCV主席 今年有一名大会主席是来自香港中文大学的信息工程系系主任汤晓鸥，他同时还是中国科学院深圳先进技术研究院的副院长兼商汤科技创始人。其他三名大会主席则分别是首尔大学的 Kyoung Mu Lee 教授、伊利诺伊大学厄巴纳-香槟分校的 David Forsyth 教授以及苏黎世联邦理工学院的 Marc Pollefeys 教授。 召开地点 本届大会最终的递交补充材料的截止日期为 3 月 29 日。大会召开时间为2019年10月27日至11月2日，举行地点是韩国首尔的 COEX 会议中心。 刚刚，计算机视觉三大顶会之一ICCV2019终于公布了它的最终论文接收结果，一共有1077篇论文被接收，接收率为25.02% 论文接收序号： 24 17 25 30 31 33 37 41 45 49 59 60 69 84 91 93 102 105 110 126 141 153 157 159 171 175 176 178 184 187 213 226 229 238 242 245 247 251 258 281 285 294 302 320 330 350 351 354 356 361 367 375 376 380 382 383 385 397 405 406 407 409 421 426 428 445 446 450 456 464 466 479 490 491 496 502 504 507 508 520 531 539 568 579 582 585 596 609 620 622 630 634 636 668 669 673 678 680 691 701 706 711 715 719 720 722 727 732 733 735 737 742 751 754 756 759 767 768 770 774 778 782 791 797 800 806 809 811 813 818 827 832 836 838 839 845 855 862 868 876 877 879 888 890 892 899 900 902 904 905 909 912 917 920 921 940 943 959 964 965 976 981 989 1001 1005 1006 1011 1017 1020 1023 1031 1032 1039 1040 1042 1045 1046 1057 1062 1067 1077 1083 1092 1093 1096 1097 1098 1104 1105 1111 1112 1113 1119 1135 1139 1142 1148 1160 1163 1165 1166 1168 1174 1180 1182 1197 1200 1205 1206 1211 1215 1223 1233 1245 1249 1252 1272 1277 1285 1288 1291 1323 1330 1334 1335 1342 1343 1356 1370 1378 1381 1384 1390 1394 1395 1403 1404 1406 1411 1412 1417 1422 1426 1428 1434 1439 1442 1452 1455 1457 1463 1477 1479 1485 1488 1501 1517 1527 1535 1538 1542 1550 1551 1552 1562 1565 1570 1574 1581 1583 1585 1586 1590 1592 1596 1597 1616 1621 1624 1630 1638 1639 1642 1643 1647 1648 1650 1652 1656 1657 1667 1672 1675 1681 1693 1700 1705 1706 1714 1743 1746 1768 1772 1773 1774 1779 1785 1788 1805 1811 1819 1820 1823 1826 1827 1829 1844 1850 1854 1855 1859 1860 1861 1863 1865 1866 1870 1874 1879 1881 1882 1911 1917 1919 1924 1926 1933 1942 1943 1959 1960 1963 1967 1970 1971 1972 1982 1983 1984 1990 2005 2010 2012 2017 2024 2029 2032 2037 2040 2043 2055 2065 2070 2077 2097 2101 2115 2126 2127 2132 2134 2140 2148 2149 2155 2157 2160 2163 2169 2177 2179 2205 2206 2209 2214 2223 2230 2235 2240 2245 2246 2247 2248 2259 2266 2267 2272 2275 2277 2282 2284 2286 2288 2289 2290 2291 2303 2304 2312 2322 2323 2336 2337 2338 2339 2344 2353 2355 2359 2385 2390 2391 2392 2397 2402 2406 2413 2419 2420 2421 2436 2437 2441 2448 2450 2454 2458 2470 2473 2478 2481 2490 2495 2498 2501 2511 2517 2521 2525 2531 2545 2547 2548 2551 2553 2555 2556 2557 2561 2563 2564 2571 2578 2580 2595 2601 2603 2607 2608 2609 2610 2613 2615 2619 2622 2633 2634 2637 2638 2642 2660 2661 2679 2683 2684 2690 2717 2725 2732 2739 2740 2768 2790 2792 2795 2796 2798 2799 2814 2820 2830 2833 2836 2838 2840 2842 2850 2855 2857 2862 2865 2872 2886 2899 2908 2912 2919 2927 2928 2939 2944 2957 2958 2962 2963 2964 2968 2979 2980 3001 3016 3034 3035 3036 3051 3058 3059 3060 3068 3072 3080 3095 3102 3104 3107 3110 3114 3116 3120 3123 3127 3128 3133 3136 3137 3139 3140 3141 3145 3151 3154 3164 3166 3172 3180 3185 3193 3197 3198 3203 3215 3220 3222 3233 3239 3242 3243 3246 3260 3272 3273 3280 3281 3286 3290 3293 3300 3315 3321 3326 3327 3339 3345 3346 3352 3359 3361 3372 3375 3378 3379 3380 3382 3391 3394 3398 3402 3403 3410 3419 3430 3435 3436 3438 3439 3443 3458 3462 3463 3464 3468 3476 3489 3492 3494 3496 3502 3505 3508 3510 3514 3518 3521 3523 3540 3544 3547 3548 3552 3554 3555 3556 3559 3571 3572 3589 3592 3593 3596 3609 3611 3618 3620 3622 3627 3632 3636 3638 3646 3652 3655 3658 3662 3665 3667 3670 3674 3676 3682 3693 3695 3700 3717 3718 3723 3729 3734 3735 3739 3740 3743 3749 3750 3758 3761 3762 3767 3768 3772 3786 3787 3788 3795 3807 3808 3813 3818 3821 3824 3832 3834 3838 3857 3860 3867 3869 3879 3882 3897 3919 3921 3923 3926 3932 3933 3937 3941 3942 3949 3964 3971 3987 3988 3992 3998 4006 4007 4009 4019 4021 4022 4024 4032 4033 4034 4042 4047 4057 4067 4075 4079 4085 4088 4090 4092 4093 4094 4097 4102 4105 4112 4113 4118 4121 4122 4124 4125 4130 4144 4151 4154 4159 4162 4164 4167 4168 4171 4176 4192 4194 4199 4211 4212 4217 4237 4245 4246 4248 4249 4253 4267 4275 4285 4289 4293 4305 4309 4311 4330 4341 4342 4343 4346 4365 4366 4367 4370 4374 4406 4410 4414 4428 4430 4434 4446 4449 4453 4481 4485 4500 4506 4509 4526 4530 4533 4534 4541 4549 4560 4562 4563 4576 4585 4599 4600 4602 4614 4618 4634 4647 4649 4660 4666 4672 4690 4697 4701 4702 4712 4721 4737 4757 4765 4766 4768 4785 4787 4794 4798 4811 4825 4835 4846 4848 4851 4856 4861 4865 4870 4874 4881 4890 4901 4903 4910 4925 4928 4943 4946 4971 4996 5005 5008 5011 5016 5018 5023 5029 5051 5052 5053 5062 5073 5088 5099 5103 5105 5112 5114 5116 5127 5128 5129 5131 5135 5136 5148 5158 5161 5162 5164 5171 5172 5174 5180 5183 5184 5195 5196 5201 5215 5223 5235 5264 5269 5274 5280 5290 5292 5296 5301 5302 5314 5321 5323 5338 5344 5348 5370 5378 5384 5393 5412 5413 5417 5423 5437 5444 5454 5455 5457 5465 5519 5532 5540 5548 5576 5582 5594 5601 5626 5649 5651 5657 5662 5672 5683 5684 5696 5698 5700 5704 5705 5725 5728 5742 5752 5797 5801 5810 5819 5823 5827 5844 5845 5853 5863 5869 5880 5892 5903 5925 5927 5935 5948 5950 5952 5957 5961 5968 6009 6021 6026 6034 6035 6036 6072 6083 6105 6132 6174 6175 6178 6191 6204 6209 6215 6221 6232 6250 6258 6267 6284 6287 6289 6294 6296 6302 6328 6329 6352 6367 6372 6379 6385 6398 6400 6403 6404 6405 6410 6414 6423 6428 6430 6433 6467 6471 6480 6483 6496 6506 6512 6519 6521 6529 6532 6534 6554 6563 6568 6578 6579 6597 6602 6608 6622 6625 6640 6668 6691 6696 6700 6740 6744 6752 6780 6783 6829 6886 6887 6929 6944 6968 6978 6981 起源人工智能研究院 - Inception Institute of Artificial Intelligence (IIAI) 28篇论文 IIAI主页：www.inceptioniai.org/ Unsupervised Video Object Segmentation via Attentive Graph Neural Networks DUAL-GLOWs: Conditional Flow-Based Generative Models for Inter-Modality Transfer in Brain Imaging Unsupervised Graph Association for Person Re-identification Relational Attention Network for Crowd Counting Attentional Neural Fields for Crowd Counting Learning Compositional Neural Information Fusion for Human Parsing RANet: Ranking Attention Network for Fast Video Object Segmentation Learning to Mask Visible Regions for Occluded Pedestrian Detection Boosted Feature Guided Refinement Network for Single-Shot Detection Deep Contextual Attention for Human-Object Interaction Detection Learning the Model Update for Siamese Trackers 3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization Learning Rich Features at High-Speed for Single-Shot Object Detection Transductive learning for zero-shot object detection Ground-to-aerial Image Geo-localization with a Hard Exemplar Reweighting Triplet Loss Towards Bridging Semantic Gap to Improve Semantic Segmentation Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks Motion Deblurring via Human-Aware Attention Network Gaussian Affinity for Max-margin Class Imbalanced Learning A Deep Step Pattern Representation for Multimodal Retinal Image Registration SegEQA: Video Segmentation based Visual Attention for Embodied Question Answering Reciprocal Multi-Layer Subspace Learning for Multi-View Clustering Scoot: A Perceptual Metric for Facial Sketches EGNet: Edge Guidance Network for Salient Object Detection PointAE: Point Auto-encoder for 3D Statistical Shape and Texture Modelling Understanding Human Gaze Communication by Spatio-temporal Graph Reasoning Optimizing the F-measure for Threshold-free Salient Object Detection SynDeMo: Synergistic Deep Feature Alignment for Joint Learning of Depth and Ego-Motion 旷视研究院 11 篇论文入选 ICCV 2019 1、Objects365: A Large-scale, High-quality Dataset for Object Detection 2、ThunderNet: Towards Real-time Generic Object Detection 3、Efficient and Accurate Arbitrary-Shaped Text Detection with PixelAggregation Network 4、Semi-supervised Skin Detection by Network with Mutual Guidance 5、Semi-Supervised Video Salient Object Detection Using Pseudo-Labels 6、Disentangled Image Matting 7、Re-ID Driven Localization Refinement for Person Search 8、Vehicle Re-identification with Viewpoint-aware Metric Learning 9、MetaPruning: Meta Learning for Automatic Neural Network ChannelPruning 10、Symmetry-constrained Rectification Network for Scene Text Recognition 11、Learning to Paint with Model-based Deep Reinforcement Learning 2019 ICCV Oral https://arxiv.org/abs/1908.00382 Interpolated Convolutional Networks for 3D Point Cloud Understanding https://arxiv.org/abs/1908.04512 Memory-Based Neighbourhood Embedding for Visual Recognition https://arxiv.org/abs/1908.04992 Learning Trajectory Dependencies for Human Motion Prediction https://arxiv.org/abs/1908.05436 Domain Adaptation for Structured Output via Discriminative Patch Representations https://arxiv.org/abs/1901.05427 Deep Non-Rigid Structure from Motion https://arxiv.org/abs/1908.00052 Scalable Place Recognition Under Appearance Change for Autonomous Driving https://arxiv.org/abs/1908.00178 Restoration of Non-rigidly Distorted Underwater Images using a Combination of Compressive Sensing and Local Polynomial Image Representations https://arxiv.org/abs/1908.01940 Consensus Maximization Tree Search Revisited https://arxiv.org/abs/1908.02021 Weakly Supervised Energy-Based Learning for Action Segmentation Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification https://arxiv.org/abs/1811.10144 Controllable Artistic Text Style Transfer via Shape-Matching GAN https://arxiv.org/abs/1905.01354 Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition https://arxiv.org/abs/1907.13369 Expectation-Maximization Attention Networks for Semantic Segmentation https://arxiv.org/abs/1907.13426 VideoBERT: A Joint Model for Video and Language Representation Learning https://arxiv.org/abs/1904.01766 CARAFE: Content-Aware ReAssembly of FEatures https://arxiv.org/pdf/1905.02188.pdf Habitat: A Platform for Embodied AI Research https://arxiv.org/abs/1904.01201 Equivariant Multi-View Networks https://arxiv.org/abs/1904.00993 PointFlow : 3D Point Cloud Generation with Continuous Normalizing Flows https://arxiv.org/abs/1906.12320 Learnable Triangulation of Human Pose https://arxiv.org/abs/1905.05754 Learning Implicit Generative Models by Matching Perceptual Features https://arxiv.org/abs/1904.02762v1 COCO-GAN: Generation by Parts via Conditional Coordinating https://arxiv.org/abs/1904.00284 SlowFast Networks for Video Recognition https://arxiv.org/abs/1812.03982 Exploring Randomly Wired Neural Networks for Image Recognition https://arxiv.org/abs/1904.01569 Can GCNs Go as Deep as CNNs? https://arxiv.org/abs/1904.03751 Deep SR-ITM: Joint Learning of Super-resolution and Inverse Tone-Mapping for 4K UHD HDR Applications https://arxiv.org/abs/1904.11176 Meta-Sim Learning to Generate Synthetic Datasets https://arxiv.org/abs/1904.11621 Deep HoughVoting for 3D Object Detection in Point Clouds https://arxiv.org/abs/1904.09664 Variational Adversarial Active Learning https://arxiv.org/abs/1904.00370 Towards Unconstrained End-to-End Text Spotting https://arxiv.org/abs/1908.09231 Non-local Recurrent Neural Memory for Supervised Sequence Modeling https://arxiv.org/abs/1908.09535 Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels https://arxiv.org/abs/1908.09597 &nbsp; 增加116篇ICCV2019文章 Similarity-Preserving Knowledge Distillation https://arxiv.org/abs/1907.09682 GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition https://arxiv.org/abs/1907.09653 Tell, Draw, and Repeat: Generating and modifying images based on continual linguistic instruction https://arxiv.org/pdf/1811.09845.pdf Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers https://arxiv.org/abs/1904.08489 nocaps: novel object captioning at scale https://arxiv.org/abs/1812.08658 ThunderNet: Towards Real-time Generic Object Detection https://arxiv.org/abs/1903.11752 Scene GraphPrediction with Limited Labels https://arxiv.org/abs/1904.11622 Ego-Pose Estimation and Forecasting as Real-Time PD Control https://arxiv.org/abs/1906.03173 The Trajectron: Probabilistic Multi-Agent Trajectory Modeling withDynamic Spatiotemporal Graphs https://arxiv.org/abs/1810.05993 End-to-End Learning of Representations for Asynchronous Event-BasedData https://arxiv.org/abs/1904.08245 Efficient Learning on Point Clouds with Basis Point Sets https://arxiv.org/abs/1908.09186 Dynamic Kernel Distillation for Efficient Pose Estimation in Videos https://arxiv.org/abs/1908.09216 Single-Stage Multi-Person Pose Machines https://arxiv.org/abs/1908.09220 Towards Unsupervised Image Captioning with Shared Multimodal Embeddings https://arxiv.org/abs/1908.09317 advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns https://arxiv.org/abs/1908.09327 Shape-Aware Human Pose and Shape Reconstruction Using Multi-View Images https://arxiv.org/abs/1908.09464 Relation Distillation Networks for Video Object Detection https://arxiv.org/abs/1908.09511 Object-Driven Multi-Layer Scene Decomposition From a Single Image https://arxiv.org/abs/1908.09521 Embarrassingly Simple Binary Representation Learning https://arxiv.org/abs/1908.09573 Moulding Humans: Non-parametric 3D Human Shape Estimation from Single Images https://arxiv.org/abs/1908.00439 Learning the Model Update for Siamese Trackers https://arxiv.org/abs/1908.00855 Distilling Knowledge From a Deep Pose Regressor Network https://arxiv.org/abs/1908.00858 Permutation-invariant Feature Restructuring for Correlation-aware Image Set-based Recognition https://arxiv.org/abs/1908.01174 ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal https://arxiv.org/abs/1908.01323 Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation https://arxiv.org/abs/1908.01491 View N-gram Network for 3D Object Retrieval https://arxiv.org/abs/1908.01958 Semi-supervised Skin Detection by Network with Mutual Guidance https://arxiv.org/abs/1908.01977 Deep Self-Learning From Noisy Labels https://arxiv.org/abs/1908.02160 Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking https://arxiv.org/abs/1908.02231 Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning https://arxiv.org/abs/1908.02441 Expert Sample Consensus Applied to Camera Re-Localization https://arxiv.org/abs/1908.02484 SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition https://arxiv.org/abs/1908.02660 GP2C: Geometric Projection Parameter Consensus for Joint 3D Pose and Focal Length Estimation in the Wild https://arxiv.org/abs/1908.02809 SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences https://arxiv.org/abs/1904.01416 Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction https://arxiv.org/abs/1907.12704 Orientation-aware Semantic Segmentation on Icosahedron Spheres https://arxiv.org/abs/1907.12849 EMPNet: Neural Localisation and Mapping using Embedded Memory Points https://arxiv.org/abs/1907.13268 SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation https://arxiv.org/abs/1907.11308 On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method https://arxiv.org/abs/1907.11684 Goal-Driven Sequential Data Abstraction https://arxiv.org/abs/1907.12336 Recursive Cascaded Networks for Unsupervised Medical Image Registration https://arxiv.org/abs/1907.12353 Learn to Scale: Generating Multipolar Normalized Density Map for Crowd Counting https://arxiv.org/abs/1907.12428 HoloGAN: Unsupervised learning of 3D representations from natural images https://arxiv.org/abs/1904.01326 MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning https://arxiv.org/abs/1903.10258 FrameNet: Learning Local Canonical Frames of 3D Surfaces from a Single RGB Image https://arxiv.org/pdf/1903.12305.pdf Face De-occlusion using 3D Morphable Model and Generative Adversarialhttp://image.inha.ac.kr/paper/ICCV2019_Xaiowei.pdf Deep Meta Learning for Real-Time Target-Aware Visual Tracking https://arxiv.org/pdf/1712.09153.pdf Switchable Whitening for Deep Representation Learning https://arxiv.org/abs/1904.09739 Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution https://arxiv.org/abs/1904.05049 Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction https://arxiv.org/abs/1902.06729 Task2Vec: Task Embedding for Meta-Learning https://arxiv.org/abs/1902.03545 ACE: Adapting to Changing Environments for Semantic Segmentation https://arxiv.org/pdf/1904.06268.pdf Few-shot Object Detection via Feature Reweighting https://arxiv.org/pdf/1812.01866.pdf Disentangling Propagation and Generation for Video Prediction https://arxiv.org/pdf/1812.00452.pdf An Empirical Study of Spatial Attention Mechanisms in Deep Networks https://arxiv.org/pdf/1904.05873.pdf Fashion++: Minimal Edits for Outfit Improvement https://arxiv.org/pdf/1904.09261.pdf Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment https://arxiv.org/pdf/1903.11649.pdf Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded https://arxiv.org/pdf/1902.03751.pdf SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation https://arxiv.org/pdf/1905.07512.pdf EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association https://arxiv.org/abs/1904.11781 Texture Fields: Learning Texture Representations in Function Space https://arxiv.org/abs/1905.07259 AMASS: Archive of Motion Capture as Surface Shapes https://arxiv.org/abs/1904.03278 End-to-end Learning for Graph Decomposition https://arxiv.org/pdf/1812.09737.pdf Towards Multi-pose Guided Virtual Try-on Network https://arxiv.org/abs/1902.11026 Learning to Reconstruct 3D Manhattan Wireframes from a Single Image https://arxiv.org/abs/1905.07482 Coherent Semantic Attention for Image Inpainting https://arxiv.org/abs/1905.12384 LayoutVAE: Stochastic Scene Layout Generation from a Label Set https://arxiv.org/abs/1907.10719 Co-Evolutionary Compression for Unpaired Image Translation https://arxiv.org/abs/1907.10804 Enhancing Adversarial Example Transferability with an Intermediate Level Attack https://arxiv.org/abs/1907.10823 Simultaneous multi-view instance detection with learned geometric soft-constraints https://arxiv.org/abs/1907.10892 Gated2Depth: Real-time Dense Lidar from Gated Images https://www.cs.princeton.edu/~fheide/papers/Gated2Depth_preprint.pdf Moment Matching for Multi-Source Domain Adaptation https://arxiv.org/abs/1812.01754 Learning Compositional Representations for Few-Shot Recognition https://sites.google.com/view/comprepr/home Digging Into Self-Supervised Monocular Depth Estimation https://arxiv.org/pdf/1806.01260.pdf Deep Interpretable Non-Rigid Structure from Motion https://arxiv.org/pdf/1902.10840.pdf PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings https://arxiv.org/pdf/1905.01296.pdf Lifelong GAN: Continual Learning for Conditional Image Generation https://arxiv.org/abs/1907.10107 Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection https://arxiv.org/abs/1907.10164 Towards Adversarially Robust Object Detection https://arxiv.org/abs/1907.10310 6-DOF GraspNet: Variational Grasp Generation for Object Manipulation https://arxiv.org/abs/1905.10520 Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction https://arxiv.org/abs/1907.10178 DAFL: Data-Free Learning of Student Networks https://arxiv.org/abs/1904.01186 Multi-adversarial Faster-RCNN for Unrestricted Object Detection https://arxiv.org/abs/1907.10343 Boosting Few-Shot Visual Learning with Self-Supervision https://arxiv.org/abs/1906.05186 A Quaternion-based Certifiably Optimal Solution to the Wahba Problem with Outliers https://arxiv.org/abs/1905.12536 Embodied Visual Recognition Rethinking ImageNet Pre-training https://arxiv.org/abs/1811.08883 TensorMask: A Foundation for Dense Object Segmentation https://arxiv.org/abs/1903.12174 3D Point Cloud Learning for Large-scale Environment Analysis and Place Recognition https://arxiv.org/abs/1812.07050 Selectivity or Invariance: Boundary-aware Salient Object Detection https://arxiv.org/pdf/1812.10066.pdf Creativity Inspired Zero-Shot Learning https://arxiv.org/abs/1904.01109 HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips https://arxiv.org/abs/1906.03327 Correlation Congruence for Knowledge Distillation https://arxiv.org/abs/1904.018029 VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research https://arxiv.org/abs/1904.03493 Episodic Training for Domain Generalization https://arxiv.org/abs/1902.00113 GarNet: A Two-stream Network for Fast and Accurate 3D Cloth Draping https://arxiv.org/abs/1811.10983v2 Semi-supervised Domain Adaptation via Minimax Entropy https://arxiv.org/abs/1904.06487 xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera https://arxiv.org/abs/1907.10045 Canonical Surface Mapping via Geometric Cycle Consistency https://arxiv.org/abs/1907.10043 Incremental Class Discovery for Semantic Segmentation with RGBD Sensing https://arxiv.org/abs/1907.10008 U4D: Unsupervised 4D Dynamic Scene Understanding https://arxiv.org/abs/1907.09905 BMN: Boundary-Matching Network for Temporal Action Proposal Generation https://arxiv.org/abs/1907.09702 SPGNet: Semantic Prediction Guidance for Scene Parsing https://arxiv.org/abs/1908.09798 Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation https://arxiv.org/abs/1811.07456 DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense https://arxiv.org/abs/1812.11017 Closed-Form Optimal Two-View Triangulation Based on Angular Errors https://arxiv.org/abs/1903.09115 Learning Combinatorial Embedding Networks for Deep Graph Matching https://arxiv.org/abs/1904.00597 A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification https://arxiv.org/abs/1904.03425 Remote Heart Rate Measurement from Highly Compressed Facial Videos: an End-to-end Deep Learning Solution with Video Enhancement https://arxiv.org/abs/1907.11921 Symmetry-constrained Rectification Network for Scene Text Recognition https://arxiv.org/abs/1908.01957 STM: SpatioTemporal and Motion Encoding for Action Recognition https://arxiv.org/abs/1908.02486 Explicit Shape Encoding for Real-Time Instance Segmentation https://arxiv.org/abs/1908.04067 Few-Shot Learning with Global Class Representations https://arxiv.org/abs/1908.05257 Symmetric Cross Entropy for Robust Learning with Noisy Labels https://arxiv.org/abs/1908.06112 Human Mesh Recovery from Monocular Images via a Skeleton-disentangled Representation https://arxiv.org/abs/1908.07172 DADA: Depth-Aware Domain Adaptation in Semantic Segmentation https://arxiv.org/abs/1904.01886 &nbsp; 增加35篇包含开源代码的ICCV2019 Bidirectional One-Shot Unsupervised Domain Mapping https://github.com/tomercohen11/BiOST Joint Monocular 3D Detection and Tracking https://arxiv.org/abs/1811.10742 https://github.com/ucbdrive/3d-vehicle-tracking MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation https://arxiv.org/abs/1906.06059 https://github.com/vita-epfl/monoloco Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data https://github.com/xw-hu/Mask-ShadowGAN Towards High-Resolution Salient Object Detection https://arxiv.org/abs/1908.07274 https://github.com/yi94code/HRSOD Confidence Regularized Self-Training https://arxiv.org/abs/1908.09822 https://github.com/yzou2/CRST Optimizing the F-measure for Threshold-free Salient Object Detectionhttp://data.kaizhao.net/publications/iccv2019fmeasure.pdf https://github.com/zeakey/iccv2019-fmeasure Perspective-Guided Convolution Networks for Crowd Counting https://github.com/Zhaoyi-Yan/PGCNet End-to-End Wireframe Parsing https://arxiv.org/abs/1905.03246 https://github.com/zhou13/lcnn Temporal Attentive Alignment for Large-Scale Video Domain Adaptation https://arxiv.org/abs/1907.12743http://github.com/cmhungsteve/TA3N From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer https://arxiv.org/abs/1908.06473 https://github. com/xhp-hust-2018-2011/S-DCNet Free-form Video Inpainting with 3D Gated Convolution and Temporal PatchGAN https://arxiv.org/abs/1904.10247 https://github.com/amjltc295/Free-Form-Video-Inpainting What Would You Expect? Anticipating Egocentric Actions with Rolling-Unrolling LSTMs and Modality Attention https://arxiv.org/pdf/1905.09035.pdf https://github.com/antoninofurnari/rulstm CompenNet++: End-to-end Full Projector Compensation https://github.com/BingyaoHuang/CompenNet-plusplus Pose-aware Dynamic Attention for Human Object Interaction Detection https://github.com/bobwan1995/Pose-aware-Dynamic-Attention-for-Human-Object-Interaction-Detection Temporally-Aggregating Spatial Encoder-Decoder for Video Saliency Detection https://github.com/kylemin/TASED-Net PU-GAN: a Point Cloud Upsampling Adversarial Network https://arxiv.org/abs/1907.10844 https://github.com/liruihui/PU-GAN A Closed-form Solution to Universal Style Transfer https://arxiv.org/abs/1906.00668 https://github.com/lu-m13/OptimalStyleTransfer Video Face Clustering with Unknown Number of Clusters https://github.com/makarandtapaswi/BallClustering_ICCV2019 TSM: Temporal Shift Module for Efficient Video Understanding https://arxiv.org/abs/1811.08383 https://github.com/mit-han-lab/temporal-shift-module Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image https://arxiv.org/abs/1907.11346 https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE 3D-RelNet: Joint Object and Relational Network for 3D Prediction https://arxiv.org/pdf/1906.02729.pdf https://github.com/nileshkulkarni/relative3d Few-shot Unsupervised Image-to-Image Translation https://arxiv.org/abs/1905.01723 https://github.com/nvlabs/FUNIT/ Metric Learning with HORDE: High-Order Regularizer for Deep Embeddings https://arxiv.org/abs/1908.02735 https://github.com/pierre-jacob/ICCV2019-Horde Model Vulnerability to Distributional Shifts over Image Transformation Sets https://arxiv.org/abs/1903.11900 https://github.com/ricvolpi/domain-shift-robustness Language-Conditioned Graph Networks for Relational Reasoning https://arxiv.org/abs/1905.04405 https://github.com/ronghanghu/lcgn Domain Intersection and Domain Difference https://github.com/sagiebenaim/DomainIntersectionDifference Probabilistic Face Embeddings https://arxiv.org/abs/1904.09658 https://github.com/seasonSH/Probabilistic-Face-Embeddings Counting with Focus for Free https://arxiv.org/abs/1903.12206 https://github.com/shizenglin/Counting-with-Focus-for-Free CCNet: Criss-Cross Attention for Semantic Segmentation https://arxiv.org/abs/1811.11721 https://github.com/speedinghzl/CCNet ABD-Net: Attentive but Diverse Person Re-Identification https://arxiv.org/abs/1908.01114 https://github.com/TAMU-VITA/ABD-Net AutoGAN: Neural Architecture Search for Generative Adversarial Networks https://github.com/TAMU-VITA/AutoGAN SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning https://github.com/TerenceCYJ/SO-HandNet Tex2Shape: Detailed Full Human Body Geometry from a Single Image https://arxiv.org/abs/1904.08645 https://github.com/thmoa/tex2shape FCOS: Fully Convolutional One-Stage Object Detectio https://arxiv.org/abs/1904.01355 https://github.com/tianzhi0549/FCOS/ &nbsp; 百度云链接：链接：https://pan.baidu.com/s/1JnbZFmQwtqk1mQMN-ZpAOw&nbsp; 提取码：mv56&nbsp; 如果百度云失效了，去公众号【计算机视觉联盟】后台回复关键词： &nbsp; &nbsp; &nbsp;ICCV2019 或在本条下方评论，我看到就会回复了","@type":"BlogPosting","url":"https://uzzz.org/2019/07/23/795552.html","headline":"ICCV 2019 ICCV 2019 论文接收列表 ICCV 2019一共接收1077篇 共4303篇投稿","dateModified":"2019-07-23T00:00:00+08:00","datePublished":"2019-07-23T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/23/795552.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>ICCV 2019 | ICCV 2019 论文接收列表 | ICCV 2019一共接收1077篇 | 共4303篇投稿</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1>Github持续更新（<span style="color:#f33b45;">8月29日更新200篇</span>）：</h1> 
  <p><span style="color:#3399ea;"><strong>（论文下载百度云见文末）</strong></span></p> 
  <p><img alt="" class="has" height="397" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190903105042637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE1Njk4NjEz,size_16,color_FFFFFF,t_70" width="858"></p> 
  <p><a href="https://github.com/Sophia-11/Awesome-ICCV2019" rel="nofollow" data-token="8f8f5650cc2d2fb9146483c42ea435fe">https://github.com/Sophia-11/Awesome-ICCV2019</a>&nbsp;</p> 
  <ul>
   <li>2019/07/26 * - 更新28篇IIAI录用论文</li> 
   <li>2019/07/28 * - 更新11篇旷视ICCV2019</li> 
   <li>2019/08/28 * - 更新31篇Oral</li> 
   <li>2019/08/29 * - 增加116篇ICCV2019文章</li> 
   <li>2019/08/29 * - 增加35篇包含开源代码的ICCV2019</li> 
  </ul>
  <h2>Table of Contents</h2> 
  <ul>
   <li><a href="https://github.com/Sophia-11/Awesome-ICCV2019/blob/master/README.md#%23ICCV%E7%AE%80%E4%BB%8B" rel="nofollow" data-token="3dbc95eb1740400e18d855d391c4e5d9">ICCV简介</a></li> 
   <li><a href="https://github.com/Sophia-11/Awesome-ICCV2019/blob/master/README.md#%23ICCV2019%E6%9C%80%E6%96%B0%E5%BD%95%E7%94%A8%E8%AE%BA%E6%96%87%E7%BC%96%E5%8F%B7" rel="nofollow" data-token="719f6b8080294316a26f5d240d77e9d5">ICCV录用编号</a></li> 
   <li><a href="https://github.com/Sophia-11/Awesome-ICCV2019/blob/master/README.md#%E8%B5%B7%E6%BA%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6%E9%99%A2" rel="nofollow" data-token="2da7351509a91a5d73b23f37c4565811">起源人工智能研究院28篇</a></li> 
   <li><a href="https://github.com/Sophia-11/Awesome-ICCV2019/blob/master/README.md#%E6%97%B7%E8%A7%86%E7%A0%94%E7%A9%B6%E9%99%A211%E7%AF%87%E8%AE%BA%E6%96%87%E5%85%A5%E9%80%89ICCV2019" rel="nofollow" data-token="5f8c1d912a15c7364c45e98731aaf0d8">旷视11篇</a></li> 
   <li><a href="https://github.com/Sophia-11/Awesome-ICCV2019/blob/master/README.md#2019ICCVOral" rel="nofollow" data-token="b0fa404c038d97aa5766323247da11c3">2019ICCVOral31篇</a></li> 
   <li><a href="https://github.com/Sophia-11/Awesome-ICCV2019/blob/master/README.md#%E5%A2%9E%E5%8A%A0116%E7%AF%87ICCV2019%E6%96%87%E7%AB%A0" rel="nofollow" data-token="97ea1a2a4e04e3ebbe0fe1860dc9b963">增加116篇ICCV2019文章</a></li> 
   <li><a href="https://github.com/Sophia-11/Awesome-ICCV2019/blob/master/README.md#%E5%A2%9E%E5%8A%A035%E7%AF%87%E5%8C%85%E5%90%AB%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%E7%9A%84ICCV2019" rel="nofollow" data-token="24051ddf2ca80dd541ad8471c46af34b">增加35篇包含开源代码的ICCV2019</a></li> 
  </ul>
  <h2>ICCV 简介</h2> 
  <p>ICCV 的全称是 IEEE International Conference on Computer Vision，即国际计算机视觉大会，由IEEE主办，与计算机视觉模式识别会议（CVPR）和欧洲计算机视觉会议（ECCV）并称计算机视觉方向的三大顶级会议，被澳大利亚ICT学术会议排名和中国计算机学会等机构评为最高级别学术会议，在业内具有极高的评价。 不同于在美国每年召开一次的CVPR和只在欧洲召开的ECCV，ICCV在世界范围内每两年召开一次。ICCV论文录用率非常低，是三大会议中公认级别最高的 。上一届提交的论文中，其中621篇被接收，录用比例达 28.9%；其中 poster、spotlight、oral 的比例分别为 24.61%、2.61% 以及 2.09%。</p> 
  <h2>ICCV主席</h2> 
  <p>今年有一名大会主席是来自香港中文大学的信息工程系系主任汤晓鸥，他同时还是中国科学院深圳先进技术研究院的副院长兼商汤科技创始人。其他三名大会主席则分别是首尔大学的 Kyoung Mu Lee 教授、伊利诺伊大学厄巴纳-香槟分校的 David Forsyth 教授以及苏黎世联邦理工学院的 Marc Pollefeys 教授。</p> 
  <h2>召开地点</h2> 
  <p>本届大会最终的递交补充材料的截止日期为 3 月 29 日。大会召开时间为2019年10月27日至11月2日，举行地点是韩国首尔的 COEX 会议中心。</p> 
  <p>刚刚，计算机视觉三大顶会之一ICCV2019终于公布了它的最终论文接收结果，一共有1077篇论文被接收，接收率为25.02%</p> 
  <h3>论文接收序号：</h3> 
  <p>24 17 25 30 31 33 37 41 45 49 59 60 69 84 91 93 102 105 110 126 141 153 157 159 171 175 176 178 184 187 213 226 229 238 242 245 247 251 258 281 285 294 302 320 330 350 351 354 356 361 367 375 376 380 382 383 385 397 405 406 407 409 421 426 428 445 446 450 456 464 466 479 490 491 496 502 504 507 508 520 531 539 568 579 582 585 596 609 620 622 630 634 636 668 669 673 678 680 691 701 706 711 715 719 720 722 727 732 733 735 737 742 751 754 756 759 767 768 770 774 778 782 791 797 800 806 809 811 813 818 827 832 836 838 839 845 855 862 868 876 877 879 888 890 892 899 900 902 904 905 909 912 917 920 921 940 943 959 964 965 976 981 989 1001 1005 1006 1011 1017 1020 1023 1031 1032 1039 1040 1042 1045 1046 1057 1062 1067 1077 1083 1092 1093 1096 1097 1098 1104 1105 1111 1112 1113 1119 1135 1139 1142 1148 1160 1163 1165 1166 1168 1174 1180 1182 1197 1200 1205 1206 1211 1215 1223 1233 1245 1249 1252 1272 1277 1285 1288 1291 1323 1330 1334 1335 1342 1343 1356 1370 1378 1381 1384 1390 1394 1395 1403 1404 1406 1411 1412 1417 1422 1426 1428 1434 1439 1442 1452 1455 1457 1463 1477 1479 1485 1488 1501 1517 1527 1535 1538 1542 1550 1551 1552 1562 1565 1570 1574 1581 1583 1585 1586 1590 1592 1596 1597 1616 1621 1624 1630 1638 1639 1642 1643 1647 1648 1650 1652 1656 1657 1667 1672 1675 1681 1693 1700 1705 1706 1714 1743 1746 1768 1772 1773 1774 1779 1785 1788 1805 1811 1819 1820 1823 1826 1827 1829 1844 1850 1854 1855 1859 1860 1861 1863 1865 1866 1870 1874 1879 1881 1882 1911 1917 1919 1924 1926 1933 1942 1943 1959 1960 1963 1967 1970 1971 1972 1982 1983 1984 1990 2005 2010 2012 2017 2024 2029 2032 2037 2040 2043 2055 2065 2070 2077 2097 2101 2115 2126 2127 2132 2134 2140 2148 2149 2155 2157 2160 2163 2169 2177 2179 2205 2206 2209 2214 2223 2230 2235 2240 2245 2246 2247 2248 2259 2266 2267 2272 2275 2277 2282 2284 2286 2288 2289 2290 2291 2303 2304 2312 2322 2323 2336 2337 2338 2339 2344 2353 2355 2359 2385 2390 2391 2392 2397 2402 2406 2413 2419 2420 2421 2436 2437 2441 2448 2450 2454 2458 2470 2473 2478 2481 2490 2495 2498 2501 2511 2517 2521 2525 2531 2545 2547 2548 2551 2553 2555 2556 2557 2561 2563 2564 2571 2578 2580 2595 2601 2603 2607 2608 2609 2610 2613 2615 2619 2622 2633 2634 2637 2638 2642 2660 2661 2679 2683 2684 2690 2717 2725 2732 2739 2740 2768 2790 2792 2795 2796 2798 2799 2814 2820 2830 2833 2836 2838 2840 2842 2850 2855 2857 2862 2865 2872 2886 2899 2908 2912 2919 2927 2928 2939 2944 2957 2958 2962 2963 2964 2968 2979 2980 3001 3016 3034 3035 3036 3051 3058 3059 3060 3068 3072 3080 3095 3102 3104 3107 3110 3114 3116 3120 3123 3127 3128 3133 3136 3137 3139 3140 3141 3145 3151 3154 3164 3166 3172 3180 3185 3193 3197 3198 3203 3215 3220 3222 3233 3239 3242 3243 3246 3260 3272 3273 3280 3281 3286 3290 3293 3300 3315 3321 3326 3327 3339 3345 3346 3352 3359 3361 3372 3375 3378 3379 3380 3382 3391 3394 3398 3402 3403 3410 3419 3430 3435 3436 3438 3439 3443 3458 3462 3463 3464 3468 3476 3489 3492 3494 3496 3502 3505 3508 3510 3514 3518 3521 3523 3540 3544 3547 3548 3552 3554 3555 3556 3559 3571 3572 3589 3592 3593 3596 3609 3611 3618 3620 3622 3627 3632 3636 3638 3646 3652 3655 3658 3662 3665 3667 3670 3674 3676 3682 3693 3695 3700 3717 3718 3723 3729 3734 3735 3739 3740 3743 3749 3750 3758 3761 3762 3767 3768 3772 3786 3787 3788 3795 3807 3808 3813 3818 3821 3824 3832 3834 3838 3857 3860 3867 3869 3879 3882 3897 3919 3921 3923 3926 3932 3933 3937 3941 3942 3949 3964 3971 3987 3988 3992 3998 4006 4007 4009 4019 4021 4022 4024 4032 4033 4034 4042 4047 4057 4067 4075 4079 4085 4088 4090 4092 4093 4094 4097 4102 4105 4112 4113 4118 4121 4122 4124 4125 4130 4144 4151 4154 4159 4162 4164 4167 4168 4171 4176 4192 4194 4199 4211 4212 4217 4237 4245 4246 4248 4249 4253 4267 4275 4285 4289 4293 4305 4309 4311 4330 4341 4342 4343 4346 4365 4366 4367 4370 4374 4406 4410 4414 4428 4430 4434 4446 4449 4453 4481 4485 4500 4506 4509 4526 4530 4533 4534 4541 4549 4560 4562 4563 4576 4585 4599 4600 4602 4614 4618 4634 4647 4649 4660 4666 4672 4690 4697 4701 4702 4712 4721 4737 4757 4765 4766 4768 4785 4787 4794 4798 4811 4825 4835 4846 4848 4851 4856 4861 4865 4870 4874 4881 4890 4901 4903 4910 4925 4928 4943 4946 4971 4996 5005 5008 5011 5016 5018 5023 5029 5051 5052 5053 5062 5073 5088 5099 5103 5105 5112 5114 5116 5127 5128 5129 5131 5135 5136 5148 5158 5161 5162 5164 5171 5172 5174 5180 5183 5184 5195 5196 5201 5215 5223 5235 5264 5269 5274 5280 5290 5292 5296 5301 5302 5314 5321 5323 5338 5344 5348 5370 5378 5384 5393 5412 5413 5417 5423 5437 5444 5454 5455 5457 5465 5519 5532 5540 5548 5576 5582 5594 5601 5626 5649 5651 5657 5662 5672 5683 5684 5696 5698 5700 5704 5705 5725 5728 5742 5752 5797 5801 5810 5819 5823 5827 5844 5845 5853 5863 5869 5880 5892 5903 5925 5927 5935 5948 5950 5952 5957 5961 5968 6009 6021 6026 6034 6035 6036 6072 6083 6105 6132 6174 6175 6178 6191 6204 6209 6215 6221 6232 6250 6258 6267 6284 6287 6289 6294 6296 6302 6328 6329 6352 6367 6372 6379 6385 6398 6400 6403 6404 6405 6410 6414 6423 6428 6430 6433 6467 6471 6480 6483 6496 6506 6512 6519 6521 6529 6532 6534 6554 6563 6568 6578 6579 6597 6602 6608 6622 6625 6640 6668 6691 6696 6700 6740 6744 6752 6780 6783 6829 6886 6887 6929 6944 6968 6978 6981</p> 
  <h1>起源人工智能研究院 - Inception Institute of Artificial Intelligence (IIAI) 28篇论文</h1> 
  <h2>IIAI主页：www.inceptioniai.org/</h2> 
  <ol>
   <li>Unsupervised Video Object Segmentation via Attentive Graph Neural Networks</li> 
   <li>DUAL-GLOWs: Conditional Flow-Based Generative Models for Inter-Modality Transfer in Brain Imaging</li> 
   <li>Unsupervised Graph Association for Person Re-identification</li> 
   <li>Relational Attention Network for Crowd Counting</li> 
   <li>Attentional Neural Fields for Crowd Counting</li> 
   <li>Learning Compositional Neural Information Fusion for Human Parsing</li> 
   <li>RANet: Ranking Attention Network for Fast Video Object Segmentation</li> 
   <li>Learning to Mask Visible Regions for Occluded Pedestrian Detection</li> 
   <li>Boosted Feature Guided Refinement Network for Single-Shot Detection</li> 
   <li>Deep Contextual Attention for Human-Object Interaction Detection</li> 
   <li>Learning the Model Update for Siamese Trackers</li> 
   <li>3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization</li> 
   <li>Learning Rich Features at High-Speed for Single-Shot Object Detection</li> 
   <li>Transductive learning for zero-shot object detection</li> 
   <li>Ground-to-aerial Image Geo-localization with a Hard Exemplar Reweighting Triplet Loss</li> 
   <li>Towards Bridging Semantic Gap to Improve Semantic Segmentation</li> 
   <li>Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks</li> 
   <li>Motion Deblurring via Human-Aware Attention Network</li> 
   <li>Gaussian Affinity for Max-margin Class Imbalanced Learning</li> 
   <li>A Deep Step Pattern Representation for Multimodal Retinal Image Registration</li> 
   <li>SegEQA: Video Segmentation based Visual Attention for Embodied Question Answering</li> 
   <li>Reciprocal Multi-Layer Subspace Learning for Multi-View Clustering</li> 
   <li>Scoot: A Perceptual Metric for Facial Sketches</li> 
   <li>EGNet: Edge Guidance Network for Salient Object Detection</li> 
   <li>PointAE: Point Auto-encoder for 3D Statistical Shape and Texture Modelling</li> 
   <li>Understanding Human Gaze Communication by Spatio-temporal Graph Reasoning</li> 
   <li>Optimizing the F-measure for Threshold-free Salient Object Detection</li> 
   <li>SynDeMo: Synergistic Deep Feature Alignment for Joint Learning of Depth and Ego-Motion</li> 
  </ol>
  <h1>旷视研究院 11 篇论文入选 ICCV 2019</h1> 
  <p>1、Objects365: A Large-scale, High-quality Dataset for Object Detection</p> 
  <p>2、ThunderNet: Towards Real-time Generic Object Detection</p> 
  <p>3、Efficient and Accurate Arbitrary-Shaped Text Detection with PixelAggregation Network</p> 
  <p>4、Semi-supervised Skin Detection by Network with Mutual Guidance</p> 
  <p>5、Semi-Supervised Video Salient Object Detection Using Pseudo-Labels</p> 
  <p>6、Disentangled Image Matting</p> 
  <p>7、Re-ID Driven Localization Refinement for Person Search</p> 
  <p>8、Vehicle Re-identification with Viewpoint-aware Metric Learning</p> 
  <p>9、MetaPruning: Meta Learning for Automatic Neural Network ChannelPruning</p> 
  <p>10、Symmetry-constrained Rectification Network for Scene Text Recognition</p> 
  <p>11、Learning to Paint with Model-based Deep Reinforcement Learning</p> 
  <h1>2019 ICCV Oral</h1> 
  <p><a href="https://arxiv.org/abs/1908.00382" rel="nofollow" data-token="0bc2fd4803158016dd7bc14c12f4b6f6">https://arxiv.org/abs/1908.00382</a></p> 
  <ol>
   <li>Interpolated Convolutional Networks for 3D Point Cloud Understanding</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.04512" rel="nofollow" data-token="70ad548cad245fede0d85651f0af4ece">https://arxiv.org/abs/1908.04512</a></p> 
  <ol>
   <li>Memory-Based Neighbourhood Embedding for Visual Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.04992" rel="nofollow" data-token="3fd2833c8030db8d0240a14140bd8d1e">https://arxiv.org/abs/1908.04992</a></p> 
  <ol>
   <li>Learning Trajectory Dependencies for Human Motion Prediction</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.05436" rel="nofollow" data-token="3da174edfecb9991fe1d7305fca1f0c6">https://arxiv.org/abs/1908.05436</a></p> 
  <ol>
   <li>Domain Adaptation for Structured Output via Discriminative Patch Representations</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1901.05427" rel="nofollow" data-token="e0658273ce985e93a8e4c8901a8d4490">https://arxiv.org/abs/1901.05427</a></p> 
  <ol>
   <li>Deep Non-Rigid Structure from Motion</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.00052" rel="nofollow" data-token="a58842c35b6103499c3afd20ea5b4785">https://arxiv.org/abs/1908.00052</a></p> 
  <ol>
   <li>Scalable Place Recognition Under Appearance Change for Autonomous Driving</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.00178" rel="nofollow" data-token="4459be328b61fed8d04b1b9dde544dd5">https://arxiv.org/abs/1908.00178</a></p> 
  <ol>
   <li>Restoration of Non-rigidly Distorted Underwater Images using a Combination of Compressive Sensing and Local Polynomial Image Representations</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.01940" rel="nofollow" data-token="69e30a219ac62bbc2ca06de04c8647f7">https://arxiv.org/abs/1908.01940</a></p> 
  <ol>
   <li>Consensus Maximization Tree Search Revisited</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.02021" rel="nofollow" data-token="1c12dca0f52c1992c04b3b7860144111">https://arxiv.org/abs/1908.02021</a></p> 
  <ol>
   <li>Weakly Supervised Energy-Based Learning for Action Segmentation</li> 
   <li>Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1811.10144" rel="nofollow" data-token="d6cfbfb008fae5363104bfa1167e34c3">https://arxiv.org/abs/1811.10144</a></p> 
  <ol>
   <li>Controllable Artistic Text Style Transfer via Shape-Matching GAN</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.01354" rel="nofollow" data-token="4f7380d7de9ca711e96bf4fa083436c3">https://arxiv.org/abs/1905.01354</a></p> 
  <ol>
   <li>Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.13369" rel="nofollow" data-token="831c95f4d4290bc71c54b8353b37d977">https://arxiv.org/abs/1907.13369</a></p> 
  <ol>
   <li>Expectation-Maximization Attention Networks for Semantic Segmentation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.13426" rel="nofollow" data-token="204d74ba92c0d0571fc50b8cc59e5b3f">https://arxiv.org/abs/1907.13426</a></p> 
  <ol>
   <li>VideoBERT: A Joint Model for Video and Language Representation Learning</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.01766" rel="nofollow" data-token="5f0ea138792996d56962295402e19644">https://arxiv.org/abs/1904.01766</a></p> 
  <ol>
   <li>CARAFE: Content-Aware ReAssembly of FEatures</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1905.02188.pdf" rel="nofollow" data-token="c389340db84836fb8d0adaa7abe3e997">https://arxiv.org/pdf/1905.02188.pdf</a></p> 
  <ol>
   <li>Habitat: A Platform for Embodied AI Research</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.01201" rel="nofollow" data-token="f206d278387570ce9b374b579e3a11eb">https://arxiv.org/abs/1904.01201</a></p> 
  <ol>
   <li>Equivariant Multi-View Networks</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.00993" rel="nofollow" data-token="92f1288ec0ae24bbe2ecc1bcaab74820">https://arxiv.org/abs/1904.00993</a></p> 
  <ol>
   <li>PointFlow : 3D Point Cloud Generation with Continuous Normalizing Flows</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1906.12320" rel="nofollow" data-token="a717cfd2d487545a07a8a97551f42ea5">https://arxiv.org/abs/1906.12320</a></p> 
  <ol>
   <li>Learnable Triangulation of Human Pose</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.05754" rel="nofollow" data-token="68cbf84eeccca1782487ecb74f271d13">https://arxiv.org/abs/1905.05754</a></p> 
  <ol>
   <li>Learning Implicit Generative Models by Matching Perceptual Features</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.02762v1" rel="nofollow" data-token="749ea46cea465a5bbd518c3da56595a2">https://arxiv.org/abs/1904.02762v1</a></p> 
  <ol>
   <li>COCO-GAN: Generation by Parts via Conditional Coordinating</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.00284" rel="nofollow" data-token="82dde5f50c20ec8e474a32cb4aa39e08">https://arxiv.org/abs/1904.00284</a></p> 
  <ol>
   <li>SlowFast Networks for Video Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1812.03982" rel="nofollow" data-token="732c4a3116a9042c78c826669108fb81">https://arxiv.org/abs/1812.03982</a></p> 
  <ol>
   <li>Exploring Randomly Wired Neural Networks for Image Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.01569" rel="nofollow" data-token="6a67c8d38828b7c4de9f95bf084ae288">https://arxiv.org/abs/1904.01569</a></p> 
  <ol>
   <li>Can GCNs Go as Deep as CNNs?</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.03751" rel="nofollow" data-token="8492c5d755c2bf2c73bcaf1ea336ae36">https://arxiv.org/abs/1904.03751</a></p> 
  <ol>
   <li>Deep SR-ITM: Joint Learning of Super-resolution and Inverse Tone-Mapping for 4K UHD HDR Applications</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.11176" rel="nofollow" data-token="6fc2f570696332bc7339d9af08478fc1">https://arxiv.org/abs/1904.11176</a></p> 
  <ol>
   <li>Meta-Sim Learning to Generate Synthetic Datasets</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.11621" rel="nofollow" data-token="454c0ec66fa3c1c74debc9bb70a00638">https://arxiv.org/abs/1904.11621</a></p> 
  <ol>
   <li>Deep HoughVoting for 3D Object Detection in Point Clouds</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.09664" rel="nofollow" data-token="9dd1b465d9f1c33b0bf93e0f114c3d57">https://arxiv.org/abs/1904.09664</a></p> 
  <ol>
   <li>Variational Adversarial Active Learning</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.00370" rel="nofollow" data-token="bb737990199ba79cac4bff8dcf449be7">https://arxiv.org/abs/1904.00370</a></p> 
  <ol>
   <li>Towards Unconstrained End-to-End Text Spotting</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09231" rel="nofollow" data-token="262aee7402671a43f78b9fd7e8a371a0">https://arxiv.org/abs/1908.09231</a></p> 
  <ol>
   <li>Non-local Recurrent Neural Memory for Supervised Sequence Modeling</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09535" rel="nofollow" data-token="d2fdec6f091e0779cd8ac845dff3084d">https://arxiv.org/abs/1908.09535</a></p> 
  <ol>
   <li>Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09597" rel="nofollow" data-token="8115f4e3167dc86c17c628be8ee7879b">https://arxiv.org/abs/1908.09597</a></p> 
  <h2>&nbsp;</h2> 
  <h1>增加116篇ICCV2019文章</h1> 
  <ol>
   <li>Similarity-Preserving Knowledge Distillation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.09682" rel="nofollow" data-token="f0fa772004437523c1629ac770ec0fbd">https://arxiv.org/abs/1907.09682</a></p> 
  <ol>
   <li>GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.09653" rel="nofollow" data-token="9541a02696716846fa4f02898e795be6">https://arxiv.org/abs/1907.09653</a></p> 
  <ol>
   <li>Tell, Draw, and Repeat: Generating and modifying images based on continual linguistic instruction</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1811.09845.pdf" rel="nofollow" data-token="55717f01600d4a9e91e783dbce31bdba">https://arxiv.org/pdf/1811.09845.pdf</a></p> 
  <ol>
   <li>Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.08489" rel="nofollow" data-token="d1d47422e5cc76776f51ed92405d4fd9">https://arxiv.org/abs/1904.08489</a></p> 
  <ol>
   <li>nocaps: novel object captioning at scale</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1812.08658" rel="nofollow" data-token="46a84d172fe8b79bb310733464351a22">https://arxiv.org/abs/1812.08658</a></p> 
  <ol>
   <li>ThunderNet: Towards Real-time Generic Object Detection</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1903.11752" rel="nofollow" data-token="c682b66b205f18979ddd73029d50ffda">https://arxiv.org/abs/1903.11752</a></p> 
  <ol>
   <li>Scene GraphPrediction with Limited Labels</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.11622" rel="nofollow" data-token="7092ac37d30900e7288b5f254e871d02">https://arxiv.org/abs/1904.11622</a></p> 
  <ol>
   <li>Ego-Pose Estimation and Forecasting as Real-Time PD Control</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1906.03173" rel="nofollow" data-token="ec11394c46622553da3a57a2d373dc88">https://arxiv.org/abs/1906.03173</a></p> 
  <ol>
   <li>The Trajectron: Probabilistic Multi-Agent Trajectory Modeling withDynamic Spatiotemporal Graphs</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1810.05993" rel="nofollow" data-token="0d819bb653b4017c2ade59bce08c53f3">https://arxiv.org/abs/1810.05993</a></p> 
  <ol>
   <li>End-to-End Learning of Representations for Asynchronous Event-BasedData</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.08245" rel="nofollow" data-token="2094627ee0421c0c5d4ddc2b51412ab6">https://arxiv.org/abs/1904.08245</a></p> 
  <ol>
   <li>Efficient Learning on Point Clouds with Basis Point Sets</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09186" rel="nofollow" data-token="7dc05b87668464ae246dd02e1f421718">https://arxiv.org/abs/1908.09186</a></p> 
  <ol>
   <li>Dynamic Kernel Distillation for Efficient Pose Estimation in Videos</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09216" rel="nofollow" data-token="e533a312939c1f01dff17d2c6822f326">https://arxiv.org/abs/1908.09216</a></p> 
  <ol>
   <li>Single-Stage Multi-Person Pose Machines</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09220" rel="nofollow" data-token="d46b7791ce14b8a0e4e9a95989c3f4e3">https://arxiv.org/abs/1908.09220</a></p> 
  <ol>
   <li>Towards Unsupervised Image Captioning with Shared Multimodal Embeddings</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09317" rel="nofollow" data-token="7d17167ee8c5a539a37e4ca56b9d699e">https://arxiv.org/abs/1908.09317</a></p> 
  <ol>
   <li>advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09327" rel="nofollow" data-token="b1b4634eb6ab114f170c09eec1c5612b">https://arxiv.org/abs/1908.09327</a></p> 
  <ol>
   <li>Shape-Aware Human Pose and Shape Reconstruction Using Multi-View Images</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09464" rel="nofollow" data-token="d2c6781589d640453f3cee965cc33101">https://arxiv.org/abs/1908.09464</a></p> 
  <ol>
   <li>Relation Distillation Networks for Video Object Detection</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09511" rel="nofollow" data-token="7f2a542f8d3cd58430bc198fc0e14b15">https://arxiv.org/abs/1908.09511</a></p> 
  <ol>
   <li>Object-Driven Multi-Layer Scene Decomposition From a Single Image</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09521" rel="nofollow" data-token="60e4084910534f6e197729f20b4947bb">https://arxiv.org/abs/1908.09521</a></p> 
  <ol>
   <li>Embarrassingly Simple Binary Representation Learning</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09573" rel="nofollow" data-token="4c391ba3a7eca5d104c9fb45fe49d649">https://arxiv.org/abs/1908.09573</a></p> 
  <ol>
   <li>Moulding Humans: Non-parametric 3D Human Shape Estimation from Single Images</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.00439" rel="nofollow" data-token="1d53d2256b55bfaecd7c395cb6c1f5e7">https://arxiv.org/abs/1908.00439</a></p> 
  <ol>
   <li>Learning the Model Update for Siamese Trackers</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.00855" rel="nofollow" data-token="05f3fe57641a2ebe847c926c1cad9418">https://arxiv.org/abs/1908.00855</a></p> 
  <ol>
   <li>Distilling Knowledge From a Deep Pose Regressor Network</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.00858" rel="nofollow" data-token="d2d6883df6c9cad0b46bd30fc60dfd66">https://arxiv.org/abs/1908.00858</a></p> 
  <ol>
   <li>Permutation-invariant Feature Restructuring for Correlation-aware Image Set-based Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.01174" rel="nofollow" data-token="63e34cb2797adc5987d62bbc5b45dd6a">https://arxiv.org/abs/1908.01174</a></p> 
  <ol>
   <li>ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.01323" rel="nofollow" data-token="f5cf374bbf46adbae2bdff90dd309bc6">https://arxiv.org/abs/1908.01323</a></p> 
  <ol>
   <li>Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.01491" rel="nofollow" data-token="8f258d9591e651e0b77718770eaf8af1">https://arxiv.org/abs/1908.01491</a></p> 
  <ol>
   <li>View N-gram Network for 3D Object Retrieval</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.01958" rel="nofollow" data-token="60b9e1255b88dfab2ca8594b2dc43ac1">https://arxiv.org/abs/1908.01958</a></p> 
  <ol>
   <li>Semi-supervised Skin Detection by Network with Mutual Guidance</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.01977" rel="nofollow" data-token="f2e01c2cc94f3384bf48fb940f97f2d7">https://arxiv.org/abs/1908.01977</a></p> 
  <ol>
   <li>Deep Self-Learning From Noisy Labels</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.02160" rel="nofollow" data-token="2bb1ee113267ea9c12855584d8b177e1">https://arxiv.org/abs/1908.02160</a></p> 
  <ol>
   <li>Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.02231" rel="nofollow" data-token="6c782fbc3b85bbcc0e811ab1dba44133">https://arxiv.org/abs/1908.02231</a></p> 
  <ol>
   <li>Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.02441" rel="nofollow" data-token="9e5f78811280d24d27dcfa1e4e8db4c7">https://arxiv.org/abs/1908.02441</a></p> 
  <ol>
   <li>Expert Sample Consensus Applied to Camera Re-Localization</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.02484" rel="nofollow" data-token="9b2e0754249486bce2e0294c14b145d0">https://arxiv.org/abs/1908.02484</a></p> 
  <ol>
   <li>SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.02660" rel="nofollow" data-token="93831f8cff0debc8db30e5a7343a66c4">https://arxiv.org/abs/1908.02660</a></p> 
  <ol>
   <li>GP2C: Geometric Projection Parameter Consensus for Joint 3D Pose and Focal Length Estimation in the Wild</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.02809" rel="nofollow" data-token="e28f74d630e7cfd963a4eaa4f5f634aa">https://arxiv.org/abs/1908.02809</a></p> 
  <ol>
   <li>SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.01416" rel="nofollow" data-token="84d5581a043bd23abb04d809fe1faf07">https://arxiv.org/abs/1904.01416</a></p> 
  <ol>
   <li>Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.12704" rel="nofollow" data-token="fd6cbeff38ec82206730a4c96d22fc1e">https://arxiv.org/abs/1907.12704</a></p> 
  <ol>
   <li>Orientation-aware Semantic Segmentation on Icosahedron Spheres</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.12849" rel="nofollow" data-token="d0fe20b7968dbcec761a2e9d57b80018">https://arxiv.org/abs/1907.12849</a></p> 
  <ol>
   <li>EMPNet: Neural Localisation and Mapping using Embedded Memory Points</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.13268" rel="nofollow" data-token="6f39835e4b15612262e510e71def112c">https://arxiv.org/abs/1907.13268</a></p> 
  <ol>
   <li>SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.11308" rel="nofollow" data-token="f4d2bcb23120a860010dddb96de5b61e">https://arxiv.org/abs/1907.11308</a></p> 
  <ol>
   <li>On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.11684" rel="nofollow" data-token="fc50d4a332d80a7187df14752e263bfc">https://arxiv.org/abs/1907.11684</a></p> 
  <ol>
   <li>Goal-Driven Sequential Data Abstraction</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.12336" rel="nofollow" data-token="449957682aa35863b8b05e9c77e0df56">https://arxiv.org/abs/1907.12336</a></p> 
  <ol>
   <li>Recursive Cascaded Networks for Unsupervised Medical Image Registration</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.12353" rel="nofollow" data-token="9daaab691532bf55c84b085055b056c0">https://arxiv.org/abs/1907.12353</a></p> 
  <ol>
   <li>Learn to Scale: Generating Multipolar Normalized Density Map for Crowd Counting</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.12428" rel="nofollow" data-token="276d275f8b9ae4b3db8a04025672afb2">https://arxiv.org/abs/1907.12428</a></p> 
  <ol>
   <li>HoloGAN: Unsupervised learning of 3D representations from natural images</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.01326" rel="nofollow" data-token="d324827b376a8a99a6c7c4b6f429f35c">https://arxiv.org/abs/1904.01326</a></p> 
  <ol>
   <li>MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1903.10258" rel="nofollow" data-token="634f05cb6ea504e16a6f2b324281b777">https://arxiv.org/abs/1903.10258</a></p> 
  <ol>
   <li>FrameNet: Learning Local Canonical Frames of 3D Surfaces from a Single RGB Image</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1903.12305.pdf" rel="nofollow" data-token="429e21828d7d27cf73f99e5e59ce0d47">https://arxiv.org/pdf/1903.12305.pdf</a></p> 
  <ol>
   <li>Face De-occlusion using 3D Morphable Model and Generative Adversarialhttp://image.inha.ac.kr/paper/ICCV2019_Xaiowei.pdf</li> 
   <li>Deep Meta Learning for Real-Time Target-Aware Visual Tracking</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1712.09153.pdf" rel="nofollow" data-token="ef76518450db3625d3b73a4c3d95bb0d">https://arxiv.org/pdf/1712.09153.pdf</a></p> 
  <ol>
   <li>Switchable Whitening for Deep Representation Learning</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.09739" rel="nofollow" data-token="26211086d09bb3eb7781483d19cefce6">https://arxiv.org/abs/1904.09739</a></p> 
  <ol>
   <li>Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.05049" rel="nofollow" data-token="505636a5ce9a8a88734ee15a52d3fc20">https://arxiv.org/abs/1904.05049</a></p> 
  <ol>
   <li>Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1902.06729" rel="nofollow" data-token="e03f105d347cf262cff7472718b9f3c3">https://arxiv.org/abs/1902.06729</a></p> 
  <ol>
   <li>Task2Vec: Task Embedding for Meta-Learning</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1902.03545" rel="nofollow" data-token="64d80992a193f57b6f2e9af11cd166c1">https://arxiv.org/abs/1902.03545</a></p> 
  <ol>
   <li>ACE: Adapting to Changing Environments for Semantic Segmentation</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1904.06268.pdf" rel="nofollow" data-token="a5eb79fdd732ef9ecec8ff0835e2b418">https://arxiv.org/pdf/1904.06268.pdf</a></p> 
  <ol>
   <li>Few-shot Object Detection via Feature Reweighting</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1812.01866.pdf" rel="nofollow" data-token="cde44ff4ddeaf4ff66edc44ea150d7cd">https://arxiv.org/pdf/1812.01866.pdf</a></p> 
  <ol>
   <li>Disentangling Propagation and Generation for Video Prediction</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1812.00452.pdf" rel="nofollow" data-token="7f4b07f8ab0db1dd58f390843be3811e">https://arxiv.org/pdf/1812.00452.pdf</a></p> 
  <ol>
   <li>An Empirical Study of Spatial Attention Mechanisms in Deep Networks</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1904.05873.pdf" rel="nofollow" data-token="b1e2d7ec37e375bac287589b3edd22bc">https://arxiv.org/pdf/1904.05873.pdf</a></p> 
  <ol>
   <li>Fashion++: Minimal Edits for Outfit Improvement</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1904.09261.pdf" rel="nofollow" data-token="f0a8c8a66032e60350f29652bd632b13">https://arxiv.org/pdf/1904.09261.pdf</a></p> 
  <ol>
   <li>Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1903.11649.pdf" rel="nofollow" data-token="28c9636a13725776021f397b8ded64aa">https://arxiv.org/pdf/1903.11649.pdf</a></p> 
  <ol>
   <li>Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1902.03751.pdf" rel="nofollow" data-token="75a4a83172a5e8535ebc876ec9cdd67c">https://arxiv.org/pdf/1902.03751.pdf</a></p> 
  <ol>
   <li>SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1905.07512.pdf" rel="nofollow" data-token="01d2f537f6ba96f5870fd75a9d551669">https://arxiv.org/pdf/1905.07512.pdf</a></p> 
  <ol>
   <li>EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.11781" rel="nofollow" data-token="f486510b43acd8401fea86b7a22c842d">https://arxiv.org/abs/1904.11781</a></p> 
  <ol>
   <li>Texture Fields: Learning Texture Representations in Function Space</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.07259" rel="nofollow" data-token="efaca2f1dff59f9f82e5eb69e830034e">https://arxiv.org/abs/1905.07259</a></p> 
  <ol>
   <li>AMASS: Archive of Motion Capture as Surface Shapes</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.03278" rel="nofollow" data-token="5cdcacd00aec93fe08847808cdc004bc">https://arxiv.org/abs/1904.03278</a></p> 
  <ol>
   <li>End-to-end Learning for Graph Decomposition</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1812.09737.pdf" rel="nofollow" data-token="5e3f2f31114bb368ab8acdccede9abde">https://arxiv.org/pdf/1812.09737.pdf</a></p> 
  <ol>
   <li>Towards Multi-pose Guided Virtual Try-on Network</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1902.11026" rel="nofollow" data-token="a511bed2f77b68b5d4b7b65d68e445c0">https://arxiv.org/abs/1902.11026</a></p> 
  <ol>
   <li>Learning to Reconstruct 3D Manhattan Wireframes from a Single Image</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.07482" rel="nofollow" data-token="662eb12fef0e8d7e43c2e2c900cdce97">https://arxiv.org/abs/1905.07482</a></p> 
  <ol>
   <li>Coherent Semantic Attention for Image Inpainting</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.12384" rel="nofollow" data-token="e7cc1fa8f0f16b9c51379c106be1c589">https://arxiv.org/abs/1905.12384</a></p> 
  <ol>
   <li>LayoutVAE: Stochastic Scene Layout Generation from a Label Set</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10719" rel="nofollow" data-token="240d7abe68ac4e06a3e7af76aeaf3afb">https://arxiv.org/abs/1907.10719</a></p> 
  <ol>
   <li>Co-Evolutionary Compression for Unpaired Image Translation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10804" rel="nofollow" data-token="5e5e2fcc35a2ebeae6aa4c7c2784d720">https://arxiv.org/abs/1907.10804</a></p> 
  <ol>
   <li>Enhancing Adversarial Example Transferability with an Intermediate Level Attack</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10823" rel="nofollow" data-token="80387d6684e98ff306779e3251c278a6">https://arxiv.org/abs/1907.10823</a></p> 
  <ol>
   <li>Simultaneous multi-view instance detection with learned geometric soft-constraints</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10892" rel="nofollow" data-token="62bd57a4a356822ebc10e44252b879d8">https://arxiv.org/abs/1907.10892</a></p> 
  <ol>
   <li>Gated2Depth: Real-time Dense Lidar from Gated Images</li> 
  </ol>
  <p><a href="https://www.cs.princeton.edu/~fheide/papers/Gated2Depth_preprint.pdf" rel="nofollow" data-token="81458525d4eb142e5566e8fd9a186233">https://www.cs.princeton.edu/~fheide/papers/Gated2Depth_preprint.pdf</a></p> 
  <ol>
   <li>Moment Matching for Multi-Source Domain Adaptation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1812.01754" rel="nofollow" data-token="817a9609ff694230d7b8d79d8326d683">https://arxiv.org/abs/1812.01754</a></p> 
  <ol>
   <li>Learning Compositional Representations for Few-Shot Recognition</li> 
  </ol>
  <p><a href="https://sites.google.com/view/comprepr/home" rel="nofollow" data-token="e978e262a5f0179d1906a65bb5289ea0">https://sites.google.com/view/comprepr/home</a></p> 
  <ol>
   <li>Digging Into Self-Supervised Monocular Depth Estimation</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1806.01260.pdf" rel="nofollow" data-token="ed3d672e21cb1f7eceae93091a66836d">https://arxiv.org/pdf/1806.01260.pdf</a></p> 
  <ol>
   <li>Deep Interpretable Non-Rigid Structure from Motion</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1902.10840.pdf" rel="nofollow" data-token="0267a6d454f6cedc74ff475bd24cc1e4">https://arxiv.org/pdf/1902.10840.pdf</a></p> 
  <ol>
   <li>PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1905.01296.pdf" rel="nofollow" data-token="466a907f40d0fba634c73a776e197a19">https://arxiv.org/pdf/1905.01296.pdf</a></p> 
  <ol>
   <li>Lifelong GAN: Continual Learning for Conditional Image Generation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10107" rel="nofollow" data-token="4e91aaf637197fcc81f70c3525e7e5e3">https://arxiv.org/abs/1907.10107</a></p> 
  <ol>
   <li>Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10164" rel="nofollow" data-token="33a009d9a3d40ada37854cdcf82ea10b">https://arxiv.org/abs/1907.10164</a></p> 
  <ol>
   <li>Towards Adversarially Robust Object Detection</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10310" rel="nofollow" data-token="fa6a342dc2ae274930aa47dbfdaecf9a">https://arxiv.org/abs/1907.10310</a></p> 
  <ol>
   <li>6-DOF GraspNet: Variational Grasp Generation for Object Manipulation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.10520" rel="nofollow" data-token="1eff5a10dad7929f91e1954d9f5dd925">https://arxiv.org/abs/1905.10520</a></p> 
  <ol>
   <li>Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10178" rel="nofollow" data-token="96c13e67f08cd9f7bfbb3fee87fbca8e">https://arxiv.org/abs/1907.10178</a></p> 
  <ol>
   <li>DAFL: Data-Free Learning of Student Networks</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.01186" rel="nofollow" data-token="d0a11eb0f6be8c6ee4eb6e63b50542f7">https://arxiv.org/abs/1904.01186</a></p> 
  <ol>
   <li>Multi-adversarial Faster-RCNN for Unrestricted Object Detection</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10343" rel="nofollow" data-token="1c95878a6f6f4026d9c4bf282e670041">https://arxiv.org/abs/1907.10343</a></p> 
  <ol>
   <li>Boosting Few-Shot Visual Learning with Self-Supervision</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1906.05186" rel="nofollow" data-token="a18a3f3b436eb4b2c84aac8090fc3ca4">https://arxiv.org/abs/1906.05186</a></p> 
  <ol>
   <li>A Quaternion-based Certifiably Optimal Solution to the Wahba Problem with Outliers</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.12536" rel="nofollow" data-token="bf710438a591af04c8dc56f26b368579">https://arxiv.org/abs/1905.12536</a></p> 
  <ol>
   <li>Embodied Visual Recognition</li> 
   <li>Rethinking ImageNet Pre-training</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1811.08883" rel="nofollow" data-token="e23892f8eda04033dd78932d5b08994e">https://arxiv.org/abs/1811.08883</a></p> 
  <ol>
   <li>TensorMask: A Foundation for Dense Object Segmentation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1903.12174" rel="nofollow" data-token="b7e515dd68d72d7dcf5a3ffcfe335eb5">https://arxiv.org/abs/1903.12174</a></p> 
  <ol>
   <li>3D Point Cloud Learning for Large-scale Environment Analysis and Place Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1812.07050" rel="nofollow" data-token="87753ad8a83a12048ff2984944c2383c">https://arxiv.org/abs/1812.07050</a></p> 
  <ol>
   <li>Selectivity or Invariance: Boundary-aware Salient Object Detection</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1812.10066.pdf" rel="nofollow" data-token="2b6874766b3d70a1888a17c29526ccd4">https://arxiv.org/pdf/1812.10066.pdf</a></p> 
  <ol>
   <li>Creativity Inspired Zero-Shot Learning</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.01109" rel="nofollow" data-token="a2b56b4ce159d6bad5786c8f67276701">https://arxiv.org/abs/1904.01109</a></p> 
  <ol>
   <li>HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1906.03327" rel="nofollow" data-token="4e218d6d564c39bffc3ef43f07f030b0">https://arxiv.org/abs/1906.03327</a></p> 
  <ol>
   <li>Correlation Congruence for Knowledge Distillation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.018029" rel="nofollow" data-token="7c1e0b31355733a1fc735b663e428ec6">https://arxiv.org/abs/1904.018029</a></p> 
  <ol>
   <li>VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.03493" rel="nofollow" data-token="a65498d013a83ef056b8a6f9714b4f88">https://arxiv.org/abs/1904.03493</a></p> 
  <ol>
   <li>Episodic Training for Domain Generalization</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1902.00113" rel="nofollow" data-token="4b4fe77f8f8db757c3b8237070589b04">https://arxiv.org/abs/1902.00113</a></p> 
  <ol>
   <li>GarNet: A Two-stream Network for Fast and Accurate 3D Cloth Draping</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1811.10983v2" rel="nofollow" data-token="90c9c5143a8349c1f6bd1641405ad1bd">https://arxiv.org/abs/1811.10983v2</a></p> 
  <ol>
   <li>Semi-supervised Domain Adaptation via Minimax Entropy</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.06487" rel="nofollow" data-token="8f939b298e8efbf992641acfe48dfb07">https://arxiv.org/abs/1904.06487</a></p> 
  <ol>
   <li>xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10045" rel="nofollow" data-token="5cd8288cbc5ad350a3f33ccc1407735f">https://arxiv.org/abs/1907.10045</a></p> 
  <ol>
   <li>Canonical Surface Mapping via Geometric Cycle Consistency</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10043" rel="nofollow" data-token="f068e33fe3f39caaf1eba1df8b93de31">https://arxiv.org/abs/1907.10043</a></p> 
  <ol>
   <li>Incremental Class Discovery for Semantic Segmentation with RGBD Sensing</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10008" rel="nofollow" data-token="cbe75801d275f763ef9a62a6d8315062">https://arxiv.org/abs/1907.10008</a></p> 
  <ol>
   <li>U4D: Unsupervised 4D Dynamic Scene Understanding</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.09905" rel="nofollow" data-token="166f131e94c4bfa89ac7c16a937f616a">https://arxiv.org/abs/1907.09905</a></p> 
  <ol>
   <li>BMN: Boundary-Matching Network for Temporal Action Proposal Generation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.09702" rel="nofollow" data-token="aa0491f21134b28f3d9f387c044bf11b">https://arxiv.org/abs/1907.09702</a></p> 
  <ol>
   <li>SPGNet: Semantic Prediction Guidance for Scene Parsing</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09798" rel="nofollow" data-token="67d67a2417195df844ac2250bbb66b57">https://arxiv.org/abs/1908.09798</a></p> 
  <ol>
   <li>Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1811.07456" rel="nofollow" data-token="46d73ab4119f1b5f9a763041a2d498b5">https://arxiv.org/abs/1811.07456</a></p> 
  <ol>
   <li>DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1812.11017" rel="nofollow" data-token="330ce3757cb83ead12aa3ce2f2a9d0cf">https://arxiv.org/abs/1812.11017</a></p> 
  <ol>
   <li>Closed-Form Optimal Two-View Triangulation Based on Angular Errors</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1903.09115" rel="nofollow" data-token="a613b495b58b08237b267704534044b5">https://arxiv.org/abs/1903.09115</a></p> 
  <ol>
   <li>Learning Combinatorial Embedding Networks for Deep Graph Matching</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.00597" rel="nofollow" data-token="35f532ae73560d4ef55a6e3b1a7d7e81">https://arxiv.org/abs/1904.00597</a></p> 
  <ol>
   <li>A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.03425" rel="nofollow" data-token="eec4dbf9887bc9cef3b9c931994fe9c8">https://arxiv.org/abs/1904.03425</a></p> 
  <ol>
   <li>Remote Heart Rate Measurement from Highly Compressed Facial Videos: an End-to-end Deep Learning Solution with Video Enhancement</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.11921" rel="nofollow" data-token="5f2bf3d4944ed2a58b335bfcbb960d30">https://arxiv.org/abs/1907.11921</a></p> 
  <ol>
   <li>Symmetry-constrained Rectification Network for Scene Text Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.01957" rel="nofollow" data-token="7b2c1990d0c54bd29c6efab19f916381">https://arxiv.org/abs/1908.01957</a></p> 
  <ol>
   <li>STM: SpatioTemporal and Motion Encoding for Action Recognition</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.02486" rel="nofollow" data-token="c1e952ed6a398e4f6f9690d0cb68f012">https://arxiv.org/abs/1908.02486</a></p> 
  <ol>
   <li>Explicit Shape Encoding for Real-Time Instance Segmentation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.04067" rel="nofollow" data-token="7a9777012a22259c72dd71acce88c7a1">https://arxiv.org/abs/1908.04067</a></p> 
  <ol>
   <li>Few-Shot Learning with Global Class Representations</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.05257" rel="nofollow" data-token="f5e58bd284a8d81459b73e059228584b">https://arxiv.org/abs/1908.05257</a></p> 
  <ol>
   <li>Symmetric Cross Entropy for Robust Learning with Noisy Labels</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.06112" rel="nofollow" data-token="b22210d3e97e7962ded43c872403b0f8">https://arxiv.org/abs/1908.06112</a></p> 
  <ol>
   <li>Human Mesh Recovery from Monocular Images via a Skeleton-disentangled Representation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.07172" rel="nofollow" data-token="bd961ed531cc16dab3005e064c8bcc8c">https://arxiv.org/abs/1908.07172</a></p> 
  <ol>
   <li>DADA: Depth-Aware Domain Adaptation in Semantic Segmentation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.01886" rel="nofollow" data-token="b6e00980f1b184b88ed9a7234ceeb230">https://arxiv.org/abs/1904.01886</a></p> 
  <h2>&nbsp;</h2> 
  <h1>增加35篇包含开源代码的ICCV2019</h1> 
  <ol>
   <li>Bidirectional One-Shot Unsupervised Domain Mapping</li> 
  </ol>
  <p><a href="https://github.com/tomercohen11/BiOST" rel="nofollow" data-token="60fdba5868478f25d87057e73cd8938c">https://github.com/tomercohen11/BiOST</a></p> 
  <ol>
   <li>Joint Monocular 3D Detection and Tracking</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1811.10742" rel="nofollow" data-token="dddcfe7bc7313fa57d5e55a35310fa33">https://arxiv.org/abs/1811.10742</a></p> 
  <p><a href="https://github.com/ucbdrive/3d-vehicle-tracking" rel="nofollow" data-token="8af2987ecaf4641da0a78d0d65fbcd69">https://github.com/ucbdrive/3d-vehicle-tracking</a></p> 
  <ol>
   <li>MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1906.06059" rel="nofollow" data-token="d17f220808dfc69e00c14c783eaf040a">https://arxiv.org/abs/1906.06059</a></p> 
  <p><a href="https://github.com/vita-epfl/monoloco" rel="nofollow" data-token="ae36d8720ee7666d8b473f347ec7442d">https://github.com/vita-epfl/monoloco</a></p> 
  <ol>
   <li>Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data</li> 
  </ol>
  <p><a href="https://github.com/xw-hu/Mask-ShadowGAN" rel="nofollow" data-token="90fee9a2a24c9e3b5e97b55712d2e58a">https://github.com/xw-hu/Mask-ShadowGAN</a></p> 
  <ol>
   <li>Towards High-Resolution Salient Object Detection</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.07274" rel="nofollow" data-token="d6be880e11a7a674169970f60c64ad17">https://arxiv.org/abs/1908.07274</a></p> 
  <p><a href="https://github.com/yi94code/HRSOD" rel="nofollow" data-token="5e4fc03adac67fe7a905b45c78ff9aff">https://github.com/yi94code/HRSOD</a></p> 
  <ol>
   <li>Confidence Regularized Self-Training</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.09822" rel="nofollow" data-token="64b4a6a43dd22c46a6ae7ebc477747c4">https://arxiv.org/abs/1908.09822</a></p> 
  <p><a href="https://github.com/yzou2/CRST" rel="nofollow" data-token="0d7ae688ace94850a7157014a0a9c041">https://github.com/yzou2/CRST</a></p> 
  <ol>
   <li>Optimizing the F-measure for Threshold-free Salient Object Detectionhttp://data.kaizhao.net/publications/iccv2019fmeasure.pdf</li> 
  </ol>
  <p><a href="https://github.com/zeakey/iccv2019-fmeasure" rel="nofollow" data-token="a5b64089b0ed0c8a5341fba7156f7e00">https://github.com/zeakey/iccv2019-fmeasure</a></p> 
  <ol>
   <li>Perspective-Guided Convolution Networks for Crowd Counting</li> 
  </ol>
  <p><a href="https://github.com/Zhaoyi-Yan/PGCNet" rel="nofollow" data-token="eb90e0a19030fa38e9cb1ff545c56034">https://github.com/Zhaoyi-Yan/PGCNet</a></p> 
  <ol>
   <li>End-to-End Wireframe Parsing</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.03246" rel="nofollow" data-token="d32f71e6311f358a46275e30712e15f3">https://arxiv.org/abs/1905.03246</a></p> 
  <p><a href="https://github.com/zhou13/lcnn" rel="nofollow" data-token="f51359451f2a4ae0e780d46965190ff5">https://github.com/zhou13/lcnn</a></p> 
  <ol>
   <li>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.12743http://github.com/cmhungsteve/TA3N" rel="nofollow" data-token="58a7c0a01e5eb3e92abe40687715d189">https://arxiv.org/abs/1907.12743http://github.com/cmhungsteve/TA3N</a></p> 
  <ol>
   <li>From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.06473" rel="nofollow" data-token="3a2a89bcc6c054e7f5b5a476f08b3657">https://arxiv.org/abs/1908.06473</a></p> 
  <p><a href="https://github/" rel="nofollow" data-token="06e5b7f1a7881ce7032d68cce083b421">https://github</a>. com/xhp-hust-2018-2011/S-DCNet</p> 
  <ol>
   <li>Free-form Video Inpainting with 3D Gated Convolution and Temporal PatchGAN</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.10247" rel="nofollow" data-token="2e8f062ef66d3307d1ee9b37a1a30805">https://arxiv.org/abs/1904.10247</a></p> 
  <p><a href="https://github.com/amjltc295/Free-Form-Video-Inpainting" rel="nofollow" data-token="59372232e3575776472ac51f57c7eb58">https://github.com/amjltc295/Free-Form-Video-Inpainting</a></p> 
  <ol>
   <li>What Would You Expect? Anticipating Egocentric Actions with Rolling-Unrolling LSTMs and Modality Attention</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1905.09035.pdf" rel="nofollow" data-token="9fd9ad22a021e63a9ddc690473960015">https://arxiv.org/pdf/1905.09035.pdf</a></p> 
  <p><a href="https://github.com/antoninofurnari/rulstm" rel="nofollow" data-token="9984117cfe3e0dff3f1487d062a504df">https://github.com/antoninofurnari/rulstm</a></p> 
  <ol>
   <li>CompenNet++: End-to-end Full Projector Compensation</li> 
  </ol>
  <p><a href="https://github.com/BingyaoHuang/CompenNet-plusplus" rel="nofollow" data-token="fd71cdb0e22c36d8e4bc999af1b309f9">https://github.com/BingyaoHuang/CompenNet-plusplus</a></p> 
  <ol>
   <li>Pose-aware Dynamic Attention for Human Object Interaction Detection</li> 
  </ol>
  <p><a href="https://github.com/bobwan1995/Pose-aware-Dynamic-Attention-for-Human-Object-Interaction-Detection" rel="nofollow" data-token="4941c1eb4bd3200ed71ca139c4605be8">https://github.com/bobwan1995/Pose-aware-Dynamic-Attention-for-Human-Object-Interaction-Detection</a></p> 
  <ol>
   <li>Temporally-Aggregating Spatial Encoder-Decoder for Video Saliency Detection</li> 
  </ol>
  <p><a href="https://github.com/kylemin/TASED-Net" rel="nofollow" data-token="d7a3bbbe7bdc5a96f7ddd910b6d74546">https://github.com/kylemin/TASED-Net</a></p> 
  <ol>
   <li>PU-GAN: a Point Cloud Upsampling Adversarial Network</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.10844" rel="nofollow" data-token="05273f41a51d221223b1fd24ec6e964b">https://arxiv.org/abs/1907.10844</a></p> 
  <p><a href="https://github.com/liruihui/PU-GAN" rel="nofollow" data-token="e75de57f938ebcca957f7af9e2de4517">https://github.com/liruihui/PU-GAN</a></p> 
  <ol>
   <li>A Closed-form Solution to Universal Style Transfer</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1906.00668" rel="nofollow" data-token="5723893c69f39cfa987d1e19fb63eaa7">https://arxiv.org/abs/1906.00668</a></p> 
  <p><a href="https://github.com/lu-m13/OptimalStyleTransfer" rel="nofollow" data-token="e3394dbbada93d30ba917745ae64836a">https://github.com/lu-m13/OptimalStyleTransfer</a></p> 
  <ol>
   <li>Video Face Clustering with Unknown Number of Clusters</li> 
  </ol>
  <p><a href="https://github.com/makarandtapaswi/BallClustering_ICCV2019" rel="nofollow" data-token="09ef07e44bb98b76868cfe8b02a8008e">https://github.com/makarandtapaswi/BallClustering_ICCV2019</a></p> 
  <ol>
   <li>TSM: Temporal Shift Module for Efficient Video Understanding</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1811.08383" rel="nofollow" data-token="5bacc6ff59245e6ffc92b00f212491e2">https://arxiv.org/abs/1811.08383</a></p> 
  <p><a href="https://github.com/mit-han-lab/temporal-shift-module" rel="nofollow" data-token="60b3da37e97ad7c40277480fd1b06b0a">https://github.com/mit-han-lab/temporal-shift-module</a></p> 
  <ol>
   <li>Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1907.11346" rel="nofollow" data-token="6f08bdee719c230e49fe271f4ff365c0">https://arxiv.org/abs/1907.11346</a></p> 
  <p><a href="https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE" rel="nofollow" data-token="36614c559ce56c3600c193992cc96d7c">https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE</a></p> 
  <ol>
   <li>3D-RelNet: Joint Object and Relational Network for 3D Prediction</li> 
  </ol>
  <p><a href="https://arxiv.org/pdf/1906.02729.pdf" rel="nofollow" data-token="336bde5edeec5f97299d92a7e245ad2f">https://arxiv.org/pdf/1906.02729.pdf</a></p> 
  <p><a href="https://github.com/nileshkulkarni/relative3d" rel="nofollow" data-token="35fce9976f46ef789e388decf0c329df">https://github.com/nileshkulkarni/relative3d</a></p> 
  <ol>
   <li>Few-shot Unsupervised Image-to-Image Translation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.01723" rel="nofollow" data-token="8aefe4c76eabb5f446e33a449ed10439">https://arxiv.org/abs/1905.01723</a></p> 
  <p><a href="https://github.com/nvlabs/FUNIT/" rel="nofollow" data-token="2455d86fbf613870c41d3b0d3c50153c">https://github.com/nvlabs/FUNIT/</a></p> 
  <ol>
   <li>Metric Learning with HORDE: High-Order Regularizer for Deep Embeddings</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.02735" rel="nofollow" data-token="aa030c0d66942a4e8fd39c4b97214afd">https://arxiv.org/abs/1908.02735</a></p> 
  <p><a href="https://github.com/pierre-jacob/ICCV2019-Horde" rel="nofollow" data-token="c71d4a7ae8ed28926e3a94b3b8182211">https://github.com/pierre-jacob/ICCV2019-Horde</a></p> 
  <ol>
   <li>Model Vulnerability to Distributional Shifts over Image Transformation Sets</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1903.11900" rel="nofollow" data-token="4b30d61633f1a11a39063501ff8ced6a">https://arxiv.org/abs/1903.11900</a></p> 
  <p><a href="https://github.com/ricvolpi/domain-shift-robustness" rel="nofollow" data-token="860b1fc6a5b556f4e45540c77e50dbae">https://github.com/ricvolpi/domain-shift-robustness</a></p> 
  <ol>
   <li>Language-Conditioned Graph Networks for Relational Reasoning</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1905.04405" rel="nofollow" data-token="c1164735e09fc22b9e59bea68235bac4">https://arxiv.org/abs/1905.04405</a></p> 
  <p><a href="https://github.com/ronghanghu/lcgn" rel="nofollow" data-token="fe1e8e484ddff09ef6654c13bddf8130">https://github.com/ronghanghu/lcgn</a></p> 
  <ol>
   <li>Domain Intersection and Domain Difference</li> 
  </ol>
  <p><a href="https://github.com/sagiebenaim/DomainIntersectionDifference" rel="nofollow" data-token="fdae4250bd37aa77facf7a24701f2aca">https://github.com/sagiebenaim/DomainIntersectionDifference</a></p> 
  <ol>
   <li>Probabilistic Face Embeddings</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.09658" rel="nofollow" data-token="ad9846c591e4a0307cad003ef00b7269">https://arxiv.org/abs/1904.09658</a></p> 
  <p><a href="https://github.com/seasonSH/Probabilistic-Face-Embeddings" rel="nofollow" data-token="e55ca9a9e7fa01507b483ac9e9ea5e6b">https://github.com/seasonSH/Probabilistic-Face-Embeddings</a></p> 
  <ol>
   <li>Counting with Focus for Free</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1903.12206" rel="nofollow" data-token="74190fcfdae4bccf2f8f41448d8e9c61">https://arxiv.org/abs/1903.12206</a></p> 
  <p><a href="https://github.com/shizenglin/Counting-with-Focus-for-Free" rel="nofollow" data-token="a3c4824286b88a57f68e1e9ec461a167">https://github.com/shizenglin/Counting-with-Focus-for-Free</a></p> 
  <ol>
   <li>CCNet: Criss-Cross Attention for Semantic Segmentation</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1811.11721" rel="nofollow" data-token="86a2578c7047573c8e76683e3f9d0df0">https://arxiv.org/abs/1811.11721</a></p> 
  <p><a href="https://github.com/speedinghzl/CCNet" rel="nofollow" data-token="a62b00f1a8d4e403d289d6096eda7012">https://github.com/speedinghzl/CCNet</a></p> 
  <ol>
   <li>ABD-Net: Attentive but Diverse Person Re-Identification</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1908.01114" rel="nofollow" data-token="debdc392ab2882acb58f9debc0639eb2">https://arxiv.org/abs/1908.01114</a></p> 
  <p><a href="https://github.com/TAMU-VITA/ABD-Net" rel="nofollow" data-token="d29ff38cef25ab6a81142c21e595abdc">https://github.com/TAMU-VITA/ABD-Net</a></p> 
  <ol>
   <li>AutoGAN: Neural Architecture Search for Generative Adversarial Networks</li> 
  </ol>
  <p><a href="https://github.com/TAMU-VITA/AutoGAN" rel="nofollow" data-token="5cf95986a42dbbd14327f1c2067e605c">https://github.com/TAMU-VITA/AutoGAN</a></p> 
  <ol>
   <li>SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning</li> 
  </ol>
  <p><a href="https://github.com/TerenceCYJ/SO-HandNet" rel="nofollow" data-token="3749a9ef11577cb002e224aa5e78dff0">https://github.com/TerenceCYJ/SO-HandNet</a></p> 
  <ol>
   <li>Tex2Shape: Detailed Full Human Body Geometry from a Single Image</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.08645" rel="nofollow" data-token="8618050f06d1443541bdfc778e5ade0e">https://arxiv.org/abs/1904.08645</a></p> 
  <p><a href="https://github.com/thmoa/tex2shape" rel="nofollow" data-token="6c0dbe2c2d2592edfc332d44f57dec6e">https://github.com/thmoa/tex2shape</a></p> 
  <ol>
   <li>FCOS: Fully Convolutional One-Stage Object Detectio</li> 
  </ol>
  <p><a href="https://arxiv.org/abs/1904.01355" rel="nofollow" data-token="2c094728be00862180217fb5fc170353">https://arxiv.org/abs/1904.01355</a></p> 
  <p><a href="https://github.com/tianzhi0549/FCOS/" rel="nofollow" data-token="61ae7dae079032e5fca1022aacad2137">https://github.com/tianzhi0549/FCOS/</a></p> 
  <p>&nbsp;</p> 
  <p>百度云链接：链接：https://pan.baidu.com/s/1JnbZFmQwtqk1mQMN-ZpAOw&nbsp;<br> 提取码：mv56&nbsp;</p> 
  <p><span style="color:#f33b45;"><strong>如果百度云失效了，去公众号【计算机视觉联盟】后台回复关键词： &nbsp; &nbsp; &nbsp;ICCV2019</strong></span></p> 
  <p><span style="color:#f33b45;"><strong>或在本条下方评论，我看到就会回复了</strong></span></p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
