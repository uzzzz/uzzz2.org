<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>2019移动广告反欺诈算法挑战赛之baseline | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="2019移动广告反欺诈算法挑战赛之baseline" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="前言： 现阶段的目标是向模型中加入尽可能多而且不同的组合特征。这个baseline统计的特征各个属性的时间特征，各个属性之间的融合特征、一些重要属性的比值特征等。分享的这个模型有以下几个特点： 第一： 原始属性只是用了6个特征，因为数据集不是很干净，所以选了一些干净的特征 第二： 对于原始特征，统计其出现的次数，以及对出现次数进行排序 第三： 对于混合属性, 考虑的方向是&nbsp;通过什么app访问的这个广告&nbsp; &nbsp;那个IP地址访问了这个广告&nbsp; 以及是什么手机访问了这个广告 第四： 对于数据集中的ip和model这个两个特征统计了比值信息 第五： 最后为了方便统计哪些特征是强特征，在最后画图显示数据集中强特征，便于以后模型的使用 第六： 这个模型现在的分数是92.73195，可以考虑把之前的原始特征都加上去，应该会有点提分的。 亲测：&nbsp; 把之前的特征加上去，再加上卡方检验能到到94.0分的 &nbsp; 多加一个os的处理又增加0.01分 &nbsp; lgb继续加特征 &nbsp; 代码： # -*- coding: utf-8 -*- # @Time : 2019/7/25 20:47 # @Author : YYLin # @Email : 854280599@qq.com # @File : My_Method_For_LGB.py # 本版本的变化有三个 删除了一些缺失值较多的属性 以及认为不重要的属性 # 增加一些混合属性 关于IP地址 app类型和广告位之间的数据统计 # 最后显示了数据集中哪些特征是重要特征 import warnings import pandas as pd from tqdm import tqdm from sklearn.preprocessing import LabelEncoder import numpy as np from sklearn.metrics import f1_score from datetime import timedelta, datetime from sklearn.model_selection import train_test_split from lightgbm import LGBMClassifier warnings.filterwarnings(&#39;ignore&#39;) pd.set_option(&#39;display.max_columns&#39;, 1000) pd.set_option(&#39;display.max_rows&#39;, None) pd.set_option(&#39;display.max_colwidth&#39;, 1000) # 定义训练使用的数据列 train_cols = [&#39;ip&#39;, &#39;apptype&#39;, &#39;model&#39;, &#39;os&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;nginxtime&#39;, &#39;label&#39;, &#39;sid&#39;] test_cols = [&#39;ip&#39;, &#39;apptype&#39;, &#39;model&#39;, &#39;os&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;nginxtime&#39;, &#39;sid&#39;] # 读取训练集以及测试集 并进行拼接操作 df_train = pd.read_csv(&#39;data/traindata.txt&#39;, sep=&#39;\t&#39;, usecols=train_cols) df_test = pd.read_csv(&#39;data/testdata.txt&#39;, sep=&#39;\t&#39;, usecols=test_cols) All_data_for_train = pd.concat([df_train, df_test], ignore_index=True).drop(columns=&#39;sid&#39;) All_data_for_train[&#39;label&#39;] = All_data_for_train[&#39;label&#39;].fillna(-1).astype(int) # print(&#39;检查一下数据是否读取正确:\n&#39;, All_data_for_train.head(5)) # 在训练集中增加 day hour mintue All_data_for_train[&#39;datetime&#39;] = pd.to_datetime(All_data_for_train[&#39;nginxtime&#39;] / 1000, unit=&#39;s&#39;) + timedelta(hours=8) All_data_for_train[&#39;hour&#39;] = All_data_for_train[&#39;datetime&#39;].dt.hour All_data_for_train[&#39;day&#39;] = All_data_for_train[&#39;datetime&#39;].dt.day - All_data_for_train[&#39;datetime&#39;].dt.day.min() All_data_for_train[&#39;minute&#39;] = All_data_for_train[&#39;datetime&#39;].dt.minute.astype(&#39;uint8&#39;) All_data_for_train.drop([&#39;nginxtime&#39;], axis=1, inplace=True) # print(&#39;检查一下数据中的时间信息是否正确:\n&#39;, All_data_for_train.head(5)) # 对model---设备进行处理 All_data_for_train[&#39;model&#39;].replace(&#39;PACM00&#39;, &quot;OPPO R15&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBAM00&#39;, &quot;OPPO A5&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBEM00&#39;, &quot;OPPO R17&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PADM00&#39;, &quot;OPPO A3&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBBM00&#39;, &quot;OPPO A7&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PAAM00&#39;, &quot;OPPO R15_1&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PACT00&#39;, &quot;OPPO R15_2&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PABT00&#39;, &quot;OPPO A5_1&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBCM10&#39;, &quot;OPPO R15x&quot;, inplace=True) # 处理属性中出现的大小写问题 All_data_for_train[&#39;model&#39;] = All_data_for_train[&#39;model&#39;].astype(&#39;str&#39;) All_data_for_train[&#39;model&#39;] = All_data_for_train[&#39;model&#39;].map(lambda x: x.upper()) All_data_for_train[&#39;os&#39;] = All_data_for_train[&#39;os&#39;].astype(&#39;str&#39;) All_data_for_train[&#39;os&#39;] = All_data_for_train[&#39;os&#39;].map(lambda x: x.upper()) # 统计属性列中单个属性出现次数 以及对结果进行排序 可以发现统计列的值比较的大 # 版本分为两个 一个是使用单独属性 一个是不使用单独属性分别测试 print(&#39;loading single attributes ...........\n&#39;) cols_for_single_atr = [&#39;ip&#39;, &#39;apptype&#39;, &#39;model&#39;, &#39;os&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;] for i in tqdm(cols_for_single_atr): lbl = LabelEncoder() All_data_for_train[i + &quot;_count&quot;] = All_data_for_train.groupby([i])[i].transform(&#39;count&#39;) All_data_for_train[i + &quot;_rank&quot;] = All_data_for_train[i + &quot;_count&quot;].rank(method=&#39;min&#39;) All_data_for_train[i] = lbl.fit_transform(All_data_for_train[i].astype(str)) # print(&#39;使用groupby之后数据的信息是:\n&#39;, All_data_for_train.head(5)) # 开始统计一些复合属性 # 第一列统计的是： 通过什么app访问的这个广告 # 第二列统计的是： 那个IP地址访问了这个广告 print(&#39;\nloading Fusion_attributes........\n&#39;) Fusion_attributes = [&#39;apptype_adunitshowid&#39;, &#39;apptype_adunitshowid_mediashowid&#39;, &#39;apptype_mediashowid&#39;, &#39;apptype_adunitshowid_model_day_hour&#39;, &#39;apptype_model_day_hour&#39;, &#39;apptype_os_adunitshowid_day_hour&#39;, &#39;ip_day&#39;, &#39;ip_apptype_model_adunitshowid_day&#39;, &#39;ip_apptype_model_day&#39;, &#39;ip_apptype_model_day_os_hour&#39;, &#39;ip_apptype_os_adunitshowid&#39;, &#39;ip_os&#39;, &#39;ip_apptype_os_adunitshowid_day&#39;] for attribute in tqdm(Fusion_attributes): name = &quot;count_&quot; + attribute dummy = &#39;label&#39; cols = attribute.split(&quot;_&quot;) cols_with_dummy = cols.copy() cols_with_dummy.append(dummy) gp = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: name}) All_data_for_train = All_data_for_train.merge(gp, on=cols, how=&#39;left&#39;) # print(&#39;经过融合属性之后数据中的值是:\n&#39;, All_data_for_train.head(5)) # 开始统计一些比值信息 print(&#39;\n loading Ratio for model: .........\n&#39;) All_data_for_train[&quot;machine&quot;] = 1000*All_data_for_train[&quot;model&quot;] + All_data_for_train[&quot;os&quot;] Ratio_Attributes = [&#39;ip_machine&#39;, &#39;machine_ip&#39;, &#39;apptype_adunitshowid&#39;, &#39;adunitshowid_apptype&#39;, &#39;apptype_mediashowid&#39;, &#39;mediashowid_apptype&#39;] for attribute in Ratio_Attributes: name = &quot;countRatio_&quot; + attribute dummy = &#39;label&#39; cols = attribute.split(&quot;_&quot;) cols_with_dummy = cols.copy() cols_with_dummy.append(dummy) # 进行属性比值的融合 gp1 = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: &#39;cnt1&#39;}) _df = All_data_for_train.merge(gp1, on=cols, how=&#39;left&#39;) gp2 = All_data_for_train[cols].groupby(by=cols[0:len(cols) - 1])[[cols[len(cols) - 1]]].count().reset_index().rename(index=str, columns={cols[len(cols) - 1]: &#39;cnt2&#39;}) _df[&#39;cnt2&#39;] = All_data_for_train.merge(gp2, on=cols[0:len(cols) - 1], how=&#39;left&#39;)[&#39;cnt2&#39;] All_data_for_train[name] = _df[&#39;cnt1&#39;] / _df[&#39;cnt2&#39;] # print(&#39;经过属性比值融合之后的属性是:&#39;, All_data_for_train.head(5)) All_data_for_train = All_data_for_train.drop(columns=&#39;datetime&#39;) Y_data = All_data_for_train.loc[All_data_for_train[&#39;label&#39;] != -1][&#39;label&#39;] X_data = All_data_for_train.loc[All_data_for_train[&#39;label&#39;] != -1].drop(columns=&#39;label&#39;) # print(&#39;训练集中的数据格式:\n&#39;, X_data.head(5)) # print(&#39;训练集中标签的格式:\n&#39;, Y_data.head(5)) X_test = All_data_for_train.loc[All_data_for_train[&#39;label&#39;] == -1].drop(columns=&#39;label&#39;) # print(&#39;测试集中的数据格式:\n&#39;, X_test.head(5)) train_x, val_x, train_y, val_y = train_test_split(X_data, Y_data, test_size=0.2) def lgb_f1(labels, preds): score = f1_score(labels, np.round(preds)) return &#39;f1&#39;, score, True # 增加了学习率 并且增大了运行的次数 lgb = LGBMClassifier(random_seed=2019, n_jobs=-1, objective=&#39;binary&#39;, learning_rate=0.1, n_estimators=6500, num_leaves=31, max_depth=-1, min_child_samples=50, min_child_weight=9, subsample_freq=1, subsample=0.7, colsample_bytree=0.7, reg_alpha=1, reg_lambda=5) lgb.fit( train_x, train_y, eval_set=[(train_x, train_y), (val_x, val_y)], eval_names=[&#39;train&#39;, &#39;val&#39;], eval_metric=lgb_f1, early_stopping_rounds=400, verbose=10, ) print(&#39;best score&#39;, lgb.best_score_) # %%------------------------------- print(&#39;predict&#39;) lgb.n_estimators = lgb.best_iteration_ lgb.fit(X_data, Y_data) test_y = lgb.predict(X_test) df_sub = pd.concat([df_test[&#39;sid&#39;], pd.Series(test_y)], axis=1) df_sub.columns = [&#39;sid&#39;, &#39;label&#39;] df_sub.to_csv(&#39;lgb_submit-{}.csv&#39;.format(datetime.now().strftime(&#39;%m%d_%H%M%S&#39;)), sep=&#39;,&#39;, index=False) # 2019 7 25 画图显示模型的重要特征 import matplotlib.pyplot as plt import seaborn as sns color = sns.color_palette() sns.set_style(&#39;darkgrid&#39;) features_list = X_data.columns.values feature_importance = lgb.feature_importances_ sorted_idx = np.argsort(feature_importance) plt.figure(figsize=(5, 7)) plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align=&#39;center&#39;) plt.yticks(range(len(sorted_idx)), features_list[sorted_idx]) plt.xlabel(&#39;Importance&#39;) plt.title(&#39;Feature importances&#39;) plt.draw() plt.show() &nbsp; # 数据集中的重要特征" />
<meta property="og:description" content="前言： 现阶段的目标是向模型中加入尽可能多而且不同的组合特征。这个baseline统计的特征各个属性的时间特征，各个属性之间的融合特征、一些重要属性的比值特征等。分享的这个模型有以下几个特点： 第一： 原始属性只是用了6个特征，因为数据集不是很干净，所以选了一些干净的特征 第二： 对于原始特征，统计其出现的次数，以及对出现次数进行排序 第三： 对于混合属性, 考虑的方向是&nbsp;通过什么app访问的这个广告&nbsp; &nbsp;那个IP地址访问了这个广告&nbsp; 以及是什么手机访问了这个广告 第四： 对于数据集中的ip和model这个两个特征统计了比值信息 第五： 最后为了方便统计哪些特征是强特征，在最后画图显示数据集中强特征，便于以后模型的使用 第六： 这个模型现在的分数是92.73195，可以考虑把之前的原始特征都加上去，应该会有点提分的。 亲测：&nbsp; 把之前的特征加上去，再加上卡方检验能到到94.0分的 &nbsp; 多加一个os的处理又增加0.01分 &nbsp; lgb继续加特征 &nbsp; 代码： # -*- coding: utf-8 -*- # @Time : 2019/7/25 20:47 # @Author : YYLin # @Email : 854280599@qq.com # @File : My_Method_For_LGB.py # 本版本的变化有三个 删除了一些缺失值较多的属性 以及认为不重要的属性 # 增加一些混合属性 关于IP地址 app类型和广告位之间的数据统计 # 最后显示了数据集中哪些特征是重要特征 import warnings import pandas as pd from tqdm import tqdm from sklearn.preprocessing import LabelEncoder import numpy as np from sklearn.metrics import f1_score from datetime import timedelta, datetime from sklearn.model_selection import train_test_split from lightgbm import LGBMClassifier warnings.filterwarnings(&#39;ignore&#39;) pd.set_option(&#39;display.max_columns&#39;, 1000) pd.set_option(&#39;display.max_rows&#39;, None) pd.set_option(&#39;display.max_colwidth&#39;, 1000) # 定义训练使用的数据列 train_cols = [&#39;ip&#39;, &#39;apptype&#39;, &#39;model&#39;, &#39;os&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;nginxtime&#39;, &#39;label&#39;, &#39;sid&#39;] test_cols = [&#39;ip&#39;, &#39;apptype&#39;, &#39;model&#39;, &#39;os&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;nginxtime&#39;, &#39;sid&#39;] # 读取训练集以及测试集 并进行拼接操作 df_train = pd.read_csv(&#39;data/traindata.txt&#39;, sep=&#39;\t&#39;, usecols=train_cols) df_test = pd.read_csv(&#39;data/testdata.txt&#39;, sep=&#39;\t&#39;, usecols=test_cols) All_data_for_train = pd.concat([df_train, df_test], ignore_index=True).drop(columns=&#39;sid&#39;) All_data_for_train[&#39;label&#39;] = All_data_for_train[&#39;label&#39;].fillna(-1).astype(int) # print(&#39;检查一下数据是否读取正确:\n&#39;, All_data_for_train.head(5)) # 在训练集中增加 day hour mintue All_data_for_train[&#39;datetime&#39;] = pd.to_datetime(All_data_for_train[&#39;nginxtime&#39;] / 1000, unit=&#39;s&#39;) + timedelta(hours=8) All_data_for_train[&#39;hour&#39;] = All_data_for_train[&#39;datetime&#39;].dt.hour All_data_for_train[&#39;day&#39;] = All_data_for_train[&#39;datetime&#39;].dt.day - All_data_for_train[&#39;datetime&#39;].dt.day.min() All_data_for_train[&#39;minute&#39;] = All_data_for_train[&#39;datetime&#39;].dt.minute.astype(&#39;uint8&#39;) All_data_for_train.drop([&#39;nginxtime&#39;], axis=1, inplace=True) # print(&#39;检查一下数据中的时间信息是否正确:\n&#39;, All_data_for_train.head(5)) # 对model---设备进行处理 All_data_for_train[&#39;model&#39;].replace(&#39;PACM00&#39;, &quot;OPPO R15&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBAM00&#39;, &quot;OPPO A5&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBEM00&#39;, &quot;OPPO R17&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PADM00&#39;, &quot;OPPO A3&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBBM00&#39;, &quot;OPPO A7&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PAAM00&#39;, &quot;OPPO R15_1&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PACT00&#39;, &quot;OPPO R15_2&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PABT00&#39;, &quot;OPPO A5_1&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBCM10&#39;, &quot;OPPO R15x&quot;, inplace=True) # 处理属性中出现的大小写问题 All_data_for_train[&#39;model&#39;] = All_data_for_train[&#39;model&#39;].astype(&#39;str&#39;) All_data_for_train[&#39;model&#39;] = All_data_for_train[&#39;model&#39;].map(lambda x: x.upper()) All_data_for_train[&#39;os&#39;] = All_data_for_train[&#39;os&#39;].astype(&#39;str&#39;) All_data_for_train[&#39;os&#39;] = All_data_for_train[&#39;os&#39;].map(lambda x: x.upper()) # 统计属性列中单个属性出现次数 以及对结果进行排序 可以发现统计列的值比较的大 # 版本分为两个 一个是使用单独属性 一个是不使用单独属性分别测试 print(&#39;loading single attributes ...........\n&#39;) cols_for_single_atr = [&#39;ip&#39;, &#39;apptype&#39;, &#39;model&#39;, &#39;os&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;] for i in tqdm(cols_for_single_atr): lbl = LabelEncoder() All_data_for_train[i + &quot;_count&quot;] = All_data_for_train.groupby([i])[i].transform(&#39;count&#39;) All_data_for_train[i + &quot;_rank&quot;] = All_data_for_train[i + &quot;_count&quot;].rank(method=&#39;min&#39;) All_data_for_train[i] = lbl.fit_transform(All_data_for_train[i].astype(str)) # print(&#39;使用groupby之后数据的信息是:\n&#39;, All_data_for_train.head(5)) # 开始统计一些复合属性 # 第一列统计的是： 通过什么app访问的这个广告 # 第二列统计的是： 那个IP地址访问了这个广告 print(&#39;\nloading Fusion_attributes........\n&#39;) Fusion_attributes = [&#39;apptype_adunitshowid&#39;, &#39;apptype_adunitshowid_mediashowid&#39;, &#39;apptype_mediashowid&#39;, &#39;apptype_adunitshowid_model_day_hour&#39;, &#39;apptype_model_day_hour&#39;, &#39;apptype_os_adunitshowid_day_hour&#39;, &#39;ip_day&#39;, &#39;ip_apptype_model_adunitshowid_day&#39;, &#39;ip_apptype_model_day&#39;, &#39;ip_apptype_model_day_os_hour&#39;, &#39;ip_apptype_os_adunitshowid&#39;, &#39;ip_os&#39;, &#39;ip_apptype_os_adunitshowid_day&#39;] for attribute in tqdm(Fusion_attributes): name = &quot;count_&quot; + attribute dummy = &#39;label&#39; cols = attribute.split(&quot;_&quot;) cols_with_dummy = cols.copy() cols_with_dummy.append(dummy) gp = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: name}) All_data_for_train = All_data_for_train.merge(gp, on=cols, how=&#39;left&#39;) # print(&#39;经过融合属性之后数据中的值是:\n&#39;, All_data_for_train.head(5)) # 开始统计一些比值信息 print(&#39;\n loading Ratio for model: .........\n&#39;) All_data_for_train[&quot;machine&quot;] = 1000*All_data_for_train[&quot;model&quot;] + All_data_for_train[&quot;os&quot;] Ratio_Attributes = [&#39;ip_machine&#39;, &#39;machine_ip&#39;, &#39;apptype_adunitshowid&#39;, &#39;adunitshowid_apptype&#39;, &#39;apptype_mediashowid&#39;, &#39;mediashowid_apptype&#39;] for attribute in Ratio_Attributes: name = &quot;countRatio_&quot; + attribute dummy = &#39;label&#39; cols = attribute.split(&quot;_&quot;) cols_with_dummy = cols.copy() cols_with_dummy.append(dummy) # 进行属性比值的融合 gp1 = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: &#39;cnt1&#39;}) _df = All_data_for_train.merge(gp1, on=cols, how=&#39;left&#39;) gp2 = All_data_for_train[cols].groupby(by=cols[0:len(cols) - 1])[[cols[len(cols) - 1]]].count().reset_index().rename(index=str, columns={cols[len(cols) - 1]: &#39;cnt2&#39;}) _df[&#39;cnt2&#39;] = All_data_for_train.merge(gp2, on=cols[0:len(cols) - 1], how=&#39;left&#39;)[&#39;cnt2&#39;] All_data_for_train[name] = _df[&#39;cnt1&#39;] / _df[&#39;cnt2&#39;] # print(&#39;经过属性比值融合之后的属性是:&#39;, All_data_for_train.head(5)) All_data_for_train = All_data_for_train.drop(columns=&#39;datetime&#39;) Y_data = All_data_for_train.loc[All_data_for_train[&#39;label&#39;] != -1][&#39;label&#39;] X_data = All_data_for_train.loc[All_data_for_train[&#39;label&#39;] != -1].drop(columns=&#39;label&#39;) # print(&#39;训练集中的数据格式:\n&#39;, X_data.head(5)) # print(&#39;训练集中标签的格式:\n&#39;, Y_data.head(5)) X_test = All_data_for_train.loc[All_data_for_train[&#39;label&#39;] == -1].drop(columns=&#39;label&#39;) # print(&#39;测试集中的数据格式:\n&#39;, X_test.head(5)) train_x, val_x, train_y, val_y = train_test_split(X_data, Y_data, test_size=0.2) def lgb_f1(labels, preds): score = f1_score(labels, np.round(preds)) return &#39;f1&#39;, score, True # 增加了学习率 并且增大了运行的次数 lgb = LGBMClassifier(random_seed=2019, n_jobs=-1, objective=&#39;binary&#39;, learning_rate=0.1, n_estimators=6500, num_leaves=31, max_depth=-1, min_child_samples=50, min_child_weight=9, subsample_freq=1, subsample=0.7, colsample_bytree=0.7, reg_alpha=1, reg_lambda=5) lgb.fit( train_x, train_y, eval_set=[(train_x, train_y), (val_x, val_y)], eval_names=[&#39;train&#39;, &#39;val&#39;], eval_metric=lgb_f1, early_stopping_rounds=400, verbose=10, ) print(&#39;best score&#39;, lgb.best_score_) # %%------------------------------- print(&#39;predict&#39;) lgb.n_estimators = lgb.best_iteration_ lgb.fit(X_data, Y_data) test_y = lgb.predict(X_test) df_sub = pd.concat([df_test[&#39;sid&#39;], pd.Series(test_y)], axis=1) df_sub.columns = [&#39;sid&#39;, &#39;label&#39;] df_sub.to_csv(&#39;lgb_submit-{}.csv&#39;.format(datetime.now().strftime(&#39;%m%d_%H%M%S&#39;)), sep=&#39;,&#39;, index=False) # 2019 7 25 画图显示模型的重要特征 import matplotlib.pyplot as plt import seaborn as sns color = sns.color_palette() sns.set_style(&#39;darkgrid&#39;) features_list = X_data.columns.values feature_importance = lgb.feature_importances_ sorted_idx = np.argsort(feature_importance) plt.figure(figsize=(5, 7)) plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align=&#39;center&#39;) plt.yticks(range(len(sorted_idx)), features_list[sorted_idx]) plt.xlabel(&#39;Importance&#39;) plt.title(&#39;Feature importances&#39;) plt.draw() plt.show() &nbsp; # 数据集中的重要特征" />
<link rel="canonical" href="https://uzzz.org/2019/07/26/793038.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/26/793038.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-26T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"前言： 现阶段的目标是向模型中加入尽可能多而且不同的组合特征。这个baseline统计的特征各个属性的时间特征，各个属性之间的融合特征、一些重要属性的比值特征等。分享的这个模型有以下几个特点： 第一： 原始属性只是用了6个特征，因为数据集不是很干净，所以选了一些干净的特征 第二： 对于原始特征，统计其出现的次数，以及对出现次数进行排序 第三： 对于混合属性, 考虑的方向是&nbsp;通过什么app访问的这个广告&nbsp; &nbsp;那个IP地址访问了这个广告&nbsp; 以及是什么手机访问了这个广告 第四： 对于数据集中的ip和model这个两个特征统计了比值信息 第五： 最后为了方便统计哪些特征是强特征，在最后画图显示数据集中强特征，便于以后模型的使用 第六： 这个模型现在的分数是92.73195，可以考虑把之前的原始特征都加上去，应该会有点提分的。 亲测：&nbsp; 把之前的特征加上去，再加上卡方检验能到到94.0分的 &nbsp; 多加一个os的处理又增加0.01分 &nbsp; lgb继续加特征 &nbsp; 代码： # -*- coding: utf-8 -*- # @Time : 2019/7/25 20:47 # @Author : YYLin # @Email : 854280599@qq.com # @File : My_Method_For_LGB.py # 本版本的变化有三个 删除了一些缺失值较多的属性 以及认为不重要的属性 # 增加一些混合属性 关于IP地址 app类型和广告位之间的数据统计 # 最后显示了数据集中哪些特征是重要特征 import warnings import pandas as pd from tqdm import tqdm from sklearn.preprocessing import LabelEncoder import numpy as np from sklearn.metrics import f1_score from datetime import timedelta, datetime from sklearn.model_selection import train_test_split from lightgbm import LGBMClassifier warnings.filterwarnings(&#39;ignore&#39;) pd.set_option(&#39;display.max_columns&#39;, 1000) pd.set_option(&#39;display.max_rows&#39;, None) pd.set_option(&#39;display.max_colwidth&#39;, 1000) # 定义训练使用的数据列 train_cols = [&#39;ip&#39;, &#39;apptype&#39;, &#39;model&#39;, &#39;os&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;nginxtime&#39;, &#39;label&#39;, &#39;sid&#39;] test_cols = [&#39;ip&#39;, &#39;apptype&#39;, &#39;model&#39;, &#39;os&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;nginxtime&#39;, &#39;sid&#39;] # 读取训练集以及测试集 并进行拼接操作 df_train = pd.read_csv(&#39;data/traindata.txt&#39;, sep=&#39;\\t&#39;, usecols=train_cols) df_test = pd.read_csv(&#39;data/testdata.txt&#39;, sep=&#39;\\t&#39;, usecols=test_cols) All_data_for_train = pd.concat([df_train, df_test], ignore_index=True).drop(columns=&#39;sid&#39;) All_data_for_train[&#39;label&#39;] = All_data_for_train[&#39;label&#39;].fillna(-1).astype(int) # print(&#39;检查一下数据是否读取正确:\\n&#39;, All_data_for_train.head(5)) # 在训练集中增加 day hour mintue All_data_for_train[&#39;datetime&#39;] = pd.to_datetime(All_data_for_train[&#39;nginxtime&#39;] / 1000, unit=&#39;s&#39;) + timedelta(hours=8) All_data_for_train[&#39;hour&#39;] = All_data_for_train[&#39;datetime&#39;].dt.hour All_data_for_train[&#39;day&#39;] = All_data_for_train[&#39;datetime&#39;].dt.day - All_data_for_train[&#39;datetime&#39;].dt.day.min() All_data_for_train[&#39;minute&#39;] = All_data_for_train[&#39;datetime&#39;].dt.minute.astype(&#39;uint8&#39;) All_data_for_train.drop([&#39;nginxtime&#39;], axis=1, inplace=True) # print(&#39;检查一下数据中的时间信息是否正确:\\n&#39;, All_data_for_train.head(5)) # 对model---设备进行处理 All_data_for_train[&#39;model&#39;].replace(&#39;PACM00&#39;, &quot;OPPO R15&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBAM00&#39;, &quot;OPPO A5&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBEM00&#39;, &quot;OPPO R17&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PADM00&#39;, &quot;OPPO A3&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBBM00&#39;, &quot;OPPO A7&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PAAM00&#39;, &quot;OPPO R15_1&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PACT00&#39;, &quot;OPPO R15_2&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PABT00&#39;, &quot;OPPO A5_1&quot;, inplace=True) All_data_for_train[&#39;model&#39;].replace(&#39;PBCM10&#39;, &quot;OPPO R15x&quot;, inplace=True) # 处理属性中出现的大小写问题 All_data_for_train[&#39;model&#39;] = All_data_for_train[&#39;model&#39;].astype(&#39;str&#39;) All_data_for_train[&#39;model&#39;] = All_data_for_train[&#39;model&#39;].map(lambda x: x.upper()) All_data_for_train[&#39;os&#39;] = All_data_for_train[&#39;os&#39;].astype(&#39;str&#39;) All_data_for_train[&#39;os&#39;] = All_data_for_train[&#39;os&#39;].map(lambda x: x.upper()) # 统计属性列中单个属性出现次数 以及对结果进行排序 可以发现统计列的值比较的大 # 版本分为两个 一个是使用单独属性 一个是不使用单独属性分别测试 print(&#39;loading single attributes ...........\\n&#39;) cols_for_single_atr = [&#39;ip&#39;, &#39;apptype&#39;, &#39;model&#39;, &#39;os&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;] for i in tqdm(cols_for_single_atr): lbl = LabelEncoder() All_data_for_train[i + &quot;_count&quot;] = All_data_for_train.groupby([i])[i].transform(&#39;count&#39;) All_data_for_train[i + &quot;_rank&quot;] = All_data_for_train[i + &quot;_count&quot;].rank(method=&#39;min&#39;) All_data_for_train[i] = lbl.fit_transform(All_data_for_train[i].astype(str)) # print(&#39;使用groupby之后数据的信息是:\\n&#39;, All_data_for_train.head(5)) # 开始统计一些复合属性 # 第一列统计的是： 通过什么app访问的这个广告 # 第二列统计的是： 那个IP地址访问了这个广告 print(&#39;\\nloading Fusion_attributes........\\n&#39;) Fusion_attributes = [&#39;apptype_adunitshowid&#39;, &#39;apptype_adunitshowid_mediashowid&#39;, &#39;apptype_mediashowid&#39;, &#39;apptype_adunitshowid_model_day_hour&#39;, &#39;apptype_model_day_hour&#39;, &#39;apptype_os_adunitshowid_day_hour&#39;, &#39;ip_day&#39;, &#39;ip_apptype_model_adunitshowid_day&#39;, &#39;ip_apptype_model_day&#39;, &#39;ip_apptype_model_day_os_hour&#39;, &#39;ip_apptype_os_adunitshowid&#39;, &#39;ip_os&#39;, &#39;ip_apptype_os_adunitshowid_day&#39;] for attribute in tqdm(Fusion_attributes): name = &quot;count_&quot; + attribute dummy = &#39;label&#39; cols = attribute.split(&quot;_&quot;) cols_with_dummy = cols.copy() cols_with_dummy.append(dummy) gp = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: name}) All_data_for_train = All_data_for_train.merge(gp, on=cols, how=&#39;left&#39;) # print(&#39;经过融合属性之后数据中的值是:\\n&#39;, All_data_for_train.head(5)) # 开始统计一些比值信息 print(&#39;\\n loading Ratio for model: .........\\n&#39;) All_data_for_train[&quot;machine&quot;] = 1000*All_data_for_train[&quot;model&quot;] + All_data_for_train[&quot;os&quot;] Ratio_Attributes = [&#39;ip_machine&#39;, &#39;machine_ip&#39;, &#39;apptype_adunitshowid&#39;, &#39;adunitshowid_apptype&#39;, &#39;apptype_mediashowid&#39;, &#39;mediashowid_apptype&#39;] for attribute in Ratio_Attributes: name = &quot;countRatio_&quot; + attribute dummy = &#39;label&#39; cols = attribute.split(&quot;_&quot;) cols_with_dummy = cols.copy() cols_with_dummy.append(dummy) # 进行属性比值的融合 gp1 = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: &#39;cnt1&#39;}) _df = All_data_for_train.merge(gp1, on=cols, how=&#39;left&#39;) gp2 = All_data_for_train[cols].groupby(by=cols[0:len(cols) - 1])[[cols[len(cols) - 1]]].count().reset_index().rename(index=str, columns={cols[len(cols) - 1]: &#39;cnt2&#39;}) _df[&#39;cnt2&#39;] = All_data_for_train.merge(gp2, on=cols[0:len(cols) - 1], how=&#39;left&#39;)[&#39;cnt2&#39;] All_data_for_train[name] = _df[&#39;cnt1&#39;] / _df[&#39;cnt2&#39;] # print(&#39;经过属性比值融合之后的属性是:&#39;, All_data_for_train.head(5)) All_data_for_train = All_data_for_train.drop(columns=&#39;datetime&#39;) Y_data = All_data_for_train.loc[All_data_for_train[&#39;label&#39;] != -1][&#39;label&#39;] X_data = All_data_for_train.loc[All_data_for_train[&#39;label&#39;] != -1].drop(columns=&#39;label&#39;) # print(&#39;训练集中的数据格式:\\n&#39;, X_data.head(5)) # print(&#39;训练集中标签的格式:\\n&#39;, Y_data.head(5)) X_test = All_data_for_train.loc[All_data_for_train[&#39;label&#39;] == -1].drop(columns=&#39;label&#39;) # print(&#39;测试集中的数据格式:\\n&#39;, X_test.head(5)) train_x, val_x, train_y, val_y = train_test_split(X_data, Y_data, test_size=0.2) def lgb_f1(labels, preds): score = f1_score(labels, np.round(preds)) return &#39;f1&#39;, score, True # 增加了学习率 并且增大了运行的次数 lgb = LGBMClassifier(random_seed=2019, n_jobs=-1, objective=&#39;binary&#39;, learning_rate=0.1, n_estimators=6500, num_leaves=31, max_depth=-1, min_child_samples=50, min_child_weight=9, subsample_freq=1, subsample=0.7, colsample_bytree=0.7, reg_alpha=1, reg_lambda=5) lgb.fit( train_x, train_y, eval_set=[(train_x, train_y), (val_x, val_y)], eval_names=[&#39;train&#39;, &#39;val&#39;], eval_metric=lgb_f1, early_stopping_rounds=400, verbose=10, ) print(&#39;best score&#39;, lgb.best_score_) # %%------------------------------- print(&#39;predict&#39;) lgb.n_estimators = lgb.best_iteration_ lgb.fit(X_data, Y_data) test_y = lgb.predict(X_test) df_sub = pd.concat([df_test[&#39;sid&#39;], pd.Series(test_y)], axis=1) df_sub.columns = [&#39;sid&#39;, &#39;label&#39;] df_sub.to_csv(&#39;lgb_submit-{}.csv&#39;.format(datetime.now().strftime(&#39;%m%d_%H%M%S&#39;)), sep=&#39;,&#39;, index=False) # 2019 7 25 画图显示模型的重要特征 import matplotlib.pyplot as plt import seaborn as sns color = sns.color_palette() sns.set_style(&#39;darkgrid&#39;) features_list = X_data.columns.values feature_importance = lgb.feature_importances_ sorted_idx = np.argsort(feature_importance) plt.figure(figsize=(5, 7)) plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align=&#39;center&#39;) plt.yticks(range(len(sorted_idx)), features_list[sorted_idx]) plt.xlabel(&#39;Importance&#39;) plt.title(&#39;Feature importances&#39;) plt.draw() plt.show() &nbsp; # 数据集中的重要特征","@type":"BlogPosting","url":"https://uzzz.org/2019/07/26/793038.html","headline":"2019移动广告反欺诈算法挑战赛之baseline","dateModified":"2019-07-26T00:00:00+08:00","datePublished":"2019-07-26T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/26/793038.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>2019移动广告反欺诈算法挑战赛之baseline</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1>前言：</h1> 
  <p>现阶段的目标是向模型中加入尽可能多而且不同的组合特征。这个baseline统计的特征各个属性的时间特征，各个属性之间的融合特征、一些重要属性的比值特征等。分享的这个模型有以下几个特点：</p> 
  <p>第一： 原始属性只是用了6个特征，因为数据集不是很干净，所以选了一些干净的特征</p> 
  <p>第二： 对于原始特征，统计其出现的次数，以及对出现次数进行排序</p> 
  <p>第三： 对于混合属性, 考虑的方向是&nbsp;通过什么app访问的这个广告&nbsp; &nbsp;那个IP地址访问了这个广告&nbsp; 以及是什么手机访问了这个广告</p> 
  <p>第四： 对于数据集中的ip和model这个两个特征统计了比值信息</p> 
  <p>第五： 最后为了方便统计哪些特征是强特征，在最后画图显示数据集中强特征，便于以后模型的使用</p> 
  <p>第六： 这个模型现在的分数是92.73195，可以考虑把之前的原始特征都加上去，应该会有点提分的。</p> 
  <h1><span style="color:#f33b45;">亲测：&nbsp; </span></h1> 
  <h1>把之前的特征加上去，再加上卡方检验能到到94.0分的</h1> 
  <p><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728191524275.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzc2Nzgx,size_16,color_FFFFFF,t_70"></p> 
  <h1>&nbsp;</h1> 
  <h1>多加一个os的处理又增加0.01分</h1> 
  <p><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728235542241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzc2Nzgx,size_16,color_FFFFFF,t_70"></p> 
  <h1>&nbsp;</h1> 
  <h1>lgb继续加特征</h1> 
  <p><img alt="" class="has" height="40" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731101715495.jpg" width="973"></p> 
  <p>&nbsp;</p> 
  <h1>代码：</h1> 
  <pre class="has">
<code class="language-python"># -*- coding: utf-8 -*-
# @Time    : 2019/7/25 20:47
# @Author  : YYLin
# @Email   : 854280599@qq.com
# @File    : My_Method_For_LGB.py
# 本版本的变化有三个   删除了一些缺失值较多的属性 以及认为不重要的属性
# 增加一些混合属性    关于IP地址 app类型和广告位之间的数据统计
# 最后显示了数据集中哪些特征是重要特征
import warnings
import pandas as pd
from tqdm import tqdm
from sklearn.preprocessing import LabelEncoder
import numpy as np
from sklearn.metrics import f1_score
from datetime import timedelta, datetime
from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', 1000)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_colwidth', 1000)

# 定义训练使用的数据列
train_cols = ['ip', 'apptype', 'model', 'os', 'adunitshowid', 'mediashowid', 'nginxtime', 'label', 'sid']
test_cols = ['ip', 'apptype', 'model', 'os', 'adunitshowid', 'mediashowid', 'nginxtime', 'sid']

# 读取训练集以及测试集 并进行拼接操作
df_train = pd.read_csv('data/traindata.txt', sep='\t', usecols=train_cols)
df_test = pd.read_csv('data/testdata.txt', sep='\t', usecols=test_cols)

All_data_for_train = pd.concat([df_train, df_test], ignore_index=True).drop(columns='sid')
All_data_for_train['label'] = All_data_for_train['label'].fillna(-1).astype(int)
# print('检查一下数据是否读取正确:\n', All_data_for_train.head(5))

# 在训练集中增加 day hour mintue
All_data_for_train['datetime'] = pd.to_datetime(All_data_for_train['nginxtime'] / 1000, unit='s') + timedelta(hours=8)
All_data_for_train['hour'] = All_data_for_train['datetime'].dt.hour
All_data_for_train['day'] = All_data_for_train['datetime'].dt.day - All_data_for_train['datetime'].dt.day.min()
All_data_for_train['minute'] = All_data_for_train['datetime'].dt.minute.astype('uint8')
All_data_for_train.drop(['nginxtime'], axis=1, inplace=True)
# print('检查一下数据中的时间信息是否正确:\n', All_data_for_train.head(5))

# 对model---设备进行处理
All_data_for_train['model'].replace('PACM00', "OPPO R15", inplace=True)
All_data_for_train['model'].replace('PBAM00', "OPPO A5", inplace=True)
All_data_for_train['model'].replace('PBEM00', "OPPO R17", inplace=True)
All_data_for_train['model'].replace('PADM00', "OPPO A3", inplace=True)
All_data_for_train['model'].replace('PBBM00', "OPPO A7", inplace=True)
All_data_for_train['model'].replace('PAAM00', "OPPO R15_1", inplace=True)
All_data_for_train['model'].replace('PACT00', "OPPO R15_2", inplace=True)
All_data_for_train['model'].replace('PABT00', "OPPO A5_1", inplace=True)
All_data_for_train['model'].replace('PBCM10', "OPPO R15x", inplace=True)

# 处理属性中出现的大小写问题
All_data_for_train['model'] = All_data_for_train['model'].astype('str')
All_data_for_train['model'] = All_data_for_train['model'].map(lambda x: x.upper())
All_data_for_train['os'] = All_data_for_train['os'].astype('str')
All_data_for_train['os'] = All_data_for_train['os'].map(lambda x: x.upper())

# 统计属性列中单个属性出现次数 以及对结果进行排序 可以发现统计列的值比较的大
# 版本分为两个 一个是使用单独属性 一个是不使用单独属性分别测试
print('loading single attributes ...........\n')
cols_for_single_atr = ['ip', 'apptype', 'model', 'os', 'adunitshowid', 'mediashowid']
for i in tqdm(cols_for_single_atr):
    lbl = LabelEncoder()
    All_data_for_train[i + "_count"] = All_data_for_train.groupby([i])[i].transform('count')
    All_data_for_train[i + "_rank"] = All_data_for_train[i + "_count"].rank(method='min')
    All_data_for_train[i] = lbl.fit_transform(All_data_for_train[i].astype(str))
# print('使用groupby之后数据的信息是:\n', All_data_for_train.head(5))

# 开始统计一些复合属性
# 第一列统计的是： 通过什么app访问的这个广告
# 第二列统计的是： 那个IP地址访问了这个广告
print('\nloading Fusion_attributes........\n')
Fusion_attributes = ['apptype_adunitshowid', 'apptype_adunitshowid_mediashowid', 'apptype_mediashowid', 'apptype_adunitshowid_model_day_hour',
                     'apptype_model_day_hour', 'apptype_os_adunitshowid_day_hour',

                     'ip_day', 'ip_apptype_model_adunitshowid_day', 'ip_apptype_model_day', 'ip_apptype_model_day_os_hour',
                     'ip_apptype_os_adunitshowid', 'ip_os', 'ip_apptype_os_adunitshowid_day']

for attribute in tqdm(Fusion_attributes):
    name = "count_" + attribute
    dummy = 'label'
    cols = attribute.split("_")
    cols_with_dummy = cols.copy()
    cols_with_dummy.append(dummy)
    gp = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: name})
    All_data_for_train = All_data_for_train.merge(gp, on=cols, how='left')
# print('经过融合属性之后数据中的值是:\n', All_data_for_train.head(5))

# 开始统计一些比值信息
print('\n loading Ratio for model: .........\n')

All_data_for_train["machine"] = 1000*All_data_for_train["model"] + All_data_for_train["os"]
Ratio_Attributes = ['ip_machine', 'machine_ip', 'apptype_adunitshowid', 'adunitshowid_apptype', 'apptype_mediashowid',
                    'mediashowid_apptype']
for attribute in Ratio_Attributes:
    name = "countRatio_" + attribute
    dummy = 'label'
    cols = attribute.split("_")
    cols_with_dummy = cols.copy()
    cols_with_dummy.append(dummy)

    # 进行属性比值的融合
    gp1 = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: 'cnt1'})
    _df = All_data_for_train.merge(gp1, on=cols, how='left')
    gp2 = All_data_for_train[cols].groupby(by=cols[0:len(cols) - 1])[[cols[len(cols) - 1]]].count().reset_index().rename(index=str, columns={cols[len(cols) - 1]: 'cnt2'})
    _df['cnt2'] = All_data_for_train.merge(gp2, on=cols[0:len(cols) - 1], how='left')['cnt2']

    All_data_for_train[name] = _df['cnt1'] / _df['cnt2']
# print('经过属性比值融合之后的属性是:', All_data_for_train.head(5))

All_data_for_train = All_data_for_train.drop(columns='datetime')
Y_data = All_data_for_train.loc[All_data_for_train['label'] != -1]['label']
X_data = All_data_for_train.loc[All_data_for_train['label'] != -1].drop(columns='label')
# print('训练集中的数据格式:\n', X_data.head(5))
# print('训练集中标签的格式:\n', Y_data.head(5))

X_test = All_data_for_train.loc[All_data_for_train['label'] == -1].drop(columns='label')
# print('测试集中的数据格式:\n', X_test.head(5))

train_x, val_x, train_y, val_y = train_test_split(X_data, Y_data, test_size=0.2)


def lgb_f1(labels, preds):
    score = f1_score(labels, np.round(preds))
    return 'f1', score, True


# 增加了学习率 并且增大了运行的次数
lgb = LGBMClassifier(random_seed=2019, n_jobs=-1, objective='binary',
                     learning_rate=0.1, n_estimators=6500, num_leaves=31, max_depth=-1,
                     min_child_samples=50, min_child_weight=9, subsample_freq=1,
                     subsample=0.7, colsample_bytree=0.7, reg_alpha=1, reg_lambda=5)

lgb.fit(
    train_x,
    train_y,
    eval_set=[(train_x, train_y), (val_x, val_y)],
    eval_names=['train', 'val'],
    eval_metric=lgb_f1,
    early_stopping_rounds=400,
    verbose=10,
)
print('best score', lgb.best_score_)

# %%-------------------------------
print('predict')
lgb.n_estimators = lgb.best_iteration_
lgb.fit(X_data, Y_data)
test_y = lgb.predict(X_test)
df_sub = pd.concat([df_test['sid'], pd.Series(test_y)], axis=1)
df_sub.columns = ['sid', 'label']
df_sub.to_csv('lgb_submit-{}.csv'.format(datetime.now().strftime('%m%d_%H%M%S')), sep=',', index=False)

# 2019 7 25 画图显示模型的重要特征
import matplotlib.pyplot as plt
import seaborn as sns

color = sns.color_palette()
sns.set_style('darkgrid')

features_list = X_data.columns.values
feature_importance = lgb.feature_importances_
sorted_idx = np.argsort(feature_importance)

plt.figure(figsize=(5, 7))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), features_list[sorted_idx])
plt.xlabel('Importance')
plt.title('Feature importances')
plt.draw()
plt.show()
</code></pre> 
  <p>&nbsp;</p> 
  <p># 数据集中的重要特征</p> 
  <p><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726140635723.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzc2Nzgx,size_16,color_FFFFFF,t_70"></p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
