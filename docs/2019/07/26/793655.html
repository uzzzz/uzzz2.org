<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>hadoop3.2.0伪分布式搭建 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="hadoop3.2.0伪分布式搭建" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="一、hadoop发展由来、历史、子项目 什么是hadoop Google Lab 开发的 Map/Reduce 和 Google File System(GFS) ，后由 apache基金会进行封装为MapReduce和HDFS组成 Hadoop的框架最核心的设计就是：HDFS和MapReduce。 HDFS为海量的数据提供了存储， 而MapReduce则为海量的数据提供了计算。 二、讲解HDFS（namenode和datanode），实现机制和优点 三、讲解MapReduce离线计算、Storm（流式计算，基于磁盘）、Spark（内存迭代计算，基于内存） 四、安装一台linux系统，版本为CentOS7 设置网卡在 /etc/sysconfig/network-scripts/ifcfg-enp0s3 ONBOOT=yes 虚拟机网络设置为桥接网络 选择对应联网方式 通过 ping www.baidu.com 来测试是否有网络 yum install net-tools 安装jdk1.8 并配置环境变量 https://www.cnblogs.com/Dylansuns/p/6974272.html 五、宿主机和虚拟机可以相互ping通，并在虚拟机设置静态IP BOOTPROTO=static DNS1=192.168.1.1 IPADDR=192.168.1.120 NETMASK=255.255.255.0 GATWAY=192.168.1.1 ZONE=public 六、创建用户，并赋权限 useradd user1 添加用户 passwd user1 设置密码 chown -R user:group 授权目录 例如：chown -R hadoop:hadoop /home // userdel –r user3 删除用户及目录 七、SSH免密登录 切换到hadoop用户下操作 ssh-keygen -t rsa 一路回车 ll -a 进入 ~/.ssh文件夹下 touch authorized_keys cat id_rsa.pub &gt;authorized_keys chmod 600 authorized_keys 测试ssh hadoop-01机器名 IP地址 设置host：root用户下 vi /etc/hosts 添加内容：ip地址 名称 查看防火墙状态 service iptables status 关闭防火墙 service iptables stop 查看防火墙开机启动状态 chkconfig iptables --list 关闭防火墙开机启动 chkconfig iptables off 关闭防火墙 systemctl stop firewalld 重启linux系统 八、下载hadoop3.2.0 https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz 九、将hadoop包上传至 /home/software 下载前进入 /home/software wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz tar -zxvf hadoop..... -C 路径 yum -y install wget 10、解压至/home/app tar -zxvf hadoop3.2.0 -C /home/app/ 11、进入hadoop3.2.0/etc/hadoop 12、修改配置文件 1)core-site.xml： fs.defaultFS hdfs://hadoop-01:9000 hadoop.tmp.dir /home/app/hadoop-3.2.0/tmp/ 2)hdfs-site.xml： dfs.replication 1 3)mapred-site.xml： mapreduce.framework.name yarn 4)yarn-site.xml： &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop-01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 进入hadoop中bin目录执行./hadoop classpath命令查看hadoop路径 --&gt; &lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;/home/app/hadoop-3.2.0/etc/hadoop:/home/app/hadoop-3.2.0/share/hadoop/common/lib/*:/home/app/hadoop-3.2.0/share/hadoop/common/*:/home/app/hadoop-3.2.0/share/hadoop/hdfs:/home/app/hadoop-3.2.0/share/hadoop/hdfs/lib/*:/home/app/hadoop-3.2.0/share/hadoop/hdfs/*:/home/app/hadoop-3.2.0/share/hadoop/mapreduce/lib/*:/home/app/hadoop-3.2.0/share/hadoop/mapreduce/*:/home/app/hadoop-3.2.0/share/hadoop/yarn:/home/app/hadoop-3.2.0/share/hadoop/yarn/lib/*:/home/app/hadoop-3.2.0/share/hadoop/yarn/*&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 13、配置hadoop环境变量 切换到root用户 vi /etc/profile source /etc/profile 使文件生效 export HADOOP_HOME=/home/app/hadoop-3.2.0 export JAVA_HOME=/home/app/jdk1.8.0_201 export JRE_HOME= J A V A H O M E / j r e e x p o r t C L A S S P A T H = . : {JAVA_HOME}/jre export CLASSPATH=.: JAVAH​OME/jreexportCLASSPATH=.:{JAVA_HOME}/lib: J R E H O M E / l i b e x p o r t P A T H = {JRE_HOME}/lib export PATH= JREH​OME/libexportPATH={JAVA_HOME}/bin: H A D O O P H O M E / s b i n : {HADOOP_HOME}/sbin: HADOOPH​OME/sbin:{HADOOP_HOME}/bin:$PATH 14、格式化namenode hadoop namenode -format 15、启动hdfs： 在hadoop/sbin目录下 ./start-dfs.sh vi hadoop-env.sh 添加java home路径：export JAVA_HOME=jdk路径 jps查看启动项 16、启动yarn：start-yarn.sh 查看进程 jps netstat -nltp 17、hdfs操作 查看目录文件：hadoop fs -ls / 存放文件：hadoop fs -put 文件 /目录 获取文件：hadoop fs -get /文件名 /目录 tmp/dfs/data/current/BP.../current/finalized/sub..0/sub...0 文件大小默认为字节 b--》k--&gt;m--&gt;g--&gt;t--&gt;p 1024 18、进入cd share/hadoop/mapreduce/ 创建一个测试文件后缀为.data 19、在hdfs上创建文件 hadoop fs -mkdir /wc hadoop fs -mkdir /wc/srcdata 20、将测试文件上传到srcdata目录 hadoop fs -put test.data /wc/srcdata 查看 hadoop fs -ls /wc/srcdata 21、执行MapReduce中wordcount算法： hadoop jar hadoop-mapreduce-examples-3.2.0.jar wordcount /wc/srcdata /wc/output 22、查看结果 hadoop fs -cat /wc/output/part-r-00000 23、通过浏览器可以访问hdfs文件系统 需要先开放namenode节点9870端口号 访问网址为：http://linux系统IP地址:9870 点击Utilities ---- Browse the file system 24、开放linux防火墙端口，hadoop3.x以上namenode web 端口改为9870 原端口号为50070 firewall-cmd --zone=public --add-port=9870/tcp --permanent 改完需要reload firewall-cmd --reload 25、退出安全模式 hadoop dfsadmin -safemode leave 26、 查看防火墙状态 service iptables status 关闭防火墙 service iptables stop 查看防火墙开机启动状态 chkconfig iptables --list 关闭防火墙开机启动 chkconfig iptables off 关闭防火墙 systemctl stop firewalld # service firewalld start; 或者 #systemctl start firewalld.service;#开启防火墙 # service firewalld stop; 或者 #systemctl stop firewalld.service;#关闭防火墙 # service firewalld restart; 或者 #systemctl restart firewalld.service; #重启防火墙 # systemctl disable firewalld.service#禁止防火墙开启自启 # systemctl enable firewalld.service#开启防火墙开启自启 27、java api操作hdfs 1) 导入jar包 &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs-client&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; 2) 操作对象 //通过configuration对象来配置文件系统 private static Configuration config = null; //文件操作系统对象 private static FileSystem fs = null; //操作hdfs的用户 private static final String HDFS_USER = &quot;hadoop&quot;; //hdfs服务地址 ip或机器名加端口号 private static final String HDFS_URL = &quot;hdfs://192.168.1.124:9000&quot;; //获取配置参数configuration对象，此对象会对xxx-site.xml文件进行解析 config = new Configuration(); //设置服务器操作地址 config.set(&quot;fs.defaultFS&quot;, HDFS_URL); //获取一个HDFS文件系统客户端实例化对象 //产生的实例究竟是哪一个文件系统的客户端，是根据conf相关参数来决定的 fs = FileSystem.get(new URI(HDFS_URL), config, HDFS_USER); 3) 操作方法 //上传文件 fs.copyFromLocalFile(new Path(&quot;D:\\hadoop-test-file\\aaa.data&quot;), new Path(&quot;/1806A/1806A.txt&quot;)); //删除 .delete(new Path(&quot;/aaa&quot;), true); //创建文件夹 .mkdirs(new Path(&quot;/111/2222/aaa&quot;)); //重命名 .rename(new Path(&quot;/123&quot;), new Path(&quot;/321&quot;)); //下载 .copyToLocalFile(new Path(&quot;/1806A/1806A.txt&quot;), new Path(&quot;d:/&quot;)); //遍历文件 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = HdfsClient.getHadoopFileSystem() .listFiles(new Path(&quot;/&quot;), true); while (listFiles.hasNext()){ LocatedFileStatus file = listFiles.next(); System.out.println(file.getPath().getName()); } //遍历文件夹和文件 FileStatus[] fileArr = HdfsClient.getHadoopFileSystem() .listStatus(new Path(&quot;/wc/output&quot;)); for(FileStatus file : fileArr){ System.out.println(file.getPath().getName() + &quot;--&quot; + (file.isDirectory() ? &quot;dir&quot; : &quot;file&quot;)); } //io流方式上传文件 SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMdd&quot;); String dateStr = sdf.format(new Date()).toString(); String filename = System.currentTimeMillis() + &quot;.txt&quot;; FSDataOutputStream fos = null; FileInputStream fis = null; try { // 文件保存在服务器上地址 fos = HdfsClient.getHdfsFileSystem() .create(new Path(&quot;/jk/&quot; + dateStr + &quot;/&quot; + filename)); // 用户上传的文件 fis = new FileInputStream(new File(&quot;D:\\hadoop-test-file\\1810B.txt&quot;)); IOUtils.copyBytes(fis, fos, 4096, false); } catch (IOException e) { e.printStackTrace(); } finally { IOUtils.closeStream(fis); IOUtils.closeStream(fos); } //io流方式从hdfs上下载文件 FSDataInputStream inputStream = null; try { inputStream = HdfsClient.getHdfsFileSystem() .open(new Path(&quot;/jk/1810b/1810B.txt&quot;)); IOUtils.copyBytes(inputStream, System.out, 4096, false); } catch (IOException e) { e.printStackTrace(); } finally { IOUtils.closeStream(inputStream); } 28、javaAPI自定义MapReduce程序 1) 新建Map测试类继承Mapper&lt;LongWritable, Text, Text, LongWritable&gt; 2) 重写map方法 // 获取文件内容 String content = value.toString(); // 将内容按空格分隔存入map集合当中 String[] contentArr = StringUtils.split(content, &#39; &#39;); for (String word : contentArr) { // 将map集合写入reduce中 context.write(new Text(word), new LongWritable(1)); } 3) 新建Reduce类继承Reducer&lt;Text, LongWritable, Text, LongWritable&gt; 4) 重写reduce方法 // 遍历map的value Iterator&lt;LongWritable&gt; iterator = values.iterator(); // 定义计数器 Long count = new Long(0); // 循环value值 while (iterator.hasNext()) { // 获取value值 LongWritable longWritable = iterator.next(); // 相同的key进行累加操作 count += longWritable.get(); } context.write(key, new LongWritable(count)); 5) 编写启动类 Configuration config = new Configuration(); Job job = Job.getInstance(config); // 设置启动类 job.setJarByClass(MapReduceRunner.class); // 设置Map类 job.setMapperClass(MapTest.class); // 设置Reduce类 job.setReducerClass(ReduceTest.class); // 设置输出的key也就是统计的关键词 job.setOutputKeyClass(Text.class); // 设置输出的value也就是统计关键词的数量 job.setOutputValueClass(LongWritable.class); // 设置MapReduce计算的文件 FileInputFormat.setInputPaths(job, &quot;/jk/1810b/1810B.txt&quot;); // 设置MapReduce计算完后的文件输出文件 // 注意：输出的目录不要存在 FileOutputFormat.setOutputPath(job, new Path(&quot;/jk/output&quot;)); job.waitForCompletion(true); 6) 将启动类打jar包，上传到服务器 执行命令：hadoop jar jar包名 包路径.启动类名" />
<meta property="og:description" content="一、hadoop发展由来、历史、子项目 什么是hadoop Google Lab 开发的 Map/Reduce 和 Google File System(GFS) ，后由 apache基金会进行封装为MapReduce和HDFS组成 Hadoop的框架最核心的设计就是：HDFS和MapReduce。 HDFS为海量的数据提供了存储， 而MapReduce则为海量的数据提供了计算。 二、讲解HDFS（namenode和datanode），实现机制和优点 三、讲解MapReduce离线计算、Storm（流式计算，基于磁盘）、Spark（内存迭代计算，基于内存） 四、安装一台linux系统，版本为CentOS7 设置网卡在 /etc/sysconfig/network-scripts/ifcfg-enp0s3 ONBOOT=yes 虚拟机网络设置为桥接网络 选择对应联网方式 通过 ping www.baidu.com 来测试是否有网络 yum install net-tools 安装jdk1.8 并配置环境变量 https://www.cnblogs.com/Dylansuns/p/6974272.html 五、宿主机和虚拟机可以相互ping通，并在虚拟机设置静态IP BOOTPROTO=static DNS1=192.168.1.1 IPADDR=192.168.1.120 NETMASK=255.255.255.0 GATWAY=192.168.1.1 ZONE=public 六、创建用户，并赋权限 useradd user1 添加用户 passwd user1 设置密码 chown -R user:group 授权目录 例如：chown -R hadoop:hadoop /home // userdel –r user3 删除用户及目录 七、SSH免密登录 切换到hadoop用户下操作 ssh-keygen -t rsa 一路回车 ll -a 进入 ~/.ssh文件夹下 touch authorized_keys cat id_rsa.pub &gt;authorized_keys chmod 600 authorized_keys 测试ssh hadoop-01机器名 IP地址 设置host：root用户下 vi /etc/hosts 添加内容：ip地址 名称 查看防火墙状态 service iptables status 关闭防火墙 service iptables stop 查看防火墙开机启动状态 chkconfig iptables --list 关闭防火墙开机启动 chkconfig iptables off 关闭防火墙 systemctl stop firewalld 重启linux系统 八、下载hadoop3.2.0 https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz 九、将hadoop包上传至 /home/software 下载前进入 /home/software wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz tar -zxvf hadoop..... -C 路径 yum -y install wget 10、解压至/home/app tar -zxvf hadoop3.2.0 -C /home/app/ 11、进入hadoop3.2.0/etc/hadoop 12、修改配置文件 1)core-site.xml： fs.defaultFS hdfs://hadoop-01:9000 hadoop.tmp.dir /home/app/hadoop-3.2.0/tmp/ 2)hdfs-site.xml： dfs.replication 1 3)mapred-site.xml： mapreduce.framework.name yarn 4)yarn-site.xml： &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop-01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 进入hadoop中bin目录执行./hadoop classpath命令查看hadoop路径 --&gt; &lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;/home/app/hadoop-3.2.0/etc/hadoop:/home/app/hadoop-3.2.0/share/hadoop/common/lib/*:/home/app/hadoop-3.2.0/share/hadoop/common/*:/home/app/hadoop-3.2.0/share/hadoop/hdfs:/home/app/hadoop-3.2.0/share/hadoop/hdfs/lib/*:/home/app/hadoop-3.2.0/share/hadoop/hdfs/*:/home/app/hadoop-3.2.0/share/hadoop/mapreduce/lib/*:/home/app/hadoop-3.2.0/share/hadoop/mapreduce/*:/home/app/hadoop-3.2.0/share/hadoop/yarn:/home/app/hadoop-3.2.0/share/hadoop/yarn/lib/*:/home/app/hadoop-3.2.0/share/hadoop/yarn/*&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 13、配置hadoop环境变量 切换到root用户 vi /etc/profile source /etc/profile 使文件生效 export HADOOP_HOME=/home/app/hadoop-3.2.0 export JAVA_HOME=/home/app/jdk1.8.0_201 export JRE_HOME= J A V A H O M E / j r e e x p o r t C L A S S P A T H = . : {JAVA_HOME}/jre export CLASSPATH=.: JAVAH​OME/jreexportCLASSPATH=.:{JAVA_HOME}/lib: J R E H O M E / l i b e x p o r t P A T H = {JRE_HOME}/lib export PATH= JREH​OME/libexportPATH={JAVA_HOME}/bin: H A D O O P H O M E / s b i n : {HADOOP_HOME}/sbin: HADOOPH​OME/sbin:{HADOOP_HOME}/bin:$PATH 14、格式化namenode hadoop namenode -format 15、启动hdfs： 在hadoop/sbin目录下 ./start-dfs.sh vi hadoop-env.sh 添加java home路径：export JAVA_HOME=jdk路径 jps查看启动项 16、启动yarn：start-yarn.sh 查看进程 jps netstat -nltp 17、hdfs操作 查看目录文件：hadoop fs -ls / 存放文件：hadoop fs -put 文件 /目录 获取文件：hadoop fs -get /文件名 /目录 tmp/dfs/data/current/BP.../current/finalized/sub..0/sub...0 文件大小默认为字节 b--》k--&gt;m--&gt;g--&gt;t--&gt;p 1024 18、进入cd share/hadoop/mapreduce/ 创建一个测试文件后缀为.data 19、在hdfs上创建文件 hadoop fs -mkdir /wc hadoop fs -mkdir /wc/srcdata 20、将测试文件上传到srcdata目录 hadoop fs -put test.data /wc/srcdata 查看 hadoop fs -ls /wc/srcdata 21、执行MapReduce中wordcount算法： hadoop jar hadoop-mapreduce-examples-3.2.0.jar wordcount /wc/srcdata /wc/output 22、查看结果 hadoop fs -cat /wc/output/part-r-00000 23、通过浏览器可以访问hdfs文件系统 需要先开放namenode节点9870端口号 访问网址为：http://linux系统IP地址:9870 点击Utilities ---- Browse the file system 24、开放linux防火墙端口，hadoop3.x以上namenode web 端口改为9870 原端口号为50070 firewall-cmd --zone=public --add-port=9870/tcp --permanent 改完需要reload firewall-cmd --reload 25、退出安全模式 hadoop dfsadmin -safemode leave 26、 查看防火墙状态 service iptables status 关闭防火墙 service iptables stop 查看防火墙开机启动状态 chkconfig iptables --list 关闭防火墙开机启动 chkconfig iptables off 关闭防火墙 systemctl stop firewalld # service firewalld start; 或者 #systemctl start firewalld.service;#开启防火墙 # service firewalld stop; 或者 #systemctl stop firewalld.service;#关闭防火墙 # service firewalld restart; 或者 #systemctl restart firewalld.service; #重启防火墙 # systemctl disable firewalld.service#禁止防火墙开启自启 # systemctl enable firewalld.service#开启防火墙开启自启 27、java api操作hdfs 1) 导入jar包 &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs-client&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; 2) 操作对象 //通过configuration对象来配置文件系统 private static Configuration config = null; //文件操作系统对象 private static FileSystem fs = null; //操作hdfs的用户 private static final String HDFS_USER = &quot;hadoop&quot;; //hdfs服务地址 ip或机器名加端口号 private static final String HDFS_URL = &quot;hdfs://192.168.1.124:9000&quot;; //获取配置参数configuration对象，此对象会对xxx-site.xml文件进行解析 config = new Configuration(); //设置服务器操作地址 config.set(&quot;fs.defaultFS&quot;, HDFS_URL); //获取一个HDFS文件系统客户端实例化对象 //产生的实例究竟是哪一个文件系统的客户端，是根据conf相关参数来决定的 fs = FileSystem.get(new URI(HDFS_URL), config, HDFS_USER); 3) 操作方法 //上传文件 fs.copyFromLocalFile(new Path(&quot;D:\\hadoop-test-file\\aaa.data&quot;), new Path(&quot;/1806A/1806A.txt&quot;)); //删除 .delete(new Path(&quot;/aaa&quot;), true); //创建文件夹 .mkdirs(new Path(&quot;/111/2222/aaa&quot;)); //重命名 .rename(new Path(&quot;/123&quot;), new Path(&quot;/321&quot;)); //下载 .copyToLocalFile(new Path(&quot;/1806A/1806A.txt&quot;), new Path(&quot;d:/&quot;)); //遍历文件 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = HdfsClient.getHadoopFileSystem() .listFiles(new Path(&quot;/&quot;), true); while (listFiles.hasNext()){ LocatedFileStatus file = listFiles.next(); System.out.println(file.getPath().getName()); } //遍历文件夹和文件 FileStatus[] fileArr = HdfsClient.getHadoopFileSystem() .listStatus(new Path(&quot;/wc/output&quot;)); for(FileStatus file : fileArr){ System.out.println(file.getPath().getName() + &quot;--&quot; + (file.isDirectory() ? &quot;dir&quot; : &quot;file&quot;)); } //io流方式上传文件 SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMdd&quot;); String dateStr = sdf.format(new Date()).toString(); String filename = System.currentTimeMillis() + &quot;.txt&quot;; FSDataOutputStream fos = null; FileInputStream fis = null; try { // 文件保存在服务器上地址 fos = HdfsClient.getHdfsFileSystem() .create(new Path(&quot;/jk/&quot; + dateStr + &quot;/&quot; + filename)); // 用户上传的文件 fis = new FileInputStream(new File(&quot;D:\\hadoop-test-file\\1810B.txt&quot;)); IOUtils.copyBytes(fis, fos, 4096, false); } catch (IOException e) { e.printStackTrace(); } finally { IOUtils.closeStream(fis); IOUtils.closeStream(fos); } //io流方式从hdfs上下载文件 FSDataInputStream inputStream = null; try { inputStream = HdfsClient.getHdfsFileSystem() .open(new Path(&quot;/jk/1810b/1810B.txt&quot;)); IOUtils.copyBytes(inputStream, System.out, 4096, false); } catch (IOException e) { e.printStackTrace(); } finally { IOUtils.closeStream(inputStream); } 28、javaAPI自定义MapReduce程序 1) 新建Map测试类继承Mapper&lt;LongWritable, Text, Text, LongWritable&gt; 2) 重写map方法 // 获取文件内容 String content = value.toString(); // 将内容按空格分隔存入map集合当中 String[] contentArr = StringUtils.split(content, &#39; &#39;); for (String word : contentArr) { // 将map集合写入reduce中 context.write(new Text(word), new LongWritable(1)); } 3) 新建Reduce类继承Reducer&lt;Text, LongWritable, Text, LongWritable&gt; 4) 重写reduce方法 // 遍历map的value Iterator&lt;LongWritable&gt; iterator = values.iterator(); // 定义计数器 Long count = new Long(0); // 循环value值 while (iterator.hasNext()) { // 获取value值 LongWritable longWritable = iterator.next(); // 相同的key进行累加操作 count += longWritable.get(); } context.write(key, new LongWritable(count)); 5) 编写启动类 Configuration config = new Configuration(); Job job = Job.getInstance(config); // 设置启动类 job.setJarByClass(MapReduceRunner.class); // 设置Map类 job.setMapperClass(MapTest.class); // 设置Reduce类 job.setReducerClass(ReduceTest.class); // 设置输出的key也就是统计的关键词 job.setOutputKeyClass(Text.class); // 设置输出的value也就是统计关键词的数量 job.setOutputValueClass(LongWritable.class); // 设置MapReduce计算的文件 FileInputFormat.setInputPaths(job, &quot;/jk/1810b/1810B.txt&quot;); // 设置MapReduce计算完后的文件输出文件 // 注意：输出的目录不要存在 FileOutputFormat.setOutputPath(job, new Path(&quot;/jk/output&quot;)); job.waitForCompletion(true); 6) 将启动类打jar包，上传到服务器 执行命令：hadoop jar jar包名 包路径.启动类名" />
<link rel="canonical" href="https://uzzz.org/2019/07/26/793655.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/26/793655.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-26T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"一、hadoop发展由来、历史、子项目 什么是hadoop Google Lab 开发的 Map/Reduce 和 Google File System(GFS) ，后由 apache基金会进行封装为MapReduce和HDFS组成 Hadoop的框架最核心的设计就是：HDFS和MapReduce。 HDFS为海量的数据提供了存储， 而MapReduce则为海量的数据提供了计算。 二、讲解HDFS（namenode和datanode），实现机制和优点 三、讲解MapReduce离线计算、Storm（流式计算，基于磁盘）、Spark（内存迭代计算，基于内存） 四、安装一台linux系统，版本为CentOS7 设置网卡在 /etc/sysconfig/network-scripts/ifcfg-enp0s3 ONBOOT=yes 虚拟机网络设置为桥接网络 选择对应联网方式 通过 ping www.baidu.com 来测试是否有网络 yum install net-tools 安装jdk1.8 并配置环境变量 https://www.cnblogs.com/Dylansuns/p/6974272.html 五、宿主机和虚拟机可以相互ping通，并在虚拟机设置静态IP BOOTPROTO=static DNS1=192.168.1.1 IPADDR=192.168.1.120 NETMASK=255.255.255.0 GATWAY=192.168.1.1 ZONE=public 六、创建用户，并赋权限 useradd user1 添加用户 passwd user1 设置密码 chown -R user:group 授权目录 例如：chown -R hadoop:hadoop /home // userdel –r user3 删除用户及目录 七、SSH免密登录 切换到hadoop用户下操作 ssh-keygen -t rsa 一路回车 ll -a 进入 ~/.ssh文件夹下 touch authorized_keys cat id_rsa.pub &gt;authorized_keys chmod 600 authorized_keys 测试ssh hadoop-01机器名 IP地址 设置host：root用户下 vi /etc/hosts 添加内容：ip地址 名称 查看防火墙状态 service iptables status 关闭防火墙 service iptables stop 查看防火墙开机启动状态 chkconfig iptables --list 关闭防火墙开机启动 chkconfig iptables off 关闭防火墙 systemctl stop firewalld 重启linux系统 八、下载hadoop3.2.0 https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz 九、将hadoop包上传至 /home/software 下载前进入 /home/software wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz tar -zxvf hadoop..... -C 路径 yum -y install wget 10、解压至/home/app tar -zxvf hadoop3.2.0 -C /home/app/ 11、进入hadoop3.2.0/etc/hadoop 12、修改配置文件 1)core-site.xml： fs.defaultFS hdfs://hadoop-01:9000 hadoop.tmp.dir /home/app/hadoop-3.2.0/tmp/ 2)hdfs-site.xml： dfs.replication 1 3)mapred-site.xml： mapreduce.framework.name yarn 4)yarn-site.xml： &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop-01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 进入hadoop中bin目录执行./hadoop classpath命令查看hadoop路径 --&gt; &lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;/home/app/hadoop-3.2.0/etc/hadoop:/home/app/hadoop-3.2.0/share/hadoop/common/lib/*:/home/app/hadoop-3.2.0/share/hadoop/common/*:/home/app/hadoop-3.2.0/share/hadoop/hdfs:/home/app/hadoop-3.2.0/share/hadoop/hdfs/lib/*:/home/app/hadoop-3.2.0/share/hadoop/hdfs/*:/home/app/hadoop-3.2.0/share/hadoop/mapreduce/lib/*:/home/app/hadoop-3.2.0/share/hadoop/mapreduce/*:/home/app/hadoop-3.2.0/share/hadoop/yarn:/home/app/hadoop-3.2.0/share/hadoop/yarn/lib/*:/home/app/hadoop-3.2.0/share/hadoop/yarn/*&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 13、配置hadoop环境变量 切换到root用户 vi /etc/profile source /etc/profile 使文件生效 export HADOOP_HOME=/home/app/hadoop-3.2.0 export JAVA_HOME=/home/app/jdk1.8.0_201 export JRE_HOME= J A V A H O M E / j r e e x p o r t C L A S S P A T H = . : {JAVA_HOME}/jre export CLASSPATH=.: JAVAH​OME/jreexportCLASSPATH=.:{JAVA_HOME}/lib: J R E H O M E / l i b e x p o r t P A T H = {JRE_HOME}/lib export PATH= JREH​OME/libexportPATH={JAVA_HOME}/bin: H A D O O P H O M E / s b i n : {HADOOP_HOME}/sbin: HADOOPH​OME/sbin:{HADOOP_HOME}/bin:$PATH 14、格式化namenode hadoop namenode -format 15、启动hdfs： 在hadoop/sbin目录下 ./start-dfs.sh vi hadoop-env.sh 添加java home路径：export JAVA_HOME=jdk路径 jps查看启动项 16、启动yarn：start-yarn.sh 查看进程 jps netstat -nltp 17、hdfs操作 查看目录文件：hadoop fs -ls / 存放文件：hadoop fs -put 文件 /目录 获取文件：hadoop fs -get /文件名 /目录 tmp/dfs/data/current/BP.../current/finalized/sub..0/sub...0 文件大小默认为字节 b--》k--&gt;m--&gt;g--&gt;t--&gt;p 1024 18、进入cd share/hadoop/mapreduce/ 创建一个测试文件后缀为.data 19、在hdfs上创建文件 hadoop fs -mkdir /wc hadoop fs -mkdir /wc/srcdata 20、将测试文件上传到srcdata目录 hadoop fs -put test.data /wc/srcdata 查看 hadoop fs -ls /wc/srcdata 21、执行MapReduce中wordcount算法： hadoop jar hadoop-mapreduce-examples-3.2.0.jar wordcount /wc/srcdata /wc/output 22、查看结果 hadoop fs -cat /wc/output/part-r-00000 23、通过浏览器可以访问hdfs文件系统 需要先开放namenode节点9870端口号 访问网址为：http://linux系统IP地址:9870 点击Utilities ---- Browse the file system 24、开放linux防火墙端口，hadoop3.x以上namenode web 端口改为9870 原端口号为50070 firewall-cmd --zone=public --add-port=9870/tcp --permanent 改完需要reload firewall-cmd --reload 25、退出安全模式 hadoop dfsadmin -safemode leave 26、 查看防火墙状态 service iptables status 关闭防火墙 service iptables stop 查看防火墙开机启动状态 chkconfig iptables --list 关闭防火墙开机启动 chkconfig iptables off 关闭防火墙 systemctl stop firewalld # service firewalld start; 或者 #systemctl start firewalld.service;#开启防火墙 # service firewalld stop; 或者 #systemctl stop firewalld.service;#关闭防火墙 # service firewalld restart; 或者 #systemctl restart firewalld.service; #重启防火墙 # systemctl disable firewalld.service#禁止防火墙开启自启 # systemctl enable firewalld.service#开启防火墙开启自启 27、java api操作hdfs 1) 导入jar包 &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs-client&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; 2) 操作对象 //通过configuration对象来配置文件系统 private static Configuration config = null; //文件操作系统对象 private static FileSystem fs = null; //操作hdfs的用户 private static final String HDFS_USER = &quot;hadoop&quot;; //hdfs服务地址 ip或机器名加端口号 private static final String HDFS_URL = &quot;hdfs://192.168.1.124:9000&quot;; //获取配置参数configuration对象，此对象会对xxx-site.xml文件进行解析 config = new Configuration(); //设置服务器操作地址 config.set(&quot;fs.defaultFS&quot;, HDFS_URL); //获取一个HDFS文件系统客户端实例化对象 //产生的实例究竟是哪一个文件系统的客户端，是根据conf相关参数来决定的 fs = FileSystem.get(new URI(HDFS_URL), config, HDFS_USER); 3) 操作方法 //上传文件 fs.copyFromLocalFile(new Path(&quot;D:\\\\hadoop-test-file\\\\aaa.data&quot;), new Path(&quot;/1806A/1806A.txt&quot;)); //删除 .delete(new Path(&quot;/aaa&quot;), true); //创建文件夹 .mkdirs(new Path(&quot;/111/2222/aaa&quot;)); //重命名 .rename(new Path(&quot;/123&quot;), new Path(&quot;/321&quot;)); //下载 .copyToLocalFile(new Path(&quot;/1806A/1806A.txt&quot;), new Path(&quot;d:/&quot;)); //遍历文件 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = HdfsClient.getHadoopFileSystem() .listFiles(new Path(&quot;/&quot;), true); while (listFiles.hasNext()){ LocatedFileStatus file = listFiles.next(); System.out.println(file.getPath().getName()); } //遍历文件夹和文件 FileStatus[] fileArr = HdfsClient.getHadoopFileSystem() .listStatus(new Path(&quot;/wc/output&quot;)); for(FileStatus file : fileArr){ System.out.println(file.getPath().getName() + &quot;--&quot; + (file.isDirectory() ? &quot;dir&quot; : &quot;file&quot;)); } //io流方式上传文件 SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMdd&quot;); String dateStr = sdf.format(new Date()).toString(); String filename = System.currentTimeMillis() + &quot;.txt&quot;; FSDataOutputStream fos = null; FileInputStream fis = null; try { // 文件保存在服务器上地址 fos = HdfsClient.getHdfsFileSystem() .create(new Path(&quot;/jk/&quot; + dateStr + &quot;/&quot; + filename)); // 用户上传的文件 fis = new FileInputStream(new File(&quot;D:\\\\hadoop-test-file\\\\1810B.txt&quot;)); IOUtils.copyBytes(fis, fos, 4096, false); } catch (IOException e) { e.printStackTrace(); } finally { IOUtils.closeStream(fis); IOUtils.closeStream(fos); } //io流方式从hdfs上下载文件 FSDataInputStream inputStream = null; try { inputStream = HdfsClient.getHdfsFileSystem() .open(new Path(&quot;/jk/1810b/1810B.txt&quot;)); IOUtils.copyBytes(inputStream, System.out, 4096, false); } catch (IOException e) { e.printStackTrace(); } finally { IOUtils.closeStream(inputStream); } 28、javaAPI自定义MapReduce程序 1) 新建Map测试类继承Mapper&lt;LongWritable, Text, Text, LongWritable&gt; 2) 重写map方法 // 获取文件内容 String content = value.toString(); // 将内容按空格分隔存入map集合当中 String[] contentArr = StringUtils.split(content, &#39; &#39;); for (String word : contentArr) { // 将map集合写入reduce中 context.write(new Text(word), new LongWritable(1)); } 3) 新建Reduce类继承Reducer&lt;Text, LongWritable, Text, LongWritable&gt; 4) 重写reduce方法 // 遍历map的value Iterator&lt;LongWritable&gt; iterator = values.iterator(); // 定义计数器 Long count = new Long(0); // 循环value值 while (iterator.hasNext()) { // 获取value值 LongWritable longWritable = iterator.next(); // 相同的key进行累加操作 count += longWritable.get(); } context.write(key, new LongWritable(count)); 5) 编写启动类 Configuration config = new Configuration(); Job job = Job.getInstance(config); // 设置启动类 job.setJarByClass(MapReduceRunner.class); // 设置Map类 job.setMapperClass(MapTest.class); // 设置Reduce类 job.setReducerClass(ReduceTest.class); // 设置输出的key也就是统计的关键词 job.setOutputKeyClass(Text.class); // 设置输出的value也就是统计关键词的数量 job.setOutputValueClass(LongWritable.class); // 设置MapReduce计算的文件 FileInputFormat.setInputPaths(job, &quot;/jk/1810b/1810B.txt&quot;); // 设置MapReduce计算完后的文件输出文件 // 注意：输出的目录不要存在 FileOutputFormat.setOutputPath(job, new Path(&quot;/jk/output&quot;)); job.waitForCompletion(true); 6) 将启动类打jar包，上传到服务器 执行命令：hadoop jar jar包名 包路径.启动类名","@type":"BlogPosting","url":"https://uzzz.org/2019/07/26/793655.html","headline":"hadoop3.2.0伪分布式搭建","dateModified":"2019-07-26T00:00:00+08:00","datePublished":"2019-07-26T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/26/793655.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>hadoop3.2.0伪分布式搭建</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>一、hadoop发展由来、历史、子项目<br> 什么是hadoop<br> Google Lab 开发的 Map/Reduce 和 Google File System(GFS) ，后由<br> apache基金会进行封装为MapReduce和HDFS组成<br> Hadoop的框架最核心的设计就是：HDFS和MapReduce。<br> HDFS为海量的数据提供了存储，<br> 而MapReduce则为海量的数据提供了计算。</p> 
  <p>二、讲解HDFS（namenode和datanode），实现机制和优点</p> 
  <p>三、讲解MapReduce离线计算、Storm（流式计算，基于磁盘）、Spark（内存迭代计算，基于内存）</p> 
  <p>四、安装一台linux系统，版本为CentOS7<br> 设置网卡在 /etc/sysconfig/network-scripts/ifcfg-enp0s3<br> ONBOOT=yes</p> 
  <pre><code>虚拟机网络设置为桥接网络 选择对应联网方式
通过 ping www.baidu.com 来测试是否有网络

yum install net-tools

安装jdk1.8 并配置环境变量
https://www.cnblogs.com/Dylansuns/p/6974272.html
</code></pre> 
  <p>五、宿主机和虚拟机可以相互ping通，并在虚拟机设置静态IP<br> BOOTPROTO=static</p> 
  <pre><code>DNS1=192.168.1.1
IPADDR=192.168.1.120
NETMASK=255.255.255.0
GATWAY=192.168.1.1
ZONE=public
</code></pre> 
  <p>六、创建用户，并赋权限<br> useradd user1 添加用户</p> 
  <pre><code>passwd user1 设置密码

chown -R user:group 授权目录 例如：chown -R hadoop:hadoop /home
</code></pre> 
  <p>// userdel –r user3 删除用户及目录</p> 
  <p>七、SSH免密登录</p> 
  <pre><code>切换到hadoop用户下操作

ssh-keygen -t rsa    

一路回车

ll -a

进入 ~/.ssh文件夹下

touch authorized_keys

cat id_rsa.pub &gt;authorized_keys

chmod 600 authorized_keys

测试ssh hadoop-01机器名 IP地址
</code></pre> 
  <p>设置host：root用户下 vi /etc/hosts<br> 添加内容：ip地址 名称</p> 
  <p>查看防火墙状态<br> service iptables status<br> 关闭防火墙<br> service iptables stop<br> 查看防火墙开机启动状态<br> chkconfig iptables --list<br> 关闭防火墙开机启动<br> chkconfig iptables off<br> 关闭防火墙<br> systemctl stop firewalld</p> 
  <pre><code>重启linux系统
</code></pre> 
  <p>八、下载hadoop3.2.0<br> <a href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz" rel="nofollow" data-token="d5bf48206744e8db4eb35ecca6b2fa10">https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz</a><br> <a href="http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz" rel="nofollow" data-token="4c289e680efcf523d41630a7a7af4fa1">http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz</a></p> 
  <p>九、将hadoop包上传至 /home/software<br> 下载前进入 /home/software<br> wget <a href="http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz" rel="nofollow" data-token="4c289e680efcf523d41630a7a7af4fa1">http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz</a></p> 
  <pre><code>tar -zxvf  hadoop..... -C 路径

yum -y install wget
</code></pre> 
  <p>10、解压至/home/app<br> tar -zxvf hadoop3.2.0 -C /home/app/</p> 
  <p>11、进入hadoop3.2.0/etc/hadoop</p> 
  <p>12、修改配置文件<br> 1)core-site.xml：<br> <br> <br> fs.defaultFS<br> hdfs://hadoop-01:9000<br> <br> <br> hadoop.tmp.dir<br> /home/app/hadoop-3.2.0/tmp/<br> <br> </p> 
  <p>2)hdfs-site.xml：<br> <br> <br> dfs.replication<br> 1<br> <br> </p> 
  <p>3)mapred-site.xml：<br> <br> <br> mapreduce.framework.name<br> yarn<br> <br> </p> 
  <p>4)yarn-site.xml：<br> </p> 
  <pre><code> &lt;!-- Site specific YARN configuration properties --&gt;

 &lt;property&gt;
  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
  &lt;value&gt;hadoop-01&lt;/value&gt;
 &lt;/property&gt;

 &lt;property&gt;
  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
 &lt;/property&gt;

&lt;!-- 进入hadoop中bin目录执行./hadoop classpath命令查看hadoop路径 --&gt;
 &lt;property&gt;
  &lt;name&gt;yarn.application.classpath&lt;/name&gt;
  &lt;value&gt;/home/app/hadoop-3.2.0/etc/hadoop:/home/app/hadoop-3.2.0/share/hadoop/common/lib/*:/home/app/hadoop-3.2.0/share/hadoop/common/*:/home/app/hadoop-3.2.0/share/hadoop/hdfs:/home/app/hadoop-3.2.0/share/hadoop/hdfs/lib/*:/home/app/hadoop-3.2.0/share/hadoop/hdfs/*:/home/app/hadoop-3.2.0/share/hadoop/mapreduce/lib/*:/home/app/hadoop-3.2.0/share/hadoop/mapreduce/*:/home/app/hadoop-3.2.0/share/hadoop/yarn:/home/app/hadoop-3.2.0/share/hadoop/yarn/lib/*:/home/app/hadoop-3.2.0/share/hadoop/yarn/*&lt;/value&gt;
 &lt;/property&gt;

&lt;/configuration&gt;
</code></pre> 
  <p>13、配置hadoop环境变量 切换到root用户 vi /etc/profile source /etc/profile 使文件生效<br> export HADOOP_HOME=/home/app/hadoop-3.2.0<br> export JAVA_HOME=/home/app/jdk1.8.0_201<br> export JRE_HOME=<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mrow>
          <mi>
           J
          </mi>
          <mi>
           A
          </mi>
          <mi>
           V
          </mi>
          <msub>
           <mi>
            A
           </mi>
           <mi>
            H
           </mi>
          </msub>
          <mi>
           O
          </mi>
          <mi>
           M
          </mi>
          <mi>
           E
          </mi>
         </mrow>
         <mi mathvariant="normal">
          /
         </mi>
         <mi>
          j
         </mi>
         <mi>
          r
         </mi>
         <mi>
          e
         </mi>
         <mi>
          e
         </mi>
         <mi>
          x
         </mi>
         <mi>
          p
         </mi>
         <mi>
          o
         </mi>
         <mi>
          r
         </mi>
         <mi>
          t
         </mi>
         <mi>
          C
         </mi>
         <mi>
          L
         </mi>
         <mi>
          A
         </mi>
         <mi>
          S
         </mi>
         <mi>
          S
         </mi>
         <mi>
          P
         </mi>
         <mi>
          A
         </mi>
         <mi>
          T
         </mi>
         <mi>
          H
         </mi>
         <mo>
          =
         </mo>
         <mi mathvariant="normal">
          .
         </mi>
         <mo>
          :
         </mo>
        </mrow>
        <annotation encoding="application/x-tex">
         {JAVA_HOME}/jre export CLASSPATH=.:
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.09618em;">J</span><span class="mord mathit">A</span><span class="mord mathit" style="margin-right: 0.22222em;">V</span><span class="mord"><span class="mord mathit">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathit" style="margin-right: 0.02778em;">O</span><span class="mord mathit" style="margin-right: 0.10903em;">M</span><span class="mord mathit" style="margin-right: 0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathit" style="margin-right: 0.05724em;">j</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">e</span><span class="mord mathit">x</span><span class="mord mathit">p</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mord mathit">t</span><span class="mord mathit" style="margin-right: 0.07153em;">C</span><span class="mord mathit">L</span><span class="mord mathit">A</span><span class="mord mathit" style="margin-right: 0.05764em;">S</span><span class="mord mathit" style="margin-right: 0.05764em;">S</span><span class="mord mathit" style="margin-right: 0.13889em;">P</span><span class="mord mathit">A</span><span class="mord mathit" style="margin-right: 0.13889em;">T</span><span class="mord mathit" style="margin-right: 0.08125em;">H</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord">.</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span></span></span></span></span>{JAVA_HOME}/lib:<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mrow>
          <mi>
           J
          </mi>
          <mi>
           R
          </mi>
          <msub>
           <mi>
            E
           </mi>
           <mi>
            H
           </mi>
          </msub>
          <mi>
           O
          </mi>
          <mi>
           M
          </mi>
          <mi>
           E
          </mi>
         </mrow>
         <mi mathvariant="normal">
          /
         </mi>
         <mi>
          l
         </mi>
         <mi>
          i
         </mi>
         <mi>
          b
         </mi>
         <mi>
          e
         </mi>
         <mi>
          x
         </mi>
         <mi>
          p
         </mi>
         <mi>
          o
         </mi>
         <mi>
          r
         </mi>
         <mi>
          t
         </mi>
         <mi>
          P
         </mi>
         <mi>
          A
         </mi>
         <mi>
          T
         </mi>
         <mi>
          H
         </mi>
         <mo>
          =
         </mo>
        </mrow>
        <annotation encoding="application/x-tex">
         {JRE_HOME}/lib export PATH=
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.09618em;">J</span><span class="mord mathit" style="margin-right: 0.00773em;">R</span><span class="mord"><span class="mord mathit" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathit" style="margin-right: 0.02778em;">O</span><span class="mord mathit" style="margin-right: 0.10903em;">M</span><span class="mord mathit" style="margin-right: 0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="mord mathit">i</span><span class="mord mathit">b</span><span class="mord mathit">e</span><span class="mord mathit">x</span><span class="mord mathit">p</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mord mathit">t</span><span class="mord mathit" style="margin-right: 0.13889em;">P</span><span class="mord mathit">A</span><span class="mord mathit" style="margin-right: 0.13889em;">T</span><span class="mord mathit" style="margin-right: 0.08125em;">H</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span></span></span></span></span>{JAVA_HOME}/bin:<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mrow>
          <mi>
           H
          </mi>
          <mi>
           A
          </mi>
          <mi>
           D
          </mi>
          <mi>
           O
          </mi>
          <mi>
           O
          </mi>
          <msub>
           <mi>
            P
           </mi>
           <mi>
            H
           </mi>
          </msub>
          <mi>
           O
          </mi>
          <mi>
           M
          </mi>
          <mi>
           E
          </mi>
         </mrow>
         <mi mathvariant="normal">
          /
         </mi>
         <mi>
          s
         </mi>
         <mi>
          b
         </mi>
         <mi>
          i
         </mi>
         <mi>
          n
         </mi>
         <mo>
          :
         </mo>
        </mrow>
        <annotation encoding="application/x-tex">
         {HADOOP_HOME}/sbin:
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.08125em;">H</span><span class="mord mathit">A</span><span class="mord mathit" style="margin-right: 0.02778em;">D</span><span class="mord mathit" style="margin-right: 0.02778em;">O</span><span class="mord mathit" style="margin-right: 0.02778em;">O</span><span class="mord"><span class="mord mathit" style="margin-right: 0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathit" style="margin-right: 0.02778em;">O</span><span class="mord mathit" style="margin-right: 0.10903em;">M</span><span class="mord mathit" style="margin-right: 0.05764em;">E</span></span><span class="mord">/</span><span class="mord mathit">s</span><span class="mord mathit">b</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span></span></span></span></span>{HADOOP_HOME}/bin:$PATH</p> 
  <p>14、格式化namenode<br> hadoop namenode -format</p> 
  <p>15、启动hdfs： 在hadoop/sbin目录下<br> ./start-dfs.sh</p> 
  <pre><code> vi hadoop-env.sh  
 添加java home路径：export JAVA_HOME=jdk路径
 
 jps查看启动项
</code></pre> 
  <p>16、启动yarn：<a href="http://start-yarn.sh" rel="nofollow" data-token="a711d5d50af3e4dde3174edf9f849989">start-yarn.sh</a><br> 查看进程<br> jps netstat -nltp</p> 
  <p>17、hdfs操作<br> 查看目录文件：hadoop fs -ls /<br> 存放文件：hadoop fs -put 文件 /目录<br> 获取文件：hadoop fs -get /文件名 /目录</p> 
  <pre><code>tmp/dfs/data/current/BP.../current/finalized/sub..0/sub...0

文件大小默认为字节 b--》k--&gt;m--&gt;g--&gt;t--&gt;p   1024
</code></pre> 
  <p>18、进入cd share/hadoop/mapreduce/<br> 创建一个测试文件后缀为.data</p> 
  <p>19、在hdfs上创建文件<br> hadoop fs -mkdir /wc<br> hadoop fs -mkdir /wc/srcdata</p> 
  <p>20、将测试文件上传到srcdata目录<br> hadoop fs -put test.data /wc/srcdata</p> 
  <pre><code>查看 hadoop fs -ls /wc/srcdata
</code></pre> 
  <p>21、执行MapReduce中wordcount算法：<br> hadoop jar hadoop-mapreduce-examples-3.2.0.jar wordcount /wc/srcdata /wc/output</p> 
  <p>22、查看结果<br> hadoop fs -cat /wc/output/part-r-00000</p> 
  <p>23、通过浏览器可以访问hdfs文件系统<br> 需要先开放namenode节点9870端口号<br> 访问网址为：<a href="http://xn--linuxIP-x68lue8731beme:9870" rel="nofollow" data-token="6d41a4ba14e12fae68d19b6e0b65462e">http://linux系统IP地址:9870</a><br> 点击Utilities ---- Browse the file system</p> 
  <p>24、开放linux防火墙端口，hadoop3.x以上namenode web 端口改为9870 原端口号为50070<br> firewall-cmd --zone=public --add-port=9870/tcp --permanent</p> 
  <pre><code>改完需要reload    firewall-cmd --reload	
</code></pre> 
  <p>25、退出安全模式<br> hadoop dfsadmin -safemode leave</p> 
  <p>26、<br> 查看防火墙状态<br> service iptables status<br> 关闭防火墙<br> service iptables stop<br> 查看防火墙开机启动状态<br> chkconfig iptables --list<br> 关闭防火墙开机启动<br> chkconfig iptables off<br> 关闭防火墙<br> systemctl stop firewalld</p> 
  <pre><code># service firewalld start;  或者 #systemctl start firewalld.service;#开启防火墙
# service firewalld stop;  或者 #systemctl stop firewalld.service;#关闭防火墙
# service firewalld restart;  或者 #systemctl restart firewalld.service;  #重启防火墙
# systemctl disable firewalld.service#禁止防火墙开启自启
# systemctl enable firewalld.service#开启防火墙开启自启
</code></pre> 
  <p>27、java api操作hdfs</p> 
  <pre><code>1) 导入jar包
	&lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
        &lt;version&gt;3.2.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
        &lt;version&gt;3.2.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-hdfs-client&lt;/artifactId&gt;
        &lt;version&gt;3.2.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;
        &lt;version&gt;3.2.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;4.11&lt;/version&gt;
    &lt;/dependency&gt;
2) 操作对象
	//通过configuration对象来配置文件系统
	private static Configuration config = null;
	//文件操作系统对象
	private static FileSystem fs = null;
	//操作hdfs的用户
	private static final String HDFS_USER = "hadoop";
	//hdfs服务地址 ip或机器名加端口号
	private static final String HDFS_URL = "hdfs://192.168.1.124:9000";
	//获取配置参数configuration对象，此对象会对xxx-site.xml文件进行解析
	config = new Configuration();
	//设置服务器操作地址
	config.set("fs.defaultFS", HDFS_URL);
	//获取一个HDFS文件系统客户端实例化对象
	//产生的实例究竟是哪一个文件系统的客户端，是根据conf相关参数来决定的
	fs = FileSystem.get(new URI(HDFS_URL), config, HDFS_USER);
3) 操作方法	
	//上传文件
	fs.copyFromLocalFile(new Path("D:\\hadoop-test-file\\aaa.data"), new Path("/1806A/1806A.txt"));
	//删除
	.delete(new Path("/aaa"), true);
	//创建文件夹
	.mkdirs(new Path("/111/2222/aaa"));
	//重命名
	.rename(new Path("/123"), new Path("/321"));
	//下载
	.copyToLocalFile(new Path("/1806A/1806A.txt"), new Path("d:/"));
	//遍历文件
	RemoteIterator&lt;LocatedFileStatus&gt; listFiles = HdfsClient.getHadoopFileSystem()
                .listFiles(new Path("/"), true);
	while (listFiles.hasNext()){
		LocatedFileStatus file = listFiles.next();
		System.out.println(file.getPath().getName());
	}
	//遍历文件夹和文件
	FileStatus[] fileArr = HdfsClient.getHadoopFileSystem()
                .listStatus(new Path("/wc/output"));
	for(FileStatus file : fileArr){
		System.out.println(file.getPath().getName() + "--" + (file.isDirectory() ? "dir" : "file"));
	}
	
	//io流方式上传文件
	SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMdd");
    String dateStr = sdf.format(new Date()).toString();
    String filename = System.currentTimeMillis() + ".txt";
    FSDataOutputStream fos = null;
    FileInputStream fis = null;
    try {
        // 文件保存在服务器上地址
        fos = HdfsClient.getHdfsFileSystem()
                .create(new Path("/jk/" + dateStr + "/" + filename));

        // 用户上传的文件
        fis = new FileInputStream(new File("D:\\hadoop-test-file\\1810B.txt"));
        IOUtils.copyBytes(fis, fos, 4096, false);
    } catch (IOException e) {
        e.printStackTrace();
    } finally {
        IOUtils.closeStream(fis);
        IOUtils.closeStream(fos);

    }
	
	//io流方式从hdfs上下载文件
	FSDataInputStream inputStream = null;
    try {
        inputStream = HdfsClient.getHdfsFileSystem()
                .open(new Path("/jk/1810b/1810B.txt"));
        IOUtils.copyBytes(inputStream, System.out, 4096, false);
    } catch (IOException e) {
        e.printStackTrace();
    } finally {
        IOUtils.closeStream(inputStream);
    }
</code></pre> 
  <p>28、javaAPI自定义MapReduce程序</p> 
  <pre><code>1) 新建Map测试类继承Mapper&lt;LongWritable, Text, Text, LongWritable&gt;

2) 重写map方法
	// 获取文件内容
    String content = value.toString();
    // 将内容按空格分隔存入map集合当中
    String[] contentArr = StringUtils.split(content, ' ');
    for (String word : contentArr) {
        // 将map集合写入reduce中
        context.write(new Text(word), new LongWritable(1));
    }

3) 新建Reduce类继承Reducer&lt;Text, LongWritable, Text, LongWritable&gt;

4) 重写reduce方法
	// 遍历map的value
    Iterator&lt;LongWritable&gt; iterator = values.iterator();

    // 定义计数器
    Long count = new Long(0);

    // 循环value值
    while (iterator.hasNext()) {
        // 获取value值
        LongWritable longWritable = iterator.next();
        // 相同的key进行累加操作
        count += longWritable.get();
    }
    context.write(key, new LongWritable(count));

5) 编写启动类
	Configuration config = new Configuration();
    Job job = Job.getInstance(config);
    // 设置启动类
    job.setJarByClass(MapReduceRunner.class);
    // 设置Map类
    job.setMapperClass(MapTest.class);
    // 设置Reduce类
    job.setReducerClass(ReduceTest.class);
    // 设置输出的key也就是统计的关键词
    job.setOutputKeyClass(Text.class);
    // 设置输出的value也就是统计关键词的数量
    job.setOutputValueClass(LongWritable.class);
    // 设置MapReduce计算的文件
    FileInputFormat.setInputPaths(job, "/jk/1810b/1810B.txt");
    // 设置MapReduce计算完后的文件输出文件
    // 注意：输出的目录不要存在
    FileOutputFormat.setOutputPath(job, new Path("/jk/output"));
    job.waitForCompletion(true);

6) 将启动类打jar包，上传到服务器
	执行命令：hadoop jar jar包名 包路径.启动类名
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
