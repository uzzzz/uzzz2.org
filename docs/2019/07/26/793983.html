<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>SparkStreaming | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="SparkStreaming" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="SparkStreaming 一、SparkStreaming简介 ​ SparkStreaming是流式处理框架，是Spark API的扩展，支持可扩展、高吞吐量、容错的实时数据流处理，实时数据的来源可以是：Kafka, Flume, Twitter, ZeroMQ或者TCP sockets，并且可以使用高级功能的复杂算子来处理流数据。例如：map,reduce,join,window 。最终，处理后的数据可以存放在文件系统，数据库等，方便实时展现 ​ SparkStreaming底层操作DStream 二、SparkStreaming与Storm区别 1 Storm是纯实时的流式处理框架，SparkStreaming是准实时的处理框架（微批处理） ​ 因为微批处理， SparkStreaming的吞吐量比Storm要高 2 Storm 的事务机制要比SparkStreaming的要完善，SparkStreaming可以手动管理事务 3 Storm支持动态资源调度，SparkStreaming也支持资源调度，但是一般情况下不开启，为了避免内存给了却要不回来的情况 4 Storm不擅长复杂的业务处理，擅长简单的汇总型计算，SparkStreaming擅长复杂的业务处理，因为在SparkStreaming中可以使用core/sql/机器算法 三、SparkStreaming 1 接收数据原理图 ​ SparkStreaming是7 *24h不间断运行, SparkStreaming启动之后,首先会启动一个job ,这个job有一个task来接收数据, task每隔一-段时间将接收来的数据封装到一-个batch中，“这段时间” 就是batchInterval。 生成的每个batch又被封装到一一个 RDD中,这个RDD又被封装到一一个DStream中, SparkStreaming底层操作的就是DStream。 DStream有 自己的Transformation类算子，懒执行，需要DStream的outputOperator类算子触发执行 2 注意事项 Ø 启动socket server 服务器：nc –lk 9999 在集群使用yum install -y nc安装 Ø receiver模式下接收数据，local的模拟线程必须大于等于2，一个线程接收数据，另一个线程执行job Ø Durations时间设置就是我们能接收的延迟度。这个需要根据集群的资源情况以及任务的执行情况来调节。 ​ localhost:4040 查看Active Jobs栏数量看是否堆积 Ø 创建JavaStreamingContext有两种方式（SparkConf,SparkContext） Ø 所有的代码逻辑完成后要有一个output operation类算子 Ø JavaStreamingContext.start() Streaming框架启动后不能再次添加业务逻辑 Ø JavaStreamingContext.stop() 无参的stop方法将SparkContext一同关闭，stop(false)，不会关闭SparkContext Ø JavaStreamingContext.stop()停止之后不能再调用start 3 算子 foreachRDD–针对当前批次的数据 注意 Ø 1 需要Action算子触发 Ø 2 在于foreachRDD内部，RDD转换算子外部之间的信息 (Driver端)每次都执行 在转换算子内部，相当于(–Excutor端–)，有数据才执行 SparkStreaming虽不能在运行过程中更改数据信息，但是更改外部文件，因此可以实现动态改变–动态广播变量 package com.cs.scala.sparkString import org.apache.spark.rdd.RDD import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} /** * foreachRDD 统计当前这一批次的数据 * * 两种创建Streaming方式 * val sc = new SparkContext(conf) * val ssc = new StreamingContext(sc,Durations.seconds(5)) * val ssc = new StreamingContext(conf,Durations.seconds(5)) */ object ForeachRDD { def main(args: Array[String]): Unit = { val conf = new SparkConf() //注意设置Master的local[]数量，因为在整个过程中，需要两个task(1接收1执行计算)，local默认是 1 conf.setAppName(&quot;foreachRDD&quot;).setMaster(&quot;local[2]&quot;) // val sc = new SparkContext(conf) val ssc = new StreamingContext(conf,Durations.seconds(5)) //监控集群端口的工具--socketTextStream val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;sxt002&quot;,port = 9999) val result: DStream[(String, Int)] = lines.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_) // result.print(100) result.foreachRDD(pairRDD=&gt;{ val newRDD: RDD[(String, Int)] = pairRDD.filter(one =&gt; { println(&quot;==========Driver=========&quot;) true }) val resultRDD: RDD[Unit] = newRDD.map(one =&gt; { println(&quot;-----Excutor-----&quot;) }) resultRDD.count() }) //框架启动之后是不能在次添加业务逻辑 // 停止之后是不能在调用start ssc.start() //表示不间断执行 7*24h ssc.awaitTermination() //可以在以后找到数据后停止SparkStreaming，若stop(ture)则一同关闭sc ssc.stop() } } updateStateByKey–针对所有批次数据 1、为Spark Streaming中每一个Key维护一份state状态，state类型可以是任意类型的， 可以是一个自定义的对象，那么更新函数也可以是自定义的 2、通过更新函数对该key的状态不断更新，对于每个新的batch而言，Spark Streaming会在使用updateStateByKey的时候为已经存在的key进行state的状态更新 3、根据key更新状态，需要设置 checkpoint来保存状态 4、默认key的状态在内存中 有一份，在checkpoint目录中有一份 5、batchInterval负责将内存文件写在磁盘上 10s为界 def main(args: Array[String]): Unit = { val conf = new SparkConf() conf.setMaster(&quot;local[2]&quot;) conf.setAppName(&quot;UpdateStateByKey&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) //设置日志级别 ssc.sparkContext.setLogLevel(&quot;ERROR&quot;) val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;c7node5&quot;,9999) val words: DStream[String] = lines.flatMap(line=&gt;{line.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word =&gt; { (word, 1) }) ssc.checkpoint(&quot;./Test_spark/data/streamingCheckpoint&quot;) /** * currentValues :当前批次某个 key 对应所有的value 组成的一个集合 * preValue : 以往批次当前key 对应的总状态值 */ val result: DStream[(String, Int)] = pairWords.updateStateByKey((currentValues: Seq[Int], preValue: Option[Int]) =&gt; { var totalValues = 0 if (!preValue.isEmpty) { totalValues += preValue.get } for(value &lt;- currentValues){ totalValues += value } Option(totalValues) }) result.print() ssc.start() ssc.awaitTermination() ssc.stop() 窗口操作–针对某一时间段批次数据 Ø 假设每隔5s 1个batch,上图中窗口长度为15s，窗口滑动间隔10s。 Ø 窗口长度和滑动间隔必须是batchInterval的整数倍。如果不是整数倍会检测报错。 Ø reduceByKeyAndWindow 窗口操作 checkpoint 保存数据 Ø 窗口操作优化的机制 实时数 = 总-旧+新 val conf = new SparkConf() conf.setAppName(&quot;windowOperator&quot;) conf.setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) ssc.sparkContext.setLogLevel(&quot;Error&quot;) val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;sxt002&quot;,9999) val words: DStream[String] = lines.flatMap(line=&gt;{line.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)}) /** * 窗口操作普通的机制 */ // val windowResult: DStream[(String, Int)] = // pairWords.reduceByKeyAndWindow((v1:Int, v2:Int)=&gt;{v1+v2},Durations.seconds(15),Durations.seconds(5)) /** * 窗口操作优化的机制 * 因为要保存状态所以要设置checkpoint路径 * 实时数 = 原总数-旧+新 */ ssc.checkpoint(&quot;./Test_spark/data/streamingCheckpoint&quot;) val windowResult: DStream[(String, Int)] = pairWords.reduceByKeyAndWindow( (v1:Int, v2:Int)=&gt;{v1+v2}, (v1:Int, v2:Int)=&gt;{v1-v2}, Durations.seconds(15), Durations.seconds(5)) windowResult.print() ssc.start() ssc.awaitTermination() ssc.stop() transform Ø transform可以拿到DStream的RDD，经过RDD的算子后，要返回RDD，返回的RDD又被封装到一个DStream Ø 过滤黑名单，如果输入的含有xxx，就过滤掉不取 Ø 作用和foreachRDD类似，–Driver端，Excutor端 --动态广播变量 val conf = new SparkConf() conf.setAppName(&quot;transform&quot;) conf.setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) // ssc.sparkContext.setLogLevel(&quot;Error&quot;) /** * 广播黑名单 */ val blackList: Broadcast[List[String]] = ssc.sparkContext.broadcast(List[String](&quot;zhangsan&quot;,&quot;lisi&quot;)) /** * 从实时数据【&quot;hello zhangsan&quot;,&quot;hello lisi&quot;】中发现 数据的第二位是黑名单人员，过滤掉 */ val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;c7node5&quot;,9999) val pairLines: DStream[(String, String)] = lines.map(line=&gt;{(line.split(&quot; &quot;)(1),line)}) val resultDStream: DStream[String] = pairLines.transform(pairRDD =&gt; { println(&quot;=======Driver=======&quot;) val filterRDD: RDD[(String, String)] = pairRDD.filter(tp =&gt; { val nameList: List[String] = blackList.value !nameList.contains(tp._1) }) val returnRDD: RDD[String] = filterRDD.map(tp =&gt; tp._2) returnRDD }) resultDStream.print() ssc.start() ssc.awaitTermination() ssc.stop() saveAsTextFiles(prefix, [suffix])–将DStream内容保存为文本文件 Ø SparkStreaming监控一个目录数据时，无法监控目录下已经存在的文件 ，可以监控增加的文件 Ø 增加的文件必须是原子性产生(文件中不能写莫名东西) Ø 命名格式是 refix-TIME_IN_MS[.suffix] result.saveAsTextFiles(&quot;./data/streamingSavePath/prefix&quot;,&quot;suffix&quot;) val conf = new SparkConf() conf.setMaster(&quot;local&quot;) conf.setAppName(&quot;saveAsTextFile&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) val lines: DStream[String] = ssc.textFileStream(&quot;./Test_spark/data/streamingCopyFile&quot;) val words: DStream[String] = lines.flatMap(line=&gt;{line.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)}) val result: DStream[(String, Int)] = pairWords.reduceByKey((v1:Int, v2:Int)=&gt;{v1+v2}) //保存的多级目录就直接写在前缀中 result.saveAsTextFiles(&quot;./Test_spark/data/streamingSavePath/prefix&quot;,&quot;suffix&quot;) ssc.start() ssc.awaitTermination() ssc.stop() Driver-HA 两方面设置： ​ 1 在提交application时，添加–supervice 如果挂点会自动重启Driver ​ 2 代码层面 StreamingContext.getOrCreate(原逻辑，新逻辑) 需要注意的是，Driver重启之后用的仍然是旧逻辑，新逻辑不会被用，除非清空checkpoint所有的目录信息 默认checkpoint 存储： 1.配置信息 2.DStream操作逻辑 3.job的执行进度 4.offset //设置checkpoint目录 val ckDir = &quot;./Test_spark/data/streamingCheckpoint&quot; def main(args: Array[String]): Unit = { val ssc: StreamingContext = StreamingContext.getOrCreate(ckDir,CreateStreamingContext) ssc.start() ssc.awaitTermination() ssc.stop() } def CreateStreamingContext() = { println(&quot;=======Create new StreamingContext =======&quot;) val conf = new SparkConf() conf.setMaster(&quot;local&quot;) conf.setAppName(&quot;DriverHA&quot;) val ssc: StreamingContext = new StreamingContext(conf,Durations.seconds(5)) ssc.sparkContext.setLogLevel(&quot;Error&quot;) ssc.checkpoint(ckDir) val lines: DStream[String] = ssc.textFileStream(&quot;./Test_spark/data/streamingCopyFile&quot;) val words: DStream[String] = lines.flatMap(line=&gt;{line.trim.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)}) val result: DStream[(String, Int)] = pairWords.reduceByKey((v1:Int, v2:Int)=&gt;{v1+v2}) result.print() //更改逻辑 // result.foreachRDD(pairRDD=&gt;{ // pairRDD.filter(one=&gt;{ // println(&quot;*********** filter *********&quot;) // true // }) // pairRDD.foreach(println) // }) ssc" />
<meta property="og:description" content="SparkStreaming 一、SparkStreaming简介 ​ SparkStreaming是流式处理框架，是Spark API的扩展，支持可扩展、高吞吐量、容错的实时数据流处理，实时数据的来源可以是：Kafka, Flume, Twitter, ZeroMQ或者TCP sockets，并且可以使用高级功能的复杂算子来处理流数据。例如：map,reduce,join,window 。最终，处理后的数据可以存放在文件系统，数据库等，方便实时展现 ​ SparkStreaming底层操作DStream 二、SparkStreaming与Storm区别 1 Storm是纯实时的流式处理框架，SparkStreaming是准实时的处理框架（微批处理） ​ 因为微批处理， SparkStreaming的吞吐量比Storm要高 2 Storm 的事务机制要比SparkStreaming的要完善，SparkStreaming可以手动管理事务 3 Storm支持动态资源调度，SparkStreaming也支持资源调度，但是一般情况下不开启，为了避免内存给了却要不回来的情况 4 Storm不擅长复杂的业务处理，擅长简单的汇总型计算，SparkStreaming擅长复杂的业务处理，因为在SparkStreaming中可以使用core/sql/机器算法 三、SparkStreaming 1 接收数据原理图 ​ SparkStreaming是7 *24h不间断运行, SparkStreaming启动之后,首先会启动一个job ,这个job有一个task来接收数据, task每隔一-段时间将接收来的数据封装到一-个batch中，“这段时间” 就是batchInterval。 生成的每个batch又被封装到一一个 RDD中,这个RDD又被封装到一一个DStream中, SparkStreaming底层操作的就是DStream。 DStream有 自己的Transformation类算子，懒执行，需要DStream的outputOperator类算子触发执行 2 注意事项 Ø 启动socket server 服务器：nc –lk 9999 在集群使用yum install -y nc安装 Ø receiver模式下接收数据，local的模拟线程必须大于等于2，一个线程接收数据，另一个线程执行job Ø Durations时间设置就是我们能接收的延迟度。这个需要根据集群的资源情况以及任务的执行情况来调节。 ​ localhost:4040 查看Active Jobs栏数量看是否堆积 Ø 创建JavaStreamingContext有两种方式（SparkConf,SparkContext） Ø 所有的代码逻辑完成后要有一个output operation类算子 Ø JavaStreamingContext.start() Streaming框架启动后不能再次添加业务逻辑 Ø JavaStreamingContext.stop() 无参的stop方法将SparkContext一同关闭，stop(false)，不会关闭SparkContext Ø JavaStreamingContext.stop()停止之后不能再调用start 3 算子 foreachRDD–针对当前批次的数据 注意 Ø 1 需要Action算子触发 Ø 2 在于foreachRDD内部，RDD转换算子外部之间的信息 (Driver端)每次都执行 在转换算子内部，相当于(–Excutor端–)，有数据才执行 SparkStreaming虽不能在运行过程中更改数据信息，但是更改外部文件，因此可以实现动态改变–动态广播变量 package com.cs.scala.sparkString import org.apache.spark.rdd.RDD import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} /** * foreachRDD 统计当前这一批次的数据 * * 两种创建Streaming方式 * val sc = new SparkContext(conf) * val ssc = new StreamingContext(sc,Durations.seconds(5)) * val ssc = new StreamingContext(conf,Durations.seconds(5)) */ object ForeachRDD { def main(args: Array[String]): Unit = { val conf = new SparkConf() //注意设置Master的local[]数量，因为在整个过程中，需要两个task(1接收1执行计算)，local默认是 1 conf.setAppName(&quot;foreachRDD&quot;).setMaster(&quot;local[2]&quot;) // val sc = new SparkContext(conf) val ssc = new StreamingContext(conf,Durations.seconds(5)) //监控集群端口的工具--socketTextStream val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;sxt002&quot;,port = 9999) val result: DStream[(String, Int)] = lines.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_) // result.print(100) result.foreachRDD(pairRDD=&gt;{ val newRDD: RDD[(String, Int)] = pairRDD.filter(one =&gt; { println(&quot;==========Driver=========&quot;) true }) val resultRDD: RDD[Unit] = newRDD.map(one =&gt; { println(&quot;-----Excutor-----&quot;) }) resultRDD.count() }) //框架启动之后是不能在次添加业务逻辑 // 停止之后是不能在调用start ssc.start() //表示不间断执行 7*24h ssc.awaitTermination() //可以在以后找到数据后停止SparkStreaming，若stop(ture)则一同关闭sc ssc.stop() } } updateStateByKey–针对所有批次数据 1、为Spark Streaming中每一个Key维护一份state状态，state类型可以是任意类型的， 可以是一个自定义的对象，那么更新函数也可以是自定义的 2、通过更新函数对该key的状态不断更新，对于每个新的batch而言，Spark Streaming会在使用updateStateByKey的时候为已经存在的key进行state的状态更新 3、根据key更新状态，需要设置 checkpoint来保存状态 4、默认key的状态在内存中 有一份，在checkpoint目录中有一份 5、batchInterval负责将内存文件写在磁盘上 10s为界 def main(args: Array[String]): Unit = { val conf = new SparkConf() conf.setMaster(&quot;local[2]&quot;) conf.setAppName(&quot;UpdateStateByKey&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) //设置日志级别 ssc.sparkContext.setLogLevel(&quot;ERROR&quot;) val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;c7node5&quot;,9999) val words: DStream[String] = lines.flatMap(line=&gt;{line.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word =&gt; { (word, 1) }) ssc.checkpoint(&quot;./Test_spark/data/streamingCheckpoint&quot;) /** * currentValues :当前批次某个 key 对应所有的value 组成的一个集合 * preValue : 以往批次当前key 对应的总状态值 */ val result: DStream[(String, Int)] = pairWords.updateStateByKey((currentValues: Seq[Int], preValue: Option[Int]) =&gt; { var totalValues = 0 if (!preValue.isEmpty) { totalValues += preValue.get } for(value &lt;- currentValues){ totalValues += value } Option(totalValues) }) result.print() ssc.start() ssc.awaitTermination() ssc.stop() 窗口操作–针对某一时间段批次数据 Ø 假设每隔5s 1个batch,上图中窗口长度为15s，窗口滑动间隔10s。 Ø 窗口长度和滑动间隔必须是batchInterval的整数倍。如果不是整数倍会检测报错。 Ø reduceByKeyAndWindow 窗口操作 checkpoint 保存数据 Ø 窗口操作优化的机制 实时数 = 总-旧+新 val conf = new SparkConf() conf.setAppName(&quot;windowOperator&quot;) conf.setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) ssc.sparkContext.setLogLevel(&quot;Error&quot;) val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;sxt002&quot;,9999) val words: DStream[String] = lines.flatMap(line=&gt;{line.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)}) /** * 窗口操作普通的机制 */ // val windowResult: DStream[(String, Int)] = // pairWords.reduceByKeyAndWindow((v1:Int, v2:Int)=&gt;{v1+v2},Durations.seconds(15),Durations.seconds(5)) /** * 窗口操作优化的机制 * 因为要保存状态所以要设置checkpoint路径 * 实时数 = 原总数-旧+新 */ ssc.checkpoint(&quot;./Test_spark/data/streamingCheckpoint&quot;) val windowResult: DStream[(String, Int)] = pairWords.reduceByKeyAndWindow( (v1:Int, v2:Int)=&gt;{v1+v2}, (v1:Int, v2:Int)=&gt;{v1-v2}, Durations.seconds(15), Durations.seconds(5)) windowResult.print() ssc.start() ssc.awaitTermination() ssc.stop() transform Ø transform可以拿到DStream的RDD，经过RDD的算子后，要返回RDD，返回的RDD又被封装到一个DStream Ø 过滤黑名单，如果输入的含有xxx，就过滤掉不取 Ø 作用和foreachRDD类似，–Driver端，Excutor端 --动态广播变量 val conf = new SparkConf() conf.setAppName(&quot;transform&quot;) conf.setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) // ssc.sparkContext.setLogLevel(&quot;Error&quot;) /** * 广播黑名单 */ val blackList: Broadcast[List[String]] = ssc.sparkContext.broadcast(List[String](&quot;zhangsan&quot;,&quot;lisi&quot;)) /** * 从实时数据【&quot;hello zhangsan&quot;,&quot;hello lisi&quot;】中发现 数据的第二位是黑名单人员，过滤掉 */ val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;c7node5&quot;,9999) val pairLines: DStream[(String, String)] = lines.map(line=&gt;{(line.split(&quot; &quot;)(1),line)}) val resultDStream: DStream[String] = pairLines.transform(pairRDD =&gt; { println(&quot;=======Driver=======&quot;) val filterRDD: RDD[(String, String)] = pairRDD.filter(tp =&gt; { val nameList: List[String] = blackList.value !nameList.contains(tp._1) }) val returnRDD: RDD[String] = filterRDD.map(tp =&gt; tp._2) returnRDD }) resultDStream.print() ssc.start() ssc.awaitTermination() ssc.stop() saveAsTextFiles(prefix, [suffix])–将DStream内容保存为文本文件 Ø SparkStreaming监控一个目录数据时，无法监控目录下已经存在的文件 ，可以监控增加的文件 Ø 增加的文件必须是原子性产生(文件中不能写莫名东西) Ø 命名格式是 refix-TIME_IN_MS[.suffix] result.saveAsTextFiles(&quot;./data/streamingSavePath/prefix&quot;,&quot;suffix&quot;) val conf = new SparkConf() conf.setMaster(&quot;local&quot;) conf.setAppName(&quot;saveAsTextFile&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) val lines: DStream[String] = ssc.textFileStream(&quot;./Test_spark/data/streamingCopyFile&quot;) val words: DStream[String] = lines.flatMap(line=&gt;{line.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)}) val result: DStream[(String, Int)] = pairWords.reduceByKey((v1:Int, v2:Int)=&gt;{v1+v2}) //保存的多级目录就直接写在前缀中 result.saveAsTextFiles(&quot;./Test_spark/data/streamingSavePath/prefix&quot;,&quot;suffix&quot;) ssc.start() ssc.awaitTermination() ssc.stop() Driver-HA 两方面设置： ​ 1 在提交application时，添加–supervice 如果挂点会自动重启Driver ​ 2 代码层面 StreamingContext.getOrCreate(原逻辑，新逻辑) 需要注意的是，Driver重启之后用的仍然是旧逻辑，新逻辑不会被用，除非清空checkpoint所有的目录信息 默认checkpoint 存储： 1.配置信息 2.DStream操作逻辑 3.job的执行进度 4.offset //设置checkpoint目录 val ckDir = &quot;./Test_spark/data/streamingCheckpoint&quot; def main(args: Array[String]): Unit = { val ssc: StreamingContext = StreamingContext.getOrCreate(ckDir,CreateStreamingContext) ssc.start() ssc.awaitTermination() ssc.stop() } def CreateStreamingContext() = { println(&quot;=======Create new StreamingContext =======&quot;) val conf = new SparkConf() conf.setMaster(&quot;local&quot;) conf.setAppName(&quot;DriverHA&quot;) val ssc: StreamingContext = new StreamingContext(conf,Durations.seconds(5)) ssc.sparkContext.setLogLevel(&quot;Error&quot;) ssc.checkpoint(ckDir) val lines: DStream[String] = ssc.textFileStream(&quot;./Test_spark/data/streamingCopyFile&quot;) val words: DStream[String] = lines.flatMap(line=&gt;{line.trim.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)}) val result: DStream[(String, Int)] = pairWords.reduceByKey((v1:Int, v2:Int)=&gt;{v1+v2}) result.print() //更改逻辑 // result.foreachRDD(pairRDD=&gt;{ // pairRDD.filter(one=&gt;{ // println(&quot;*********** filter *********&quot;) // true // }) // pairRDD.foreach(println) // }) ssc" />
<link rel="canonical" href="https://uzzz.org/2019/07/26/793983.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/26/793983.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-26T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"SparkStreaming 一、SparkStreaming简介 ​ SparkStreaming是流式处理框架，是Spark API的扩展，支持可扩展、高吞吐量、容错的实时数据流处理，实时数据的来源可以是：Kafka, Flume, Twitter, ZeroMQ或者TCP sockets，并且可以使用高级功能的复杂算子来处理流数据。例如：map,reduce,join,window 。最终，处理后的数据可以存放在文件系统，数据库等，方便实时展现 ​ SparkStreaming底层操作DStream 二、SparkStreaming与Storm区别 1 Storm是纯实时的流式处理框架，SparkStreaming是准实时的处理框架（微批处理） ​ 因为微批处理， SparkStreaming的吞吐量比Storm要高 2 Storm 的事务机制要比SparkStreaming的要完善，SparkStreaming可以手动管理事务 3 Storm支持动态资源调度，SparkStreaming也支持资源调度，但是一般情况下不开启，为了避免内存给了却要不回来的情况 4 Storm不擅长复杂的业务处理，擅长简单的汇总型计算，SparkStreaming擅长复杂的业务处理，因为在SparkStreaming中可以使用core/sql/机器算法 三、SparkStreaming 1 接收数据原理图 ​ SparkStreaming是7 *24h不间断运行, SparkStreaming启动之后,首先会启动一个job ,这个job有一个task来接收数据, task每隔一-段时间将接收来的数据封装到一-个batch中，“这段时间” 就是batchInterval。 生成的每个batch又被封装到一一个 RDD中,这个RDD又被封装到一一个DStream中, SparkStreaming底层操作的就是DStream。 DStream有 自己的Transformation类算子，懒执行，需要DStream的outputOperator类算子触发执行 2 注意事项 Ø 启动socket server 服务器：nc –lk 9999 在集群使用yum install -y nc安装 Ø receiver模式下接收数据，local的模拟线程必须大于等于2，一个线程接收数据，另一个线程执行job Ø Durations时间设置就是我们能接收的延迟度。这个需要根据集群的资源情况以及任务的执行情况来调节。 ​ localhost:4040 查看Active Jobs栏数量看是否堆积 Ø 创建JavaStreamingContext有两种方式（SparkConf,SparkContext） Ø 所有的代码逻辑完成后要有一个output operation类算子 Ø JavaStreamingContext.start() Streaming框架启动后不能再次添加业务逻辑 Ø JavaStreamingContext.stop() 无参的stop方法将SparkContext一同关闭，stop(false)，不会关闭SparkContext Ø JavaStreamingContext.stop()停止之后不能再调用start 3 算子 foreachRDD–针对当前批次的数据 注意 Ø 1 需要Action算子触发 Ø 2 在于foreachRDD内部，RDD转换算子外部之间的信息 (Driver端)每次都执行 在转换算子内部，相当于(–Excutor端–)，有数据才执行 SparkStreaming虽不能在运行过程中更改数据信息，但是更改外部文件，因此可以实现动态改变–动态广播变量 package com.cs.scala.sparkString import org.apache.spark.rdd.RDD import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} /** * foreachRDD 统计当前这一批次的数据 * * 两种创建Streaming方式 * val sc = new SparkContext(conf) * val ssc = new StreamingContext(sc,Durations.seconds(5)) * val ssc = new StreamingContext(conf,Durations.seconds(5)) */ object ForeachRDD { def main(args: Array[String]): Unit = { val conf = new SparkConf() //注意设置Master的local[]数量，因为在整个过程中，需要两个task(1接收1执行计算)，local默认是 1 conf.setAppName(&quot;foreachRDD&quot;).setMaster(&quot;local[2]&quot;) // val sc = new SparkContext(conf) val ssc = new StreamingContext(conf,Durations.seconds(5)) //监控集群端口的工具--socketTextStream val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;sxt002&quot;,port = 9999) val result: DStream[(String, Int)] = lines.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_) // result.print(100) result.foreachRDD(pairRDD=&gt;{ val newRDD: RDD[(String, Int)] = pairRDD.filter(one =&gt; { println(&quot;==========Driver=========&quot;) true }) val resultRDD: RDD[Unit] = newRDD.map(one =&gt; { println(&quot;-----Excutor-----&quot;) }) resultRDD.count() }) //框架启动之后是不能在次添加业务逻辑 // 停止之后是不能在调用start ssc.start() //表示不间断执行 7*24h ssc.awaitTermination() //可以在以后找到数据后停止SparkStreaming，若stop(ture)则一同关闭sc ssc.stop() } } updateStateByKey–针对所有批次数据 1、为Spark Streaming中每一个Key维护一份state状态，state类型可以是任意类型的， 可以是一个自定义的对象，那么更新函数也可以是自定义的 2、通过更新函数对该key的状态不断更新，对于每个新的batch而言，Spark Streaming会在使用updateStateByKey的时候为已经存在的key进行state的状态更新 3、根据key更新状态，需要设置 checkpoint来保存状态 4、默认key的状态在内存中 有一份，在checkpoint目录中有一份 5、batchInterval负责将内存文件写在磁盘上 10s为界 def main(args: Array[String]): Unit = { val conf = new SparkConf() conf.setMaster(&quot;local[2]&quot;) conf.setAppName(&quot;UpdateStateByKey&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) //设置日志级别 ssc.sparkContext.setLogLevel(&quot;ERROR&quot;) val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;c7node5&quot;,9999) val words: DStream[String] = lines.flatMap(line=&gt;{line.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word =&gt; { (word, 1) }) ssc.checkpoint(&quot;./Test_spark/data/streamingCheckpoint&quot;) /** * currentValues :当前批次某个 key 对应所有的value 组成的一个集合 * preValue : 以往批次当前key 对应的总状态值 */ val result: DStream[(String, Int)] = pairWords.updateStateByKey((currentValues: Seq[Int], preValue: Option[Int]) =&gt; { var totalValues = 0 if (!preValue.isEmpty) { totalValues += preValue.get } for(value &lt;- currentValues){ totalValues += value } Option(totalValues) }) result.print() ssc.start() ssc.awaitTermination() ssc.stop() 窗口操作–针对某一时间段批次数据 Ø 假设每隔5s 1个batch,上图中窗口长度为15s，窗口滑动间隔10s。 Ø 窗口长度和滑动间隔必须是batchInterval的整数倍。如果不是整数倍会检测报错。 Ø reduceByKeyAndWindow 窗口操作 checkpoint 保存数据 Ø 窗口操作优化的机制 实时数 = 总-旧+新 val conf = new SparkConf() conf.setAppName(&quot;windowOperator&quot;) conf.setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) ssc.sparkContext.setLogLevel(&quot;Error&quot;) val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;sxt002&quot;,9999) val words: DStream[String] = lines.flatMap(line=&gt;{line.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)}) /** * 窗口操作普通的机制 */ // val windowResult: DStream[(String, Int)] = // pairWords.reduceByKeyAndWindow((v1:Int, v2:Int)=&gt;{v1+v2},Durations.seconds(15),Durations.seconds(5)) /** * 窗口操作优化的机制 * 因为要保存状态所以要设置checkpoint路径 * 实时数 = 原总数-旧+新 */ ssc.checkpoint(&quot;./Test_spark/data/streamingCheckpoint&quot;) val windowResult: DStream[(String, Int)] = pairWords.reduceByKeyAndWindow( (v1:Int, v2:Int)=&gt;{v1+v2}, (v1:Int, v2:Int)=&gt;{v1-v2}, Durations.seconds(15), Durations.seconds(5)) windowResult.print() ssc.start() ssc.awaitTermination() ssc.stop() transform Ø transform可以拿到DStream的RDD，经过RDD的算子后，要返回RDD，返回的RDD又被封装到一个DStream Ø 过滤黑名单，如果输入的含有xxx，就过滤掉不取 Ø 作用和foreachRDD类似，–Driver端，Excutor端 --动态广播变量 val conf = new SparkConf() conf.setAppName(&quot;transform&quot;) conf.setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) // ssc.sparkContext.setLogLevel(&quot;Error&quot;) /** * 广播黑名单 */ val blackList: Broadcast[List[String]] = ssc.sparkContext.broadcast(List[String](&quot;zhangsan&quot;,&quot;lisi&quot;)) /** * 从实时数据【&quot;hello zhangsan&quot;,&quot;hello lisi&quot;】中发现 数据的第二位是黑名单人员，过滤掉 */ val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;c7node5&quot;,9999) val pairLines: DStream[(String, String)] = lines.map(line=&gt;{(line.split(&quot; &quot;)(1),line)}) val resultDStream: DStream[String] = pairLines.transform(pairRDD =&gt; { println(&quot;=======Driver=======&quot;) val filterRDD: RDD[(String, String)] = pairRDD.filter(tp =&gt; { val nameList: List[String] = blackList.value !nameList.contains(tp._1) }) val returnRDD: RDD[String] = filterRDD.map(tp =&gt; tp._2) returnRDD }) resultDStream.print() ssc.start() ssc.awaitTermination() ssc.stop() saveAsTextFiles(prefix, [suffix])–将DStream内容保存为文本文件 Ø SparkStreaming监控一个目录数据时，无法监控目录下已经存在的文件 ，可以监控增加的文件 Ø 增加的文件必须是原子性产生(文件中不能写莫名东西) Ø 命名格式是 refix-TIME_IN_MS[.suffix] result.saveAsTextFiles(&quot;./data/streamingSavePath/prefix&quot;,&quot;suffix&quot;) val conf = new SparkConf() conf.setMaster(&quot;local&quot;) conf.setAppName(&quot;saveAsTextFile&quot;) val ssc = new StreamingContext(conf,Durations.seconds(5)) val lines: DStream[String] = ssc.textFileStream(&quot;./Test_spark/data/streamingCopyFile&quot;) val words: DStream[String] = lines.flatMap(line=&gt;{line.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)}) val result: DStream[(String, Int)] = pairWords.reduceByKey((v1:Int, v2:Int)=&gt;{v1+v2}) //保存的多级目录就直接写在前缀中 result.saveAsTextFiles(&quot;./Test_spark/data/streamingSavePath/prefix&quot;,&quot;suffix&quot;) ssc.start() ssc.awaitTermination() ssc.stop() Driver-HA 两方面设置： ​ 1 在提交application时，添加–supervice 如果挂点会自动重启Driver ​ 2 代码层面 StreamingContext.getOrCreate(原逻辑，新逻辑) 需要注意的是，Driver重启之后用的仍然是旧逻辑，新逻辑不会被用，除非清空checkpoint所有的目录信息 默认checkpoint 存储： 1.配置信息 2.DStream操作逻辑 3.job的执行进度 4.offset //设置checkpoint目录 val ckDir = &quot;./Test_spark/data/streamingCheckpoint&quot; def main(args: Array[String]): Unit = { val ssc: StreamingContext = StreamingContext.getOrCreate(ckDir,CreateStreamingContext) ssc.start() ssc.awaitTermination() ssc.stop() } def CreateStreamingContext() = { println(&quot;=======Create new StreamingContext =======&quot;) val conf = new SparkConf() conf.setMaster(&quot;local&quot;) conf.setAppName(&quot;DriverHA&quot;) val ssc: StreamingContext = new StreamingContext(conf,Durations.seconds(5)) ssc.sparkContext.setLogLevel(&quot;Error&quot;) ssc.checkpoint(ckDir) val lines: DStream[String] = ssc.textFileStream(&quot;./Test_spark/data/streamingCopyFile&quot;) val words: DStream[String] = lines.flatMap(line=&gt;{line.trim.split(&quot; &quot;)}) val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)}) val result: DStream[(String, Int)] = pairWords.reduceByKey((v1:Int, v2:Int)=&gt;{v1+v2}) result.print() //更改逻辑 // result.foreachRDD(pairRDD=&gt;{ // pairRDD.filter(one=&gt;{ // println(&quot;*********** filter *********&quot;) // true // }) // pairRDD.foreach(println) // }) ssc","@type":"BlogPosting","url":"https://uzzz.org/2019/07/26/793983.html","headline":"SparkStreaming","dateModified":"2019-07-26T00:00:00+08:00","datePublished":"2019-07-26T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/26/793983.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>SparkStreaming</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h1><a id="SparkStreaming_0"></a>SparkStreaming</h1> 
  <h2><a id="SparkStreaming_2"></a>一、SparkStreaming简介</h2> 
  <p>​ SparkStreaming是流式处理框架，是Spark API的扩展，支持可扩展、高吞吐量、容错的实时数据流处理，实时数据的来源可以是：Kafka, Flume, Twitter, ZeroMQ或者TCP sockets，并且可以使用高级功能的复杂算子来处理流数据。例如：map,reduce,join,window 。最终，处理后的数据可以存放在文件系统，数据库等，方便实时展现</p> 
  <p>​ SparkStreaming底层操作DStream</p> 
  <h2><a id="SparkStreamingStorm_8"></a>二、SparkStreaming与Storm区别</h2> 
  <p>1 Storm是纯实时的流式处理框架，SparkStreaming是准实时的处理框架（微批处理）</p> 
  <p>​ 因为微批处理， SparkStreaming的吞吐量比Storm要高</p> 
  <p>2 Storm 的事务机制要比SparkStreaming的要完善，SparkStreaming可以手动管理事务</p> 
  <p>3 Storm支持动态资源调度，SparkStreaming也支持资源调度，但是一般情况下不开启，为了避免内存给了却要不回来的情况</p> 
  <p>4 Storm不擅长复杂的业务处理，擅长简单的汇总型计算，SparkStreaming擅长复杂的业务处理，因为在SparkStreaming中可以使用core/sql/机器算法</p> 
  <h2><a id="SparkStreaming_20"></a>三、SparkStreaming</h2> 
  <h3><a id="1__22"></a>1 接收数据原理图</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726210311881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTI5OTIx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>​ SparkStreaming是7 *24h不间断运行, SparkStreaming启动之后,首先会启动一个job ,这个job有一个task来接收数据, task每隔一-段时间将接收来的数据封装到一-个batch中，“这段时间” 就是batchInterval。 生成的每个batch又被封装到一一个 RDD中,这个RDD又被封装到一一个DStream中, SparkStreaming底层操作的就是DStream。 DStream有 自己的Transformation类算子，懒执行，需要DStream的outputOperator类算子触发执行</p> 
  <h3><a id="2__28"></a>2 注意事项</h3> 
  <p>Ø 启动socket server 服务器：nc –lk 9999 在集群使用yum install -y nc安装</p> 
  <p>Ø receiver模式下接收数据，local的模拟线程必须大于等于2，一个线程接收数据，另一个线程执行job</p> 
  <p>Ø Durations时间设置就是我们能接收的延迟度。这个需要根据集群的资源情况以及任务的执行情况来调节。</p> 
  <p>​ localhost:4040 查看Active Jobs栏数量看是否堆积</p> 
  <p>Ø 创建JavaStreamingContext有两种方式（SparkConf,SparkContext）</p> 
  <p>Ø 所有的代码逻辑完成后要有一个output operation类算子</p> 
  <p>Ø JavaStreamingContext.start() Streaming框架启动后不能再次添加业务逻辑</p> 
  <p>Ø JavaStreamingContext.stop() 无参的stop方法将SparkContext一同关闭，stop(false)，不会关闭SparkContext</p> 
  <p>Ø JavaStreamingContext.stop()停止之后不能再调用start</p> 
  <h3><a id="3__50"></a>3 算子</h3> 
  <h4><a id="foreachRDD_52"></a>foreachRDD–针对当前批次的数据</h4> 
  <p>注意</p> 
  <p>Ø 1 需要Action算子触发</p> 
  <p>Ø 2 在于foreachRDD内部，RDD转换算子外部之间的信息 (<mark>Driver端</mark>)每次都执行</p> 
  <p>在转换算子内部，相当于(–Excutor端–)，有数据才执行</p> 
  <p>SparkStreaming虽不能在运行过程中更改数据信息，但是更改外部文件，因此可以实现动态改变–动态广播变量</p> 
  <pre><code>package com.cs.scala.sparkString

import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Durations, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * foreachRDD  统计当前这一批次的数据
  *
  * 两种创建Streaming方式
  *       val sc = new SparkContext(conf)
  *       val ssc = new StreamingContext(sc,Durations.seconds(5))
  *   val ssc = new StreamingContext(conf,Durations.seconds(5))
  */
object ForeachRDD {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    //注意设置Master的local[]数量，因为在整个过程中，需要两个task(1接收1执行计算)，local默认是 1
    conf.setAppName("foreachRDD").setMaster("local[2]")
//    val sc = new SparkContext(conf)
    val ssc = new StreamingContext(conf,Durations.seconds(5))

    //监控集群端口的工具--socketTextStream
    val lines: ReceiverInputDStream[String] = ssc.socketTextStream("sxt002",port = 9999)
    val result: DStream[(String, Int)] = lines.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)

//    result.print(100)
    result.foreachRDD(pairRDD=&gt;{
      val newRDD: RDD[(String, Int)] = pairRDD.filter(one =&gt; {
        println("==========Driver=========")
        true
      })
      val resultRDD: RDD[Unit] = newRDD.map(one =&gt; {
        println("-----Excutor-----")
      })
      resultRDD.count()

    })

    //框架启动之后是不能在次添加业务逻辑
    // 停止之后是不能在调用start
    ssc.start()
    //表示不间断执行 7*24h
    ssc.awaitTermination()
    //可以在以后找到数据后停止SparkStreaming，若stop(ture)则一同关闭sc
    ssc.stop()
  }
}
</code></pre> 
  <h4><a id="updateStateByKey_118"></a>updateStateByKey–针对所有批次数据</h4> 
  <p>1、为Spark Streaming中每一个Key维护一份state状态，state类型可以是任意类型的， 可以是一个自定义的对象，那么更新函数也可以是自定义的</p> 
  <p>2、通过更新函数对该key的状态不断更新，对于每个新的batch而言，Spark Streaming会在使用updateStateByKey的时候为已经存在的key进行state的状态更新</p> 
  <p>3、根据key更新状态，需要设置 checkpoint来保存状态</p> 
  <p>4、默认key的状态在内存中 有一份，在checkpoint目录中有一份</p> 
  <p>5、batchInterval负责将内存文件写在磁盘上 10s为界</p> 
  <pre><code>def main(args: Array[String]): Unit = {
  val conf = new SparkConf()
  conf.setMaster("local[2]")
  conf.setAppName("UpdateStateByKey")
  val ssc = new StreamingContext(conf,Durations.seconds(5))
  //设置日志级别
  ssc.sparkContext.setLogLevel("ERROR")
  val lines: ReceiverInputDStream[String] = ssc.socketTextStream("c7node5",9999)
  val words: DStream[String] = lines.flatMap(line=&gt;{line.split(" ")})
  val pairWords: DStream[(String, Int)] = words.map(word =&gt; {
    (word, 1)
  })

  ssc.checkpoint("./Test_spark/data/streamingCheckpoint")

  /**
    * currentValues :当前批次某个 key 对应所有的value 组成的一个集合
    * preValue : 以往批次当前key 对应的总状态值
    */
  val result: DStream[(String, Int)] = pairWords.updateStateByKey((currentValues: Seq[Int], preValue: Option[Int]) =&gt; {
    var totalValues = 0
    if (!preValue.isEmpty) {
      totalValues += preValue.get
    }
    for(value &lt;- currentValues){
      totalValues += value
    }

    Option(totalValues)
  })
  result.print()
  ssc.start()
  ssc.awaitTermination()
  ssc.stop()
</code></pre> 
  <h4><a id="_169"></a>窗口操作–针对某一时间段批次数据</h4> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726210337915.png" alt="在这里插入图片描述"></p> 
  <p>Ø 假设每隔5s 1个batch,上图中窗口长度为15s，窗口滑动间隔10s。</p> 
  <p>Ø 窗口长度和滑动间隔必须是batchInterval的整数倍。如果不是整数倍会检测报错。</p> 
  <p>Ø reduceByKeyAndWindow 窗口操作 checkpoint 保存数据</p> 
  <p>Ø 窗口操作优化的机制 实时数 = 总-旧+新</p> 
  <pre><code> val conf = new SparkConf()
    conf.setAppName("windowOperator")
    conf.setMaster("local[2]")
    val ssc = new StreamingContext(conf,Durations.seconds(5))
    ssc.sparkContext.setLogLevel("Error")
    val lines: ReceiverInputDStream[String] = ssc.socketTextStream("sxt002",9999)
    val words: DStream[String] = lines.flatMap(line=&gt;{line.split(" ")})
    val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)})

    /**
      * 窗口操作普通的机制
      */
//    val windowResult: DStream[(String, Int)] =
//      pairWords.reduceByKeyAndWindow((v1:Int, v2:Int)=&gt;{v1+v2},Durations.seconds(15),Durations.seconds(5))

    /**
      * 窗口操作优化的机制
      * 因为要保存状态所以要设置checkpoint路径
      * 实时数 = 原总数-旧+新
      */
    ssc.checkpoint("./Test_spark/data/streamingCheckpoint")
    val windowResult: DStream[(String, Int)] = pairWords.reduceByKeyAndWindow(
      (v1:Int, v2:Int)=&gt;{v1+v2},
      (v1:Int, v2:Int)=&gt;{v1-v2},
      Durations.seconds(15),
      Durations.seconds(5))

    windowResult.print()

    ssc.start()
    ssc.awaitTermination()
    ssc.stop()
</code></pre> 
  <h4><a id="transform_215"></a>transform</h4> 
  <p>Ø transform可以拿到DStream的RDD，经过RDD的算子后，要返回RDD，返回的RDD又被封装到一个DStream</p> 
  <p>Ø 过滤黑名单，如果输入的含有xxx，就过滤掉不取</p> 
  <p>Ø 作用和foreachRDD类似，–Driver端，Excutor端 --动态广播变量</p> 
  <pre><code>    val conf = new SparkConf()
    conf.setAppName("transform")
    conf.setMaster("local[2]")
    val ssc = new StreamingContext(conf,Durations.seconds(5))
//    ssc.sparkContext.setLogLevel("Error")
    /**
      * 广播黑名单
      */
    val blackList: Broadcast[List[String]] = ssc.sparkContext.broadcast(List[String]("zhangsan","lisi"))

    /**
      * 从实时数据【"hello zhangsan","hello lisi"】中发现 数据的第二位是黑名单人员，过滤掉
      */
    val lines: ReceiverInputDStream[String] = ssc.socketTextStream("c7node5",9999)
    val pairLines: DStream[(String, String)] = lines.map(line=&gt;{(line.split(" ")(1),line)})
    val resultDStream: DStream[String] = pairLines.transform(pairRDD =&gt; {
      println("=======Driver=======")
      val filterRDD: RDD[(String, String)] = pairRDD.filter(tp =&gt; {
        val nameList: List[String] = blackList.value
        !nameList.contains(tp._1)
      })
      val returnRDD: RDD[String] = filterRDD.map(tp =&gt; tp._2)
      returnRDD
    })

    resultDStream.print()

    ssc.start()
    ssc.awaitTermination()
    ssc.stop()
</code></pre> 
  <h4><a id="saveAsTextFilesprefix_suffixDStream_258"></a>saveAsTextFiles(prefix, [suffix])–将DStream内容保存为文本文件</h4> 
  <p>Ø SparkStreaming监控一个目录数据时，无法监控目录下已经存在的文件 ，可以监控增加的文件</p> 
  <p>Ø 增加的文件必须是原子性产生(文件中不能写莫名东西)</p> 
  <p>Ø 命名格式是 refix-TIME_IN_MS[.suffix]</p> 
  <pre><code>result.saveAsTextFiles("./data/streamingSavePath/prefix","suffix")
</code></pre> 
  <pre><code>val conf = new SparkConf()
conf.setMaster("local")
conf.setAppName("saveAsTextFile")
val ssc = new StreamingContext(conf,Durations.seconds(5))
val lines: DStream[String] = ssc.textFileStream("./Test_spark/data/streamingCopyFile")
val words: DStream[String] = lines.flatMap(line=&gt;{line.split(" ")})
val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)})
val result: DStream[(String, Int)] = pairWords.reduceByKey((v1:Int, v2:Int)=&gt;{v1+v2})
//保存的多级目录就直接写在前缀中
result.saveAsTextFiles("./Test_spark/data/streamingSavePath/prefix","suffix")
ssc.start()
ssc.awaitTermination()
ssc.stop()
</code></pre> 
  <h4><a id="DriverHA_288"></a>Driver-HA</h4> 
  <p>两方面设置：</p> 
  <p>​ 1 在提交application时，添加–supervice 如果挂点会自动重启Driver</p> 
  <p>​ 2 代码层面 StreamingContext.getOrCreate(原逻辑，新逻辑)</p> 
  <p>需要注意的是，Driver重启之后用的仍然是旧逻辑，新逻辑不会被用，除非清空checkpoint所有的目录信息</p> 
  <pre><code>默认checkpoint 存储：
    1.配置信息
    2.DStream操作逻辑
    3.job的执行进度
    4.offset
</code></pre> 
  <pre><code>  //设置checkpoint目录
  val ckDir = "./Test_spark/data/streamingCheckpoint"
  def main(args: Array[String]): Unit = {

    val ssc: StreamingContext = StreamingContext.getOrCreate(ckDir,CreateStreamingContext)
    ssc.start()
    ssc.awaitTermination()
    ssc.stop()

  }

  def CreateStreamingContext() = {
    println("=======Create new StreamingContext =======")
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("DriverHA")
    val ssc: StreamingContext = new StreamingContext(conf,Durations.seconds(5))
    ssc.sparkContext.setLogLevel("Error")

    ssc.checkpoint(ckDir)
    val lines: DStream[String] = ssc.textFileStream("./Test_spark/data/streamingCopyFile")
    val words: DStream[String] = lines.flatMap(line=&gt;{line.trim.split(" ")})
    val pairWords: DStream[(String, Int)] = words.map(word=&gt;{(word,1)})
    val result: DStream[(String, Int)] = pairWords.reduceByKey((v1:Int, v2:Int)=&gt;{v1+v2})

    result.print()

	//更改逻辑
//    result.foreachRDD(pairRDD=&gt;{
//      pairRDD.filter(one=&gt;{
//        println("*********** filter *********")
//        true
//      })
//      pairRDD.foreach(println)
//    })

    ssc
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
