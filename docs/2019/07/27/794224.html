<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>SparkStreaming入门总结 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="SparkStreaming入门总结" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Spark Streaming http://spark.apache.org/docs/2.2.1/streaming-programming-guide.html Spark Streaming 的checkPoint： 提供故障容错 To summarize, metadata checkpointing is primarily needed for recovery from driver failures, whereas data or RDD checkpointing is necessary even for basic functioning if stateful transformations are used. DStream的理解：一系列带有时间间隔的RDD 操作DStream的3个重要的原语 代码之前准备：集群安装 安装netcat yum -y install nc window operations 具体窗口大小和窗口间隔，比如需要需要展示的是上一次和这一次的10个数据 package zygDemo1 import org.apache.spark.{SparkConf, SparkContext} import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} /** * Created by HP on 2019/7/16 15:51 */ object WindowOperationsDemo { def main(args: Array[String]): Unit = { //初始化环境 val conf=new SparkConf().setAppName(&quot;WindowOperationsDemo&quot;).setMaster(&quot;local[*]&quot;) val sc=new SparkContext(conf) //实例化StreamingContext val strCon: StreamingContext = new StreamingContext(sc,Durations.milliseconds(1000)) //获取NetCat模拟发送的数据 val msg: ReceiverInputDStream[String] = strCon.socketTextStream(&quot;mini2&quot;,9999) //wordcount简单数据拼接 word word hello （world，1）（word，1）（hello，1） val tup: DStream[(String, Int)] = msg.flatMap(_.split(&quot; &quot;)).map((_,1)) //窗口聚合 参数（聚合func，窗口大小，滑动间隔）（窗口解析见图） val sumd: DStream[(String, Int)] = tup.reduceByKeyAndWindow((x:Int, y:Int)=&gt;(x+y),Durations.seconds(10),Durations.seconds(10)) sumd.print() //提交任务到集群 strCon.start() //线程等待，等待处理任务 strCon.awaitTermination() } } updateStateByKey 历史数据批次累加 package zygDemo1 import org.apache.spark.streaming.dstream.DStream import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.{HashPartitioner, SparkConf} /** * Created by HP on 2019/7/16 11:24 */ object UpdateStateByKeyDemo{ def main(args: Array[String]): Unit = { val conf=new SparkConf().setAppName(&quot;UpdateStateByKeyDemo&quot;).setMaster(&quot;local[2]&quot;) val strCon: StreamingContext = new StreamingContext(conf,Durations.milliseconds(2000))//conf和批处理时间间隔 //设置检查点 strCon.checkpoint(&quot;d://zyg-20190716&quot;) //输入流 val msg=strCon.socketTextStream(&quot;mini2&quot;,9999) //处理逻辑 val tup=msg.flatMap(_.split(&quot; &quot;)).map((_,1)) //历史数据和当前数据聚合，制定分区器，是否获取分区信息 val sumd: DStream[(String, Int)] = tup.updateStateByKey(func,new HashPartitioner(strCon.sparkContext.defaultParallelism),true) sumd.print() strCon.start() strCon.awaitTermination() } //参数（word，（1,1,1））（hello，（1,1）） //历史数据和当前数据的聚合逻辑（迭代器参数：key，（1,1），历史数据） val func=(it:Iterator[(String,Seq[Int],Option[Int])])=&gt;{ it.map(x=&gt;{ (x._1,x._2.sum+x._3.getOrElse(0)) }) } } transform 允许将任意RDD到RDD函数应用于DStream package zygDemo1 import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} /** * Created by HP on 2019/7/16 10:26 * nc -lk 9999 启动netcat(网络服务) */ object StreamingWC { def main(args: Array[String]): Unit = { val conf=new SparkConf().setAppName(&quot;StreamingWC&quot;).setMaster(&quot;local[*]&quot;) val sc=new SparkContext(conf) val strCon: StreamingContext = new StreamingContext(sc,Durations.milliseconds(1000)) //获取NetCat的数据 val msg: ReceiverInputDStream[String] = strCon.socketTextStream(&quot;mini2&quot;,9999) //数据处理逻辑 val sumd: DStream[(String, Int)] = msg.transform(rdd =&gt; { rdd.flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_ + _) }) sumd.print() //提交任务到集群 strCon.start() //线程等待，等待处理任务 strCon.awaitTermination() } }" />
<meta property="og:description" content="Spark Streaming http://spark.apache.org/docs/2.2.1/streaming-programming-guide.html Spark Streaming 的checkPoint： 提供故障容错 To summarize, metadata checkpointing is primarily needed for recovery from driver failures, whereas data or RDD checkpointing is necessary even for basic functioning if stateful transformations are used. DStream的理解：一系列带有时间间隔的RDD 操作DStream的3个重要的原语 代码之前准备：集群安装 安装netcat yum -y install nc window operations 具体窗口大小和窗口间隔，比如需要需要展示的是上一次和这一次的10个数据 package zygDemo1 import org.apache.spark.{SparkConf, SparkContext} import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} /** * Created by HP on 2019/7/16 15:51 */ object WindowOperationsDemo { def main(args: Array[String]): Unit = { //初始化环境 val conf=new SparkConf().setAppName(&quot;WindowOperationsDemo&quot;).setMaster(&quot;local[*]&quot;) val sc=new SparkContext(conf) //实例化StreamingContext val strCon: StreamingContext = new StreamingContext(sc,Durations.milliseconds(1000)) //获取NetCat模拟发送的数据 val msg: ReceiverInputDStream[String] = strCon.socketTextStream(&quot;mini2&quot;,9999) //wordcount简单数据拼接 word word hello （world，1）（word，1）（hello，1） val tup: DStream[(String, Int)] = msg.flatMap(_.split(&quot; &quot;)).map((_,1)) //窗口聚合 参数（聚合func，窗口大小，滑动间隔）（窗口解析见图） val sumd: DStream[(String, Int)] = tup.reduceByKeyAndWindow((x:Int, y:Int)=&gt;(x+y),Durations.seconds(10),Durations.seconds(10)) sumd.print() //提交任务到集群 strCon.start() //线程等待，等待处理任务 strCon.awaitTermination() } } updateStateByKey 历史数据批次累加 package zygDemo1 import org.apache.spark.streaming.dstream.DStream import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.{HashPartitioner, SparkConf} /** * Created by HP on 2019/7/16 11:24 */ object UpdateStateByKeyDemo{ def main(args: Array[String]): Unit = { val conf=new SparkConf().setAppName(&quot;UpdateStateByKeyDemo&quot;).setMaster(&quot;local[2]&quot;) val strCon: StreamingContext = new StreamingContext(conf,Durations.milliseconds(2000))//conf和批处理时间间隔 //设置检查点 strCon.checkpoint(&quot;d://zyg-20190716&quot;) //输入流 val msg=strCon.socketTextStream(&quot;mini2&quot;,9999) //处理逻辑 val tup=msg.flatMap(_.split(&quot; &quot;)).map((_,1)) //历史数据和当前数据聚合，制定分区器，是否获取分区信息 val sumd: DStream[(String, Int)] = tup.updateStateByKey(func,new HashPartitioner(strCon.sparkContext.defaultParallelism),true) sumd.print() strCon.start() strCon.awaitTermination() } //参数（word，（1,1,1））（hello，（1,1）） //历史数据和当前数据的聚合逻辑（迭代器参数：key，（1,1），历史数据） val func=(it:Iterator[(String,Seq[Int],Option[Int])])=&gt;{ it.map(x=&gt;{ (x._1,x._2.sum+x._3.getOrElse(0)) }) } } transform 允许将任意RDD到RDD函数应用于DStream package zygDemo1 import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} /** * Created by HP on 2019/7/16 10:26 * nc -lk 9999 启动netcat(网络服务) */ object StreamingWC { def main(args: Array[String]): Unit = { val conf=new SparkConf().setAppName(&quot;StreamingWC&quot;).setMaster(&quot;local[*]&quot;) val sc=new SparkContext(conf) val strCon: StreamingContext = new StreamingContext(sc,Durations.milliseconds(1000)) //获取NetCat的数据 val msg: ReceiverInputDStream[String] = strCon.socketTextStream(&quot;mini2&quot;,9999) //数据处理逻辑 val sumd: DStream[(String, Int)] = msg.transform(rdd =&gt; { rdd.flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_ + _) }) sumd.print() //提交任务到集群 strCon.start() //线程等待，等待处理任务 strCon.awaitTermination() } }" />
<link rel="canonical" href="https://uzzz.org/2019/07/27/794224.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/27/794224.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-27T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Spark Streaming http://spark.apache.org/docs/2.2.1/streaming-programming-guide.html Spark Streaming 的checkPoint： 提供故障容错 To summarize, metadata checkpointing is primarily needed for recovery from driver failures, whereas data or RDD checkpointing is necessary even for basic functioning if stateful transformations are used. DStream的理解：一系列带有时间间隔的RDD 操作DStream的3个重要的原语 代码之前准备：集群安装 安装netcat yum -y install nc window operations 具体窗口大小和窗口间隔，比如需要需要展示的是上一次和这一次的10个数据 package zygDemo1 import org.apache.spark.{SparkConf, SparkContext} import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} /** * Created by HP on 2019/7/16 15:51 */ object WindowOperationsDemo { def main(args: Array[String]): Unit = { //初始化环境 val conf=new SparkConf().setAppName(&quot;WindowOperationsDemo&quot;).setMaster(&quot;local[*]&quot;) val sc=new SparkContext(conf) //实例化StreamingContext val strCon: StreamingContext = new StreamingContext(sc,Durations.milliseconds(1000)) //获取NetCat模拟发送的数据 val msg: ReceiverInputDStream[String] = strCon.socketTextStream(&quot;mini2&quot;,9999) //wordcount简单数据拼接 word word hello （world，1）（word，1）（hello，1） val tup: DStream[(String, Int)] = msg.flatMap(_.split(&quot; &quot;)).map((_,1)) //窗口聚合 参数（聚合func，窗口大小，滑动间隔）（窗口解析见图） val sumd: DStream[(String, Int)] = tup.reduceByKeyAndWindow((x:Int, y:Int)=&gt;(x+y),Durations.seconds(10),Durations.seconds(10)) sumd.print() //提交任务到集群 strCon.start() //线程等待，等待处理任务 strCon.awaitTermination() } } updateStateByKey 历史数据批次累加 package zygDemo1 import org.apache.spark.streaming.dstream.DStream import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.{HashPartitioner, SparkConf} /** * Created by HP on 2019/7/16 11:24 */ object UpdateStateByKeyDemo{ def main(args: Array[String]): Unit = { val conf=new SparkConf().setAppName(&quot;UpdateStateByKeyDemo&quot;).setMaster(&quot;local[2]&quot;) val strCon: StreamingContext = new StreamingContext(conf,Durations.milliseconds(2000))//conf和批处理时间间隔 //设置检查点 strCon.checkpoint(&quot;d://zyg-20190716&quot;) //输入流 val msg=strCon.socketTextStream(&quot;mini2&quot;,9999) //处理逻辑 val tup=msg.flatMap(_.split(&quot; &quot;)).map((_,1)) //历史数据和当前数据聚合，制定分区器，是否获取分区信息 val sumd: DStream[(String, Int)] = tup.updateStateByKey(func,new HashPartitioner(strCon.sparkContext.defaultParallelism),true) sumd.print() strCon.start() strCon.awaitTermination() } //参数（word，（1,1,1））（hello，（1,1）） //历史数据和当前数据的聚合逻辑（迭代器参数：key，（1,1），历史数据） val func=(it:Iterator[(String,Seq[Int],Option[Int])])=&gt;{ it.map(x=&gt;{ (x._1,x._2.sum+x._3.getOrElse(0)) }) } } transform 允许将任意RDD到RDD函数应用于DStream package zygDemo1 import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} import org.apache.spark.streaming.{Durations, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} /** * Created by HP on 2019/7/16 10:26 * nc -lk 9999 启动netcat(网络服务) */ object StreamingWC { def main(args: Array[String]): Unit = { val conf=new SparkConf().setAppName(&quot;StreamingWC&quot;).setMaster(&quot;local[*]&quot;) val sc=new SparkContext(conf) val strCon: StreamingContext = new StreamingContext(sc,Durations.milliseconds(1000)) //获取NetCat的数据 val msg: ReceiverInputDStream[String] = strCon.socketTextStream(&quot;mini2&quot;,9999) //数据处理逻辑 val sumd: DStream[(String, Int)] = msg.transform(rdd =&gt; { rdd.flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_ + _) }) sumd.print() //提交任务到集群 strCon.start() //线程等待，等待处理任务 strCon.awaitTermination() } }","@type":"BlogPosting","url":"https://uzzz.org/2019/07/27/794224.html","headline":"SparkStreaming入门总结","dateModified":"2019-07-27T00:00:00+08:00","datePublished":"2019-07-27T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/27/794224.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>SparkStreaming入门总结</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h1><a id="Spark_Streaming_0"></a>Spark Streaming</h1> 
  <p><a href="http://spark.apache.org/docs/2.2.1/streaming-programming-guide.html" rel="nofollow" data-token="3833ded9fddc12c95a918dfd2e2151d0">http://spark.apache.org/docs/2.2.1/streaming-programming-guide.html</a></p> 
  <p>Spark Streaming 的checkPoint：</p> 
  <p>提供故障容错</p> 
  <p>To summarize, metadata checkpointing is primarily needed for recovery from driver failures, whereas data or RDD checkpointing is necessary even for basic functioning if stateful transformations are used.</p> 
  <p>DStream的理解：一系列带有时间间隔的RDD</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190727092019244.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzU1Nzcx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>操作DStream的3个重要的原语</p> 
  <p>代码之前准备：集群安装</p> 
  <p>安装netcat</p> 
  <p>yum -y install nc</p> 
  <h2><a id="window_operations_24"></a>window operations</h2> 
  <p>具体窗口大小和窗口间隔，比如需要需要展示的是上一次和这一次的10个数据</p> 
  <pre><code class="prism language-scala">package zygDemo1


import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.streaming.{Durations, StreamingContext}
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}

/**
  * Created by HP on 2019/7/16 15:51
  */
object WindowOperationsDemo {
  def main(args: Array[String]): Unit = {
    //初始化环境
    val conf=new SparkConf().setAppName("WindowOperationsDemo").setMaster("local[*]")
    val sc=new SparkContext(conf)
    //实例化StreamingContext
    val strCon: StreamingContext = new StreamingContext(sc,Durations.milliseconds(1000))

    //获取NetCat模拟发送的数据
    val msg: ReceiverInputDStream[String] = strCon.socketTextStream("mini2",9999)
    //wordcount简单数据拼接 word word hello （world，1）（word，1）（hello，1）
    val tup: DStream[(String, Int)] = msg.flatMap(_.split(" ")).map((_,1))
    //窗口聚合 参数（聚合func，窗口大小，滑动间隔）（窗口解析见图）
    val sumd: DStream[(String, Int)] = tup.reduceByKeyAndWindow((x:Int, y:Int)=&gt;(x+y),Durations.seconds(10),Durations.seconds(10))

    sumd.print()

    //提交任务到集群
    strCon.start()
    //线程等待，等待处理任务
    strCon.awaitTermination()

  }

}

</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190727091928392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzU1Nzcx,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败(img-rzrpSynW-1564190323807)(C:\Users\HP\AppData\Roaming\Typora\typora-user-images\1563278550013.png)]"></p> 
  <h2><a id="updateStateByKey_69"></a>updateStateByKey</h2> 
  <p>历史数据批次累加</p> 
  <pre><code class="prism language-scala">package zygDemo1

import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Durations, StreamingContext}
import org.apache.spark.{HashPartitioner, SparkConf}

/**
  * Created by HP on 2019/7/16 11:24
  */
object UpdateStateByKeyDemo{
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("UpdateStateByKeyDemo").setMaster("local[2]")
    
    val strCon: StreamingContext = new StreamingContext(conf,Durations.milliseconds(2000))//conf和批处理时间间隔
    //设置检查点
    strCon.checkpoint("d://zyg-20190716")
    //输入流
    val msg=strCon.socketTextStream("mini2",9999)

    //处理逻辑
    val tup=msg.flatMap(_.split(" ")).map((_,1))
    //历史数据和当前数据聚合，制定分区器，是否获取分区信息
    val sumd: DStream[(String, Int)] = tup.updateStateByKey(func,new HashPartitioner(strCon.sparkContext.defaultParallelism),true)

    sumd.print()

    strCon.start()
    strCon.awaitTermination()

  }
  //参数（word，（1,1,1））（hello，（1,1））
  //历史数据和当前数据的聚合逻辑（迭代器参数：key，（1,1），历史数据）
  val func=(it:Iterator[(String,Seq[Int],Option[Int])])=&gt;{
  it.map(x=&gt;{
    (x._1,x._2.sum+x._3.getOrElse(0))
  })

  }

}
</code></pre> 
  <h2><a id="transform_118"></a>transform</h2> 
  <p>允许将任意RDD到RDD函数应用于DStream</p> 
  <pre><code class="prism language-scala">package zygDemo1

import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Durations, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by HP on 2019/7/16 10:26
  * nc -lk 9999 启动netcat(网络服务)
  */
object StreamingWC {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setAppName("StreamingWC").setMaster("local[*]")
    val sc=new SparkContext(conf)
    val strCon: StreamingContext = new StreamingContext(sc,Durations.milliseconds(1000))

    //获取NetCat的数据
    val msg: ReceiverInputDStream[String] = strCon.socketTextStream("mini2",9999)
    //数据处理逻辑
    val sumd: DStream[(String, Int)] = msg.transform(rdd =&gt; {
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _)
    })

    sumd.print()
    //提交任务到集群
    strCon.start()
    //线程等待，等待处理任务
    strCon.awaitTermination()

  }

}
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
