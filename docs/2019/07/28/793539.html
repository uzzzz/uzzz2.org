<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Druid实战–数据摄入案例 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Druid实战–数据摄入案例" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Durid摄入数据的方式 &nbsp; &nbsp; &nbsp;通过imply的UI界面配置摄入数据规范Ingestion Spec &nbsp; &nbsp; &nbsp;手动创建摄入数据规范Ingestion Spec，通过http请求执行 &nbsp;说明：下面以摄入kafka的数据为例进行步骤解析,kafka的数据格式json {&quot;current_timestamp&quot;:&quot;1560321490664&quot;,&quot;age12&quot;:&quot;22&quot;,&quot;name&quot;:&quot;wangzh&quot;,&quot;type&quot;:&quot;String&quot;} UI界面摄入kafka数据 第一步：点击右上角的 Load data 我们选择kafka并点进去 说明：我们也可以点击other，然后通过编辑Ingestion Spec的方式摄入kafka的数据 进入页面后需要我们配置kafka的broker地址，需要摄入数据的topic名称，以及topic中数据的格式，推荐使用json格式 点击执行过一会，解析topic中的数据，检查数据正确继续点击下一步 接下来会让我们选择时间列以及时间戳的格式 ，一定要和数据中时间戳的格式一样，以及上不上卷 最后设置数据列，以及聚合的时间间隔，data source的名称可以去掉不需要的字段 到此界面的方式摄入kafka的数据结束。 自定义Ingestion Spec摄入kafka数据 &nbsp;第一步 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;自定义Ingestion Spec需要我们自己创建一个json文件,kafka-index.json ,编写Ingestion Spec，然后通过http的方式执行即可，下面展示kafka的自定义Ingestion Spec案例 { &quot;type&quot;: &quot;kafka&quot;, &quot;dataSchema&quot;: { &quot;dataSource&quot;: &quot;kafka2druid&quot;, &quot;parser&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;parseSpec&quot;: { &quot;format&quot;: &quot;json&quot;, &quot;dimensionsSpec&quot;: { &quot;dimensions&quot;: [ &quot;age&quot;, &quot;name&quot;, &quot;type&quot; ] }, &quot;timestampSpec&quot;: { &quot;column&quot;: &quot;current_timestamp&quot;, &quot;format&quot;: &quot;millis&quot; } } }, &quot;metricsSpec&quot;: [], &quot;granularitySpec&quot;: { &quot;type&quot;: &quot;uniform&quot;, &quot;segmentGranularity&quot;: &quot;HOUR&quot;, &quot;queryGranularity&quot;: { &quot;type&quot;: &quot;none&quot; }, &quot;rollup&quot;: false, &quot;intervals&quot;: null }, &quot;transformSpec&quot;: { &quot;filter&quot;: null, &quot;transforms&quot;: [] } }, &quot;tuningConfig&quot;: { &quot;type&quot;: &quot;kafka&quot;, &quot;maxRowsInMemory&quot;: 1000000, &quot;maxBytesInMemory&quot;: 0, &quot;maxRowsPerSegment&quot;: 5000000, &quot;maxTotalRows&quot;: null, &quot;intermediatePersistPeriod&quot;: &quot;PT10M&quot;, &quot;basePersistDirectory&quot;: &quot;/Users/apple/Desktop/imply-2.9.16/var/tmp/1563703527303-0&quot;, &quot;maxPendingPersists&quot;: 0, &quot;indexSpec&quot;: { &quot;bitmap&quot;: { &quot;type&quot;: &quot;concise&quot; }, &quot;dimensionCompression&quot;: &quot;lz4&quot;, &quot;metricCompression&quot;: &quot;lz4&quot;, &quot;longEncoding&quot;: &quot;longs&quot; }, &quot;buildV9Directly&quot;: true, &quot;reportParseExceptions&quot;: false, &quot;handoffConditionTimeout&quot;: 0, &quot;resetOffsetAutomatically&quot;: false, &quot;segmentWriteOutMediumFactory&quot;: null, &quot;workerThreads&quot;: null, &quot;chatThreads&quot;: null, &quot;chatRetries&quot;: 8, &quot;httpTimeout&quot;: &quot;PT10S&quot;, &quot;shutdownTimeout&quot;: &quot;PT80S&quot;, &quot;offsetFetchPeriod&quot;: &quot;PT30S&quot;, &quot;intermediateHandoffPeriod&quot;: &quot;P2147483647D&quot;, &quot;logParseExceptions&quot;: false, &quot;maxParseExceptions&quot;: 100, &quot;maxSavedParseExceptions&quot;: 10, &quot;skipSequenceNumberAvailabilityCheck&quot;: false }, &quot;ioConfig&quot;: { &quot;topic&quot;: &quot;flink_log&quot;, &quot;replicas&quot;: 1, &quot;taskCount&quot;: 1, &quot;taskDuration&quot;: &quot;PT3600S&quot;, &quot;consumerProperties&quot;: { &quot;bootstrap.servers&quot;: &quot;ip1:9092,ip2,ip3:9092&quot; } &quot;pollTimeout&quot;: 100, &quot;startDelay&quot;: &quot;PT5S&quot;, &quot;period&quot;: &quot;PT30S&quot;, &quot;useEarliestOffset&quot;: false, &quot;completionTimeout&quot;: &quot;PT1800S&quot;, &quot;lateMessageRejectionPeriod&quot;: null, &quot;earlyMessageRejectionPeriod&quot;: null, &quot;stream&quot;: &quot;flink_log&quot;, &quot;useEarliestSequenceNumber&quot;: false }, &quot;context&quot;: null, &quot;suspended&quot;: false } 第二步：执行摄入数据的命令 curl -X &#39;POST&#39; -H &#39;Content-Type:application/json&#39; -d @kafka-index.json http://host:8090/druid/indexer/v1/task 自定义Ingestion Spec摄入hdfs数据 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;步骤和摄入kafka的数据一样下面只展示摄入hdfs的自定义Ingestion Spec，其中xxxx是hdfs的域名 { &quot;type&quot; : &quot;index_hadoop&quot;, &quot;spec&quot; : { &quot;dataSchema&quot; : { &quot;dataSource&quot; : &quot;rollup-tutorial&quot;, &quot;parser&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;parseSpec&quot; : { &quot;format&quot; : &quot;json&quot;, &quot;dimensionsSpec&quot; : { &quot;dimensions&quot; : [ &quot;http_user_agent&quot;, &quot;user_id&quot;, &quot;event&quot;, &quot;label&quot; ] }, &quot;timestampSpec&quot;: { &quot;column&quot;: &quot;create_time&quot;, &quot;format&quot;: &quot;iso&quot; } } }, &quot;metricsSpec&quot; : [], &quot;granularitySpec&quot; : { &quot;type&quot; : &quot;uniform&quot;, &quot;segmentGranularity&quot; : &quot;day&quot;, &quot;queryGranularity&quot; : &quot;none&quot;, &quot;intervals&quot; : [&quot;20019-06-01/2019-0-03&quot;], &quot;rollup&quot; : false } }, &quot;ioConfig&quot; : { &quot;type&quot; : &quot;hadoop&quot;, &quot;inputSpec&quot; : { &quot;type&quot; : &quot;static&quot;, &quot;paths&quot; : &quot;hdfs://xxxx/data/test.json&quot; } }, &quot;tuningConfig&quot; : { &quot;type&quot; : &quot;hadoop&quot;, &quot;targetPartitionSize&quot; : 5000000, &quot;maxRowsInMemory&quot; : 25000, &quot;forceExtendableShardSpecs&quot; : true, &quot;jobProperties&quot; : { &quot;mapreduce.job.classloader&quot; : &quot;true&quot; } } }, &quot;hadoopDependencyCoordinates&quot; : [ &quot;org.apache.hadoop:hadoop-client:3.1.1&quot; ] } &nbsp; 扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦 扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦 扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="Durid摄入数据的方式 &nbsp; &nbsp; &nbsp;通过imply的UI界面配置摄入数据规范Ingestion Spec &nbsp; &nbsp; &nbsp;手动创建摄入数据规范Ingestion Spec，通过http请求执行 &nbsp;说明：下面以摄入kafka的数据为例进行步骤解析,kafka的数据格式json {&quot;current_timestamp&quot;:&quot;1560321490664&quot;,&quot;age12&quot;:&quot;22&quot;,&quot;name&quot;:&quot;wangzh&quot;,&quot;type&quot;:&quot;String&quot;} UI界面摄入kafka数据 第一步：点击右上角的 Load data 我们选择kafka并点进去 说明：我们也可以点击other，然后通过编辑Ingestion Spec的方式摄入kafka的数据 进入页面后需要我们配置kafka的broker地址，需要摄入数据的topic名称，以及topic中数据的格式，推荐使用json格式 点击执行过一会，解析topic中的数据，检查数据正确继续点击下一步 接下来会让我们选择时间列以及时间戳的格式 ，一定要和数据中时间戳的格式一样，以及上不上卷 最后设置数据列，以及聚合的时间间隔，data source的名称可以去掉不需要的字段 到此界面的方式摄入kafka的数据结束。 自定义Ingestion Spec摄入kafka数据 &nbsp;第一步 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;自定义Ingestion Spec需要我们自己创建一个json文件,kafka-index.json ,编写Ingestion Spec，然后通过http的方式执行即可，下面展示kafka的自定义Ingestion Spec案例 { &quot;type&quot;: &quot;kafka&quot;, &quot;dataSchema&quot;: { &quot;dataSource&quot;: &quot;kafka2druid&quot;, &quot;parser&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;parseSpec&quot;: { &quot;format&quot;: &quot;json&quot;, &quot;dimensionsSpec&quot;: { &quot;dimensions&quot;: [ &quot;age&quot;, &quot;name&quot;, &quot;type&quot; ] }, &quot;timestampSpec&quot;: { &quot;column&quot;: &quot;current_timestamp&quot;, &quot;format&quot;: &quot;millis&quot; } } }, &quot;metricsSpec&quot;: [], &quot;granularitySpec&quot;: { &quot;type&quot;: &quot;uniform&quot;, &quot;segmentGranularity&quot;: &quot;HOUR&quot;, &quot;queryGranularity&quot;: { &quot;type&quot;: &quot;none&quot; }, &quot;rollup&quot;: false, &quot;intervals&quot;: null }, &quot;transformSpec&quot;: { &quot;filter&quot;: null, &quot;transforms&quot;: [] } }, &quot;tuningConfig&quot;: { &quot;type&quot;: &quot;kafka&quot;, &quot;maxRowsInMemory&quot;: 1000000, &quot;maxBytesInMemory&quot;: 0, &quot;maxRowsPerSegment&quot;: 5000000, &quot;maxTotalRows&quot;: null, &quot;intermediatePersistPeriod&quot;: &quot;PT10M&quot;, &quot;basePersistDirectory&quot;: &quot;/Users/apple/Desktop/imply-2.9.16/var/tmp/1563703527303-0&quot;, &quot;maxPendingPersists&quot;: 0, &quot;indexSpec&quot;: { &quot;bitmap&quot;: { &quot;type&quot;: &quot;concise&quot; }, &quot;dimensionCompression&quot;: &quot;lz4&quot;, &quot;metricCompression&quot;: &quot;lz4&quot;, &quot;longEncoding&quot;: &quot;longs&quot; }, &quot;buildV9Directly&quot;: true, &quot;reportParseExceptions&quot;: false, &quot;handoffConditionTimeout&quot;: 0, &quot;resetOffsetAutomatically&quot;: false, &quot;segmentWriteOutMediumFactory&quot;: null, &quot;workerThreads&quot;: null, &quot;chatThreads&quot;: null, &quot;chatRetries&quot;: 8, &quot;httpTimeout&quot;: &quot;PT10S&quot;, &quot;shutdownTimeout&quot;: &quot;PT80S&quot;, &quot;offsetFetchPeriod&quot;: &quot;PT30S&quot;, &quot;intermediateHandoffPeriod&quot;: &quot;P2147483647D&quot;, &quot;logParseExceptions&quot;: false, &quot;maxParseExceptions&quot;: 100, &quot;maxSavedParseExceptions&quot;: 10, &quot;skipSequenceNumberAvailabilityCheck&quot;: false }, &quot;ioConfig&quot;: { &quot;topic&quot;: &quot;flink_log&quot;, &quot;replicas&quot;: 1, &quot;taskCount&quot;: 1, &quot;taskDuration&quot;: &quot;PT3600S&quot;, &quot;consumerProperties&quot;: { &quot;bootstrap.servers&quot;: &quot;ip1:9092,ip2,ip3:9092&quot; } &quot;pollTimeout&quot;: 100, &quot;startDelay&quot;: &quot;PT5S&quot;, &quot;period&quot;: &quot;PT30S&quot;, &quot;useEarliestOffset&quot;: false, &quot;completionTimeout&quot;: &quot;PT1800S&quot;, &quot;lateMessageRejectionPeriod&quot;: null, &quot;earlyMessageRejectionPeriod&quot;: null, &quot;stream&quot;: &quot;flink_log&quot;, &quot;useEarliestSequenceNumber&quot;: false }, &quot;context&quot;: null, &quot;suspended&quot;: false } 第二步：执行摄入数据的命令 curl -X &#39;POST&#39; -H &#39;Content-Type:application/json&#39; -d @kafka-index.json http://host:8090/druid/indexer/v1/task 自定义Ingestion Spec摄入hdfs数据 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;步骤和摄入kafka的数据一样下面只展示摄入hdfs的自定义Ingestion Spec，其中xxxx是hdfs的域名 { &quot;type&quot; : &quot;index_hadoop&quot;, &quot;spec&quot; : { &quot;dataSchema&quot; : { &quot;dataSource&quot; : &quot;rollup-tutorial&quot;, &quot;parser&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;parseSpec&quot; : { &quot;format&quot; : &quot;json&quot;, &quot;dimensionsSpec&quot; : { &quot;dimensions&quot; : [ &quot;http_user_agent&quot;, &quot;user_id&quot;, &quot;event&quot;, &quot;label&quot; ] }, &quot;timestampSpec&quot;: { &quot;column&quot;: &quot;create_time&quot;, &quot;format&quot;: &quot;iso&quot; } } }, &quot;metricsSpec&quot; : [], &quot;granularitySpec&quot; : { &quot;type&quot; : &quot;uniform&quot;, &quot;segmentGranularity&quot; : &quot;day&quot;, &quot;queryGranularity&quot; : &quot;none&quot;, &quot;intervals&quot; : [&quot;20019-06-01/2019-0-03&quot;], &quot;rollup&quot; : false } }, &quot;ioConfig&quot; : { &quot;type&quot; : &quot;hadoop&quot;, &quot;inputSpec&quot; : { &quot;type&quot; : &quot;static&quot;, &quot;paths&quot; : &quot;hdfs://xxxx/data/test.json&quot; } }, &quot;tuningConfig&quot; : { &quot;type&quot; : &quot;hadoop&quot;, &quot;targetPartitionSize&quot; : 5000000, &quot;maxRowsInMemory&quot; : 25000, &quot;forceExtendableShardSpecs&quot; : true, &quot;jobProperties&quot; : { &quot;mapreduce.job.classloader&quot; : &quot;true&quot; } } }, &quot;hadoopDependencyCoordinates&quot; : [ &quot;org.apache.hadoop:hadoop-client:3.1.1&quot; ] } &nbsp; 扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦 扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦 扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/07/28/793539.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/28/793539.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-28T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Durid摄入数据的方式 &nbsp; &nbsp; &nbsp;通过imply的UI界面配置摄入数据规范Ingestion Spec &nbsp; &nbsp; &nbsp;手动创建摄入数据规范Ingestion Spec，通过http请求执行 &nbsp;说明：下面以摄入kafka的数据为例进行步骤解析,kafka的数据格式json {&quot;current_timestamp&quot;:&quot;1560321490664&quot;,&quot;age12&quot;:&quot;22&quot;,&quot;name&quot;:&quot;wangzh&quot;,&quot;type&quot;:&quot;String&quot;} UI界面摄入kafka数据 第一步：点击右上角的 Load data 我们选择kafka并点进去 说明：我们也可以点击other，然后通过编辑Ingestion Spec的方式摄入kafka的数据 进入页面后需要我们配置kafka的broker地址，需要摄入数据的topic名称，以及topic中数据的格式，推荐使用json格式 点击执行过一会，解析topic中的数据，检查数据正确继续点击下一步 接下来会让我们选择时间列以及时间戳的格式 ，一定要和数据中时间戳的格式一样，以及上不上卷 最后设置数据列，以及聚合的时间间隔，data source的名称可以去掉不需要的字段 到此界面的方式摄入kafka的数据结束。 自定义Ingestion Spec摄入kafka数据 &nbsp;第一步 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;自定义Ingestion Spec需要我们自己创建一个json文件,kafka-index.json ,编写Ingestion Spec，然后通过http的方式执行即可，下面展示kafka的自定义Ingestion Spec案例 { &quot;type&quot;: &quot;kafka&quot;, &quot;dataSchema&quot;: { &quot;dataSource&quot;: &quot;kafka2druid&quot;, &quot;parser&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;parseSpec&quot;: { &quot;format&quot;: &quot;json&quot;, &quot;dimensionsSpec&quot;: { &quot;dimensions&quot;: [ &quot;age&quot;, &quot;name&quot;, &quot;type&quot; ] }, &quot;timestampSpec&quot;: { &quot;column&quot;: &quot;current_timestamp&quot;, &quot;format&quot;: &quot;millis&quot; } } }, &quot;metricsSpec&quot;: [], &quot;granularitySpec&quot;: { &quot;type&quot;: &quot;uniform&quot;, &quot;segmentGranularity&quot;: &quot;HOUR&quot;, &quot;queryGranularity&quot;: { &quot;type&quot;: &quot;none&quot; }, &quot;rollup&quot;: false, &quot;intervals&quot;: null }, &quot;transformSpec&quot;: { &quot;filter&quot;: null, &quot;transforms&quot;: [] } }, &quot;tuningConfig&quot;: { &quot;type&quot;: &quot;kafka&quot;, &quot;maxRowsInMemory&quot;: 1000000, &quot;maxBytesInMemory&quot;: 0, &quot;maxRowsPerSegment&quot;: 5000000, &quot;maxTotalRows&quot;: null, &quot;intermediatePersistPeriod&quot;: &quot;PT10M&quot;, &quot;basePersistDirectory&quot;: &quot;/Users/apple/Desktop/imply-2.9.16/var/tmp/1563703527303-0&quot;, &quot;maxPendingPersists&quot;: 0, &quot;indexSpec&quot;: { &quot;bitmap&quot;: { &quot;type&quot;: &quot;concise&quot; }, &quot;dimensionCompression&quot;: &quot;lz4&quot;, &quot;metricCompression&quot;: &quot;lz4&quot;, &quot;longEncoding&quot;: &quot;longs&quot; }, &quot;buildV9Directly&quot;: true, &quot;reportParseExceptions&quot;: false, &quot;handoffConditionTimeout&quot;: 0, &quot;resetOffsetAutomatically&quot;: false, &quot;segmentWriteOutMediumFactory&quot;: null, &quot;workerThreads&quot;: null, &quot;chatThreads&quot;: null, &quot;chatRetries&quot;: 8, &quot;httpTimeout&quot;: &quot;PT10S&quot;, &quot;shutdownTimeout&quot;: &quot;PT80S&quot;, &quot;offsetFetchPeriod&quot;: &quot;PT30S&quot;, &quot;intermediateHandoffPeriod&quot;: &quot;P2147483647D&quot;, &quot;logParseExceptions&quot;: false, &quot;maxParseExceptions&quot;: 100, &quot;maxSavedParseExceptions&quot;: 10, &quot;skipSequenceNumberAvailabilityCheck&quot;: false }, &quot;ioConfig&quot;: { &quot;topic&quot;: &quot;flink_log&quot;, &quot;replicas&quot;: 1, &quot;taskCount&quot;: 1, &quot;taskDuration&quot;: &quot;PT3600S&quot;, &quot;consumerProperties&quot;: { &quot;bootstrap.servers&quot;: &quot;ip1:9092,ip2,ip3:9092&quot; } &quot;pollTimeout&quot;: 100, &quot;startDelay&quot;: &quot;PT5S&quot;, &quot;period&quot;: &quot;PT30S&quot;, &quot;useEarliestOffset&quot;: false, &quot;completionTimeout&quot;: &quot;PT1800S&quot;, &quot;lateMessageRejectionPeriod&quot;: null, &quot;earlyMessageRejectionPeriod&quot;: null, &quot;stream&quot;: &quot;flink_log&quot;, &quot;useEarliestSequenceNumber&quot;: false }, &quot;context&quot;: null, &quot;suspended&quot;: false } 第二步：执行摄入数据的命令 curl -X &#39;POST&#39; -H &#39;Content-Type:application/json&#39; -d @kafka-index.json http://host:8090/druid/indexer/v1/task 自定义Ingestion Spec摄入hdfs数据 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;步骤和摄入kafka的数据一样下面只展示摄入hdfs的自定义Ingestion Spec，其中xxxx是hdfs的域名 { &quot;type&quot; : &quot;index_hadoop&quot;, &quot;spec&quot; : { &quot;dataSchema&quot; : { &quot;dataSource&quot; : &quot;rollup-tutorial&quot;, &quot;parser&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;parseSpec&quot; : { &quot;format&quot; : &quot;json&quot;, &quot;dimensionsSpec&quot; : { &quot;dimensions&quot; : [ &quot;http_user_agent&quot;, &quot;user_id&quot;, &quot;event&quot;, &quot;label&quot; ] }, &quot;timestampSpec&quot;: { &quot;column&quot;: &quot;create_time&quot;, &quot;format&quot;: &quot;iso&quot; } } }, &quot;metricsSpec&quot; : [], &quot;granularitySpec&quot; : { &quot;type&quot; : &quot;uniform&quot;, &quot;segmentGranularity&quot; : &quot;day&quot;, &quot;queryGranularity&quot; : &quot;none&quot;, &quot;intervals&quot; : [&quot;20019-06-01/2019-0-03&quot;], &quot;rollup&quot; : false } }, &quot;ioConfig&quot; : { &quot;type&quot; : &quot;hadoop&quot;, &quot;inputSpec&quot; : { &quot;type&quot; : &quot;static&quot;, &quot;paths&quot; : &quot;hdfs://xxxx/data/test.json&quot; } }, &quot;tuningConfig&quot; : { &quot;type&quot; : &quot;hadoop&quot;, &quot;targetPartitionSize&quot; : 5000000, &quot;maxRowsInMemory&quot; : 25000, &quot;forceExtendableShardSpecs&quot; : true, &quot;jobProperties&quot; : { &quot;mapreduce.job.classloader&quot; : &quot;true&quot; } } }, &quot;hadoopDependencyCoordinates&quot; : [ &quot;org.apache.hadoop:hadoop-client:3.1.1&quot; ] } &nbsp; 扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦 扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦 扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/07/28/793539.html","headline":"Druid实战–数据摄入案例","dateModified":"2019-07-28T00:00:00+08:00","datePublished":"2019-07-28T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/28/793539.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Druid实战--数据摄入案例</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1>Durid摄入数据的方式</h1> 
  <ol>
   <li>&nbsp; &nbsp; &nbsp;通过imply的UI界面配置摄入数据规范Ingestion Spec</li> 
   <li>&nbsp; &nbsp; &nbsp;手动创建摄入数据规范Ingestion Spec，通过http请求执行</li> 
  </ol>
  <blockquote> 
   <p>&nbsp;说明：下面以摄入kafka的数据为例进行步骤解析,kafka的数据格式json</p> 
   <p>{"current_timestamp":"1560321490664","age12":"22","name":"wangzh","type":"String"}</p> 
  </blockquote> 
  <h1>UI界面摄入kafka数据</h1> 
  <p>第一步：点击右上角的 Load data</p> 
  <p><img alt="" class="has" height="183" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728181741295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FBNTE4MTg5,size_16,color_FFFFFF,t_70" width="599"></p> 
  <p>我们选择kafka并点进去</p> 
  <blockquote> 
   <p><span style="color:#f33b45;"><strong>说明：我们也可以点击other，然后通过编辑Ingestion Spec的方式摄入kafka的数据</strong></span></p> 
  </blockquote> 
  <p><img alt="" class="has" height="264" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728181834401.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FBNTE4MTg5,size_16,color_FFFFFF,t_70" width="688"></p> 
  <p>进入页面后需要我们配置kafka的broker地址，需要摄入数据的topic名称，以及topic中数据的格式，推荐使用json格式</p> 
  <p><img alt="" class="has" height="252" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728182050478.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FBNTE4MTg5,size_16,color_FFFFFF,t_70" width="711"></p> 
  <p>点击执行过一会，解析topic中的数据，检查数据正确继续点击下一步</p> 
  <p><img alt="" class="has" height="183" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190729160152653.png" width="793"></p> 
  <p>接下来会让我们选择时间列以及时间戳的格式 ，<strong><span style="color:#f33b45;">一定要和数据中时间戳的格式一样</span></strong>，以及上不上卷</p> 
  <p><img alt="" class="has" height="226" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728183125901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FBNTE4MTg5,size_16,color_FFFFFF,t_70" width="483"></p> 
  <p>最后设置数据列，以及聚合的时间间隔，data source的名称可以去掉不需要的字段</p> 
  <p><img alt="" class="has" height="260" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728183217902.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FBNTE4MTg5,size_16,color_FFFFFF,t_70" width="685"></p> 
  <p><img alt="" class="has" height="311" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019072818334218.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FBNTE4MTg5,size_16,color_FFFFFF,t_70" width="502"></p> 
  <p>到此界面的方式摄入kafka的数据结束。</p> 
  <h1>自定义Ingestion Spec摄入kafka数据</h1> 
  <h3><strong><span style="color:#f33b45;">&nbsp;第一步</span></strong></h3> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;自定义Ingestion Spec需要我们自己创建一个json文件,kafka-index.json ,编写Ingestion Spec，然后通过http的方式执行即可，下面展示kafka的自定义Ingestion Spec案例</p> 
  <pre class="has">
<code>{
  "type": "kafka",
  "dataSchema": {
    "dataSource": "kafka2druid",
    "parser": {
      "type": "string",
      "parseSpec": {
        "format": "json",
        "dimensionsSpec": {
          "dimensions": [
            "age",
            "name",
            "type"
          ]
        },
        "timestampSpec": {
          "column": "current_timestamp",
          "format": "millis"
        }
      }
    },
    "metricsSpec": [],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": {
        "type": "none"
      },
      "rollup": false,
      "intervals": null
    },
    "transformSpec": {
      "filter": null,
      "transforms": []
    }
  },
  "tuningConfig": {
    "type": "kafka",
    "maxRowsInMemory": 1000000,
    "maxBytesInMemory": 0,
    "maxRowsPerSegment": 5000000,
    "maxTotalRows": null,
    "intermediatePersistPeriod": "PT10M",
    "basePersistDirectory": "/Users/apple/Desktop/imply-2.9.16/var/tmp/1563703527303-0",
    "maxPendingPersists": 0,
    "indexSpec": {
      "bitmap": {
        "type": "concise"
      },
      "dimensionCompression": "lz4",
      "metricCompression": "lz4",
      "longEncoding": "longs"
    },
    "buildV9Directly": true,
    "reportParseExceptions": false,
    "handoffConditionTimeout": 0,
    "resetOffsetAutomatically": false,
    "segmentWriteOutMediumFactory": null,
    "workerThreads": null,
    "chatThreads": null,
    "chatRetries": 8,
    "httpTimeout": "PT10S",
    "shutdownTimeout": "PT80S",
    "offsetFetchPeriod": "PT30S",
    "intermediateHandoffPeriod": "P2147483647D",
    "logParseExceptions": false,
    "maxParseExceptions": 100,
    "maxSavedParseExceptions": 10,
    "skipSequenceNumberAvailabilityCheck": false
  },
  "ioConfig": {
    "topic": "flink_log",
    "replicas": 1,
    "taskCount": 1,
    "taskDuration": "PT3600S",
    "consumerProperties": {
      "bootstrap.servers": "ip1:9092,ip2,ip3:9092"
    }
    "pollTimeout": 100,
    "startDelay": "PT5S",
    "period": "PT30S",
    "useEarliestOffset": false,
    "completionTimeout": "PT1800S",
    "lateMessageRejectionPeriod": null,
    "earlyMessageRejectionPeriod": null,
    "stream": "flink_log",
    "useEarliestSequenceNumber": false
  },
  "context": null,
  "suspended": false
}</code></pre> 
  <h3><span style="color:#f33b45;">第二步：执行摄入数据的命令</span></h3> 
  <pre class="has">
<code>curl -X 'POST' -H 'Content-Type:application/json' -d @kafka-index.json http://host:8090/druid/indexer/v1/task</code></pre> 
  <h1>自定义Ingestion Spec摄入hdfs数据</h1> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;步骤和摄入kafka的数据一样下面只展示摄入hdfs的自定义Ingestion Spec，其中xxxx是hdfs的域名</p> 
  <pre class="has">
<code>{
    "type" : "index_hadoop",
    "spec" : {
            "dataSchema" : {
            "dataSource" : "rollup-tutorial",
            "parser" : {
                    "type" : "string",
                    "parseSpec" : {
                    "format" : "json",
                    "dimensionsSpec" : {
                        "dimensions" : [
                                "http_user_agent",
                                "user_id",
                                "event",
                                "label"
                             ]
                     },
                    "timestampSpec": {
                        "column": "create_time",
                        "format": "iso"
                      }
                  }
               },
              "metricsSpec" : [],
              "granularitySpec" : {
                    "type" : "uniform",
                    "segmentGranularity" : "day",
                    "queryGranularity" : "none",
                    "intervals" : ["20019-06-01/2019-0-03"],
                    "rollup" : false
                }
            },
         "ioConfig" : {
                "type" : "hadoop",
                "inputSpec" : {
                      "type" : "static",
                      "paths" : "hdfs://xxxx/data/test.json"
                  }
           },
        "tuningConfig" : {
            "type" : "hadoop",
            "targetPartitionSize" : 5000000,
            "maxRowsInMemory" : 25000,
            "forceExtendableShardSpecs" : true,
            "jobProperties" : {
                    "mapreduce.job.classloader" : "true"
               }
          }
        },
        "hadoopDependencyCoordinates" : [
              "org.apache.hadoop:hadoop-client:3.1.1"
          ]
  }
</code></pre> 
  <p>&nbsp;</p> 
  <p>扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦</p> 
  <p>扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦</p> 
  <p>扫一扫加入大数据技术交流群，了解更多大数据技术，还有免费资料等你哦</p> 
  <p><img alt="" class="has" height="200" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190119110905660.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FBNTE4MTg5,size_16,color_FFFFFF,t_70" width="200"></p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h1>&nbsp;</h1> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
