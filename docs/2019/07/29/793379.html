<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Flink零基础学习（一）理解和搭建demo | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Flink零基础学习（一）理解和搭建demo" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="网上关于Flink介绍的文章很多，可以自行百度，向来喜欢研究技术解决实际问题，主要谈我是怎么入坑的 学java出身没怎么接触大数据，也分不清楚Hadoop和Spark的业务场景好坏之分，只是在工作中遇到GPS异常数据处理时，项目中时时会出现一些单靠现有的知识解决不了的问题，想着有没有更好的架构或者java相关的处理办法呢，而恰好Flink是基于java代码设计的，于是开始了 简单的介绍还是要的 一 应用场景： 大量数据不断产生：金融交易数据，互联网订单数，GPS定位数据，传感器信号，移动终端数据，以及网络流量监控，服务器产生的日志；这些数据共同点都是实时从不同数据源产生，然后再传输到下游。日常的智能推荐，复杂事件处理，实时欺诈检测，流数据分析，及时报表。 二.优势特点: 1高吞吐，低延迟，高性能 2支持事时间概念，支持有状态计算（将计算的结果放在内存或者文件中，等下一个事件获取中间结果，有效降低了资源消耗） 3有很好的容错机制，基于分布式的快照CheckPoints,Save points恢复到原来的计算状态 4基于JVM实现的独立内存管理，将所有数据对象转成二进制在内存中，降低内存空间和GC带来的问题 来实现我们的第一个Flink项目 三.安装Flink 官网下载https://flink.apache.org/downloads.html#apache-flink-181 下载后，直接去E:\Flink\flink-1.8.1-bin-scala_2.11\flink-1.8.1\bin\start-cluster.bat（安装目录的bat文件），然后查看本地8081 当然，这些是为后面做铺垫，还是回归到我们demo上来 四.创建maven项目 当然第一次需要将flink所需的jar按add进来，仓库jar包地址如下 &nbsp; 构建完成结构如下（多出2个默认批处理和流处理的java文件） 五创建流处理的main方法 我这里的demo是对文件的单词进行统计次数小功能，将他打印到控制台和指定文件内 import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; /** * 利用读取文件 * @author zhouxl */ public class ReadFile { public static void main(String[] args) throws Exception { //第一步设定环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //第二步；指定数据源，读取文件 DataStreamSource&lt;String&gt; stream =env.readTextFile(&quot;本地文件地址，可相对和绝对路径&quot;); //第三步：计数 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(new Tokenizer()) .keyBy(0) .sum(1); //将统计数据打印控制台 sum.print(); //将统计数据打印到本地文件 sum.writeAsText(&quot;自定义本地文件路径&quot;); env.execute(&quot;单词统计数据&quot;); } public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; { @Override //s是行数据，将每行数据进行切割 public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) { String[] tokens = s.toLowerCase().split(&quot;\\W+&quot;); //token是切出来的单词 for (String token: tokens) { if (token.length() &gt; 0) { //写入集合统计 collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); } } } } } 直接运行，会生成本地文件夹打开如下： 前面是分出的单词数，后面的统计个数，至于控制台日志内容以及分析，接着会在下一次讲到！" />
<meta property="og:description" content="网上关于Flink介绍的文章很多，可以自行百度，向来喜欢研究技术解决实际问题，主要谈我是怎么入坑的 学java出身没怎么接触大数据，也分不清楚Hadoop和Spark的业务场景好坏之分，只是在工作中遇到GPS异常数据处理时，项目中时时会出现一些单靠现有的知识解决不了的问题，想着有没有更好的架构或者java相关的处理办法呢，而恰好Flink是基于java代码设计的，于是开始了 简单的介绍还是要的 一 应用场景： 大量数据不断产生：金融交易数据，互联网订单数，GPS定位数据，传感器信号，移动终端数据，以及网络流量监控，服务器产生的日志；这些数据共同点都是实时从不同数据源产生，然后再传输到下游。日常的智能推荐，复杂事件处理，实时欺诈检测，流数据分析，及时报表。 二.优势特点: 1高吞吐，低延迟，高性能 2支持事时间概念，支持有状态计算（将计算的结果放在内存或者文件中，等下一个事件获取中间结果，有效降低了资源消耗） 3有很好的容错机制，基于分布式的快照CheckPoints,Save points恢复到原来的计算状态 4基于JVM实现的独立内存管理，将所有数据对象转成二进制在内存中，降低内存空间和GC带来的问题 来实现我们的第一个Flink项目 三.安装Flink 官网下载https://flink.apache.org/downloads.html#apache-flink-181 下载后，直接去E:\Flink\flink-1.8.1-bin-scala_2.11\flink-1.8.1\bin\start-cluster.bat（安装目录的bat文件），然后查看本地8081 当然，这些是为后面做铺垫，还是回归到我们demo上来 四.创建maven项目 当然第一次需要将flink所需的jar按add进来，仓库jar包地址如下 &nbsp; 构建完成结构如下（多出2个默认批处理和流处理的java文件） 五创建流处理的main方法 我这里的demo是对文件的单词进行统计次数小功能，将他打印到控制台和指定文件内 import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; /** * 利用读取文件 * @author zhouxl */ public class ReadFile { public static void main(String[] args) throws Exception { //第一步设定环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //第二步；指定数据源，读取文件 DataStreamSource&lt;String&gt; stream =env.readTextFile(&quot;本地文件地址，可相对和绝对路径&quot;); //第三步：计数 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(new Tokenizer()) .keyBy(0) .sum(1); //将统计数据打印控制台 sum.print(); //将统计数据打印到本地文件 sum.writeAsText(&quot;自定义本地文件路径&quot;); env.execute(&quot;单词统计数据&quot;); } public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; { @Override //s是行数据，将每行数据进行切割 public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) { String[] tokens = s.toLowerCase().split(&quot;\\W+&quot;); //token是切出来的单词 for (String token: tokens) { if (token.length() &gt; 0) { //写入集合统计 collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); } } } } } 直接运行，会生成本地文件夹打开如下： 前面是分出的单词数，后面的统计个数，至于控制台日志内容以及分析，接着会在下一次讲到！" />
<link rel="canonical" href="https://uzzz.org/2019/07/29/793379.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/29/793379.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-29T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"网上关于Flink介绍的文章很多，可以自行百度，向来喜欢研究技术解决实际问题，主要谈我是怎么入坑的 学java出身没怎么接触大数据，也分不清楚Hadoop和Spark的业务场景好坏之分，只是在工作中遇到GPS异常数据处理时，项目中时时会出现一些单靠现有的知识解决不了的问题，想着有没有更好的架构或者java相关的处理办法呢，而恰好Flink是基于java代码设计的，于是开始了 简单的介绍还是要的 一 应用场景： 大量数据不断产生：金融交易数据，互联网订单数，GPS定位数据，传感器信号，移动终端数据，以及网络流量监控，服务器产生的日志；这些数据共同点都是实时从不同数据源产生，然后再传输到下游。日常的智能推荐，复杂事件处理，实时欺诈检测，流数据分析，及时报表。 二.优势特点: 1高吞吐，低延迟，高性能 2支持事时间概念，支持有状态计算（将计算的结果放在内存或者文件中，等下一个事件获取中间结果，有效降低了资源消耗） 3有很好的容错机制，基于分布式的快照CheckPoints,Save points恢复到原来的计算状态 4基于JVM实现的独立内存管理，将所有数据对象转成二进制在内存中，降低内存空间和GC带来的问题 来实现我们的第一个Flink项目 三.安装Flink 官网下载https://flink.apache.org/downloads.html#apache-flink-181 下载后，直接去E:\\Flink\\flink-1.8.1-bin-scala_2.11\\flink-1.8.1\\bin\\start-cluster.bat（安装目录的bat文件），然后查看本地8081 当然，这些是为后面做铺垫，还是回归到我们demo上来 四.创建maven项目 当然第一次需要将flink所需的jar按add进来，仓库jar包地址如下 &nbsp; 构建完成结构如下（多出2个默认批处理和流处理的java文件） 五创建流处理的main方法 我这里的demo是对文件的单词进行统计次数小功能，将他打印到控制台和指定文件内 import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; /** * 利用读取文件 * @author zhouxl */ public class ReadFile { public static void main(String[] args) throws Exception { //第一步设定环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //第二步；指定数据源，读取文件 DataStreamSource&lt;String&gt; stream =env.readTextFile(&quot;本地文件地址，可相对和绝对路径&quot;); //第三步：计数 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(new Tokenizer()) .keyBy(0) .sum(1); //将统计数据打印控制台 sum.print(); //将统计数据打印到本地文件 sum.writeAsText(&quot;自定义本地文件路径&quot;); env.execute(&quot;单词统计数据&quot;); } public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; { @Override //s是行数据，将每行数据进行切割 public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) { String[] tokens = s.toLowerCase().split(&quot;\\\\W+&quot;); //token是切出来的单词 for (String token: tokens) { if (token.length() &gt; 0) { //写入集合统计 collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); } } } } } 直接运行，会生成本地文件夹打开如下： 前面是分出的单词数，后面的统计个数，至于控制台日志内容以及分析，接着会在下一次讲到！","@type":"BlogPosting","url":"https://uzzz.org/2019/07/29/793379.html","headline":"Flink零基础学习（一）理解和搭建demo","dateModified":"2019-07-29T00:00:00+08:00","datePublished":"2019-07-29T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/29/793379.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Flink零基础学习（一）理解和搭建demo</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>网上关于Flink介绍的文章很多，可以自行百度，向来喜欢研究技术解决实际问题，主要谈我是怎么入坑的</p> 
  <p>学java出身没怎么接触大数据，也分不清楚Hadoop和Spark的业务场景好坏之分，只是在工作中遇到GPS异常数据处理时，项目中时时会出现一些单靠现有的知识解决不了的问题，想着有没有更好的架构或者java相关的处理办法呢，而恰好Flink是基于java代码设计的，于是开始了</p> 
  <p>简单的介绍还是要的</p> 
  <p><strong>一 应用场景</strong>：</p> 
  <p>大量数据不断产生：金融交易数据，互联网订单数，GPS定位数据，传感器信号，移动终端数据，以及网络流量监控，服务器产生的日志；这些数据共同点都是实时从不同数据源产生，然后再传输到下游。日常的智能推荐，复杂事件处理，实时欺诈检测，流数据分析，及时报表。</p> 
  <p><strong>二.优势特点:</strong></p> 
  <p>1高吞吐，低延迟，高性能</p> 
  <p>2支持事时间概念，支持有状态计算（将计算的结果放在内存或者文件中，等下一个事件获取中间结果，有效降低了资源消耗）</p> 
  <p>3有很好的容错机制，基于分布式的快照CheckPoints,Save points恢复到原来的计算状态</p> 
  <p>4基于JVM实现的独立内存管理，将所有数据对象转成二进制在内存中，降低内存空间和GC带来的问题</p> 
  <p>来实现我们的第一个Flink项目</p> 
  <p><strong>三.安装Flink</strong></p> 
  <p>官网下载<a href="https://flink.apache.org/downloads.html#apache-flink-181" rel="nofollow" data-token="fb8ed2db06abf0f8b0f1d93c64099d0f">https://flink.apache.org/downloads.html#apache-flink-181</a></p> 
  <p><img alt="" class="has" height="325" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019072915454441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjUwMzc4,size_16,color_FFFFFF,t_70" width="871"></p> 
  <p>下载后，直接去E:\Flink\flink-1.8.1-bin-scala_2.11\flink-1.8.1\bin\start-cluster.bat（安装目录的bat文件），然后查看本地8081</p> 
  <p><img alt="" class="has" height="806" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190729154934706.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjUwMzc4,size_16,color_FFFFFF,t_70" width="1200"></p> 
  <p>当然，这些是为后面做铺垫，还是回归到我们demo上来</p> 
  <p><strong>四.创建maven项目</strong></p> 
  <p><img alt="" class="has" height="849" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019072915531378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjUwMzc4,size_16,color_FFFFFF,t_70" width="1032"></p> 
  <p>当然第一次需要将flink所需的jar按add进来，仓库jar包地址如下</p> 
  <p><img alt="" class="has" height="549" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190729155629622.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjUwMzc4,size_16,color_FFFFFF,t_70" width="1200"></p> 
  <p>&nbsp;</p> 
  <p>构建完成结构如下（多出2个默认批处理和流处理的java文件）</p> 
  <p><img alt="" class="has" height="713" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190729155822481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjUwMzc4,size_16,color_FFFFFF,t_70" width="1200"></p> 
  <p><strong>五创建流处理的main方法</strong></p> 
  <p><strong>我这里的demo是对文件的单词进行统计次数小功能，将他打印到控制台和指定文件内</strong></p> 
  <pre>
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;

/**
 * 利用读取文件
 * @author zhouxl 
 */
public class ReadFile {

    public static void main(String[] args) throws Exception {
        //第一步设定环境
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //第二步；指定数据源，读取文件
        DataStreamSource&lt;String&gt; stream =env.readTextFile("本地文件地址，可相对和绝对路径");
        //第三步：计数
        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum =  stream.flatMap(new Tokenizer())
                .keyBy(0)
                .sum(1);
         //将统计数据打印控制台
        sum.print();
        //将统计数据打印到本地文件
        sum.writeAsText("自定义本地文件路径");

        env.execute("单词统计数据");

    }

    public static final class Tokenizer implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; {
        @Override
        //s是行数据，将每行数据进行切割
        public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) {
            String[] tokens = s.toLowerCase().split("\\W+");
            //token是切出来的单词
            for (String token: tokens) {
                if (token.length() &gt; 0) {
                    //写入集合统计
                    collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1));
                }
            }
        }
    }
}</pre> 
  <p>直接运行，会生成本地文件夹打开如下：</p> 
  <p><img alt="" class="has" height="684" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019072916093729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjUwMzc4,size_16,color_FFFFFF,t_70" width="726"></p> 
  <p>前面是分出的单词数，后面的统计个数，至于控制台日志内容以及分析，接着会在下一次讲到！</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
