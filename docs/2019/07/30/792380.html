<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>TensorFlow笔记（11） GoolgeNet | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="TensorFlow笔记（11） GoolgeNet" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TensorFlow笔记（11） GoolgeNet 1. Inception块 2. 数据读取 3. 构建模型 4. 训练模型 5. 评估模型 6. 模型预测 1. Inception块 根据 深度学习笔记（30） Inception网络 可以了解到 可以利用Inception块构建经典网络InceptionNet，也就是GoolgeNet 那么，以CIFAR-10 数据集的分类为例，采用GoolgeNet模型来解决问题 2. 数据读取 CIFAR-10 数据集的分类是机器学习中一个公开的基准测试问题 其任务是对一组32x32RGB的图像进行分类，这些图像涵盖了10个类别： 飞机， 汽车， 鸟， 猫， 鹿， 狗， 青蛙， 马， 船以及卡车 利用网上的 CIFAR-10 数据集 获取数据集压缩文件： 这里使用的是python版本 CIFAR-10 python version 载入数据集合： import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import os import urllib.request import tarfile import pickle as p from sklearn.preprocessing import OneHotEncoder # 下载 cifar10 url = &#39;https://www.cs.toronto.edu/-kriz/cifar-10-python.tar.gz&#39; filepath = &#39;../data/cifar-10-python.tar.gz&#39; if not os.path.isfile(filepath): result = urllib.request.urlretrieve(url, filepath) print(&#39;downloaded&#39;, result) else: print(&#39;Data file already exists.&#39;) # 解压 cifar10 if not os.path.exists(&quot;../data/cifar-10-batches-py&quot;): tfile = tarfile.open(&quot;../data/cifar-10-python.tar.gz&quot;, &#39;r:gz&#39;) result = tfile.extractall(&quot;../data/&quot;) print(&#39;Extracted to ./../data/cifar-10-batches-py/&#39;) else: print(&#39;Directory already exists.&#39;) 下载并解压数据集 当然可以自己在 CIFAR-10 数据集 网站上下载在工程的data文件夹下，然后解压 加载数据集 def load_CIFAR_batch(filename): &quot;&quot;&quot;oad single batch of cifar&quot;&quot;&quot; with open(filename, &#39;rb&#39;)as f: # 一个样本由标签和图像数据组成 # &lt;1 xlabel&gt;&lt;3072 xpixel&gt; (3072-32x32x3) data_dict = p.load(f, encoding=&#39;bytes&#39;) images = data_dict[b&#39;data&#39;] labels = data_dict[b&#39;labels&#39;] # 把原始数据结构调整为: BCWH images = images.reshape(10000, 3, 32, 32) # tensorflow处理图像数据的结构: BWHC # #把通道数据C移动到最后一个维度 images = images.transpose(0, 2, 3, 1) labels = np.array(labels) return images, labels def load_CIFAR_data(data_dir): &quot;&quot;&quot;load CIFAR data&quot;&quot;&quot; images_train = [] labels_train = [] for i in range(5): f = os.path.join(data_dir, &#39;data_batch_{0}&#39;.format(i + 1)) print(&#39;loading&#39;, f) # 调用loadCIFARbatch(获得批量的图像及其对应的标签 image_batch, label_batch = load_CIFAR_batch(f) images_train.append(image_batch) labels_train.append(label_batch) Xtrain = np.concatenate(images_train) Ytrain = np.concatenate(labels_train) del image_batch, label_batch Xtest, Ytest = load_CIFAR_batch(os.path.join(data_dir, &#39;test_batch&#39;)) print(&#39;finished loadding CIFAR-10 data&#39;) # 返回训练集的图象和标签，测试集的图像和标签 return Xtrain, Ytrain, Xtest, Ytest data_dir = &#39;../data/cifar-10-batches-py/&#39; Xtrain, Ytrain, Xtest, Ytest = load_CIFAR_data(data_dir) # loading ../data/cifar-10-batches-py/data_batch_1 # loading ../data/cifar-10-batches-py/data_batch_2 # loading ../data/cifar-10-batches-py/data_batch_3 # loading ../data/cifar-10-batches-py/data_batch_4 # loading ../data/cifar-10-batches-py/data_batch_5 # finished ../loadding CIFAR-10 data 查看数据集数量 print(&#39;training data shape:&#39;, Xtrain.shape) print(&#39;training labels shape:&#39;, Ytrain.shape) print(&#39;test data shape:&#39;, Xtest.shape) print(&#39;test labels shape:&#39;, Ytest.shape) # training data shape: (50000, 32, 32, 3) # training labels shape: (50000,) # test data shape: (10000, 32, 32, 3) # test labels shape: (10000,) 32x32 RGB3通道尺寸图片，训练图片50000张，测试图片10000张， 定义标签字典 对应类别信息查看：http://www.cs.toronto.edu/~kriz/cifar.html label_dict = {0: &quot;airplane&quot;, 1: &quot;automobile&quot;, 2: &quot;bird&quot;, 3: &quot;cat&quot;, 4: &quot;deer&quot;, 5: &quot;dog&quot;, 6: &quot;frog&quot;, 7: &quot;horse&quot;, 8: &quot;ship&quot;, 9: &quot;truck&quot;} 查看第11张图片并查看对应的label imshow_num = 10 plt.imshow(Xtrain[imshow_num]) plt.show() print(&quot;Xtrain[{0}] label:&quot;.format(imshow_num), label_dict[Ytrain[imshow_num]]) # Xtrain[10] label: deer 虽然模模糊糊，但还可以猜出来是十个类别中的鹿 图片进行数字标准化并对比数据 Xtrain_mean0 = np.mean(Xtrain.astype(&#39;float32&#39;)[:, :, :, 0]) Xtrain_mean1 = np.mean(Xtrain.astype(&#39;float32&#39;)[:, :, :, 1]) Xtrain_mean2 = np.mean(Xtrain.astype(&#39;float32&#39;)[:, :, :, 2]) Xtrain_mean = [Xtrain_mean0, Xtrain_mean1, Xtrain_mean2] Xtrain_std0 = np.std(Xtrain.astype(&#39;float32&#39;)[:, :, :, 0]) Xtrain_std1 = np.std(Xtrain.astype(&#39;float32&#39;)[:, :, :, 1]) Xtrain_std2 = np.std(Xtrain.astype(&#39;float32&#39;)[:, :, :, 2]) Xtrain_std = [Xtrain_std0, Xtrain_std1, Xtrain_std2] def data_norm(x, x_mean, x_std): x_norm = x.astype(&#39;float32&#39;) x_norm[:, :, :, 0] = (x_norm[:, :, :, 0] - x_mean[0]) / x_std[0] x_norm[:, :, :, 1] = (x_norm[:, :, :, 1] - x_mean[1]) / x_std[1] x_norm[:, :, :, 2] = (x_norm[:, :, :, 2] - x_mean[2]) / x_std[2] return x_norm Xtrain_norm = data_norm(Xtrain, Xtrain_mean, Xtrain_std) Xtest_norm = data_norm(Xtest, Xtrain_mean, Xtrain_std) # 对比图像数据 print(&quot;Xtrain[0][0][0] data:&quot;, Xtrain[0][0][0]) print(&quot;Xtrain_norm[0][0][0] data:&quot;, Xtrain_norm[0][0][0]) # Xtrain[0][0][0] data: [59 62 63] # Xtrain_normalize[0][0][0] data: [-1.0526042 -0.9816644 -0.7625441] 现在标准化过的数据Xtrain_norm的对应的32x32x3个像素均值为0，标准差为1 测试集的标准化设置的均值和方差需以训练集为准 不然训练的模型对测试图片作用就没那么好了 标签数据处理并对比数据 encoder = OneHotEncoder(sparse=False, categories=&#39;auto&#39;) yy = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]] encoder.fit(yy) Ytrain_reshape = Ytrain.reshape(-1, 1) Ytrain_onehot = encoder.transform(Ytrain_reshape) Ytest_reshape = Ytest.reshape(-1, 1) Ytest_onehot = encoder.transform(Ytest_reshape) print(&quot;Ytrain[10] data:&quot;, Ytrain[10]) print(&quot;Ytrain_onehot[10] data:&quot;, Ytrain_onehot[10]) # Ytrain[10] data: 4 # Ytrain_onehot[10] data: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] 现在的标签数据Ytrain_onehot，Ytest_onehot 已采用独热编码形式 3. 构建模型 定义训练数据的占位符， x是32x32x3个像素点的特征值， y是10分类的标签值： x = tf.placeholder(tf.float32, [None, 32, 32, 3], name=&quot;X&quot;) y = tf.placeholder(tf.float32, [None, 10], name=&quot;Y&quot;) shape中 None 表示行的数量未知 在实际训练时决定一次代入多少行样本 展开图片 x 为了使用卷积层，需把x变成一个4d向量 其第1维对应样本数， -1表示任意数量 其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数 x_image = tf.reshape(x, [-1, 32, 32, 3]) 定义权重初始化函数： 定义权重W 初始化函数 ：从标准差0.1的截断正态分布中输出随机值 标准正态分布生生成的数据在负无穷到正无穷 但是截断式正态分布生成的数据在均值-2倍的标准差，均值+2倍的标准差这个范围内def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) 定义权重b 初始化函数 ：数值为0.1def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 定义 卷积 函数：# 定义1步长的 same卷积 def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # 定义1步长的 valid卷积 def conv2d_v(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;VALID&#39;) # 定义2步长的 same卷积 def conv2d_2(x, W): return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) TensorFow的卷积函数：tf.nn.conv2d()用法可在TensorFlow笔记（8） LeNet-5卷积神经网络中查看 定义 池化 函数：# 定义步长为1 大小3x3的 max pooling def max_pool_3x3(x): return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # 定义步长为2 大小3x3的 max pooling def max_pool_3x3_2(x): return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) # 定义步长为3 大小5x5的 avg pooling def avg_pool_5x5(x): return tf.nn.avg_pool(x, ksize=[1, 5, 5, 1], strides=[1, 3, 3, 1], padding=&#39;VALID&#39;) # 定义步长为1 大小7x7的 avg pooling def avg_pool_7x7(x): return tf.nn.avg_pool(x, ksize=[1, 7, 7, 1], strides=[1, 1, 1, 1], padding=&#39;VALID&#39;) TensorFow的池化函数：平均池化tf.nn.avg_pool()与最大池化tf.nn.max_pool()用法相似 同样可在TensorFlow笔记（8） LeNet-5卷积神经网络中查看 配置inception块： def inception(x, channel_in, filters_num): # 第一分支 conv 1x1+1s with tf.variable_scope(&quot;branch1&quot;): branch1_w_conv1 = weight_variable([1, 1, channel_in, filters_num[0]]) branch1_b_conv1 = bias_variable([filters_num[0]]) branch1_h_conv1 = tf.nn.relu(conv2d(x, branch1_w_conv1) + branch1_b_conv1) # 第二分支 conv 1x1+1s &gt; conv 3x3+1s with tf.variable_scope(&quot;branch2&quot;): branch2_w_conv1 = weight_variable([1, 1, channel_in, filters_num[1]]) branch2_b_conv1 = bias_variable([filters_num[1]]) branch2_h_conv1 = tf.nn.relu(conv2d(x, branch2_w_conv1) + branch2_b_conv1) branch2_w_conv2 = weight_variable([3, 3, filters_num[1], filters_num[2]]) branch2_b_conv2 = bias_variable([filters_num[2]]) branch2_h_conv2 = tf.nn.relu(conv2d(branch2_h_conv1, branch2_w_conv2) + branch2_b_conv2) # 第三分支 conv 1x1+1s &gt; conv 5x5+1s with tf.variable_scope(&quot;branch3&quot;): branch3_w_conv1 = weight_variable([1, 1, channel_in, filters_num[3]]) branch3_b_conv1 = bias_variable([filters_num[3]]) branch3_h_conv1 = tf.nn.relu(conv2d(x, branch3_w_conv1) + branch3_b_conv1) branch3_w_conv2 = weight_variable([5, 5, filters_num[3], filters_num[4]]) branch3_b_conv2 = bias_variable([filters_num[4]]) branch3_h_conv2 = tf.nn.relu(conv2d(branch3_h_conv1, branch3_w_conv2) + branch3_b_conv2) # 第四分支 max_pool 3x3+1s &gt; conv 1x1+1s with tf.variable_scope(&quot;branch4&quot;): branch4_pool1 = max_pool_3x3(x) branch4_w_conv1 = weight_variable([1, 1, channel_in, filters_num[5]]) branch4_b_conv1 = bias_variable([filters_num[5]]) branch4_h_conv1 = tf.nn.relu(conv2d(branch4_pool1, branch4_w_conv1) + branch4_b_conv1) # 输出叠加 with tf.variable_scope(&quot;depth_concat&quot;): concat = tf.concat([branch1_h_conv1, branch2_h_conv2, branch3_h_conv2, branch4_h_conv1], axis=3) return concat 第一二层：压缩图片 第一二层为什么混着来呢，因为其实作用都是调整和预处理图片 以确保第一次进入inception块时为28x28x192的尺寸 而cifar10图片尺寸较小，为配合后续的计算，调整一些原有结构压缩成28x28x192 # 第一层 w_conv1 = weight_variable([3, 3, 3, 64]) b_conv1 = bias_variable([64]) h_conv1 = tf.nn.relu(conv2d_v(x_image, w_conv1) + b_conv1) h_pool1 = max_pool_3x3(h_conv1) hh_lrn1 = tf.nn.lrn(h_pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75) # 第二层 w_conv2 = weight_variable([1, 1, 64, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(hh_lrn1, w_conv2) + b_conv2) w_conv3 = weight_variable([3, 3, 64, 192]) b_conv3 = bias_variable([192]) h_conv3 = tf.nn.relu(conv2d_v(h_conv2, w_conv3) + b_conv3) h_lrn2 = tf.nn.lrn(h_conv3, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75) h_pool2 = max_pool_3x3(h_lrn2) TensorFow的局部响应归一化函数：tf.nn.lrn()的使用可在 TensorFlow笔记（9） ResNet 中查看 第三阶段 inception3 第三阶段的过程：Previous Activation &gt; inception 3a &gt; inception 3b &gt; max_pool 3x3+2s # inception 3a：conv 1x1 64 + conv 1x1 96 &gt; conv 3x3 128 + conv 1x1 16 &gt; conv 3x3 32 + max_pool 3x3 &gt; conv 3x3 32 filters_num_3a = [64, 96, 128, 16, 32, 32] # 设置inception 3a的通道数 inception_3a = inception(h_pool2, 192, filters_num_3a) # inception 3b：conv 1x1 128 + conv 1x1 128 &gt; conv 3x3 192 + conv 1x1 32 &gt; conv 3x3 96 + max_pool 3x3 &gt; conv 3x3 64 filters_num_3b = [128, 128, 192, 32, 96, 64] inception_3b = inception(inception_3a, 256, filters_num_3b) h_pool3 = max_pool_3x3_2(inception_3b) 第四阶段 inception4 第四阶段的过程：Previous Activation &gt; inception 4a &gt; inception 4b &gt; inception 4c+(softmax0) &gt; inception 4d &gt; inception 4e+(softmax1) &gt; max_pool 3x3+2s # inception 4a filters_num_4a = [192, 96, 208, 16, 48, 64] inception_4a = inception(h_pool3, 480, filters_num_4a) # inception 4b filters_num_4b = [160, 112, 224, 24, 64, 64] inception_4b = inception(inception_4a, 512, filters_num_4b) # softmax0 h_pool4 = avg_pool_5x5(inception_4a) # &gt; conv 1x1 128 w_conv4 = weight_variable([1, 1, 512, 128]) b_conv4 = bias_variable([128]) h_conv4 = tf.nn.relu(conv2d(h_pool4, w_conv4) + b_conv4) h_flat1 = tf.reshape(h_conv4, shape=[-1, 4 * 4 * 128]) # 重新展开 # &gt; fc 1x1 1024 W_fc1 = weight_variable([4 * 4 * 128, 1024]) b_fc1 = bias_variable([1024]) h_fc1 = tf.nn.relu(tf.matmul(h_flat1, W_fc1) + b_fc1) # &gt; fc 1x1 10 dropout_rate0 = tf.placeholder(&quot;float&quot;) h_fc1_drop = tf.nn.dropout(h_fc1, rate=dropout_rate0) W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) h_fc2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 # &gt; softmax0 pred0 = tf.nn.softmax(h_fc2) # inception 4c filters_num_4c = [128, 128, 256, 24, 64, 64] inception_4c = inception(inception_4b, 512, filters_num_4c) # inception 4d filters_num_4d = [112, 144, 288, 32, 64, 64] inception_4d = inception(inception_4c, 512, filters_num_4d) # inception 4e filters_num_4e = [256, 160, 320, 32, 128, 128] inception_4e = inception(inception_4d, 528, filters_num_4e) h_pool5 = max_pool_3x3_2(inception_4e) # softmax1 h_pool6 = avg_pool_5x5(inception_4d) # &gt; conv 1x1 128 w_conv5 = weight_variable([1, 1, 528, 128]) b_conv5 = bias_variable([128]) h_conv5 = tf.nn.relu(conv2d(h_pool6, w_conv5) + b_conv5) h_flat2 = tf.reshape(h_conv5, shape=[-1, 4 * 4 * 128]) # 重新展开 # &gt; fc 1x1 1024 W_fc3 = weight_variable([4 * 4 * 128, 1024]) b_fc3 = bias_variable([1024]) h_fc3 = tf.nn.relu(tf.matmul(h_flat2, W_fc3) + b_fc3) # &gt; fc 1x1 10 dropout_rate1 = tf.placeholder(&quot;float&quot;) h_fc3_drop = tf.nn.dropout(h_fc1, rate=dropout_rate1) W_fc4 = weight_variable([1024, 10]) b_fc4 = bias_variable([10]) h_fc4 = tf.matmul(h_fc3_drop, W_fc4) + b_fc4 # &gt; softmax1 pred1 = tf.nn.softmax(h_fc4) TensorFow的dropout 函数：tf.nn.dropout()的使用可在TensorFlow笔记（9） ResNet中查看 第五阶段 最终输出 第五阶段的过程：inception 5a &gt; inception 5b &gt; avg_pool_7x7 &gt; softmax2 # inception 5a filters_num_5a = [256, 160, 320, 32, 128, 128] inception_5a = inception(h_pool5, 832, filters_num_5a) # inception 5b filters_num_5b = [384, 192, 384, 48, 128, 128] inception_5b = inception(inception_5a, 832, filters_num_5b) # avg_pool_7x7 h_pool7 = avg_pool_7x7(inception_5b) h_flat3 = tf.reshape(h_pool7, shape=[-1, 1 * 1 * 1024]) # 重新展开 # softmax2 dropout_rate2 = tf.placeholder(&quot;float&quot;) h_pool7_drop = tf.nn.dropout(h_flat3, rate=dropout_rate2) W_fc5 = weight_variable([1024, 10]) b_fc5 = bias_variable([10]) h_fc5 = tf.matmul(h_pool7_drop, W_fc5) + b_fc2 pred2 = tf.nn.softmax(h_fc5) 整合softmax forward = h_fc5 * 0.4 + h_fc4 * 0.3 + h_fc2 * 0.3 pred = pred2 * 0.4 + pred1 * 0.3 + pred0 * 0.3 定义损失函数 使用TensoFlow提供的结合Softmax的交叉熵损失函数定义方法：softmax_cross_entropy_with_logits_v2 交叉熵损失函数其实就是逻辑回归损失函数的前半部 − y ∗ l o g ( p r e d ) - y * log(pred) −y∗log(pred) 忽略了 − ( 1 − y ) ∗ l o g ( 1 − p r e d ) -(1 - y) * log(1 - pred) −(1−y)∗log(1−pred) with tf.name_scope(&quot;LossFunction&quot;): loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=forward, labels=y)) 4. 训练模型 设置超参数： train_epochs = 20 # 迭代次数 learning_rate = 0.001 # 学习率 定义Adam优化器，设置学习率和优化目标损失最小化： optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function) 定义预测类别匹配情况 correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) tf.equal(A, B) ：对比这两个矩阵或者向量的相等的元素，相等返回 True，相反返回 False tf.argmax(input,axis) ：根据axis取值的不同返回每行或者每列最大值的索引，axis 表示维度，0：第一维度（行），1：第二维度（列），-1：最后一个维度 其实，这里的最终求得的索引，恰好就表示图片上的数字 定义准确率，将布尔值转化成浮点数，再求平均值 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 读取模型与创建会话： # 定义保存模型 saver = tf.train.Saver() save_dir = &quot;../save_path/GoogleNet/&quot; # 定义保存模型编号 save_step = 0 # 恢复保存模型 ckpt_dir = tf.train.latest_checkpoint(save_dir) sess = tf.Session() # 建立会话 if ckpt_dir != None: saver.restore(sess, ckpt_dir) print(&quot;Finished loading&quot;, ckpt_dir) save_step = int(input(&quot;Set the load save step:&quot;)) else: # 变量初始化 sess.run(tf.initialize_all_variables()) print(&quot;Finished initialize&quot;) 关于如何保存模型，可在TensorFlow笔记（10） CheckPoint中查看 如果是读取模型继续训练时，例如最近保存的是序号12的模型 # Finished loading ../save_path/GoogleNet/model-12 Set the load save step: 则输入现在加载的模型序号12，然后回车，之后的训练保存的模型就从13开始 Set the load save step:12 设置批次大小和数量： 如果在处理完整个5万个训练图片的训练集之后才进行一次训练 这样的处理速度相对缓慢 如果在处理完整个5万个训练图片的训练集之前先让梯度下降法处理一部分 算法速度会更快 可以把训练集分割为小一点的子集训练 如128张训练图片，然后就进行梯度下降法处理 这种梯度下降法处理方法称之为Mini-batch 梯度下降 具体可参考深度学习笔记（9） 优化算法（一） # 每个批次的大小，每次放入的大小，每次放入 128张图片 以矩阵的方式 batch_size = 128 # 计算一共有多少个批次，数量整除大小训练出有多少批次 n_batch = len(Ytrain_onehot) // batch_size # 定义显示训练过程中验证的间隔批量次数 display_test_num = n_batch // 10 # 定义显示训练过程.的间隔批量次数 display_train_num = display_test_num // 10 # 定义批次训练数据的占位符 x_batch = tf.placeholder(tf.float32, [None, 32, 32, 3], name=&quot;X_batch&quot;) # 随机上下翻转批次图片 Xtrain_batch_up = tf.image.random_flip_up_down(x_batch) # 随机左右翻转批次图片 Xtrain_batch_lf = tf.image.random_flip_left_right(Xtrain_batch_up) # 对角旋转批次图片 # Xtrain_batch_tp = tf.image.transpose_image(x_batch) # 定义训练集批次函数 def get_train_batch(num, size): Xtrain_batch = Xtrain_norm_shuffle[num * size:(num + 1) * size] Ytrain_batch = Ytrain_onehot_shuffle[num * size:(num + 1) * size] # 随机翻转数据 with tf.Session() as sess_batch: Xtrain_batch = sess_batch.run(Xtrain_batch_lf, feed_dict={x_batch: Xtrain_batch}) return Xtrain_batch, Ytrain_batch 批次迭代训练，其中 dropout 随机丢弃的概率都为0.4，显示迭代过程中的信息： for epoch in range(train_epochs): # 打乱训练数据集 index = [i for i in range(len(Ytrain_onehot))] random.shuffle(index) Xtrain_norm_shuffle = Xtrain_norm[index] Ytrain_onehot_shuffle = Ytrain_onehot[index] # 批次迭代训练 for batch in range(0, n_batch): xs, ys = get_train_batch(batch, batch_size) sess.run(optimizer, feed_dict={x: xs, y: ys, dropout_rate0: 0.4, dropout_rate1: 0.4, dropout_rate2: 0.4}) if (batch + 1) % display_train_num == 0: print(&quot;.&quot;, end=&quot;&quot;) if (batch + 1) % display_test_num == 0: # 保存模型 save_step += 1 save_path = saver.save(sess, save_dir + &quot;model&quot;, global_step=save_step) print(&quot;Complete save &quot;, save_path) # 批次训练完成之后，使用测试数据计算误差与准确率 loss, acc = sess.run([loss_function, accuracy], feed_dict={x: Xtest_norm[0:512], y: Ytest_onehot[0:512], dropout_rate0: 0, dropout_rate1: 0, dropout_rate2: 0}) learning_rate = 0.95 * learning_rate # 学习率衰减 # ..........Complete save D:/save_path/GoogleNet/model-1 # TrainEpoch= 01 TrainBatch= 0039 Loss= 1.958486080 TestAccuracy= 0.1973 # ... # ..........Complete save D:/save_path/GoogleNet/model-10 # TrainEpoch= 01 TrainBatch= 0390 Loss= 1.628237653 TestAccuracy= 0.4268 # ... # ..........Complete save D:/save_path/GoogleNet/model-200 # TrainEpoch= 20 TrainBatch= 0390 Loss= 0.510145028 TestAccuracy= 0.8066 数据训练量比较大，而设备有限，为了保护设备而断断续续的训练 过一遍数据，抽取测试集前512张图片分类的准确率到达42.68% 最后一共历遍训练集20次，抽取测试集前512张图片分类的准确率到达 80.66% 5. 评估模型 测试集上评估模型预测的准确率 test_total_batch = int(len(Xtest_norm) / batch_size) test_acc_sum = 0.0 for i in range(test_total_batch): test_image_batch = Xtest_norm[i * batch_size:(i + 1) * batch_size] test_label_batch = Ytest_onehot[i * batch_size:(i + 1) * batch_size] test_batch_acc = sess.run(accuracy, feed_dict={x: test_image_batch, y: test_label_batch, dropout_rate0: 0, dropout_rate1: 0, dropout_rate2: 0}) test_acc_sum += test_batch_acc test_acc = float(test_acc_sum / test_total_batch) print(&quot;Test accuracy:{:.6f}&quot;.format(test_acc)) # Test accuracy:0.804688 测试集的准确率达到80% 6. 模型预测 查看预测结果 # 转换第1-10张测试图片pred预测结果独热编码格式为数字0-9 prediction_result = sess.run(tf.argmax(pred, 1), feed_dict={x: Xtest_normalize[0:10], dropout_rate0: 0, dropout_rate1: 0, dropout_rate2: 0}) # 查看第1-10张测试图片的预测结果 print(prediction_result) # [3 8 8 0 6 6 1 6 3 1] 但是这样没办法知道，预测的到底是不是正确的 预测结果可视化比对 定义可视化函数： def plot_images_labels_prediction(images, labels, prediction, idx, num=10): fig = plt.gcf() fig.set_size_inches(12, 6) if num &gt; 10: num = 10 for i in range(0, num): ax = plt.subplot(2, 5, 1 + i) ax.imshow(images[idx], cmap=&quot;binary&quot;) title = str(i) + &#39;,&#39; + label_dict[labels[idx]] if len(prediction) &gt; 0: title += &#39;=&gt;&#39; + label_dict[prediction[idx]] ax.set_title(title, fontsize=10) idx += 1 plt.show() 可视化第1-10张测试图片的预测结果对比 plot_images_labels_prediction(Xtest, Ytest, rediction_result, 0, 10) 这次的预测都判断正确，还行。 [1] python的代码地址: https://github.com/JoveH-H/TensorFlow/blob/master/py/8.GoogleNet.py [2] jupyter notebook的代码地址: （待补充） [3] MNIST 数据集 t10k-images-idx3-ubyte.gz http://www.cs.toronto.edu/~kriz/cifar.html 相关推荐： 深度学习笔记（30） Inception网络 深度学习笔记（26） 卷积神经网络 TensorFlow笔记（10） CheckPoint TensorFlow笔记（9） ResNet TensorFlow笔记（8） LeNet-5卷积神经网络 谢谢！" />
<meta property="og:description" content="TensorFlow笔记（11） GoolgeNet 1. Inception块 2. 数据读取 3. 构建模型 4. 训练模型 5. 评估模型 6. 模型预测 1. Inception块 根据 深度学习笔记（30） Inception网络 可以了解到 可以利用Inception块构建经典网络InceptionNet，也就是GoolgeNet 那么，以CIFAR-10 数据集的分类为例，采用GoolgeNet模型来解决问题 2. 数据读取 CIFAR-10 数据集的分类是机器学习中一个公开的基准测试问题 其任务是对一组32x32RGB的图像进行分类，这些图像涵盖了10个类别： 飞机， 汽车， 鸟， 猫， 鹿， 狗， 青蛙， 马， 船以及卡车 利用网上的 CIFAR-10 数据集 获取数据集压缩文件： 这里使用的是python版本 CIFAR-10 python version 载入数据集合： import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import os import urllib.request import tarfile import pickle as p from sklearn.preprocessing import OneHotEncoder # 下载 cifar10 url = &#39;https://www.cs.toronto.edu/-kriz/cifar-10-python.tar.gz&#39; filepath = &#39;../data/cifar-10-python.tar.gz&#39; if not os.path.isfile(filepath): result = urllib.request.urlretrieve(url, filepath) print(&#39;downloaded&#39;, result) else: print(&#39;Data file already exists.&#39;) # 解压 cifar10 if not os.path.exists(&quot;../data/cifar-10-batches-py&quot;): tfile = tarfile.open(&quot;../data/cifar-10-python.tar.gz&quot;, &#39;r:gz&#39;) result = tfile.extractall(&quot;../data/&quot;) print(&#39;Extracted to ./../data/cifar-10-batches-py/&#39;) else: print(&#39;Directory already exists.&#39;) 下载并解压数据集 当然可以自己在 CIFAR-10 数据集 网站上下载在工程的data文件夹下，然后解压 加载数据集 def load_CIFAR_batch(filename): &quot;&quot;&quot;oad single batch of cifar&quot;&quot;&quot; with open(filename, &#39;rb&#39;)as f: # 一个样本由标签和图像数据组成 # &lt;1 xlabel&gt;&lt;3072 xpixel&gt; (3072-32x32x3) data_dict = p.load(f, encoding=&#39;bytes&#39;) images = data_dict[b&#39;data&#39;] labels = data_dict[b&#39;labels&#39;] # 把原始数据结构调整为: BCWH images = images.reshape(10000, 3, 32, 32) # tensorflow处理图像数据的结构: BWHC # #把通道数据C移动到最后一个维度 images = images.transpose(0, 2, 3, 1) labels = np.array(labels) return images, labels def load_CIFAR_data(data_dir): &quot;&quot;&quot;load CIFAR data&quot;&quot;&quot; images_train = [] labels_train = [] for i in range(5): f = os.path.join(data_dir, &#39;data_batch_{0}&#39;.format(i + 1)) print(&#39;loading&#39;, f) # 调用loadCIFARbatch(获得批量的图像及其对应的标签 image_batch, label_batch = load_CIFAR_batch(f) images_train.append(image_batch) labels_train.append(label_batch) Xtrain = np.concatenate(images_train) Ytrain = np.concatenate(labels_train) del image_batch, label_batch Xtest, Ytest = load_CIFAR_batch(os.path.join(data_dir, &#39;test_batch&#39;)) print(&#39;finished loadding CIFAR-10 data&#39;) # 返回训练集的图象和标签，测试集的图像和标签 return Xtrain, Ytrain, Xtest, Ytest data_dir = &#39;../data/cifar-10-batches-py/&#39; Xtrain, Ytrain, Xtest, Ytest = load_CIFAR_data(data_dir) # loading ../data/cifar-10-batches-py/data_batch_1 # loading ../data/cifar-10-batches-py/data_batch_2 # loading ../data/cifar-10-batches-py/data_batch_3 # loading ../data/cifar-10-batches-py/data_batch_4 # loading ../data/cifar-10-batches-py/data_batch_5 # finished ../loadding CIFAR-10 data 查看数据集数量 print(&#39;training data shape:&#39;, Xtrain.shape) print(&#39;training labels shape:&#39;, Ytrain.shape) print(&#39;test data shape:&#39;, Xtest.shape) print(&#39;test labels shape:&#39;, Ytest.shape) # training data shape: (50000, 32, 32, 3) # training labels shape: (50000,) # test data shape: (10000, 32, 32, 3) # test labels shape: (10000,) 32x32 RGB3通道尺寸图片，训练图片50000张，测试图片10000张， 定义标签字典 对应类别信息查看：http://www.cs.toronto.edu/~kriz/cifar.html label_dict = {0: &quot;airplane&quot;, 1: &quot;automobile&quot;, 2: &quot;bird&quot;, 3: &quot;cat&quot;, 4: &quot;deer&quot;, 5: &quot;dog&quot;, 6: &quot;frog&quot;, 7: &quot;horse&quot;, 8: &quot;ship&quot;, 9: &quot;truck&quot;} 查看第11张图片并查看对应的label imshow_num = 10 plt.imshow(Xtrain[imshow_num]) plt.show() print(&quot;Xtrain[{0}] label:&quot;.format(imshow_num), label_dict[Ytrain[imshow_num]]) # Xtrain[10] label: deer 虽然模模糊糊，但还可以猜出来是十个类别中的鹿 图片进行数字标准化并对比数据 Xtrain_mean0 = np.mean(Xtrain.astype(&#39;float32&#39;)[:, :, :, 0]) Xtrain_mean1 = np.mean(Xtrain.astype(&#39;float32&#39;)[:, :, :, 1]) Xtrain_mean2 = np.mean(Xtrain.astype(&#39;float32&#39;)[:, :, :, 2]) Xtrain_mean = [Xtrain_mean0, Xtrain_mean1, Xtrain_mean2] Xtrain_std0 = np.std(Xtrain.astype(&#39;float32&#39;)[:, :, :, 0]) Xtrain_std1 = np.std(Xtrain.astype(&#39;float32&#39;)[:, :, :, 1]) Xtrain_std2 = np.std(Xtrain.astype(&#39;float32&#39;)[:, :, :, 2]) Xtrain_std = [Xtrain_std0, Xtrain_std1, Xtrain_std2] def data_norm(x, x_mean, x_std): x_norm = x.astype(&#39;float32&#39;) x_norm[:, :, :, 0] = (x_norm[:, :, :, 0] - x_mean[0]) / x_std[0] x_norm[:, :, :, 1] = (x_norm[:, :, :, 1] - x_mean[1]) / x_std[1] x_norm[:, :, :, 2] = (x_norm[:, :, :, 2] - x_mean[2]) / x_std[2] return x_norm Xtrain_norm = data_norm(Xtrain, Xtrain_mean, Xtrain_std) Xtest_norm = data_norm(Xtest, Xtrain_mean, Xtrain_std) # 对比图像数据 print(&quot;Xtrain[0][0][0] data:&quot;, Xtrain[0][0][0]) print(&quot;Xtrain_norm[0][0][0] data:&quot;, Xtrain_norm[0][0][0]) # Xtrain[0][0][0] data: [59 62 63] # Xtrain_normalize[0][0][0] data: [-1.0526042 -0.9816644 -0.7625441] 现在标准化过的数据Xtrain_norm的对应的32x32x3个像素均值为0，标准差为1 测试集的标准化设置的均值和方差需以训练集为准 不然训练的模型对测试图片作用就没那么好了 标签数据处理并对比数据 encoder = OneHotEncoder(sparse=False, categories=&#39;auto&#39;) yy = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]] encoder.fit(yy) Ytrain_reshape = Ytrain.reshape(-1, 1) Ytrain_onehot = encoder.transform(Ytrain_reshape) Ytest_reshape = Ytest.reshape(-1, 1) Ytest_onehot = encoder.transform(Ytest_reshape) print(&quot;Ytrain[10] data:&quot;, Ytrain[10]) print(&quot;Ytrain_onehot[10] data:&quot;, Ytrain_onehot[10]) # Ytrain[10] data: 4 # Ytrain_onehot[10] data: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] 现在的标签数据Ytrain_onehot，Ytest_onehot 已采用独热编码形式 3. 构建模型 定义训练数据的占位符， x是32x32x3个像素点的特征值， y是10分类的标签值： x = tf.placeholder(tf.float32, [None, 32, 32, 3], name=&quot;X&quot;) y = tf.placeholder(tf.float32, [None, 10], name=&quot;Y&quot;) shape中 None 表示行的数量未知 在实际训练时决定一次代入多少行样本 展开图片 x 为了使用卷积层，需把x变成一个4d向量 其第1维对应样本数， -1表示任意数量 其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数 x_image = tf.reshape(x, [-1, 32, 32, 3]) 定义权重初始化函数： 定义权重W 初始化函数 ：从标准差0.1的截断正态分布中输出随机值 标准正态分布生生成的数据在负无穷到正无穷 但是截断式正态分布生成的数据在均值-2倍的标准差，均值+2倍的标准差这个范围内def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) 定义权重b 初始化函数 ：数值为0.1def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 定义 卷积 函数：# 定义1步长的 same卷积 def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # 定义1步长的 valid卷积 def conv2d_v(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;VALID&#39;) # 定义2步长的 same卷积 def conv2d_2(x, W): return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) TensorFow的卷积函数：tf.nn.conv2d()用法可在TensorFlow笔记（8） LeNet-5卷积神经网络中查看 定义 池化 函数：# 定义步长为1 大小3x3的 max pooling def max_pool_3x3(x): return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # 定义步长为2 大小3x3的 max pooling def max_pool_3x3_2(x): return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) # 定义步长为3 大小5x5的 avg pooling def avg_pool_5x5(x): return tf.nn.avg_pool(x, ksize=[1, 5, 5, 1], strides=[1, 3, 3, 1], padding=&#39;VALID&#39;) # 定义步长为1 大小7x7的 avg pooling def avg_pool_7x7(x): return tf.nn.avg_pool(x, ksize=[1, 7, 7, 1], strides=[1, 1, 1, 1], padding=&#39;VALID&#39;) TensorFow的池化函数：平均池化tf.nn.avg_pool()与最大池化tf.nn.max_pool()用法相似 同样可在TensorFlow笔记（8） LeNet-5卷积神经网络中查看 配置inception块： def inception(x, channel_in, filters_num): # 第一分支 conv 1x1+1s with tf.variable_scope(&quot;branch1&quot;): branch1_w_conv1 = weight_variable([1, 1, channel_in, filters_num[0]]) branch1_b_conv1 = bias_variable([filters_num[0]]) branch1_h_conv1 = tf.nn.relu(conv2d(x, branch1_w_conv1) + branch1_b_conv1) # 第二分支 conv 1x1+1s &gt; conv 3x3+1s with tf.variable_scope(&quot;branch2&quot;): branch2_w_conv1 = weight_variable([1, 1, channel_in, filters_num[1]]) branch2_b_conv1 = bias_variable([filters_num[1]]) branch2_h_conv1 = tf.nn.relu(conv2d(x, branch2_w_conv1) + branch2_b_conv1) branch2_w_conv2 = weight_variable([3, 3, filters_num[1], filters_num[2]]) branch2_b_conv2 = bias_variable([filters_num[2]]) branch2_h_conv2 = tf.nn.relu(conv2d(branch2_h_conv1, branch2_w_conv2) + branch2_b_conv2) # 第三分支 conv 1x1+1s &gt; conv 5x5+1s with tf.variable_scope(&quot;branch3&quot;): branch3_w_conv1 = weight_variable([1, 1, channel_in, filters_num[3]]) branch3_b_conv1 = bias_variable([filters_num[3]]) branch3_h_conv1 = tf.nn.relu(conv2d(x, branch3_w_conv1) + branch3_b_conv1) branch3_w_conv2 = weight_variable([5, 5, filters_num[3], filters_num[4]]) branch3_b_conv2 = bias_variable([filters_num[4]]) branch3_h_conv2 = tf.nn.relu(conv2d(branch3_h_conv1, branch3_w_conv2) + branch3_b_conv2) # 第四分支 max_pool 3x3+1s &gt; conv 1x1+1s with tf.variable_scope(&quot;branch4&quot;): branch4_pool1 = max_pool_3x3(x) branch4_w_conv1 = weight_variable([1, 1, channel_in, filters_num[5]]) branch4_b_conv1 = bias_variable([filters_num[5]]) branch4_h_conv1 = tf.nn.relu(conv2d(branch4_pool1, branch4_w_conv1) + branch4_b_conv1) # 输出叠加 with tf.variable_scope(&quot;depth_concat&quot;): concat = tf.concat([branch1_h_conv1, branch2_h_conv2, branch3_h_conv2, branch4_h_conv1], axis=3) return concat 第一二层：压缩图片 第一二层为什么混着来呢，因为其实作用都是调整和预处理图片 以确保第一次进入inception块时为28x28x192的尺寸 而cifar10图片尺寸较小，为配合后续的计算，调整一些原有结构压缩成28x28x192 # 第一层 w_conv1 = weight_variable([3, 3, 3, 64]) b_conv1 = bias_variable([64]) h_conv1 = tf.nn.relu(conv2d_v(x_image, w_conv1) + b_conv1) h_pool1 = max_pool_3x3(h_conv1) hh_lrn1 = tf.nn.lrn(h_pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75) # 第二层 w_conv2 = weight_variable([1, 1, 64, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(hh_lrn1, w_conv2) + b_conv2) w_conv3 = weight_variable([3, 3, 64, 192]) b_conv3 = bias_variable([192]) h_conv3 = tf.nn.relu(conv2d_v(h_conv2, w_conv3) + b_conv3) h_lrn2 = tf.nn.lrn(h_conv3, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75) h_pool2 = max_pool_3x3(h_lrn2) TensorFow的局部响应归一化函数：tf.nn.lrn()的使用可在 TensorFlow笔记（9） ResNet 中查看 第三阶段 inception3 第三阶段的过程：Previous Activation &gt; inception 3a &gt; inception 3b &gt; max_pool 3x3+2s # inception 3a：conv 1x1 64 + conv 1x1 96 &gt; conv 3x3 128 + conv 1x1 16 &gt; conv 3x3 32 + max_pool 3x3 &gt; conv 3x3 32 filters_num_3a = [64, 96, 128, 16, 32, 32] # 设置inception 3a的通道数 inception_3a = inception(h_pool2, 192, filters_num_3a) # inception 3b：conv 1x1 128 + conv 1x1 128 &gt; conv 3x3 192 + conv 1x1 32 &gt; conv 3x3 96 + max_pool 3x3 &gt; conv 3x3 64 filters_num_3b = [128, 128, 192, 32, 96, 64] inception_3b = inception(inception_3a, 256, filters_num_3b) h_pool3 = max_pool_3x3_2(inception_3b) 第四阶段 inception4 第四阶段的过程：Previous Activation &gt; inception 4a &gt; inception 4b &gt; inception 4c+(softmax0) &gt; inception 4d &gt; inception 4e+(softmax1) &gt; max_pool 3x3+2s # inception 4a filters_num_4a = [192, 96, 208, 16, 48, 64] inception_4a = inception(h_pool3, 480, filters_num_4a) # inception 4b filters_num_4b = [160, 112, 224, 24, 64, 64] inception_4b = inception(inception_4a, 512, filters_num_4b) # softmax0 h_pool4 = avg_pool_5x5(inception_4a) # &gt; conv 1x1 128 w_conv4 = weight_variable([1, 1, 512, 128]) b_conv4 = bias_variable([128]) h_conv4 = tf.nn.relu(conv2d(h_pool4, w_conv4) + b_conv4) h_flat1 = tf.reshape(h_conv4, shape=[-1, 4 * 4 * 128]) # 重新展开 # &gt; fc 1x1 1024 W_fc1 = weight_variable([4 * 4 * 128, 1024]) b_fc1 = bias_variable([1024]) h_fc1 = tf.nn.relu(tf.matmul(h_flat1, W_fc1) + b_fc1) # &gt; fc 1x1 10 dropout_rate0 = tf.placeholder(&quot;float&quot;) h_fc1_drop = tf.nn.dropout(h_fc1, rate=dropout_rate0) W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) h_fc2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 # &gt; softmax0 pred0 = tf.nn.softmax(h_fc2) # inception 4c filters_num_4c = [128, 128, 256, 24, 64, 64] inception_4c = inception(inception_4b, 512, filters_num_4c) # inception 4d filters_num_4d = [112, 144, 288, 32, 64, 64] inception_4d = inception(inception_4c, 512, filters_num_4d) # inception 4e filters_num_4e = [256, 160, 320, 32, 128, 128] inception_4e = inception(inception_4d, 528, filters_num_4e) h_pool5 = max_pool_3x3_2(inception_4e) # softmax1 h_pool6 = avg_pool_5x5(inception_4d) # &gt; conv 1x1 128 w_conv5 = weight_variable([1, 1, 528, 128]) b_conv5 = bias_variable([128]) h_conv5 = tf.nn.relu(conv2d(h_pool6, w_conv5) + b_conv5) h_flat2 = tf.reshape(h_conv5, shape=[-1, 4 * 4 * 128]) # 重新展开 # &gt; fc 1x1 1024 W_fc3 = weight_variable([4 * 4 * 128, 1024]) b_fc3 = bias_variable([1024]) h_fc3 = tf.nn.relu(tf.matmul(h_flat2, W_fc3) + b_fc3) # &gt; fc 1x1 10 dropout_rate1 = tf.placeholder(&quot;float&quot;) h_fc3_drop = tf.nn.dropout(h_fc1, rate=dropout_rate1) W_fc4 = weight_variable([1024, 10]) b_fc4 = bias_variable([10]) h_fc4 = tf.matmul(h_fc3_drop, W_fc4) + b_fc4 # &gt; softmax1 pred1 = tf.nn.softmax(h_fc4) TensorFow的dropout 函数：tf.nn.dropout()的使用可在TensorFlow笔记（9） ResNet中查看 第五阶段 最终输出 第五阶段的过程：inception 5a &gt; inception 5b &gt; avg_pool_7x7 &gt; softmax2 # inception 5a filters_num_5a = [256, 160, 320, 32, 128, 128] inception_5a = inception(h_pool5, 832, filters_num_5a) # inception 5b filters_num_5b = [384, 192, 384, 48, 128, 128] inception_5b = inception(inception_5a, 832, filters_num_5b) # avg_pool_7x7 h_pool7 = avg_pool_7x7(inception_5b) h_flat3 = tf.reshape(h_pool7, shape=[-1, 1 * 1 * 1024]) # 重新展开 # softmax2 dropout_rate2 = tf.placeholder(&quot;float&quot;) h_pool7_drop = tf.nn.dropout(h_flat3, rate=dropout_rate2) W_fc5 = weight_variable([1024, 10]) b_fc5 = bias_variable([10]) h_fc5 = tf.matmul(h_pool7_drop, W_fc5) + b_fc2 pred2 = tf.nn.softmax(h_fc5) 整合softmax forward = h_fc5 * 0.4 + h_fc4 * 0.3 + h_fc2 * 0.3 pred = pred2 * 0.4 + pred1 * 0.3 + pred0 * 0.3 定义损失函数 使用TensoFlow提供的结合Softmax的交叉熵损失函数定义方法：softmax_cross_entropy_with_logits_v2 交叉熵损失函数其实就是逻辑回归损失函数的前半部 − y ∗ l o g ( p r e d ) - y * log(pred) −y∗log(pred) 忽略了 − ( 1 − y ) ∗ l o g ( 1 − p r e d ) -(1 - y) * log(1 - pred) −(1−y)∗log(1−pred) with tf.name_scope(&quot;LossFunction&quot;): loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=forward, labels=y)) 4. 训练模型 设置超参数： train_epochs = 20 # 迭代次数 learning_rate = 0.001 # 学习率 定义Adam优化器，设置学习率和优化目标损失最小化： optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function) 定义预测类别匹配情况 correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) tf.equal(A, B) ：对比这两个矩阵或者向量的相等的元素，相等返回 True，相反返回 False tf.argmax(input,axis) ：根据axis取值的不同返回每行或者每列最大值的索引，axis 表示维度，0：第一维度（行），1：第二维度（列），-1：最后一个维度 其实，这里的最终求得的索引，恰好就表示图片上的数字 定义准确率，将布尔值转化成浮点数，再求平均值 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 读取模型与创建会话： # 定义保存模型 saver = tf.train.Saver() save_dir = &quot;../save_path/GoogleNet/&quot; # 定义保存模型编号 save_step = 0 # 恢复保存模型 ckpt_dir = tf.train.latest_checkpoint(save_dir) sess = tf.Session() # 建立会话 if ckpt_dir != None: saver.restore(sess, ckpt_dir) print(&quot;Finished loading&quot;, ckpt_dir) save_step = int(input(&quot;Set the load save step:&quot;)) else: # 变量初始化 sess.run(tf.initialize_all_variables()) print(&quot;Finished initialize&quot;) 关于如何保存模型，可在TensorFlow笔记（10） CheckPoint中查看 如果是读取模型继续训练时，例如最近保存的是序号12的模型 # Finished loading ../save_path/GoogleNet/model-12 Set the load save step: 则输入现在加载的模型序号12，然后回车，之后的训练保存的模型就从13开始 Set the load save step:12 设置批次大小和数量： 如果在处理完整个5万个训练图片的训练集之后才进行一次训练 这样的处理速度相对缓慢 如果在处理完整个5万个训练图片的训练集之前先让梯度下降法处理一部分 算法速度会更快 可以把训练集分割为小一点的子集训练 如128张训练图片，然后就进行梯度下降法处理 这种梯度下降法处理方法称之为Mini-batch 梯度下降 具体可参考深度学习笔记（9） 优化算法（一） # 每个批次的大小，每次放入的大小，每次放入 128张图片 以矩阵的方式 batch_size = 128 # 计算一共有多少个批次，数量整除大小训练出有多少批次 n_batch = len(Ytrain_onehot) // batch_size # 定义显示训练过程中验证的间隔批量次数 display_test_num = n_batch // 10 # 定义显示训练过程.的间隔批量次数 display_train_num = display_test_num // 10 # 定义批次训练数据的占位符 x_batch = tf.placeholder(tf.float32, [None, 32, 32, 3], name=&quot;X_batch&quot;) # 随机上下翻转批次图片 Xtrain_batch_up = tf.image.random_flip_up_down(x_batch) # 随机左右翻转批次图片 Xtrain_batch_lf = tf.image.random_flip_left_right(Xtrain_batch_up) # 对角旋转批次图片 # Xtrain_batch_tp = tf.image.transpose_image(x_batch) # 定义训练集批次函数 def get_train_batch(num, size): Xtrain_batch = Xtrain_norm_shuffle[num * size:(num + 1) * size] Ytrain_batch = Ytrain_onehot_shuffle[num * size:(num + 1) * size] # 随机翻转数据 with tf.Session() as sess_batch: Xtrain_batch = sess_batch.run(Xtrain_batch_lf, feed_dict={x_batch: Xtrain_batch}) return Xtrain_batch, Ytrain_batch 批次迭代训练，其中 dropout 随机丢弃的概率都为0.4，显示迭代过程中的信息： for epoch in range(train_epochs): # 打乱训练数据集 index = [i for i in range(len(Ytrain_onehot))] random.shuffle(index) Xtrain_norm_shuffle = Xtrain_norm[index] Ytrain_onehot_shuffle = Ytrain_onehot[index] # 批次迭代训练 for batch in range(0, n_batch): xs, ys = get_train_batch(batch, batch_size) sess.run(optimizer, feed_dict={x: xs, y: ys, dropout_rate0: 0.4, dropout_rate1: 0.4, dropout_rate2: 0.4}) if (batch + 1) % display_train_num == 0: print(&quot;.&quot;, end=&quot;&quot;) if (batch + 1) % display_test_num == 0: # 保存模型 save_step += 1 save_path = saver.save(sess, save_dir + &quot;model&quot;, global_step=save_step) print(&quot;Complete save &quot;, save_path) # 批次训练完成之后，使用测试数据计算误差与准确率 loss, acc = sess.run([loss_function, accuracy], feed_dict={x: Xtest_norm[0:512], y: Ytest_onehot[0:512], dropout_rate0: 0, dropout_rate1: 0, dropout_rate2: 0}) learning_rate = 0.95 * learning_rate # 学习率衰减 # ..........Complete save D:/save_path/GoogleNet/model-1 # TrainEpoch= 01 TrainBatch= 0039 Loss= 1.958486080 TestAccuracy= 0.1973 # ... # ..........Complete save D:/save_path/GoogleNet/model-10 # TrainEpoch= 01 TrainBatch= 0390 Loss= 1.628237653 TestAccuracy= 0.4268 # ... # ..........Complete save D:/save_path/GoogleNet/model-200 # TrainEpoch= 20 TrainBatch= 0390 Loss= 0.510145028 TestAccuracy= 0.8066 数据训练量比较大，而设备有限，为了保护设备而断断续续的训练 过一遍数据，抽取测试集前512张图片分类的准确率到达42.68% 最后一共历遍训练集20次，抽取测试集前512张图片分类的准确率到达 80.66% 5. 评估模型 测试集上评估模型预测的准确率 test_total_batch = int(len(Xtest_norm) / batch_size) test_acc_sum = 0.0 for i in range(test_total_batch): test_image_batch = Xtest_norm[i * batch_size:(i + 1) * batch_size] test_label_batch = Ytest_onehot[i * batch_size:(i + 1) * batch_size] test_batch_acc = sess.run(accuracy, feed_dict={x: test_image_batch, y: test_label_batch, dropout_rate0: 0, dropout_rate1: 0, dropout_rate2: 0}) test_acc_sum += test_batch_acc test_acc = float(test_acc_sum / test_total_batch) print(&quot;Test accuracy:{:.6f}&quot;.format(test_acc)) # Test accuracy:0.804688 测试集的准确率达到80% 6. 模型预测 查看预测结果 # 转换第1-10张测试图片pred预测结果独热编码格式为数字0-9 prediction_result = sess.run(tf.argmax(pred, 1), feed_dict={x: Xtest_normalize[0:10], dropout_rate0: 0, dropout_rate1: 0, dropout_rate2: 0}) # 查看第1-10张测试图片的预测结果 print(prediction_result) # [3 8 8 0 6 6 1 6 3 1] 但是这样没办法知道，预测的到底是不是正确的 预测结果可视化比对 定义可视化函数： def plot_images_labels_prediction(images, labels, prediction, idx, num=10): fig = plt.gcf() fig.set_size_inches(12, 6) if num &gt; 10: num = 10 for i in range(0, num): ax = plt.subplot(2, 5, 1 + i) ax.imshow(images[idx], cmap=&quot;binary&quot;) title = str(i) + &#39;,&#39; + label_dict[labels[idx]] if len(prediction) &gt; 0: title += &#39;=&gt;&#39; + label_dict[prediction[idx]] ax.set_title(title, fontsize=10) idx += 1 plt.show() 可视化第1-10张测试图片的预测结果对比 plot_images_labels_prediction(Xtest, Ytest, rediction_result, 0, 10) 这次的预测都判断正确，还行。 [1] python的代码地址: https://github.com/JoveH-H/TensorFlow/blob/master/py/8.GoogleNet.py [2] jupyter notebook的代码地址: （待补充） [3] MNIST 数据集 t10k-images-idx3-ubyte.gz http://www.cs.toronto.edu/~kriz/cifar.html 相关推荐： 深度学习笔记（30） Inception网络 深度学习笔记（26） 卷积神经网络 TensorFlow笔记（10） CheckPoint TensorFlow笔记（9） ResNet TensorFlow笔记（8） LeNet-5卷积神经网络 谢谢！" />
<link rel="canonical" href="https://uzzz.org/2019/07/30/792380.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/30/792380.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-30T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"TensorFlow笔记（11） GoolgeNet 1. Inception块 2. 数据读取 3. 构建模型 4. 训练模型 5. 评估模型 6. 模型预测 1. Inception块 根据 深度学习笔记（30） Inception网络 可以了解到 可以利用Inception块构建经典网络InceptionNet，也就是GoolgeNet 那么，以CIFAR-10 数据集的分类为例，采用GoolgeNet模型来解决问题 2. 数据读取 CIFAR-10 数据集的分类是机器学习中一个公开的基准测试问题 其任务是对一组32x32RGB的图像进行分类，这些图像涵盖了10个类别： 飞机， 汽车， 鸟， 猫， 鹿， 狗， 青蛙， 马， 船以及卡车 利用网上的 CIFAR-10 数据集 获取数据集压缩文件： 这里使用的是python版本 CIFAR-10 python version 载入数据集合： import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import os import urllib.request import tarfile import pickle as p from sklearn.preprocessing import OneHotEncoder # 下载 cifar10 url = &#39;https://www.cs.toronto.edu/-kriz/cifar-10-python.tar.gz&#39; filepath = &#39;../data/cifar-10-python.tar.gz&#39; if not os.path.isfile(filepath): result = urllib.request.urlretrieve(url, filepath) print(&#39;downloaded&#39;, result) else: print(&#39;Data file already exists.&#39;) # 解压 cifar10 if not os.path.exists(&quot;../data/cifar-10-batches-py&quot;): tfile = tarfile.open(&quot;../data/cifar-10-python.tar.gz&quot;, &#39;r:gz&#39;) result = tfile.extractall(&quot;../data/&quot;) print(&#39;Extracted to ./../data/cifar-10-batches-py/&#39;) else: print(&#39;Directory already exists.&#39;) 下载并解压数据集 当然可以自己在 CIFAR-10 数据集 网站上下载在工程的data文件夹下，然后解压 加载数据集 def load_CIFAR_batch(filename): &quot;&quot;&quot;oad single batch of cifar&quot;&quot;&quot; with open(filename, &#39;rb&#39;)as f: # 一个样本由标签和图像数据组成 # &lt;1 xlabel&gt;&lt;3072 xpixel&gt; (3072-32x32x3) data_dict = p.load(f, encoding=&#39;bytes&#39;) images = data_dict[b&#39;data&#39;] labels = data_dict[b&#39;labels&#39;] # 把原始数据结构调整为: BCWH images = images.reshape(10000, 3, 32, 32) # tensorflow处理图像数据的结构: BWHC # #把通道数据C移动到最后一个维度 images = images.transpose(0, 2, 3, 1) labels = np.array(labels) return images, labels def load_CIFAR_data(data_dir): &quot;&quot;&quot;load CIFAR data&quot;&quot;&quot; images_train = [] labels_train = [] for i in range(5): f = os.path.join(data_dir, &#39;data_batch_{0}&#39;.format(i + 1)) print(&#39;loading&#39;, f) # 调用loadCIFARbatch(获得批量的图像及其对应的标签 image_batch, label_batch = load_CIFAR_batch(f) images_train.append(image_batch) labels_train.append(label_batch) Xtrain = np.concatenate(images_train) Ytrain = np.concatenate(labels_train) del image_batch, label_batch Xtest, Ytest = load_CIFAR_batch(os.path.join(data_dir, &#39;test_batch&#39;)) print(&#39;finished loadding CIFAR-10 data&#39;) # 返回训练集的图象和标签，测试集的图像和标签 return Xtrain, Ytrain, Xtest, Ytest data_dir = &#39;../data/cifar-10-batches-py/&#39; Xtrain, Ytrain, Xtest, Ytest = load_CIFAR_data(data_dir) # loading ../data/cifar-10-batches-py/data_batch_1 # loading ../data/cifar-10-batches-py/data_batch_2 # loading ../data/cifar-10-batches-py/data_batch_3 # loading ../data/cifar-10-batches-py/data_batch_4 # loading ../data/cifar-10-batches-py/data_batch_5 # finished ../loadding CIFAR-10 data 查看数据集数量 print(&#39;training data shape:&#39;, Xtrain.shape) print(&#39;training labels shape:&#39;, Ytrain.shape) print(&#39;test data shape:&#39;, Xtest.shape) print(&#39;test labels shape:&#39;, Ytest.shape) # training data shape: (50000, 32, 32, 3) # training labels shape: (50000,) # test data shape: (10000, 32, 32, 3) # test labels shape: (10000,) 32x32 RGB3通道尺寸图片，训练图片50000张，测试图片10000张， 定义标签字典 对应类别信息查看：http://www.cs.toronto.edu/~kriz/cifar.html label_dict = {0: &quot;airplane&quot;, 1: &quot;automobile&quot;, 2: &quot;bird&quot;, 3: &quot;cat&quot;, 4: &quot;deer&quot;, 5: &quot;dog&quot;, 6: &quot;frog&quot;, 7: &quot;horse&quot;, 8: &quot;ship&quot;, 9: &quot;truck&quot;} 查看第11张图片并查看对应的label imshow_num = 10 plt.imshow(Xtrain[imshow_num]) plt.show() print(&quot;Xtrain[{0}] label:&quot;.format(imshow_num), label_dict[Ytrain[imshow_num]]) # Xtrain[10] label: deer 虽然模模糊糊，但还可以猜出来是十个类别中的鹿 图片进行数字标准化并对比数据 Xtrain_mean0 = np.mean(Xtrain.astype(&#39;float32&#39;)[:, :, :, 0]) Xtrain_mean1 = np.mean(Xtrain.astype(&#39;float32&#39;)[:, :, :, 1]) Xtrain_mean2 = np.mean(Xtrain.astype(&#39;float32&#39;)[:, :, :, 2]) Xtrain_mean = [Xtrain_mean0, Xtrain_mean1, Xtrain_mean2] Xtrain_std0 = np.std(Xtrain.astype(&#39;float32&#39;)[:, :, :, 0]) Xtrain_std1 = np.std(Xtrain.astype(&#39;float32&#39;)[:, :, :, 1]) Xtrain_std2 = np.std(Xtrain.astype(&#39;float32&#39;)[:, :, :, 2]) Xtrain_std = [Xtrain_std0, Xtrain_std1, Xtrain_std2] def data_norm(x, x_mean, x_std): x_norm = x.astype(&#39;float32&#39;) x_norm[:, :, :, 0] = (x_norm[:, :, :, 0] - x_mean[0]) / x_std[0] x_norm[:, :, :, 1] = (x_norm[:, :, :, 1] - x_mean[1]) / x_std[1] x_norm[:, :, :, 2] = (x_norm[:, :, :, 2] - x_mean[2]) / x_std[2] return x_norm Xtrain_norm = data_norm(Xtrain, Xtrain_mean, Xtrain_std) Xtest_norm = data_norm(Xtest, Xtrain_mean, Xtrain_std) # 对比图像数据 print(&quot;Xtrain[0][0][0] data:&quot;, Xtrain[0][0][0]) print(&quot;Xtrain_norm[0][0][0] data:&quot;, Xtrain_norm[0][0][0]) # Xtrain[0][0][0] data: [59 62 63] # Xtrain_normalize[0][0][0] data: [-1.0526042 -0.9816644 -0.7625441] 现在标准化过的数据Xtrain_norm的对应的32x32x3个像素均值为0，标准差为1 测试集的标准化设置的均值和方差需以训练集为准 不然训练的模型对测试图片作用就没那么好了 标签数据处理并对比数据 encoder = OneHotEncoder(sparse=False, categories=&#39;auto&#39;) yy = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]] encoder.fit(yy) Ytrain_reshape = Ytrain.reshape(-1, 1) Ytrain_onehot = encoder.transform(Ytrain_reshape) Ytest_reshape = Ytest.reshape(-1, 1) Ytest_onehot = encoder.transform(Ytest_reshape) print(&quot;Ytrain[10] data:&quot;, Ytrain[10]) print(&quot;Ytrain_onehot[10] data:&quot;, Ytrain_onehot[10]) # Ytrain[10] data: 4 # Ytrain_onehot[10] data: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] 现在的标签数据Ytrain_onehot，Ytest_onehot 已采用独热编码形式 3. 构建模型 定义训练数据的占位符， x是32x32x3个像素点的特征值， y是10分类的标签值： x = tf.placeholder(tf.float32, [None, 32, 32, 3], name=&quot;X&quot;) y = tf.placeholder(tf.float32, [None, 10], name=&quot;Y&quot;) shape中 None 表示行的数量未知 在实际训练时决定一次代入多少行样本 展开图片 x 为了使用卷积层，需把x变成一个4d向量 其第1维对应样本数， -1表示任意数量 其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数 x_image = tf.reshape(x, [-1, 32, 32, 3]) 定义权重初始化函数： 定义权重W 初始化函数 ：从标准差0.1的截断正态分布中输出随机值 标准正态分布生生成的数据在负无穷到正无穷 但是截断式正态分布生成的数据在均值-2倍的标准差，均值+2倍的标准差这个范围内def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) 定义权重b 初始化函数 ：数值为0.1def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 定义 卷积 函数：# 定义1步长的 same卷积 def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # 定义1步长的 valid卷积 def conv2d_v(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;VALID&#39;) # 定义2步长的 same卷积 def conv2d_2(x, W): return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) TensorFow的卷积函数：tf.nn.conv2d()用法可在TensorFlow笔记（8） LeNet-5卷积神经网络中查看 定义 池化 函数：# 定义步长为1 大小3x3的 max pooling def max_pool_3x3(x): return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) # 定义步长为2 大小3x3的 max pooling def max_pool_3x3_2(x): return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) # 定义步长为3 大小5x5的 avg pooling def avg_pool_5x5(x): return tf.nn.avg_pool(x, ksize=[1, 5, 5, 1], strides=[1, 3, 3, 1], padding=&#39;VALID&#39;) # 定义步长为1 大小7x7的 avg pooling def avg_pool_7x7(x): return tf.nn.avg_pool(x, ksize=[1, 7, 7, 1], strides=[1, 1, 1, 1], padding=&#39;VALID&#39;) TensorFow的池化函数：平均池化tf.nn.avg_pool()与最大池化tf.nn.max_pool()用法相似 同样可在TensorFlow笔记（8） LeNet-5卷积神经网络中查看 配置inception块： def inception(x, channel_in, filters_num): # 第一分支 conv 1x1+1s with tf.variable_scope(&quot;branch1&quot;): branch1_w_conv1 = weight_variable([1, 1, channel_in, filters_num[0]]) branch1_b_conv1 = bias_variable([filters_num[0]]) branch1_h_conv1 = tf.nn.relu(conv2d(x, branch1_w_conv1) + branch1_b_conv1) # 第二分支 conv 1x1+1s &gt; conv 3x3+1s with tf.variable_scope(&quot;branch2&quot;): branch2_w_conv1 = weight_variable([1, 1, channel_in, filters_num[1]]) branch2_b_conv1 = bias_variable([filters_num[1]]) branch2_h_conv1 = tf.nn.relu(conv2d(x, branch2_w_conv1) + branch2_b_conv1) branch2_w_conv2 = weight_variable([3, 3, filters_num[1], filters_num[2]]) branch2_b_conv2 = bias_variable([filters_num[2]]) branch2_h_conv2 = tf.nn.relu(conv2d(branch2_h_conv1, branch2_w_conv2) + branch2_b_conv2) # 第三分支 conv 1x1+1s &gt; conv 5x5+1s with tf.variable_scope(&quot;branch3&quot;): branch3_w_conv1 = weight_variable([1, 1, channel_in, filters_num[3]]) branch3_b_conv1 = bias_variable([filters_num[3]]) branch3_h_conv1 = tf.nn.relu(conv2d(x, branch3_w_conv1) + branch3_b_conv1) branch3_w_conv2 = weight_variable([5, 5, filters_num[3], filters_num[4]]) branch3_b_conv2 = bias_variable([filters_num[4]]) branch3_h_conv2 = tf.nn.relu(conv2d(branch3_h_conv1, branch3_w_conv2) + branch3_b_conv2) # 第四分支 max_pool 3x3+1s &gt; conv 1x1+1s with tf.variable_scope(&quot;branch4&quot;): branch4_pool1 = max_pool_3x3(x) branch4_w_conv1 = weight_variable([1, 1, channel_in, filters_num[5]]) branch4_b_conv1 = bias_variable([filters_num[5]]) branch4_h_conv1 = tf.nn.relu(conv2d(branch4_pool1, branch4_w_conv1) + branch4_b_conv1) # 输出叠加 with tf.variable_scope(&quot;depth_concat&quot;): concat = tf.concat([branch1_h_conv1, branch2_h_conv2, branch3_h_conv2, branch4_h_conv1], axis=3) return concat 第一二层：压缩图片 第一二层为什么混着来呢，因为其实作用都是调整和预处理图片 以确保第一次进入inception块时为28x28x192的尺寸 而cifar10图片尺寸较小，为配合后续的计算，调整一些原有结构压缩成28x28x192 # 第一层 w_conv1 = weight_variable([3, 3, 3, 64]) b_conv1 = bias_variable([64]) h_conv1 = tf.nn.relu(conv2d_v(x_image, w_conv1) + b_conv1) h_pool1 = max_pool_3x3(h_conv1) hh_lrn1 = tf.nn.lrn(h_pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75) # 第二层 w_conv2 = weight_variable([1, 1, 64, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(hh_lrn1, w_conv2) + b_conv2) w_conv3 = weight_variable([3, 3, 64, 192]) b_conv3 = bias_variable([192]) h_conv3 = tf.nn.relu(conv2d_v(h_conv2, w_conv3) + b_conv3) h_lrn2 = tf.nn.lrn(h_conv3, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75) h_pool2 = max_pool_3x3(h_lrn2) TensorFow的局部响应归一化函数：tf.nn.lrn()的使用可在 TensorFlow笔记（9） ResNet 中查看 第三阶段 inception3 第三阶段的过程：Previous Activation &gt; inception 3a &gt; inception 3b &gt; max_pool 3x3+2s # inception 3a：conv 1x1 64 + conv 1x1 96 &gt; conv 3x3 128 + conv 1x1 16 &gt; conv 3x3 32 + max_pool 3x3 &gt; conv 3x3 32 filters_num_3a = [64, 96, 128, 16, 32, 32] # 设置inception 3a的通道数 inception_3a = inception(h_pool2, 192, filters_num_3a) # inception 3b：conv 1x1 128 + conv 1x1 128 &gt; conv 3x3 192 + conv 1x1 32 &gt; conv 3x3 96 + max_pool 3x3 &gt; conv 3x3 64 filters_num_3b = [128, 128, 192, 32, 96, 64] inception_3b = inception(inception_3a, 256, filters_num_3b) h_pool3 = max_pool_3x3_2(inception_3b) 第四阶段 inception4 第四阶段的过程：Previous Activation &gt; inception 4a &gt; inception 4b &gt; inception 4c+(softmax0) &gt; inception 4d &gt; inception 4e+(softmax1) &gt; max_pool 3x3+2s # inception 4a filters_num_4a = [192, 96, 208, 16, 48, 64] inception_4a = inception(h_pool3, 480, filters_num_4a) # inception 4b filters_num_4b = [160, 112, 224, 24, 64, 64] inception_4b = inception(inception_4a, 512, filters_num_4b) # softmax0 h_pool4 = avg_pool_5x5(inception_4a) # &gt; conv 1x1 128 w_conv4 = weight_variable([1, 1, 512, 128]) b_conv4 = bias_variable([128]) h_conv4 = tf.nn.relu(conv2d(h_pool4, w_conv4) + b_conv4) h_flat1 = tf.reshape(h_conv4, shape=[-1, 4 * 4 * 128]) # 重新展开 # &gt; fc 1x1 1024 W_fc1 = weight_variable([4 * 4 * 128, 1024]) b_fc1 = bias_variable([1024]) h_fc1 = tf.nn.relu(tf.matmul(h_flat1, W_fc1) + b_fc1) # &gt; fc 1x1 10 dropout_rate0 = tf.placeholder(&quot;float&quot;) h_fc1_drop = tf.nn.dropout(h_fc1, rate=dropout_rate0) W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) h_fc2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 # &gt; softmax0 pred0 = tf.nn.softmax(h_fc2) # inception 4c filters_num_4c = [128, 128, 256, 24, 64, 64] inception_4c = inception(inception_4b, 512, filters_num_4c) # inception 4d filters_num_4d = [112, 144, 288, 32, 64, 64] inception_4d = inception(inception_4c, 512, filters_num_4d) # inception 4e filters_num_4e = [256, 160, 320, 32, 128, 128] inception_4e = inception(inception_4d, 528, filters_num_4e) h_pool5 = max_pool_3x3_2(inception_4e) # softmax1 h_pool6 = avg_pool_5x5(inception_4d) # &gt; conv 1x1 128 w_conv5 = weight_variable([1, 1, 528, 128]) b_conv5 = bias_variable([128]) h_conv5 = tf.nn.relu(conv2d(h_pool6, w_conv5) + b_conv5) h_flat2 = tf.reshape(h_conv5, shape=[-1, 4 * 4 * 128]) # 重新展开 # &gt; fc 1x1 1024 W_fc3 = weight_variable([4 * 4 * 128, 1024]) b_fc3 = bias_variable([1024]) h_fc3 = tf.nn.relu(tf.matmul(h_flat2, W_fc3) + b_fc3) # &gt; fc 1x1 10 dropout_rate1 = tf.placeholder(&quot;float&quot;) h_fc3_drop = tf.nn.dropout(h_fc1, rate=dropout_rate1) W_fc4 = weight_variable([1024, 10]) b_fc4 = bias_variable([10]) h_fc4 = tf.matmul(h_fc3_drop, W_fc4) + b_fc4 # &gt; softmax1 pred1 = tf.nn.softmax(h_fc4) TensorFow的dropout 函数：tf.nn.dropout()的使用可在TensorFlow笔记（9） ResNet中查看 第五阶段 最终输出 第五阶段的过程：inception 5a &gt; inception 5b &gt; avg_pool_7x7 &gt; softmax2 # inception 5a filters_num_5a = [256, 160, 320, 32, 128, 128] inception_5a = inception(h_pool5, 832, filters_num_5a) # inception 5b filters_num_5b = [384, 192, 384, 48, 128, 128] inception_5b = inception(inception_5a, 832, filters_num_5b) # avg_pool_7x7 h_pool7 = avg_pool_7x7(inception_5b) h_flat3 = tf.reshape(h_pool7, shape=[-1, 1 * 1 * 1024]) # 重新展开 # softmax2 dropout_rate2 = tf.placeholder(&quot;float&quot;) h_pool7_drop = tf.nn.dropout(h_flat3, rate=dropout_rate2) W_fc5 = weight_variable([1024, 10]) b_fc5 = bias_variable([10]) h_fc5 = tf.matmul(h_pool7_drop, W_fc5) + b_fc2 pred2 = tf.nn.softmax(h_fc5) 整合softmax forward = h_fc5 * 0.4 + h_fc4 * 0.3 + h_fc2 * 0.3 pred = pred2 * 0.4 + pred1 * 0.3 + pred0 * 0.3 定义损失函数 使用TensoFlow提供的结合Softmax的交叉熵损失函数定义方法：softmax_cross_entropy_with_logits_v2 交叉熵损失函数其实就是逻辑回归损失函数的前半部 − y ∗ l o g ( p r e d ) - y * log(pred) −y∗log(pred) 忽略了 − ( 1 − y ) ∗ l o g ( 1 − p r e d ) -(1 - y) * log(1 - pred) −(1−y)∗log(1−pred) with tf.name_scope(&quot;LossFunction&quot;): loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=forward, labels=y)) 4. 训练模型 设置超参数： train_epochs = 20 # 迭代次数 learning_rate = 0.001 # 学习率 定义Adam优化器，设置学习率和优化目标损失最小化： optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function) 定义预测类别匹配情况 correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) tf.equal(A, B) ：对比这两个矩阵或者向量的相等的元素，相等返回 True，相反返回 False tf.argmax(input,axis) ：根据axis取值的不同返回每行或者每列最大值的索引，axis 表示维度，0：第一维度（行），1：第二维度（列），-1：最后一个维度 其实，这里的最终求得的索引，恰好就表示图片上的数字 定义准确率，将布尔值转化成浮点数，再求平均值 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 读取模型与创建会话： # 定义保存模型 saver = tf.train.Saver() save_dir = &quot;../save_path/GoogleNet/&quot; # 定义保存模型编号 save_step = 0 # 恢复保存模型 ckpt_dir = tf.train.latest_checkpoint(save_dir) sess = tf.Session() # 建立会话 if ckpt_dir != None: saver.restore(sess, ckpt_dir) print(&quot;Finished loading&quot;, ckpt_dir) save_step = int(input(&quot;Set the load save step:&quot;)) else: # 变量初始化 sess.run(tf.initialize_all_variables()) print(&quot;Finished initialize&quot;) 关于如何保存模型，可在TensorFlow笔记（10） CheckPoint中查看 如果是读取模型继续训练时，例如最近保存的是序号12的模型 # Finished loading ../save_path/GoogleNet/model-12 Set the load save step: 则输入现在加载的模型序号12，然后回车，之后的训练保存的模型就从13开始 Set the load save step:12 设置批次大小和数量： 如果在处理完整个5万个训练图片的训练集之后才进行一次训练 这样的处理速度相对缓慢 如果在处理完整个5万个训练图片的训练集之前先让梯度下降法处理一部分 算法速度会更快 可以把训练集分割为小一点的子集训练 如128张训练图片，然后就进行梯度下降法处理 这种梯度下降法处理方法称之为Mini-batch 梯度下降 具体可参考深度学习笔记（9） 优化算法（一） # 每个批次的大小，每次放入的大小，每次放入 128张图片 以矩阵的方式 batch_size = 128 # 计算一共有多少个批次，数量整除大小训练出有多少批次 n_batch = len(Ytrain_onehot) // batch_size # 定义显示训练过程中验证的间隔批量次数 display_test_num = n_batch // 10 # 定义显示训练过程.的间隔批量次数 display_train_num = display_test_num // 10 # 定义批次训练数据的占位符 x_batch = tf.placeholder(tf.float32, [None, 32, 32, 3], name=&quot;X_batch&quot;) # 随机上下翻转批次图片 Xtrain_batch_up = tf.image.random_flip_up_down(x_batch) # 随机左右翻转批次图片 Xtrain_batch_lf = tf.image.random_flip_left_right(Xtrain_batch_up) # 对角旋转批次图片 # Xtrain_batch_tp = tf.image.transpose_image(x_batch) # 定义训练集批次函数 def get_train_batch(num, size): Xtrain_batch = Xtrain_norm_shuffle[num * size:(num + 1) * size] Ytrain_batch = Ytrain_onehot_shuffle[num * size:(num + 1) * size] # 随机翻转数据 with tf.Session() as sess_batch: Xtrain_batch = sess_batch.run(Xtrain_batch_lf, feed_dict={x_batch: Xtrain_batch}) return Xtrain_batch, Ytrain_batch 批次迭代训练，其中 dropout 随机丢弃的概率都为0.4，显示迭代过程中的信息： for epoch in range(train_epochs): # 打乱训练数据集 index = [i for i in range(len(Ytrain_onehot))] random.shuffle(index) Xtrain_norm_shuffle = Xtrain_norm[index] Ytrain_onehot_shuffle = Ytrain_onehot[index] # 批次迭代训练 for batch in range(0, n_batch): xs, ys = get_train_batch(batch, batch_size) sess.run(optimizer, feed_dict={x: xs, y: ys, dropout_rate0: 0.4, dropout_rate1: 0.4, dropout_rate2: 0.4}) if (batch + 1) % display_train_num == 0: print(&quot;.&quot;, end=&quot;&quot;) if (batch + 1) % display_test_num == 0: # 保存模型 save_step += 1 save_path = saver.save(sess, save_dir + &quot;model&quot;, global_step=save_step) print(&quot;Complete save &quot;, save_path) # 批次训练完成之后，使用测试数据计算误差与准确率 loss, acc = sess.run([loss_function, accuracy], feed_dict={x: Xtest_norm[0:512], y: Ytest_onehot[0:512], dropout_rate0: 0, dropout_rate1: 0, dropout_rate2: 0}) learning_rate = 0.95 * learning_rate # 学习率衰减 # ..........Complete save D:/save_path/GoogleNet/model-1 # TrainEpoch= 01 TrainBatch= 0039 Loss= 1.958486080 TestAccuracy= 0.1973 # ... # ..........Complete save D:/save_path/GoogleNet/model-10 # TrainEpoch= 01 TrainBatch= 0390 Loss= 1.628237653 TestAccuracy= 0.4268 # ... # ..........Complete save D:/save_path/GoogleNet/model-200 # TrainEpoch= 20 TrainBatch= 0390 Loss= 0.510145028 TestAccuracy= 0.8066 数据训练量比较大，而设备有限，为了保护设备而断断续续的训练 过一遍数据，抽取测试集前512张图片分类的准确率到达42.68% 最后一共历遍训练集20次，抽取测试集前512张图片分类的准确率到达 80.66% 5. 评估模型 测试集上评估模型预测的准确率 test_total_batch = int(len(Xtest_norm) / batch_size) test_acc_sum = 0.0 for i in range(test_total_batch): test_image_batch = Xtest_norm[i * batch_size:(i + 1) * batch_size] test_label_batch = Ytest_onehot[i * batch_size:(i + 1) * batch_size] test_batch_acc = sess.run(accuracy, feed_dict={x: test_image_batch, y: test_label_batch, dropout_rate0: 0, dropout_rate1: 0, dropout_rate2: 0}) test_acc_sum += test_batch_acc test_acc = float(test_acc_sum / test_total_batch) print(&quot;Test accuracy:{:.6f}&quot;.format(test_acc)) # Test accuracy:0.804688 测试集的准确率达到80% 6. 模型预测 查看预测结果 # 转换第1-10张测试图片pred预测结果独热编码格式为数字0-9 prediction_result = sess.run(tf.argmax(pred, 1), feed_dict={x: Xtest_normalize[0:10], dropout_rate0: 0, dropout_rate1: 0, dropout_rate2: 0}) # 查看第1-10张测试图片的预测结果 print(prediction_result) # [3 8 8 0 6 6 1 6 3 1] 但是这样没办法知道，预测的到底是不是正确的 预测结果可视化比对 定义可视化函数： def plot_images_labels_prediction(images, labels, prediction, idx, num=10): fig = plt.gcf() fig.set_size_inches(12, 6) if num &gt; 10: num = 10 for i in range(0, num): ax = plt.subplot(2, 5, 1 + i) ax.imshow(images[idx], cmap=&quot;binary&quot;) title = str(i) + &#39;,&#39; + label_dict[labels[idx]] if len(prediction) &gt; 0: title += &#39;=&gt;&#39; + label_dict[prediction[idx]] ax.set_title(title, fontsize=10) idx += 1 plt.show() 可视化第1-10张测试图片的预测结果对比 plot_images_labels_prediction(Xtest, Ytest, rediction_result, 0, 10) 这次的预测都判断正确，还行。 [1] python的代码地址: https://github.com/JoveH-H/TensorFlow/blob/master/py/8.GoogleNet.py [2] jupyter notebook的代码地址: （待补充） [3] MNIST 数据集 t10k-images-idx3-ubyte.gz http://www.cs.toronto.edu/~kriz/cifar.html 相关推荐： 深度学习笔记（30） Inception网络 深度学习笔记（26） 卷积神经网络 TensorFlow笔记（10） CheckPoint TensorFlow笔记（9） ResNet TensorFlow笔记（8） LeNet-5卷积神经网络 谢谢！","@type":"BlogPosting","url":"https://uzzz.org/2019/07/30/792380.html","headline":"TensorFlow笔记（11） GoolgeNet","dateModified":"2019-07-30T00:00:00+08:00","datePublished":"2019-07-30T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/30/792380.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>TensorFlow笔记（11） GoolgeNet</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>TensorFlow笔记（11） GoolgeNet</h3>
   <ul>
    <li><a href="#1_Inception_2" rel="nofollow" data-token="a1c7b9fa2e61b3ad4cadcf825fb54167">1. Inception块</a></li>
    <li><a href="#2__11" rel="nofollow" data-token="402cb85d1eef8966e4007b04b125067b">2. 数据读取</a></li>
    <li><a href="#3__191" rel="nofollow" data-token="5e3e90844e76f7c2cab5501368cdb599">3. 构建模型</a></li>
    <li><a href="#4__444" rel="nofollow" data-token="c5200652377629e6ba16e1b59955b0c7">4. 训练模型</a></li>
    <li><a href="#5__586" rel="nofollow" data-token="d6855979430974094cbc7b2538de8314">5. 评估模型</a></li>
    <li><a href="#6__609" rel="nofollow" data-token="71fbdc154289cd074af5ff7f225033af">6. 模型预测</a></li>
   </ul>
  </div>
  <p></p> 
  <h1><a id="1_Inception_2"></a>1. Inception块</h1> 
  <p>根据 <a href="https://blog.csdn.net/qq_32618327/article/details/93479440" rel="nofollow" data-token="d9c8b6ca79cc76940f2427012f425a00">深度学习笔记（30） Inception网络</a> 可以了解到</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190626141246422.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjE4MzI3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br> 可以利用Inception块构建经典网络InceptionNet，也就是GoolgeNet<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019062614133956.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjE4MzI3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br> 那么，以CIFAR-10 数据集的分类为例，采用<strong>GoolgeNet</strong>模型来解决问题</p> 
  <hr> 
  <h1><a id="2__11"></a>2. 数据读取</h1> 
  <p>CIFAR-10 数据集的分类是机器学习中一个公开的基准测试问题<br> 其任务是对一组32x32RGB的图像进行分类，这些图像涵盖了10个类别：<br> 飞机， 汽车， 鸟， 猫， 鹿， 狗， 青蛙， 马， 船以及卡车<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019072110584698.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9qb3ZlaC1oLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br> 利用网上的 <a href="http://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow" data-token="fecec1939431790ed9961ab6eba6697e">CIFAR-10 数据集</a> 获取数据集压缩文件：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721111547843.#pic_center" alt="在这里插入图片描述"><br> 这里使用的是python版本 <code>CIFAR-10 python version</code></p> 
  <ul> 
   <li> <p>载入数据集合：</p> <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> os
<span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request
<span class="token keyword">import</span> tarfile
<span class="token keyword">import</span> pickle <span class="token keyword">as</span> p
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> OneHotEncoder

<span class="token comment"># 下载 cifar10</span>
url <span class="token operator">=</span> <span class="token string">'https://www.cs.toronto.edu/-kriz/cifar-10-python.tar.gz'</span>
filepath <span class="token operator">=</span> <span class="token string">'../data/cifar-10-python.tar.gz'</span>
<span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>filepath<span class="token punctuation">)</span><span class="token punctuation">:</span>
    result <span class="token operator">=</span> urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlretrieve<span class="token punctuation">(</span>url<span class="token punctuation">,</span> filepath<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'downloaded'</span><span class="token punctuation">,</span> result<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Data file already exists.'</span><span class="token punctuation">)</span>

<span class="token comment"># 解压 cifar10</span>
<span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token string">"../data/cifar-10-batches-py"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    tfile <span class="token operator">=</span> tarfile<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"../data/cifar-10-python.tar.gz"</span><span class="token punctuation">,</span> <span class="token string">'r:gz'</span><span class="token punctuation">)</span>
    result <span class="token operator">=</span> tfile<span class="token punctuation">.</span>extractall<span class="token punctuation">(</span><span class="token string">"../data/"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Extracted to ./../data/cifar-10-batches-py/'</span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Directory already exists.'</span><span class="token punctuation">)</span>
</code></pre> <p>下载并解压数据集<br> 当然可以自己在 <a href="http://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow" data-token="fecec1939431790ed9961ab6eba6697e">CIFAR-10 数据集</a> 网站上下载在工程的<code>data</code>文件夹下，然后解压<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721112735215.#pic_center" alt="在这里插入图片描述"></p> </li> 
   <li> <p>加载数据集</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">load_CIFAR_batch</span><span class="token punctuation">(</span>filename<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""oad single batch of cifar"""</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>filename<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span><span class="token keyword">as</span> f<span class="token punctuation">:</span>
        <span class="token comment"># 一个样本由标签和图像数据组成</span>
        <span class="token comment"># &lt;1 xlabel&gt;&lt;3072 xpixel&gt; (3072-32x32x3)</span>
        data_dict <span class="token operator">=</span> p<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'bytes'</span><span class="token punctuation">)</span>
        images <span class="token operator">=</span> data_dict<span class="token punctuation">[</span>b<span class="token string">'data'</span><span class="token punctuation">]</span>
        labels <span class="token operator">=</span> data_dict<span class="token punctuation">[</span>b<span class="token string">'labels'</span><span class="token punctuation">]</span>
        <span class="token comment"># 把原始数据结构调整为: BCWH</span>
        images <span class="token operator">=</span> images<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>
        <span class="token comment"># tensorflow处理图像数据的结构: BWHC</span>
        <span class="token comment"># #把通道数据C移动到最后一个维度</span>
        images <span class="token operator">=</span> images<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        labels <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>
        <span class="token keyword">return</span> images<span class="token punctuation">,</span> labels


<span class="token keyword">def</span> <span class="token function">load_CIFAR_data</span><span class="token punctuation">(</span>data_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""load CIFAR data"""</span>
    images_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    labels_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        f <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> <span class="token string">'data_batch_{0}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'loading'</span><span class="token punctuation">,</span> f<span class="token punctuation">)</span>
        <span class="token comment"># 调用loadCIFARbatch(获得批量的图像及其对应的标签</span>
        image_batch<span class="token punctuation">,</span> label_batch <span class="token operator">=</span> load_CIFAR_batch<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
        images_train<span class="token punctuation">.</span>append<span class="token punctuation">(</span>image_batch<span class="token punctuation">)</span>
        labels_train<span class="token punctuation">.</span>append<span class="token punctuation">(</span>label_batch<span class="token punctuation">)</span>
        Xtrain <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span>images_train<span class="token punctuation">)</span>
        Ytrain <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span>labels_train<span class="token punctuation">)</span>
        <span class="token keyword">del</span> image_batch<span class="token punctuation">,</span> label_batch
    Xtest<span class="token punctuation">,</span> Ytest <span class="token operator">=</span> load_CIFAR_batch<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> <span class="token string">'test_batch'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'finished loadding CIFAR-10 data'</span><span class="token punctuation">)</span>
    <span class="token comment"># 返回训练集的图象和标签，测试集的图像和标签</span>
    <span class="token keyword">return</span> Xtrain<span class="token punctuation">,</span> Ytrain<span class="token punctuation">,</span> Xtest<span class="token punctuation">,</span> Ytest


data_dir <span class="token operator">=</span> <span class="token string">'../data/cifar-10-batches-py/'</span>
Xtrain<span class="token punctuation">,</span> Ytrain<span class="token punctuation">,</span> Xtest<span class="token punctuation">,</span> Ytest <span class="token operator">=</span> load_CIFAR_data<span class="token punctuation">(</span>data_dir<span class="token punctuation">)</span>

<span class="token comment"># loading ../data/cifar-10-batches-py/data_batch_1</span>
<span class="token comment"># loading ../data/cifar-10-batches-py/data_batch_2</span>
<span class="token comment"># loading ../data/cifar-10-batches-py/data_batch_3</span>
<span class="token comment"># loading ../data/cifar-10-batches-py/data_batch_4</span>
<span class="token comment"># loading ../data/cifar-10-batches-py/data_batch_5</span>
<span class="token comment"># finished ../loadding CIFAR-10 data</span>
</code></pre> </li> 
   <li> <p>查看数据集数量</p> <pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'training data shape:'</span><span class="token punctuation">,</span> Xtrain<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'training labels shape:'</span><span class="token punctuation">,</span> Ytrain<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'test data shape:'</span><span class="token punctuation">,</span> Xtest<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'test labels shape:'</span><span class="token punctuation">,</span> Ytest<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token comment"># training data shape: (50000, 32, 32, 3)</span>
<span class="token comment"># training labels shape: (50000,)</span>
<span class="token comment"># test data shape: (10000, 32, 32, 3)</span>
<span class="token comment"># test labels shape: (10000,)</span>
</code></pre> <p>32x32 RGB3通道尺寸图片，训练图片50000张，测试图片10000张，</p> </li> 
   <li> <p>定义标签字典<br> 对应类别信息查看：<a href="http://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow" data-token="fecec1939431790ed9961ab6eba6697e">http://www.cs.toronto.edu/~kriz/cifar.html</a></p> <pre><code class="prism language-python">label_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">0</span><span class="token punctuation">:</span> <span class="token string">"airplane"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"automobile"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">:</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span> <span class="token string">"deer"</span><span class="token punctuation">,</span>
              <span class="token number">5</span><span class="token punctuation">:</span> <span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">:</span> <span class="token string">"frog"</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">:</span> <span class="token string">"horse"</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">:</span> <span class="token string">"ship"</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">:</span> <span class="token string">"truck"</span><span class="token punctuation">}</span>
</code></pre> </li> 
   <li> <p>查看第11张图片并查看对应的label</p> <pre><code class="prism language-python">imshow_num <span class="token operator">=</span> <span class="token number">10</span>
plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>Xtrain<span class="token punctuation">[</span>imshow_num<span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Xtrain[{0}] label:"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>imshow_num<span class="token punctuation">)</span><span class="token punctuation">,</span> label_dict<span class="token punctuation">[</span>Ytrain<span class="token punctuation">[</span>imshow_num<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># Xtrain[10] label: deer</span>
</code></pre> <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721113902529.#pic_center" alt="在这里插入图片描述"><br> 虽然模模糊糊，但还可以猜出来是十个类别中的鹿</p> </li> 
   <li> <p>图片进行数字标准化并对比数据</p> <pre><code class="prism language-python">Xtrain_mean0 <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>Xtrain<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xtrain_mean1 <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>Xtrain<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xtrain_mean2 <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>Xtrain<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xtrain_mean <span class="token operator">=</span> <span class="token punctuation">[</span>Xtrain_mean0<span class="token punctuation">,</span> Xtrain_mean1<span class="token punctuation">,</span> Xtrain_mean2<span class="token punctuation">]</span>

Xtrain_std0 <span class="token operator">=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>Xtrain<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xtrain_std1 <span class="token operator">=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>Xtrain<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xtrain_std2 <span class="token operator">=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>Xtrain<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xtrain_std <span class="token operator">=</span> <span class="token punctuation">[</span>Xtrain_std0<span class="token punctuation">,</span> Xtrain_std1<span class="token punctuation">,</span> Xtrain_std2<span class="token punctuation">]</span>


<span class="token keyword">def</span> <span class="token function">data_norm</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> x_mean<span class="token punctuation">,</span> x_std<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x_norm <span class="token operator">=</span> x<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span>
    x_norm<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>x_norm<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> x_mean<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> x_std<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    x_norm<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>x_norm<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> x_mean<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> x_std<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    x_norm<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>x_norm<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">-</span> x_mean<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> x_std<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> x_norm


Xtrain_norm <span class="token operator">=</span> data_norm<span class="token punctuation">(</span>Xtrain<span class="token punctuation">,</span> Xtrain_mean<span class="token punctuation">,</span> Xtrain_std<span class="token punctuation">)</span>
Xtest_norm <span class="token operator">=</span> data_norm<span class="token punctuation">(</span>Xtest<span class="token punctuation">,</span> Xtrain_mean<span class="token punctuation">,</span> Xtrain_std<span class="token punctuation">)</span>

<span class="token comment"># 对比图像数据</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Xtrain[0][0][0] data:"</span><span class="token punctuation">,</span> Xtrain<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Xtrain_norm[0][0][0] data:"</span><span class="token punctuation">,</span> Xtrain_norm<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># Xtrain[0][0][0] data: [59 62 63]</span>
<span class="token comment"># Xtrain_normalize[0][0][0] data: [-1.0526042 -0.9816644 -0.7625441]</span>
</code></pre> <p>现在标准化过的数据Xtrain_norm的对应的32x32x3个像素均值为0，标准差为1<br> 测试集的标准化设置的均值和方差需以训练集为准<br> 不然训练的模型对测试图片作用就没那么好了</p> </li> 
   <li> <p>标签数据处理并对比数据</p> <pre><code class="prism language-python">encoder <span class="token operator">=</span> OneHotEncoder<span class="token punctuation">(</span>sparse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> categories<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>
yy <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
encoder<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>yy<span class="token punctuation">)</span>
Ytrain_reshape <span class="token operator">=</span> Ytrain<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
Ytrain_onehot <span class="token operator">=</span> encoder<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>Ytrain_reshape<span class="token punctuation">)</span>
Ytest_reshape <span class="token operator">=</span> Ytest<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
Ytest_onehot <span class="token operator">=</span> encoder<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>Ytest_reshape<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Ytrain[10] data:"</span><span class="token punctuation">,</span> Ytrain<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Ytrain_onehot[10] data:"</span><span class="token punctuation">,</span> Ytrain_onehot<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># Ytrain[10] data: 4</span>
<span class="token comment"># Ytrain_onehot[10] data: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]</span>
</code></pre> <p>现在的标签数据Ytrain_onehot，Ytest_onehot 已采用独热编码形式</p> </li> 
  </ul> 
  <hr> 
  <h1><a id="3__191"></a>3. 构建模型</h1> 
  <ul> 
   <li> <p>定义训练数据的占位符， x是32x32x3个像素点的特征值， y是10分类的标签值：</p> <pre><code class="prism language-python">x <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"X"</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"Y"</span><span class="token punctuation">)</span>
</code></pre> <p>shape中 None 表示行的数量未知<br> 在实际训练时决定一次代入多少行样本</p> </li> 
   <li> <p>展开图片 x<br> 为了使用卷积层，需把x变成一个4d向量<br> 其第1维对应样本数， -1表示任意数量<br> 其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数</p> <pre><code class="prism language-python">x_image <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>定义权重初始化函数：</p> 
    <ul> 
     <li>定义<strong>权重W</strong> 初始化函数 ：从标准差0.1的<strong>截断正态分布</strong>中输出随机值<br> 标准正态分布生生成的数据在负无穷到正无穷<br> 但是截断式正态分布生成的数据在均值-2倍的标准差，均值+2倍的标准差这个范围内<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">weight_variable</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
	initial <span class="token operator">=</span> tf<span class="token punctuation">.</span>truncated_normal<span class="token punctuation">(</span>shape<span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
	<span class="token keyword">return</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>initial<span class="token punctuation">)</span>
</code></pre> </li> 
     <li>定义<strong>权重b</strong> 初始化函数 ：数值为0.1<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">bias_variable</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
	initial <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">,</span> shape<span class="token operator">=</span>shape<span class="token punctuation">)</span>
	<span class="token keyword">return</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>initial<span class="token punctuation">)</span>
</code></pre> </li> 
     <li>定义 <strong>卷积</strong> 函数：<pre><code class="prism language-python"><span class="token comment"># 定义1步长的 same卷积</span>
<span class="token keyword">def</span> <span class="token function">conv2d</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>


<span class="token comment"># 定义1步长的 valid卷积</span>
<span class="token keyword">def</span> <span class="token function">conv2d_v</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'VALID'</span><span class="token punctuation">)</span>


<span class="token comment"># 定义2步长的 same卷积</span>
<span class="token keyword">def</span> <span class="token function">conv2d_2</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>
</code></pre> TensorFow的卷积函数：<code>tf.nn.conv2d()</code>用法可在<a href="https://blog.csdn.net/qq_32618327/article/details/91786928" rel="nofollow" data-token="cc71f228b9dd6425afafd259b3003de3">TensorFlow笔记（8） LeNet-5卷积神经网络</a>中查看</li> 
     <li>定义 <strong>池化</strong> 函数：<pre><code class="prism language-python"><span class="token comment"># 定义步长为1 大小3x3的 max pooling</span>
<span class="token keyword">def</span> <span class="token function">max_pool_3x3</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>x<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>


<span class="token comment"># 定义步长为2 大小3x3的 max pooling</span>
<span class="token keyword">def</span> <span class="token function">max_pool_3x3_2</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>x<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>


<span class="token comment"># 定义步长为3 大小5x5的 avg pooling</span>
<span class="token keyword">def</span> <span class="token function">avg_pool_5x5</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'VALID'</span><span class="token punctuation">)</span>


<span class="token comment"># 定义步长为1 大小7x7的 avg pooling</span>
<span class="token keyword">def</span> <span class="token function">avg_pool_7x7</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'VALID'</span><span class="token punctuation">)</span>
</code></pre> TensorFow的池化函数：平均池化<code>tf.nn.avg_pool()</code>与最大池化<code>tf.nn.max_pool()</code>用法相似<br> 同样可在<a href="https://blog.csdn.net/qq_32618327/article/details/91786928" rel="nofollow" data-token="cc71f228b9dd6425afafd259b3003de3">TensorFlow笔记（8） LeNet-5卷积神经网络</a>中查看</li> 
    </ul> </li> 
   <li> <p>配置<strong>inception块</strong>：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190626141246422.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjE4MzI3,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">inception</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> channel_in<span class="token punctuation">,</span> filters_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 第一分支 conv 1x1+1s</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"branch1"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        branch1_w_conv1 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> channel_in<span class="token punctuation">,</span> filters_num<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch1_b_conv1 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>filters_num<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch1_h_conv1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> branch1_w_conv1<span class="token punctuation">)</span> <span class="token operator">+</span> branch1_b_conv1<span class="token punctuation">)</span>
    <span class="token comment"># 第二分支 conv 1x1+1s &gt; conv 3x3+1s</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"branch2"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        branch2_w_conv1 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> channel_in<span class="token punctuation">,</span> filters_num<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch2_b_conv1 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>filters_num<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch2_h_conv1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> branch2_w_conv1<span class="token punctuation">)</span> <span class="token operator">+</span> branch2_b_conv1<span class="token punctuation">)</span>
        branch2_w_conv2 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> filters_num<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> filters_num<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch2_b_conv2 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>filters_num<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch2_h_conv2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>branch2_h_conv1<span class="token punctuation">,</span> branch2_w_conv2<span class="token punctuation">)</span> <span class="token operator">+</span> branch2_b_conv2<span class="token punctuation">)</span>
    <span class="token comment"># 第三分支 conv 1x1+1s &gt; conv 5x5+1s</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"branch3"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        branch3_w_conv1 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> channel_in<span class="token punctuation">,</span> filters_num<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch3_b_conv1 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>filters_num<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch3_h_conv1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> branch3_w_conv1<span class="token punctuation">)</span> <span class="token operator">+</span> branch3_b_conv1<span class="token punctuation">)</span>
        branch3_w_conv2 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> filters_num<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> filters_num<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch3_b_conv2 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>filters_num<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch3_h_conv2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>branch3_h_conv1<span class="token punctuation">,</span> branch3_w_conv2<span class="token punctuation">)</span> <span class="token operator">+</span> branch3_b_conv2<span class="token punctuation">)</span>
    <span class="token comment"># 第四分支 max_pool 3x3+1s &gt; conv 1x1+1s</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"branch4"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        branch4_pool1 <span class="token operator">=</span> max_pool_3x3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        branch4_w_conv1 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> channel_in<span class="token punctuation">,</span> filters_num<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch4_b_conv1 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span>filters_num<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        branch4_h_conv1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>branch4_pool1<span class="token punctuation">,</span> branch4_w_conv1<span class="token punctuation">)</span> <span class="token operator">+</span> branch4_b_conv1<span class="token punctuation">)</span>
    <span class="token comment"># 输出叠加</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"depth_concat"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        concat <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>branch1_h_conv1<span class="token punctuation">,</span> branch2_h_conv2<span class="token punctuation">,</span> branch3_h_conv2<span class="token punctuation">,</span> branch4_h_conv1<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> concat
</code></pre> </li> 
   <li> <p>第一二层：压缩图片<br> 第一二层为什么混着来呢，因为其实作用都是调整和预处理图片<br> 以确保第一次进入inception块时为28x28x192的尺寸<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721155740942.#pic_center" alt="在这里插入图片描述"><br> 而cifar10图片尺寸较小，为配合后续的计算，调整一些原有结构压缩成28x28x192</p> <pre><code class="prism language-python"><span class="token comment"># 第一层</span>
w_conv1 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_conv1 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_conv1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d_v<span class="token punctuation">(</span>x_image<span class="token punctuation">,</span> w_conv1<span class="token punctuation">)</span> <span class="token operator">+</span> b_conv1<span class="token punctuation">)</span>
h_pool1 <span class="token operator">=</span> max_pool_3x3<span class="token punctuation">(</span>h_conv1<span class="token punctuation">)</span>
hh_lrn1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>lrn<span class="token punctuation">(</span>h_pool1<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.001</span> <span class="token operator">/</span> <span class="token number">9.0</span><span class="token punctuation">,</span> beta<span class="token operator">=</span><span class="token number">0.75</span><span class="token punctuation">)</span>
<span class="token comment"># 第二层</span>
w_conv2 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_conv2 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_conv2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>hh_lrn1<span class="token punctuation">,</span> w_conv2<span class="token punctuation">)</span> <span class="token operator">+</span> b_conv2<span class="token punctuation">)</span>
w_conv3 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_conv3 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">192</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_conv3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d_v<span class="token punctuation">(</span>h_conv2<span class="token punctuation">,</span> w_conv3<span class="token punctuation">)</span> <span class="token operator">+</span> b_conv3<span class="token punctuation">)</span>
h_lrn2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>lrn<span class="token punctuation">(</span>h_conv3<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.001</span> <span class="token operator">/</span> <span class="token number">9.0</span><span class="token punctuation">,</span> beta<span class="token operator">=</span><span class="token number">0.75</span><span class="token punctuation">)</span>
h_pool2 <span class="token operator">=</span> max_pool_3x3<span class="token punctuation">(</span>h_lrn2<span class="token punctuation">)</span>
</code></pre> <p>TensorFow的局部响应归一化函数：<code>tf.nn.lrn()</code>的使用可在 <a href="https://blog.csdn.net/qq_32618327/article/details/94379612" rel="nofollow" data-token="ef73e5f5bdd92b6a721f027d46ae1cb3">TensorFlow笔记（9） ResNet</a> 中查看</p> </li> 
   <li> <p>第三阶段 <strong>inception3</strong><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721161236671.#pic_center" alt="在这里插入图片描述"><br> 第三阶段的过程：Previous Activation &gt; inception 3a &gt; inception 3b &gt; max_pool 3x3+2s</p> <pre><code class="prism language-python"><span class="token comment"># inception 3a：conv 1x1 64 + conv 1x1 96 &gt; conv 3x3 128 + conv 1x1 16 &gt; conv 3x3 32 + max_pool 3x3 &gt; conv 3x3 32</span>
filters_num_3a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span>  <span class="token comment"># 设置inception 3a的通道数</span>
inception_3a <span class="token operator">=</span> inception<span class="token punctuation">(</span>h_pool2<span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> filters_num_3a<span class="token punctuation">)</span>

<span class="token comment"># inception 3b：conv 1x1 128 + conv 1x1 128 &gt; conv 3x3 192 + conv 1x1 32 &gt; conv 3x3 96 + max_pool 3x3 &gt; conv 3x3 64</span>
filters_num_3b <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span>
inception_3b <span class="token operator">=</span> inception<span class="token punctuation">(</span>inception_3a<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> filters_num_3b<span class="token punctuation">)</span>

h_pool3 <span class="token operator">=</span> max_pool_3x3_2<span class="token punctuation">(</span>inception_3b<span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>第四阶段 <strong>inception4</strong><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721162808167.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9qb3ZlaC1oLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br> 第四阶段的过程：Previous Activation &gt; inception 4a &gt; inception 4b &gt; inception 4c+(softmax0) &gt; inception 4d &gt; inception 4e+(softmax1) &gt; max_pool 3x3+2s</p> <pre><code class="prism language-python"><span class="token comment"># inception 4a</span>
filters_num_4a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">208</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">48</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span>
inception_4a <span class="token operator">=</span> inception<span class="token punctuation">(</span>h_pool3<span class="token punctuation">,</span> <span class="token number">480</span><span class="token punctuation">,</span> filters_num_4a<span class="token punctuation">)</span>
<span class="token comment"># inception 4b</span>
filters_num_4b <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span>
inception_4b <span class="token operator">=</span> inception<span class="token punctuation">(</span>inception_4a<span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> filters_num_4b<span class="token punctuation">)</span>
<span class="token comment"># softmax0</span>
h_pool4 <span class="token operator">=</span> avg_pool_5x5<span class="token punctuation">(</span>inception_4a<span class="token punctuation">)</span>
<span class="token comment"># &gt; conv 1x1 128</span>
w_conv4 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_conv4 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_conv4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>h_pool4<span class="token punctuation">,</span> w_conv4<span class="token punctuation">)</span> <span class="token operator">+</span> b_conv4<span class="token punctuation">)</span>
h_flat1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>h_conv4<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 重新展开</span>
<span class="token comment"># &gt; fc 1x1 1024</span>
W_fc1 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_fc1 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_fc1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>h_flat1<span class="token punctuation">,</span> W_fc1<span class="token punctuation">)</span> <span class="token operator">+</span> b_fc1<span class="token punctuation">)</span>
<span class="token comment"># &gt; fc 1x1 10</span>
dropout_rate0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span><span class="token string">"float"</span><span class="token punctuation">)</span>
h_fc1_drop <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>h_fc1<span class="token punctuation">,</span> rate<span class="token operator">=</span>dropout_rate0<span class="token punctuation">)</span>
W_fc2 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_fc2 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_fc2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>h_fc1_drop<span class="token punctuation">,</span> W_fc2<span class="token punctuation">)</span> <span class="token operator">+</span> b_fc2
<span class="token comment"># &gt; softmax0</span>
pred0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>h_fc2<span class="token punctuation">)</span>
<span class="token comment"># inception 4c</span>
filters_num_4c <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span>
inception_4c <span class="token operator">=</span> inception<span class="token punctuation">(</span>inception_4b<span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> filters_num_4c<span class="token punctuation">)</span>
<span class="token comment"># inception 4d</span>
filters_num_4d <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">112</span><span class="token punctuation">,</span> <span class="token number">144</span><span class="token punctuation">,</span> <span class="token number">288</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span>
inception_4d <span class="token operator">=</span> inception<span class="token punctuation">(</span>inception_4c<span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> filters_num_4d<span class="token punctuation">)</span>
<span class="token comment"># inception 4e</span>
filters_num_4e <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">320</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span>
inception_4e <span class="token operator">=</span> inception<span class="token punctuation">(</span>inception_4d<span class="token punctuation">,</span> <span class="token number">528</span><span class="token punctuation">,</span> filters_num_4e<span class="token punctuation">)</span>
h_pool5 <span class="token operator">=</span> max_pool_3x3_2<span class="token punctuation">(</span>inception_4e<span class="token punctuation">)</span>
<span class="token comment"># softmax1</span>
h_pool6 <span class="token operator">=</span> avg_pool_5x5<span class="token punctuation">(</span>inception_4d<span class="token punctuation">)</span>
<span class="token comment"># &gt; conv 1x1 128</span>
w_conv5 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">528</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_conv5 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_conv5 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>h_pool6<span class="token punctuation">,</span> w_conv5<span class="token punctuation">)</span> <span class="token operator">+</span> b_conv5<span class="token punctuation">)</span>
h_flat2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>h_conv5<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 重新展开</span>
<span class="token comment"># &gt; fc 1x1 1024</span>
W_fc3 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">*</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_fc3 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_fc3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>h_flat2<span class="token punctuation">,</span> W_fc3<span class="token punctuation">)</span> <span class="token operator">+</span> b_fc3<span class="token punctuation">)</span>
<span class="token comment"># &gt; fc 1x1 10</span>
dropout_rate1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span><span class="token string">"float"</span><span class="token punctuation">)</span>
h_fc3_drop <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>h_fc1<span class="token punctuation">,</span> rate<span class="token operator">=</span>dropout_rate1<span class="token punctuation">)</span>
W_fc4 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_fc4 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_fc4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>h_fc3_drop<span class="token punctuation">,</span> W_fc4<span class="token punctuation">)</span> <span class="token operator">+</span> b_fc4
<span class="token comment"># &gt; softmax1</span>
pred1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>h_fc4<span class="token punctuation">)</span>
</code></pre> <p>TensorFow的dropout 函数：<code>tf.nn.dropout()</code>的使用可在<a href="https://blog.csdn.net/qq_32618327/article/details/94379612" rel="nofollow" data-token="ef73e5f5bdd92b6a721f027d46ae1cb3">TensorFlow笔记（9） ResNet</a>中查看</p> </li> 
   <li> <p>第五阶段 最终输出<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721163755373.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9qb3ZlaC1oLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br> 第五阶段的过程：inception 5a &gt; inception 5b &gt; avg_pool_7x7 &gt; softmax2</p> <pre><code class="prism language-python"><span class="token comment"># inception 5a</span>
filters_num_5a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">320</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span>
inception_5a <span class="token operator">=</span> inception<span class="token punctuation">(</span>h_pool5<span class="token punctuation">,</span> <span class="token number">832</span><span class="token punctuation">,</span> filters_num_5a<span class="token punctuation">)</span>
<span class="token comment"># inception 5b</span>
filters_num_5b <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">384</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">,</span> <span class="token number">48</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span>
inception_5b <span class="token operator">=</span> inception<span class="token punctuation">(</span>inception_5a<span class="token punctuation">,</span> <span class="token number">832</span><span class="token punctuation">,</span> filters_num_5b<span class="token punctuation">)</span>

<span class="token comment"># avg_pool_7x7</span>
h_pool7 <span class="token operator">=</span> avg_pool_7x7<span class="token punctuation">(</span>inception_5b<span class="token punctuation">)</span>
h_flat3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>h_pool7<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">*</span> <span class="token number">1</span> <span class="token operator">*</span> <span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 重新展开</span>

<span class="token comment"># softmax2</span>
dropout_rate2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span><span class="token string">"float"</span><span class="token punctuation">)</span>
h_pool7_drop <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>h_flat3<span class="token punctuation">,</span> rate<span class="token operator">=</span>dropout_rate2<span class="token punctuation">)</span>
W_fc5 <span class="token operator">=</span> weight_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b_fc5 <span class="token operator">=</span> bias_variable<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
h_fc5 <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>h_pool7_drop<span class="token punctuation">,</span> W_fc5<span class="token punctuation">)</span> <span class="token operator">+</span> b_fc2
pred2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>h_fc5<span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>整合softmax</p> <pre><code class="prism language-python">forward <span class="token operator">=</span> h_fc5 <span class="token operator">*</span> <span class="token number">0.4</span> <span class="token operator">+</span> h_fc4 <span class="token operator">*</span> <span class="token number">0.3</span> <span class="token operator">+</span> h_fc2 <span class="token operator">*</span> <span class="token number">0.3</span>
pred <span class="token operator">=</span> pred2 <span class="token operator">*</span> <span class="token number">0.4</span> <span class="token operator">+</span> pred1 <span class="token operator">*</span> <span class="token number">0.3</span> <span class="token operator">+</span> pred0 <span class="token operator">*</span> <span class="token number">0.3</span>
</code></pre> </li> 
   <li> <p>定义损失函数<br> 使用TensoFlow提供的结合Softmax的交叉熵损失函数定义方法：<em><strong>softmax_cross_entropy_with_logits_v2</strong></em><br> 交叉熵损失函数其实就是逻辑回归损失函数的前半部 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
        <math>
         <semantics>
          <mrow>
           <mo>
            −
           </mo>
           <mi>
            y
           </mi>
           <mo>
            ∗
           </mo>
           <mi>
            l
           </mi>
           <mi>
            o
           </mi>
           <mi>
            g
           </mi>
           <mo>
            (
           </mo>
           <mi>
            p
           </mi>
           <mi>
            r
           </mi>
           <mi>
            e
           </mi>
           <mi>
            d
           </mi>
           <mo>
            )
           </mo>
          </mrow>
          <annotation encoding="application/x-tex">
           - y * log(pred)
          </annotation>
         </semantics>
        </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.77777em; vertical-align: -0.19444em;"></span><span class="mord">−</span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right: 0.03588em;">g</span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mclose">)</span></span></span></span></span><br> 忽略了 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
        <math>
         <semantics>
          <mrow>
           <mo>
            −
           </mo>
           <mo>
            (
           </mo>
           <mn>
            1
           </mn>
           <mo>
            −
           </mo>
           <mi>
            y
           </mi>
           <mo>
            )
           </mo>
           <mo>
            ∗
           </mo>
           <mi>
            l
           </mi>
           <mi>
            o
           </mi>
           <mi>
            g
           </mi>
           <mo>
            (
           </mo>
           <mn>
            1
           </mn>
           <mo>
            −
           </mo>
           <mi>
            p
           </mi>
           <mi>
            r
           </mi>
           <mi>
            e
           </mi>
           <mi>
            d
           </mi>
           <mo>
            )
           </mo>
          </mrow>
          <annotation encoding="application/x-tex">
           -(1 - y) * log(1 - pred)
          </annotation>
         </semantics>
        </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">−</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right: 0.03588em;">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit">p</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit">d</span><span class="mclose">)</span></span></span></span></span></p> <pre><code class="prism language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"LossFunction"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	loss_function <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax_cross_entropy_with_logits_v2<span class="token punctuation">(</span>logits<span class="token operator">=</span>forward<span class="token punctuation">,</span> labels<span class="token operator">=</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li> 
  </ul> 
  <hr> 
  <h1><a id="4__444"></a>4. 训练模型</h1> 
  <ul> 
   <li> <p>设置超参数：</p> <pre><code class="prism language-python">train_epochs <span class="token operator">=</span> <span class="token number">20</span>  <span class="token comment"># 迭代次数</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.001</span>  <span class="token comment"># 学习率</span>
</code></pre> </li> 
   <li> <p>定义Adam优化器，设置学习率和优化目标损失最小化：</p> <pre><code class="prism language-python">optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span><span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss_function<span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>定义预测类别匹配情况</p> <pre><code class="prism language-python">correct_prediction <span class="token operator">=</span> tf<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <p><em><strong>tf.equal(A, B)</strong></em> ：对比这两个矩阵或者向量的相等的元素，相等返回 <em>True</em>，相反返回 <em>False</em><br> <em><strong>tf.argmax(input,axis)</strong></em> ：根据axis取值的不同返回每行或者每列最大值的索引，<em>axis</em> 表示维度，0：第一维度（行），1：第二维度（列），-1：最后一个维度<br> 其实，这里的最终求得的索引，恰好就表示图片上的数字</p> </li> 
   <li> <p>定义准确率，将布尔值转化成浮点数，再求平均值</p> <pre><code class="prism language-python">accuracy <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>correct_prediction<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>读取模型与创建会话：</p> <pre><code class="prism language-python"><span class="token comment"># 定义保存模型</span>
saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span><span class="token punctuation">)</span>
save_dir <span class="token operator">=</span> <span class="token string">"../save_path/GoogleNet/"</span>

<span class="token comment"># 定义保存模型编号</span>
save_step <span class="token operator">=</span> <span class="token number">0</span>

<span class="token comment"># 恢复保存模型</span>
ckpt_dir <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>latest_checkpoint<span class="token punctuation">(</span>save_dir<span class="token punctuation">)</span>
sess <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 建立会话</span>
<span class="token keyword">if</span> ckpt_dir <span class="token operator">!=</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    saver<span class="token punctuation">.</span>restore<span class="token punctuation">(</span>sess<span class="token punctuation">,</span> ckpt_dir<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Finished loading"</span><span class="token punctuation">,</span> ckpt_dir<span class="token punctuation">)</span>
    save_step <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">(</span><span class="token string">"Set the load save step:"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token comment"># 变量初始化</span>
    sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>initialize_all_variables<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Finished initialize"</span><span class="token punctuation">)</span>
</code></pre> <p>关于如何保存模型，可在<a href="https://blog.csdn.net/qq_32618327/article/details/96835324" rel="nofollow" data-token="297b4b653c8c8f8766559b9a47470e34">TensorFlow笔记（10） CheckPoint</a>中查看<br> 如果是读取模型继续训练时，例如最近保存的是序号12的模型</p> <pre><code class="prism language-python"><span class="token comment"># Finished loading ../save_path/GoogleNet/model-12</span>
Set the load save step<span class="token punctuation">:</span>
</code></pre> <p>则输入现在加载的模型序号12，然后回车，之后的训练保存的模型就从13开始</p> <pre><code class="prism language-python">Set the load save step<span class="token punctuation">:</span><span class="token number">12</span>
</code></pre> </li> 
   <li> <p>设置批次大小和数量：<br> 如果在处理完整个5万个训练图片的训练集之后才进行一次训练<br> 这样的处理速度相对缓慢<br> 如果在处理完整个5万个训练图片的训练集之前先让梯度下降法处理一部分<br> 算法速度会更快<br> 可以把训练集分割为小一点的子集训练<br> 如128张训练图片，然后就进行梯度下降法处理<br> 这种梯度下降法处理方法称之为<strong>Mini-batch 梯度下降</strong><br> 具体可参考<a href="https://blog.csdn.net/qq_32618327/article/details/90339408" rel="nofollow" data-token="7eb25720ea75004ca0e275ce1c4e4b2a">深度学习笔记（9） 优化算法（一）</a></p> <pre><code class="prism language-python"><span class="token comment"># 每个批次的大小，每次放入的大小，每次放入 128张图片 以矩阵的方式</span>
batch_size <span class="token operator">=</span> <span class="token number">128</span>

<span class="token comment"># 计算一共有多少个批次，数量整除大小训练出有多少批次</span>
n_batch <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>Ytrain_onehot<span class="token punctuation">)</span> <span class="token operator">//</span> batch_size

<span class="token comment"># 定义显示训练过程中验证的间隔批量次数</span>
display_test_num <span class="token operator">=</span> n_batch <span class="token operator">//</span> <span class="token number">10</span>

<span class="token comment"># 定义显示训练过程.的间隔批量次数</span>
display_train_num <span class="token operator">=</span> display_test_num <span class="token operator">//</span> <span class="token number">10</span>

<span class="token comment"># 定义批次训练数据的占位符</span>
x_batch <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"X_batch"</span><span class="token punctuation">)</span>
<span class="token comment"># 随机上下翻转批次图片</span>
Xtrain_batch_up <span class="token operator">=</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>random_flip_up_down<span class="token punctuation">(</span>x_batch<span class="token punctuation">)</span>
<span class="token comment"># 随机左右翻转批次图片</span>
Xtrain_batch_lf <span class="token operator">=</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>random_flip_left_right<span class="token punctuation">(</span>Xtrain_batch_up<span class="token punctuation">)</span>
<span class="token comment"># 对角旋转批次图片</span>
<span class="token comment"># Xtrain_batch_tp = tf.image.transpose_image(x_batch)</span>


<span class="token comment"># 定义训练集批次函数</span>
<span class="token keyword">def</span> <span class="token function">get_train_batch</span><span class="token punctuation">(</span>num<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    Xtrain_batch <span class="token operator">=</span> Xtrain_norm_shuffle<span class="token punctuation">[</span>num <span class="token operator">*</span> size<span class="token punctuation">:</span><span class="token punctuation">(</span>num <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> size<span class="token punctuation">]</span>
    Ytrain_batch <span class="token operator">=</span> Ytrain_onehot_shuffle<span class="token punctuation">[</span>num <span class="token operator">*</span> size<span class="token punctuation">:</span><span class="token punctuation">(</span>num <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> size<span class="token punctuation">]</span>
    <span class="token comment"># 随机翻转数据</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess_batch<span class="token punctuation">:</span>
        Xtrain_batch <span class="token operator">=</span> sess_batch<span class="token punctuation">.</span>run<span class="token punctuation">(</span>Xtrain_batch_lf<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>x_batch<span class="token punctuation">:</span> Xtrain_batch<span class="token punctuation">}</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> Xtrain_batch<span class="token punctuation">,</span> Ytrain_batch

</code></pre> </li> 
   <li> <p>批次迭代训练，其中 dropout 随机丢弃的概率都为0.4，显示迭代过程中的信息：</p> <pre><code class="prism language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>train_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 打乱训练数据集</span>
    index <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>Ytrain_onehot<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>index<span class="token punctuation">)</span>
    Xtrain_norm_shuffle <span class="token operator">=</span> Xtrain_norm<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
    Ytrain_onehot_shuffle <span class="token operator">=</span> Ytrain_onehot<span class="token punctuation">[</span>index<span class="token punctuation">]</span>

    <span class="token comment"># 批次迭代训练</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> n_batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
        xs<span class="token punctuation">,</span> ys <span class="token operator">=</span> get_train_batch<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
        sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>x<span class="token punctuation">:</span> xs<span class="token punctuation">,</span> y<span class="token punctuation">:</span> ys<span class="token punctuation">,</span> dropout_rate0<span class="token punctuation">:</span> <span class="token number">0.4</span><span class="token punctuation">,</span> dropout_rate1<span class="token punctuation">:</span> <span class="token number">0.4</span><span class="token punctuation">,</span> dropout_rate2<span class="token punctuation">:</span> <span class="token number">0.4</span><span class="token punctuation">}</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>batch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> display_train_num <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">,</span> end<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>batch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> display_test_num <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># 保存模型</span>
            save_step <span class="token operator">+=</span> <span class="token number">1</span>
            save_path <span class="token operator">=</span> saver<span class="token punctuation">.</span>save<span class="token punctuation">(</span>sess<span class="token punctuation">,</span> save_dir <span class="token operator">+</span> <span class="token string">"model"</span><span class="token punctuation">,</span> global_step<span class="token operator">=</span>save_step<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Complete save "</span><span class="token punctuation">,</span> save_path<span class="token punctuation">)</span>
            <span class="token comment"># 批次训练完成之后，使用测试数据计算误差与准确率</span>
            loss<span class="token punctuation">,</span> acc <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>loss_function<span class="token punctuation">,</span> accuracy<span class="token punctuation">]</span><span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>x<span class="token punctuation">:</span> Xtest_norm<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">512</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                                       y<span class="token punctuation">:</span> Ytest_onehot<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">512</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                                       dropout_rate0<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
                                                                       dropout_rate1<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
                                                                       dropout_rate2<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
	learning_rate <span class="token operator">=</span> <span class="token number">0.95</span> <span class="token operator">*</span> learning_rate <span class="token comment"># 学习率衰减</span>
	
<span class="token comment"># ..........Complete save D:/save_path/GoogleNet/model-1</span>
<span class="token comment"># TrainEpoch= 01 TrainBatch= 0039 Loss= 1.958486080 TestAccuracy= 0.1973</span>
<span class="token comment"># ...</span>
<span class="token comment"># ..........Complete save D:/save_path/GoogleNet/model-10</span>
<span class="token comment"># TrainEpoch= 01 TrainBatch= 0390 Loss= 1.628237653 TestAccuracy= 0.4268</span>
<span class="token comment"># ...</span>
<span class="token comment"># ..........Complete save D:/save_path/GoogleNet/model-200</span>
<span class="token comment"># TrainEpoch= 20 TrainBatch= 0390 Loss= 0.510145028 TestAccuracy= 0.8066</span>
</code></pre> <p>数据训练量比较大，而设备有限，为了保护设备而断断续续的训练<br> 过一遍数据，抽取测试集前512张图片分类的准确率到达42.68%<br> 最后一共历遍训练集20次，抽取测试集前512张图片分类的准确率到达 80.66%</p> </li> 
  </ul> 
  <hr> 
  <h1><a id="5__586"></a>5. 评估模型</h1> 
  <p>测试集上评估模型预测的准确率</p> 
  <pre><code class="prism language-python">test_total_batch <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>Xtest_norm<span class="token punctuation">)</span> <span class="token operator">/</span> batch_size<span class="token punctuation">)</span>
test_acc_sum <span class="token operator">=</span> <span class="token number">0.0</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>test_total_batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    test_image_batch <span class="token operator">=</span> Xtest_norm<span class="token punctuation">[</span>i <span class="token operator">*</span> batch_size<span class="token punctuation">:</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> batch_size<span class="token punctuation">]</span>
    test_label_batch <span class="token operator">=</span> Ytest_onehot<span class="token punctuation">[</span>i <span class="token operator">*</span> batch_size<span class="token punctuation">:</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> batch_size<span class="token punctuation">]</span>
    test_batch_acc <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>accuracy<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>x<span class="token punctuation">:</span> test_image_batch<span class="token punctuation">,</span>
                                                   y<span class="token punctuation">:</span> test_label_batch<span class="token punctuation">,</span>
                                                   dropout_rate0<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
                                                   dropout_rate1<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
                                                   dropout_rate2<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
    test_acc_sum <span class="token operator">+=</span> test_batch_acc
test_acc <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span>test_acc_sum <span class="token operator">/</span> test_total_batch<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Test accuracy:{:.6f}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># Test accuracy:0.804688</span>
</code></pre> 
  <p>测试集的准确率达到80%</p> 
  <hr> 
  <h1><a id="6__609"></a>6. 模型预测</h1> 
  <ul> 
   <li> <p>查看预测结果</p> <pre><code class="prism language-python"><span class="token comment"># 转换第1-10张测试图片pred预测结果独热编码格式为数字0-9</span>
prediction_result <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>x<span class="token punctuation">:</span> Xtest_normalize<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                        	dropout_rate0<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
                                                        	dropout_rate1<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
                                                        	dropout_rate2<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token comment"># 查看第1-10张测试图片的预测结果</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>prediction_result<span class="token punctuation">)</span>

<span class="token comment"># [3 8 8 0 6 6 1 6 3 1]</span>
</code></pre> <p>但是这样没办法知道，预测的到底是不是正确的</p> </li> 
   <li> <p>预测结果可视化比对<br> 定义可视化函数：</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">plot_images_labels_prediction</span><span class="token punctuation">(</span>images<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> prediction<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> num<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>gcf<span class="token punctuation">(</span><span class="token punctuation">)</span>
    fig<span class="token punctuation">.</span>set_size_inches<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> num <span class="token operator">&gt;</span> <span class="token number">10</span><span class="token punctuation">:</span>
        num <span class="token operator">=</span> <span class="token number">10</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num<span class="token punctuation">)</span><span class="token punctuation">:</span>
        ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">+</span> i<span class="token punctuation">)</span>
        ax<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>images<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"binary"</span><span class="token punctuation">)</span>
        title <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">','</span> <span class="token operator">+</span> label_dict<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>prediction<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            title <span class="token operator">+=</span> <span class="token string">'=&gt;'</span> <span class="token operator">+</span> label_dict<span class="token punctuation">[</span>prediction<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">]</span>
        ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span>title<span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
        idx <span class="token operator">+=</span> <span class="token number">1</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> </li> 
   <li> <p>可视化第1-10张测试图片的预测结果对比</p> <pre><code class="prism language-python">plot_images_labels_prediction<span class="token punctuation">(</span>Xtest<span class="token punctuation">,</span> Ytest<span class="token punctuation">,</span> rediction_result<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
</code></pre> <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190724151029400.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9qb3ZlaC1oLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p> <p>这次的预测都判断正确，还行。</p> </li> 
  </ul> 
  <hr> 
  <p>[1] python的代码地址:<br> <a href="https://github.com/JoveH-H/TensorFlow/blob/master/py/8.GoogleNet.py" rel="nofollow" data-token="b6b5f29eb3bb9b1f059a9d3d6fc238bd">https://github.com/JoveH-H/TensorFlow/blob/master/py/8.GoogleNet.py</a><br> [2] jupyter notebook的代码地址:<br> （待补充）<br> [3] MNIST 数据集 t10k-images-idx3-ubyte.gz<br> <a href="http://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow" data-token="fecec1939431790ed9961ab6eba6697e">http://www.cs.toronto.edu/~kriz/cifar.html</a></p> 
  <hr> 
  <p>相关推荐：</p> 
  <p><a href="https://blog.csdn.net/qq_32618327/article/details/93479440" rel="nofollow" data-token="d9c8b6ca79cc76940f2427012f425a00">深度学习笔记（30） Inception网络</a><br> <a href="https://blog.csdn.net/qq_32618327/article/details/91350707" rel="nofollow" data-token="bece28b431f8054f752f8f930fc1650f">深度学习笔记（26） 卷积神经网络</a><br> <a href="https://blog.csdn.net/qq_32618327/article/details/96835324" rel="nofollow" data-token="297b4b653c8c8f8766559b9a47470e34">TensorFlow笔记（10） CheckPoint</a><br> <a href="https://blog.csdn.net/qq_32618327/article/details/94379612" rel="nofollow" data-token="ef73e5f5bdd92b6a721f027d46ae1cb3">TensorFlow笔记（9） ResNet</a><br> <a href="https://blog.csdn.net/qq_32618327/article/details/91786928" rel="nofollow" data-token="cc71f228b9dd6425afafd259b3003de3">TensorFlow笔记（8） LeNet-5卷积神经网络</a></p> 
  <hr> 
  <p>谢谢！</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
