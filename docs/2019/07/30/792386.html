<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>聚类总结（一）K-means、层次、DBSCAN、均值漂移、K-Means 与 KNN | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="聚类总结（一）K-means、层次、DBSCAN、均值漂移、K-Means 与 KNN" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 一、K-means简介 1.1 K-means简介 1.1.1 K值的确定 1.1.2 K-means 成本函数（利用SSE选择k） 1.2 层次聚类 1.3 DBSCAN - 基于密度的聚类算法 1.3.1 简介 1.3.2 具体步骤 1.4 均值漂移聚类 1.4.1 简介 1.4.2 步骤 二、代码 2.1 原理推导 2.2 make_blobs 三、总结 3.1 K-Means 与 KNN 3.2 K-Means的主要优点有 3.3 K-Means的主要缺点有 一、K-means简介 1.1 K-means简介   K-means是无监督的聚类算法。其主要思想是选择K个点作为初始聚类中心， 将每个对象分配到最近的中心形成K个簇，重新计算每个簇的中心，重复以上迭代步骤，直到簇不再变化或达到指定迭代次数为止。，让簇内的点尽量紧密的连接在一起，而让簇间的距离尽量的大。   K-means每次计算质心，第一次是随机产生质心，第二次开始，是根据第一次分类后，每类的平均值作为质心，所以叫K-means聚类。   K-means如果簇中存在异常点,将导致均值偏差比较严重。例如： 一个簇中有2、4、6、8、100五个数据,那么新的质点为24,显然这个质点离绝大多数点都比较远;在当前情况下,使用中位数6可能比使用均值的想法更好,使用中位数的聚类方式叫做K- Mediods聚类(K中值聚类)。 1.1.1 K值的确定   在实际的应用中，主要两种方法进行K值的确定： 经验法：在实际的工作中，可以结合业务的场景和需求，来决定分几类以确定K值。 肘部法则：在使用聚类算法时，如果没有指定聚类的数量，即K值，则可以通过肘部法则来进行对K值得确定。肘部法则是通过成本函数来刻画的，其是通过将不同K值的成本函数刻画出来，随着K值的增大，平均畸变程度会不断减小且每个类包含的样本数会减少，于是样本离其重心会更近。但是，随着值继续增大，平均畸变程度的改善效果会不断减低。因此找出在K值增大的过程中，畸变程度下降幅度最大的位置所对应的K较为合理。 1.1.2 K-means 成本函数（利用SSE选择k）   k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散。 畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点。   指定一个i值，即可能的最大类簇数。然后将类簇数从1开始递增，一直到i，计算出i个簇内误方差(SSE)。根据数据的潜在模式，当设定的类簇数不断逼近真实类簇数时，SSE呈现快速下降态势，而当设定类簇数超过真实类簇数时，SSE也会继续下降，当下降会迅速趋于缓慢。通过画出K-SSE曲线，找出下降途中的拐点，即可较好的确定K值。   从图中可以看出，k值从1到2时，平均畸变程度变化最大。超过2以后，平均畸变程度变化显著降低。因此最佳的k是2。 1.2 层次聚类   尽管k-means的原理很简单，然而层次聚类法的原理更简单。它的基本过程如下： 每一个样本点视为一个簇； 计算各个簇之间的距离，最近的两个簇聚合成一个新簇； 重复以上过程直至最后只有一簇。   层次聚类不指定具体的簇数，而只关注簇之间的远近，最终会形成一个树形图。 1.3 DBSCAN - 基于密度的聚类算法 1.3.1 简介   DBSCAN（Density-Based Spatial Clustering of Application with Noise） - 基于密度的聚类算法.   基于密度的聚类方法与其他方法的一个根本区别是：它不是基于各种各样的距离度量的，而是基于密度的。因此它能克服基于距离的算法只能发现“类圆形”的聚类的缺点。   DBSCAN的指导思想是：用一个点的∈邻域内的邻居点数衡量该点所在空间的密度，只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去 1.3.2 具体步骤   与均值漂移聚类类似，DBSCAN也是基于密度的聚类算法。 具体步骤： 首先确定半径r和minPoints. 从一个没有被访问过的任意数据点开始，以这个点为中心，r为半径的圆内包含的点的数量是否大于或等于minPoints，如果大于或等于minPoints则改点被标记为central point,反之则会被标记为noise point。 重复1的步骤，如果一个noise point存在于某个central point为半径的圆内，则这个点被标记为边缘点，反之仍为noise point。重复步骤1，知道所有的点都被访问过。 优点：不需要知道簇的数量 缺点：需要确定距离r和minPoints 1.4 均值漂移聚类 1.4.1 简介   均值漂移聚类是基于滑动窗口的算法，来找到数据点的密集区域。这是一个基于质心的算法，通过将中心点的候选点更新为滑动窗口内点的均值来完成，来定位每个组/类的中心点。然后对这些候选窗口进行相似窗口进行去除，最终形成中心点集及相应的分组。 1.4.2 步骤 具体步骤： 确定滑动窗口半径r，以随机选取的中心点C半径为r的圆形滑动窗口开始滑动。均值漂移类似一种爬山算法，在每一次迭代中向密度更高的区域移动，直到收敛。 每一次滑动到新的区域，计算滑动窗口内的均值来作为中心点，滑动窗口内的点的数量为窗口内的密度。在每一次移动中，窗口会想密度更高的区域移动。 移动窗口，计算窗口内的中心点以及窗口内的密度，知道没有方向在窗口内可以容纳更多的点，即一直移动到圆内密度不再增加为止。 步骤一到三会产生很多个滑动窗口，当多个滑动窗口重叠时，保留包含最多点的窗口，然后根据数据点所在的滑动窗口进行聚类。 二、代码 2.1 原理推导 from scipy.io import loadmat import pandas as pd import seaborn as sb import matplotlib.pyplot as plt import numpy as np &#39;&#39; &#39;&#39;&#39; K-means是一个迭代的，无监督的聚类算法，将类似的实例组合成簇。 该算法通过猜测每个簇的 初始聚类中心开始，然后重复将实例分配给最近的簇，并重新计算该簇的聚类中心。 &#39;&#39;&#39; def find_closest_centroids(X, centroids): # 初始质心centroids m = X.shape[0] # m是样本数 k = centroids.shape[0] idx = np.zeros(m) for i in range(m): min_dist = 1000000 # 初始距离 样本与质心 for j in range(k): # k是聚类中心数 dist = np.sum((X[i, :] - centroids[j, :]) ** 2) print(&#39;dist&#39;, dist) # 300个样本，输出900个dist（计算每个样本到质心的距离） if dist &lt; min_dist: min_dist = dist idx[i] = j return idx data = loadmat(&#39;./data/ex7data2.mat&#39;) print(&#39;data.keys()&#39;, data.keys()) # data.keys() dict_keys([&#39;__header__&#39;, &#39;__version__&#39;, &#39;__globals__&#39;, &#39;X&#39;]) X = data[&#39;X&#39;] print(&#39;X.shape&#39;, X.shape) # X.shape (300, 2) initial_centroids = np.array([[3, 3], [6, 2], [8, 5]]) idx = find_closest_centroids(X, initial_centroids) print(&#39;idx.shape&#39;, idx.shape) # idx.shape (300,),即预测的结果 print(&#39;idx[0:3]&#39;, idx[0:3]) # idx[0:3] [0. 2. 1.] # 输出与文本中的预期值匹配（记住我们的数组是从0开始索引的，而不是从1开始索引的， #接下来，我们需要一个函数来计算簇的聚类中心。 聚类中心只是当前分配给簇的所有样本的平均值。 data2 = pd.DataFrame(data.get(&#39;X&#39;), columns=[&#39;X1&#39;, &#39;X2&#39;]) print(&#39;data2.head()&#39;,data2.head()) sb.set(context=&#39;notebook&#39;, style=&#39;white&#39;) sb.lmplot(&#39;X1&#39;, &#39;X2&#39;, data=data2, fit_reg=False)#lmplot是用来绘制回归图的 plt.show() def compute_centroids(X, idx, k):#计算质心 m, n = X.shape centroids = np.zeros((k, n)) for i in range(k): indices = np.where(idx == i) centroids[i, :] = (np.sum(X[indices, :], axis=1) / len(indices[0])).ravel() return centroids compute_centroids(X, idx, 3) &#39;&#39;&#39; array([[2.42830111, 3.15792418], [5.81350331, 2.63365645], [7.11938687, 3.6166844 ]]) &#39;&#39;&#39; def run_k_means(X, initial_centroids, max_iters): m, n = X.shape k = initial_centroids.shape[0] idx = np.zeros(m) centroids = initial_centroids for i in range(max_iters): idx = find_closest_centroids(X, centroids) centroids = compute_centroids(X, idx, k) return idx, centroids idx, centroids = run_k_means(X, initial_centroids, 10) cluster1 = X[np.where(idx == 0)[0],:] cluster2 = X[np.where(idx == 1)[0],:] cluster3 = X[np.where(idx == 2)[0],:] fig, ax = plt.subplots(figsize=(12,8)) ax.scatter(cluster1[:,0], cluster1[:,1], s=30, color=&#39;r&#39;, label=&#39;Cluster 1&#39;) ax.scatter(cluster2[:,0], cluster2[:,1], s=30, color=&#39;g&#39;, label=&#39;Cluster 2&#39;) ax.scatter(cluster3[:,0], cluster3[:,1], s=30, color=&#39;b&#39;, label=&#39;Cluster 3&#39;) ax.legend() plt.show() 2.2 make_blobs # 数据构建。:创建的团状的数据集合，数据分布呈高斯分布状况。 from sklearn.datasets import make_blobs N = 1000 centers = 4 X, Y = make_blobs(n_samples=N, n_features=2, centers=centers, random_state=0) # 模型构建:导入K-Means算法包，其底层就是按照前面讲的算法步骤一步一步创建的 from sklearn.cluster import KMeans # 给出要划分的中心点树k，也可以给出算法中止条件，迭代次数，或者簇中心变化率。 km = KMeans(n_clusters=centers, init=&#39;random&#39;, random_state=28) km.fit(X) # 模型的预测 y_hat = km.predict(X) # 求出模型的中心点坐标，并且得到，样本到中心点的总距离，也就是前面提到的损失函数。 print(&quot;所有样本距离所属簇中心点的总距离和为:%.5f&quot; % km.inertia_) print(&quot;所有的中心点聚类中心坐标:&quot;) cluter_centers = km.cluster_centers_ print(cluter_centers) print(&quot;score其实就是所有样本点离所属簇中心点距离和的相反数:&quot;) print(km.score(X)) &#39;&#39;&#39;运行结果 所有样本距离所属簇中心点的总距离和为:1734.01601 所有的中心点聚类中心坐标: [[ 1.99871335 0.79038817] [-1.31360134 7.86561393] [-1.51473374 2.8755229 ] [ 0.97790397 4.28661633]] score其实就是所有样本点离所属簇中心点距离和的相反数: -1734.0160089537276 &#39;&#39;&#39; # 画图，把原始数据和最终预测数据在图上表现出来 import matplotlib.pyplot as plt import matplotlib as mpl cm = mpl.colors.ListedColormap(list(&#39;rgby&#39;)) plt.figure(figsize=(15, 9), facecolor=&#39;w&#39;) plt.subplot(121) plt.title(u&#39;原始数据&#39;) plt.grid(True) plt.scatter(X[:, 0], X[:, 1], c=Y, s=30, cmap=cm, edgecolors=&#39;none&#39;) plt.subplot(122) plt.scatter(X[:, 0], X[:, 1], c=y_hat, s=30, cmap=cm, edgecolors=&#39;none&#39;) plt.title(u&#39;K-Means算法聚类结果&#39;) plt.grid(True) plt.show() 三、总结 3.1 K-Means 与 KNN K-Means是无监督学习的聚类算法，没有样本输出；而KNN是监督学习的分类算法，有对应的类别输出。KNN基本不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。 当然，两者也有一些相似点，两个算法都包含一个过程，即找出和某一个点最近的点。两者都利用了最近邻(nearest neighbors)的思想。 3.2 K-Means的主要优点有  　1）原理比较简单，实现也是很容易，收敛速度快。  　2）聚类效果较优。  　3）算法的可解释度比较强。  　4）主要需要调参的参数仅仅是簇数k。 3.3 K-Means的主要缺点有  　1）K值的选取不好把握  　2）对于不是凸的数据集比较难收敛  　3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。  　4） 采用迭代方法，得到的结果只是局部最优。  　5） 对噪音和异常点比较的敏感。 　　　 　　　 　6）每一次迭代都要重新计算各个点与质心的距离，然后排序，时间成本较高。" />
<meta property="og:description" content="文章目录 一、K-means简介 1.1 K-means简介 1.1.1 K值的确定 1.1.2 K-means 成本函数（利用SSE选择k） 1.2 层次聚类 1.3 DBSCAN - 基于密度的聚类算法 1.3.1 简介 1.3.2 具体步骤 1.4 均值漂移聚类 1.4.1 简介 1.4.2 步骤 二、代码 2.1 原理推导 2.2 make_blobs 三、总结 3.1 K-Means 与 KNN 3.2 K-Means的主要优点有 3.3 K-Means的主要缺点有 一、K-means简介 1.1 K-means简介   K-means是无监督的聚类算法。其主要思想是选择K个点作为初始聚类中心， 将每个对象分配到最近的中心形成K个簇，重新计算每个簇的中心，重复以上迭代步骤，直到簇不再变化或达到指定迭代次数为止。，让簇内的点尽量紧密的连接在一起，而让簇间的距离尽量的大。   K-means每次计算质心，第一次是随机产生质心，第二次开始，是根据第一次分类后，每类的平均值作为质心，所以叫K-means聚类。   K-means如果簇中存在异常点,将导致均值偏差比较严重。例如： 一个簇中有2、4、6、8、100五个数据,那么新的质点为24,显然这个质点离绝大多数点都比较远;在当前情况下,使用中位数6可能比使用均值的想法更好,使用中位数的聚类方式叫做K- Mediods聚类(K中值聚类)。 1.1.1 K值的确定   在实际的应用中，主要两种方法进行K值的确定： 经验法：在实际的工作中，可以结合业务的场景和需求，来决定分几类以确定K值。 肘部法则：在使用聚类算法时，如果没有指定聚类的数量，即K值，则可以通过肘部法则来进行对K值得确定。肘部法则是通过成本函数来刻画的，其是通过将不同K值的成本函数刻画出来，随着K值的增大，平均畸变程度会不断减小且每个类包含的样本数会减少，于是样本离其重心会更近。但是，随着值继续增大，平均畸变程度的改善效果会不断减低。因此找出在K值增大的过程中，畸变程度下降幅度最大的位置所对应的K较为合理。 1.1.2 K-means 成本函数（利用SSE选择k）   k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散。 畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点。   指定一个i值，即可能的最大类簇数。然后将类簇数从1开始递增，一直到i，计算出i个簇内误方差(SSE)。根据数据的潜在模式，当设定的类簇数不断逼近真实类簇数时，SSE呈现快速下降态势，而当设定类簇数超过真实类簇数时，SSE也会继续下降，当下降会迅速趋于缓慢。通过画出K-SSE曲线，找出下降途中的拐点，即可较好的确定K值。   从图中可以看出，k值从1到2时，平均畸变程度变化最大。超过2以后，平均畸变程度变化显著降低。因此最佳的k是2。 1.2 层次聚类   尽管k-means的原理很简单，然而层次聚类法的原理更简单。它的基本过程如下： 每一个样本点视为一个簇； 计算各个簇之间的距离，最近的两个簇聚合成一个新簇； 重复以上过程直至最后只有一簇。   层次聚类不指定具体的簇数，而只关注簇之间的远近，最终会形成一个树形图。 1.3 DBSCAN - 基于密度的聚类算法 1.3.1 简介   DBSCAN（Density-Based Spatial Clustering of Application with Noise） - 基于密度的聚类算法.   基于密度的聚类方法与其他方法的一个根本区别是：它不是基于各种各样的距离度量的，而是基于密度的。因此它能克服基于距离的算法只能发现“类圆形”的聚类的缺点。   DBSCAN的指导思想是：用一个点的∈邻域内的邻居点数衡量该点所在空间的密度，只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去 1.3.2 具体步骤   与均值漂移聚类类似，DBSCAN也是基于密度的聚类算法。 具体步骤： 首先确定半径r和minPoints. 从一个没有被访问过的任意数据点开始，以这个点为中心，r为半径的圆内包含的点的数量是否大于或等于minPoints，如果大于或等于minPoints则改点被标记为central point,反之则会被标记为noise point。 重复1的步骤，如果一个noise point存在于某个central point为半径的圆内，则这个点被标记为边缘点，反之仍为noise point。重复步骤1，知道所有的点都被访问过。 优点：不需要知道簇的数量 缺点：需要确定距离r和minPoints 1.4 均值漂移聚类 1.4.1 简介   均值漂移聚类是基于滑动窗口的算法，来找到数据点的密集区域。这是一个基于质心的算法，通过将中心点的候选点更新为滑动窗口内点的均值来完成，来定位每个组/类的中心点。然后对这些候选窗口进行相似窗口进行去除，最终形成中心点集及相应的分组。 1.4.2 步骤 具体步骤： 确定滑动窗口半径r，以随机选取的中心点C半径为r的圆形滑动窗口开始滑动。均值漂移类似一种爬山算法，在每一次迭代中向密度更高的区域移动，直到收敛。 每一次滑动到新的区域，计算滑动窗口内的均值来作为中心点，滑动窗口内的点的数量为窗口内的密度。在每一次移动中，窗口会想密度更高的区域移动。 移动窗口，计算窗口内的中心点以及窗口内的密度，知道没有方向在窗口内可以容纳更多的点，即一直移动到圆内密度不再增加为止。 步骤一到三会产生很多个滑动窗口，当多个滑动窗口重叠时，保留包含最多点的窗口，然后根据数据点所在的滑动窗口进行聚类。 二、代码 2.1 原理推导 from scipy.io import loadmat import pandas as pd import seaborn as sb import matplotlib.pyplot as plt import numpy as np &#39;&#39; &#39;&#39;&#39; K-means是一个迭代的，无监督的聚类算法，将类似的实例组合成簇。 该算法通过猜测每个簇的 初始聚类中心开始，然后重复将实例分配给最近的簇，并重新计算该簇的聚类中心。 &#39;&#39;&#39; def find_closest_centroids(X, centroids): # 初始质心centroids m = X.shape[0] # m是样本数 k = centroids.shape[0] idx = np.zeros(m) for i in range(m): min_dist = 1000000 # 初始距离 样本与质心 for j in range(k): # k是聚类中心数 dist = np.sum((X[i, :] - centroids[j, :]) ** 2) print(&#39;dist&#39;, dist) # 300个样本，输出900个dist（计算每个样本到质心的距离） if dist &lt; min_dist: min_dist = dist idx[i] = j return idx data = loadmat(&#39;./data/ex7data2.mat&#39;) print(&#39;data.keys()&#39;, data.keys()) # data.keys() dict_keys([&#39;__header__&#39;, &#39;__version__&#39;, &#39;__globals__&#39;, &#39;X&#39;]) X = data[&#39;X&#39;] print(&#39;X.shape&#39;, X.shape) # X.shape (300, 2) initial_centroids = np.array([[3, 3], [6, 2], [8, 5]]) idx = find_closest_centroids(X, initial_centroids) print(&#39;idx.shape&#39;, idx.shape) # idx.shape (300,),即预测的结果 print(&#39;idx[0:3]&#39;, idx[0:3]) # idx[0:3] [0. 2. 1.] # 输出与文本中的预期值匹配（记住我们的数组是从0开始索引的，而不是从1开始索引的， #接下来，我们需要一个函数来计算簇的聚类中心。 聚类中心只是当前分配给簇的所有样本的平均值。 data2 = pd.DataFrame(data.get(&#39;X&#39;), columns=[&#39;X1&#39;, &#39;X2&#39;]) print(&#39;data2.head()&#39;,data2.head()) sb.set(context=&#39;notebook&#39;, style=&#39;white&#39;) sb.lmplot(&#39;X1&#39;, &#39;X2&#39;, data=data2, fit_reg=False)#lmplot是用来绘制回归图的 plt.show() def compute_centroids(X, idx, k):#计算质心 m, n = X.shape centroids = np.zeros((k, n)) for i in range(k): indices = np.where(idx == i) centroids[i, :] = (np.sum(X[indices, :], axis=1) / len(indices[0])).ravel() return centroids compute_centroids(X, idx, 3) &#39;&#39;&#39; array([[2.42830111, 3.15792418], [5.81350331, 2.63365645], [7.11938687, 3.6166844 ]]) &#39;&#39;&#39; def run_k_means(X, initial_centroids, max_iters): m, n = X.shape k = initial_centroids.shape[0] idx = np.zeros(m) centroids = initial_centroids for i in range(max_iters): idx = find_closest_centroids(X, centroids) centroids = compute_centroids(X, idx, k) return idx, centroids idx, centroids = run_k_means(X, initial_centroids, 10) cluster1 = X[np.where(idx == 0)[0],:] cluster2 = X[np.where(idx == 1)[0],:] cluster3 = X[np.where(idx == 2)[0],:] fig, ax = plt.subplots(figsize=(12,8)) ax.scatter(cluster1[:,0], cluster1[:,1], s=30, color=&#39;r&#39;, label=&#39;Cluster 1&#39;) ax.scatter(cluster2[:,0], cluster2[:,1], s=30, color=&#39;g&#39;, label=&#39;Cluster 2&#39;) ax.scatter(cluster3[:,0], cluster3[:,1], s=30, color=&#39;b&#39;, label=&#39;Cluster 3&#39;) ax.legend() plt.show() 2.2 make_blobs # 数据构建。:创建的团状的数据集合，数据分布呈高斯分布状况。 from sklearn.datasets import make_blobs N = 1000 centers = 4 X, Y = make_blobs(n_samples=N, n_features=2, centers=centers, random_state=0) # 模型构建:导入K-Means算法包，其底层就是按照前面讲的算法步骤一步一步创建的 from sklearn.cluster import KMeans # 给出要划分的中心点树k，也可以给出算法中止条件，迭代次数，或者簇中心变化率。 km = KMeans(n_clusters=centers, init=&#39;random&#39;, random_state=28) km.fit(X) # 模型的预测 y_hat = km.predict(X) # 求出模型的中心点坐标，并且得到，样本到中心点的总距离，也就是前面提到的损失函数。 print(&quot;所有样本距离所属簇中心点的总距离和为:%.5f&quot; % km.inertia_) print(&quot;所有的中心点聚类中心坐标:&quot;) cluter_centers = km.cluster_centers_ print(cluter_centers) print(&quot;score其实就是所有样本点离所属簇中心点距离和的相反数:&quot;) print(km.score(X)) &#39;&#39;&#39;运行结果 所有样本距离所属簇中心点的总距离和为:1734.01601 所有的中心点聚类中心坐标: [[ 1.99871335 0.79038817] [-1.31360134 7.86561393] [-1.51473374 2.8755229 ] [ 0.97790397 4.28661633]] score其实就是所有样本点离所属簇中心点距离和的相反数: -1734.0160089537276 &#39;&#39;&#39; # 画图，把原始数据和最终预测数据在图上表现出来 import matplotlib.pyplot as plt import matplotlib as mpl cm = mpl.colors.ListedColormap(list(&#39;rgby&#39;)) plt.figure(figsize=(15, 9), facecolor=&#39;w&#39;) plt.subplot(121) plt.title(u&#39;原始数据&#39;) plt.grid(True) plt.scatter(X[:, 0], X[:, 1], c=Y, s=30, cmap=cm, edgecolors=&#39;none&#39;) plt.subplot(122) plt.scatter(X[:, 0], X[:, 1], c=y_hat, s=30, cmap=cm, edgecolors=&#39;none&#39;) plt.title(u&#39;K-Means算法聚类结果&#39;) plt.grid(True) plt.show() 三、总结 3.1 K-Means 与 KNN K-Means是无监督学习的聚类算法，没有样本输出；而KNN是监督学习的分类算法，有对应的类别输出。KNN基本不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。 当然，两者也有一些相似点，两个算法都包含一个过程，即找出和某一个点最近的点。两者都利用了最近邻(nearest neighbors)的思想。 3.2 K-Means的主要优点有  　1）原理比较简单，实现也是很容易，收敛速度快。  　2）聚类效果较优。  　3）算法的可解释度比较强。  　4）主要需要调参的参数仅仅是簇数k。 3.3 K-Means的主要缺点有  　1）K值的选取不好把握  　2）对于不是凸的数据集比较难收敛  　3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。  　4） 采用迭代方法，得到的结果只是局部最优。  　5） 对噪音和异常点比较的敏感。 　　　 　　　 　6）每一次迭代都要重新计算各个点与质心的距离，然后排序，时间成本较高。" />
<link rel="canonical" href="https://uzzz.org/2019/07/30/792386.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/30/792386.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-30T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 一、K-means简介 1.1 K-means简介 1.1.1 K值的确定 1.1.2 K-means 成本函数（利用SSE选择k） 1.2 层次聚类 1.3 DBSCAN - 基于密度的聚类算法 1.3.1 简介 1.3.2 具体步骤 1.4 均值漂移聚类 1.4.1 简介 1.4.2 步骤 二、代码 2.1 原理推导 2.2 make_blobs 三、总结 3.1 K-Means 与 KNN 3.2 K-Means的主要优点有 3.3 K-Means的主要缺点有 一、K-means简介 1.1 K-means简介   K-means是无监督的聚类算法。其主要思想是选择K个点作为初始聚类中心， 将每个对象分配到最近的中心形成K个簇，重新计算每个簇的中心，重复以上迭代步骤，直到簇不再变化或达到指定迭代次数为止。，让簇内的点尽量紧密的连接在一起，而让簇间的距离尽量的大。   K-means每次计算质心，第一次是随机产生质心，第二次开始，是根据第一次分类后，每类的平均值作为质心，所以叫K-means聚类。   K-means如果簇中存在异常点,将导致均值偏差比较严重。例如： 一个簇中有2、4、6、8、100五个数据,那么新的质点为24,显然这个质点离绝大多数点都比较远;在当前情况下,使用中位数6可能比使用均值的想法更好,使用中位数的聚类方式叫做K- Mediods聚类(K中值聚类)。 1.1.1 K值的确定   在实际的应用中，主要两种方法进行K值的确定： 经验法：在实际的工作中，可以结合业务的场景和需求，来决定分几类以确定K值。 肘部法则：在使用聚类算法时，如果没有指定聚类的数量，即K值，则可以通过肘部法则来进行对K值得确定。肘部法则是通过成本函数来刻画的，其是通过将不同K值的成本函数刻画出来，随着K值的增大，平均畸变程度会不断减小且每个类包含的样本数会减少，于是样本离其重心会更近。但是，随着值继续增大，平均畸变程度的改善效果会不断减低。因此找出在K值增大的过程中，畸变程度下降幅度最大的位置所对应的K较为合理。 1.1.2 K-means 成本函数（利用SSE选择k）   k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散。 畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点。   指定一个i值，即可能的最大类簇数。然后将类簇数从1开始递增，一直到i，计算出i个簇内误方差(SSE)。根据数据的潜在模式，当设定的类簇数不断逼近真实类簇数时，SSE呈现快速下降态势，而当设定类簇数超过真实类簇数时，SSE也会继续下降，当下降会迅速趋于缓慢。通过画出K-SSE曲线，找出下降途中的拐点，即可较好的确定K值。   从图中可以看出，k值从1到2时，平均畸变程度变化最大。超过2以后，平均畸变程度变化显著降低。因此最佳的k是2。 1.2 层次聚类   尽管k-means的原理很简单，然而层次聚类法的原理更简单。它的基本过程如下： 每一个样本点视为一个簇； 计算各个簇之间的距离，最近的两个簇聚合成一个新簇； 重复以上过程直至最后只有一簇。   层次聚类不指定具体的簇数，而只关注簇之间的远近，最终会形成一个树形图。 1.3 DBSCAN - 基于密度的聚类算法 1.3.1 简介   DBSCAN（Density-Based Spatial Clustering of Application with Noise） - 基于密度的聚类算法.   基于密度的聚类方法与其他方法的一个根本区别是：它不是基于各种各样的距离度量的，而是基于密度的。因此它能克服基于距离的算法只能发现“类圆形”的聚类的缺点。   DBSCAN的指导思想是：用一个点的∈邻域内的邻居点数衡量该点所在空间的密度，只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去 1.3.2 具体步骤   与均值漂移聚类类似，DBSCAN也是基于密度的聚类算法。 具体步骤： 首先确定半径r和minPoints. 从一个没有被访问过的任意数据点开始，以这个点为中心，r为半径的圆内包含的点的数量是否大于或等于minPoints，如果大于或等于minPoints则改点被标记为central point,反之则会被标记为noise point。 重复1的步骤，如果一个noise point存在于某个central point为半径的圆内，则这个点被标记为边缘点，反之仍为noise point。重复步骤1，知道所有的点都被访问过。 优点：不需要知道簇的数量 缺点：需要确定距离r和minPoints 1.4 均值漂移聚类 1.4.1 简介   均值漂移聚类是基于滑动窗口的算法，来找到数据点的密集区域。这是一个基于质心的算法，通过将中心点的候选点更新为滑动窗口内点的均值来完成，来定位每个组/类的中心点。然后对这些候选窗口进行相似窗口进行去除，最终形成中心点集及相应的分组。 1.4.2 步骤 具体步骤： 确定滑动窗口半径r，以随机选取的中心点C半径为r的圆形滑动窗口开始滑动。均值漂移类似一种爬山算法，在每一次迭代中向密度更高的区域移动，直到收敛。 每一次滑动到新的区域，计算滑动窗口内的均值来作为中心点，滑动窗口内的点的数量为窗口内的密度。在每一次移动中，窗口会想密度更高的区域移动。 移动窗口，计算窗口内的中心点以及窗口内的密度，知道没有方向在窗口内可以容纳更多的点，即一直移动到圆内密度不再增加为止。 步骤一到三会产生很多个滑动窗口，当多个滑动窗口重叠时，保留包含最多点的窗口，然后根据数据点所在的滑动窗口进行聚类。 二、代码 2.1 原理推导 from scipy.io import loadmat import pandas as pd import seaborn as sb import matplotlib.pyplot as plt import numpy as np &#39;&#39; &#39;&#39;&#39; K-means是一个迭代的，无监督的聚类算法，将类似的实例组合成簇。 该算法通过猜测每个簇的 初始聚类中心开始，然后重复将实例分配给最近的簇，并重新计算该簇的聚类中心。 &#39;&#39;&#39; def find_closest_centroids(X, centroids): # 初始质心centroids m = X.shape[0] # m是样本数 k = centroids.shape[0] idx = np.zeros(m) for i in range(m): min_dist = 1000000 # 初始距离 样本与质心 for j in range(k): # k是聚类中心数 dist = np.sum((X[i, :] - centroids[j, :]) ** 2) print(&#39;dist&#39;, dist) # 300个样本，输出900个dist（计算每个样本到质心的距离） if dist &lt; min_dist: min_dist = dist idx[i] = j return idx data = loadmat(&#39;./data/ex7data2.mat&#39;) print(&#39;data.keys()&#39;, data.keys()) # data.keys() dict_keys([&#39;__header__&#39;, &#39;__version__&#39;, &#39;__globals__&#39;, &#39;X&#39;]) X = data[&#39;X&#39;] print(&#39;X.shape&#39;, X.shape) # X.shape (300, 2) initial_centroids = np.array([[3, 3], [6, 2], [8, 5]]) idx = find_closest_centroids(X, initial_centroids) print(&#39;idx.shape&#39;, idx.shape) # idx.shape (300,),即预测的结果 print(&#39;idx[0:3]&#39;, idx[0:3]) # idx[0:3] [0. 2. 1.] # 输出与文本中的预期值匹配（记住我们的数组是从0开始索引的，而不是从1开始索引的， #接下来，我们需要一个函数来计算簇的聚类中心。 聚类中心只是当前分配给簇的所有样本的平均值。 data2 = pd.DataFrame(data.get(&#39;X&#39;), columns=[&#39;X1&#39;, &#39;X2&#39;]) print(&#39;data2.head()&#39;,data2.head()) sb.set(context=&#39;notebook&#39;, style=&#39;white&#39;) sb.lmplot(&#39;X1&#39;, &#39;X2&#39;, data=data2, fit_reg=False)#lmplot是用来绘制回归图的 plt.show() def compute_centroids(X, idx, k):#计算质心 m, n = X.shape centroids = np.zeros((k, n)) for i in range(k): indices = np.where(idx == i) centroids[i, :] = (np.sum(X[indices, :], axis=1) / len(indices[0])).ravel() return centroids compute_centroids(X, idx, 3) &#39;&#39;&#39; array([[2.42830111, 3.15792418], [5.81350331, 2.63365645], [7.11938687, 3.6166844 ]]) &#39;&#39;&#39; def run_k_means(X, initial_centroids, max_iters): m, n = X.shape k = initial_centroids.shape[0] idx = np.zeros(m) centroids = initial_centroids for i in range(max_iters): idx = find_closest_centroids(X, centroids) centroids = compute_centroids(X, idx, k) return idx, centroids idx, centroids = run_k_means(X, initial_centroids, 10) cluster1 = X[np.where(idx == 0)[0],:] cluster2 = X[np.where(idx == 1)[0],:] cluster3 = X[np.where(idx == 2)[0],:] fig, ax = plt.subplots(figsize=(12,8)) ax.scatter(cluster1[:,0], cluster1[:,1], s=30, color=&#39;r&#39;, label=&#39;Cluster 1&#39;) ax.scatter(cluster2[:,0], cluster2[:,1], s=30, color=&#39;g&#39;, label=&#39;Cluster 2&#39;) ax.scatter(cluster3[:,0], cluster3[:,1], s=30, color=&#39;b&#39;, label=&#39;Cluster 3&#39;) ax.legend() plt.show() 2.2 make_blobs # 数据构建。:创建的团状的数据集合，数据分布呈高斯分布状况。 from sklearn.datasets import make_blobs N = 1000 centers = 4 X, Y = make_blobs(n_samples=N, n_features=2, centers=centers, random_state=0) # 模型构建:导入K-Means算法包，其底层就是按照前面讲的算法步骤一步一步创建的 from sklearn.cluster import KMeans # 给出要划分的中心点树k，也可以给出算法中止条件，迭代次数，或者簇中心变化率。 km = KMeans(n_clusters=centers, init=&#39;random&#39;, random_state=28) km.fit(X) # 模型的预测 y_hat = km.predict(X) # 求出模型的中心点坐标，并且得到，样本到中心点的总距离，也就是前面提到的损失函数。 print(&quot;所有样本距离所属簇中心点的总距离和为:%.5f&quot; % km.inertia_) print(&quot;所有的中心点聚类中心坐标:&quot;) cluter_centers = km.cluster_centers_ print(cluter_centers) print(&quot;score其实就是所有样本点离所属簇中心点距离和的相反数:&quot;) print(km.score(X)) &#39;&#39;&#39;运行结果 所有样本距离所属簇中心点的总距离和为:1734.01601 所有的中心点聚类中心坐标: [[ 1.99871335 0.79038817] [-1.31360134 7.86561393] [-1.51473374 2.8755229 ] [ 0.97790397 4.28661633]] score其实就是所有样本点离所属簇中心点距离和的相反数: -1734.0160089537276 &#39;&#39;&#39; # 画图，把原始数据和最终预测数据在图上表现出来 import matplotlib.pyplot as plt import matplotlib as mpl cm = mpl.colors.ListedColormap(list(&#39;rgby&#39;)) plt.figure(figsize=(15, 9), facecolor=&#39;w&#39;) plt.subplot(121) plt.title(u&#39;原始数据&#39;) plt.grid(True) plt.scatter(X[:, 0], X[:, 1], c=Y, s=30, cmap=cm, edgecolors=&#39;none&#39;) plt.subplot(122) plt.scatter(X[:, 0], X[:, 1], c=y_hat, s=30, cmap=cm, edgecolors=&#39;none&#39;) plt.title(u&#39;K-Means算法聚类结果&#39;) plt.grid(True) plt.show() 三、总结 3.1 K-Means 与 KNN K-Means是无监督学习的聚类算法，没有样本输出；而KNN是监督学习的分类算法，有对应的类别输出。KNN基本不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。 当然，两者也有一些相似点，两个算法都包含一个过程，即找出和某一个点最近的点。两者都利用了最近邻(nearest neighbors)的思想。 3.2 K-Means的主要优点有  　1）原理比较简单，实现也是很容易，收敛速度快。  　2）聚类效果较优。  　3）算法的可解释度比较强。  　4）主要需要调参的参数仅仅是簇数k。 3.3 K-Means的主要缺点有  　1）K值的选取不好把握  　2）对于不是凸的数据集比较难收敛  　3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。  　4） 采用迭代方法，得到的结果只是局部最优。  　5） 对噪音和异常点比较的敏感。 　　　 　　　 　6）每一次迭代都要重新计算各个点与质心的距离，然后排序，时间成本较高。","@type":"BlogPosting","url":"https://uzzz.org/2019/07/30/792386.html","headline":"聚类总结（一）K-means、层次、DBSCAN、均值漂移、K-Means 与 KNN","dateModified":"2019-07-30T00:00:00+08:00","datePublished":"2019-07-30T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/30/792386.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>聚类总结（一）K-means、层次、DBSCAN、均值漂移、K-Means 与 KNN</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <li><a href="#Kmeans_1" rel="nofollow" data-token="101c5612d11f61392923186ee35fc577">一、K-means简介</a></li>
    <ul>
     <li><a href="#11_Kmeans_2" rel="nofollow" data-token="cffb6967df9256791b78301b24b39514">1.1 K-means简介</a></li>
     <ul>
      <li><a href="#111_K_8" rel="nofollow" data-token="ac20e49b7144640e3b044c69a6f6be8b">1.1.1 K值的确定</a></li>
      <li><a href="#112_Kmeans_SSEk_13" rel="nofollow" data-token="ce3d19c1b459395ec726053c2859abeb">1.1.2 K-means 成本函数（利用SSE选择k）</a></li>
     </ul>
     <li><a href="#12__19" rel="nofollow" data-token="fc0df8ce9daadd456dc269f7c314f5db">1.2 层次聚类</a></li>
     <li><a href="#13_DBSCAN___26" rel="nofollow" data-token="abb506eb212220680c3794e2adb76e35">1.3 DBSCAN - 基于密度的聚类算法</a></li>
     <ul>
      <li><a href="#131__27" rel="nofollow" data-token="cfac058defbd0971b567a54c0fa0460f">1.3.1 简介</a></li>
      <li><a href="#132__31" rel="nofollow" data-token="7ab566a0a6929b536e2847d0889abe40">1.3.2 具体步骤</a></li>
     </ul>
     <li><a href="#14__38" rel="nofollow" data-token="67e67ed6629f4feb11d6038af349d5cd">1.4 均值漂移聚类</a></li>
     <ul>
      <li><a href="#141__39" rel="nofollow" data-token="b20a790a7487f4680dea08a93b7573c8">1.4.1 简介</a></li>
      <li><a href="#142__41" rel="nofollow" data-token="8fb51265f816872460f5c6c00e5f6860">1.4.2 步骤</a></li>
     </ul>
    </ul>
    <li><a href="#_48" rel="nofollow" data-token="15b3c13f54167a26b440da7309704ffa">二、代码</a></li>
    <ul>
     <li><a href="#21__49" rel="nofollow" data-token="7bd62de025b0d642f0b607c27314496c">2.1 原理推导</a></li>
     <li><a href="#22_make_blobs_152" rel="nofollow" data-token="f9f8cf79a002cc4b8a5a904bf0cbe73e">2.2 make_blobs</a></li>
    </ul>
    <li><a href="#_217" rel="nofollow" data-token="7aaf0428766e014b2da032c221b3ec59">三、总结</a></li>
    <ul>
     <li><a href="#31_KMeans__KNN_218" rel="nofollow" data-token="01c6eb9f3e4ffe195dd6efa75b9fad55">3.1 K-Means 与 KNN</a></li>
     <li><a href="#32_KMeans_221" rel="nofollow" data-token="17e7dff0600667db133926d52c833556">3.2 K-Means的主要优点有</a></li>
     <li><a href="#33_KMeans_229" rel="nofollow" data-token="59733c5de169347b8e328dd5db86537f">3.3 K-Means的主要缺点有</a></li>
    </ul>
   </ul>
  </div>
  <p></p> 
  <h1><a id="Kmeans_1"></a>一、K-means简介</h1> 
  <h2><a id="11_Kmeans_2"></a>1.1 K-means简介</h2> 
  <p>  K-means是无监督的聚类算法。其主要思想是选择K个点作为初始聚类中心， 将每个对象分配到最近的中心形成K个簇，重新计算每个簇的中心，重复以上迭代步骤，直到簇不再变化或达到指定迭代次数为止。，让簇内的点尽量紧密的连接在一起，而让簇间的距离尽量的大。<br>   K-means每次计算质心，第一次是随机产生质心，第二次开始，是根据第一次分类后，<strong>每类的平均值作为质心</strong>，所以叫K-means聚类。<br>   K-means如果簇中存在异常点,将导致均值偏差比较严重。例如：<br> 一个簇中有2、4、6、8、100五个数据,那么新的质点为24,显然这个质点离绝大多数点都比较远;在当前情况下,使用中位数6可能比使用均值的想法更好,使用中位数的聚类方式叫做<strong>K- Mediods聚类(K中值聚类)</strong>。</p> 
  <h3><a id="111_K_8"></a>1.1.1 K值的确定</h3> 
  <p>  在实际的应用中，主要两种方法进行K值的确定：</p> 
  <ul> 
   <li><strong>经验法</strong>：在实际的工作中，可以结合业务的场景和需求，来决定分几类以确定K值。</li> 
   <li><strong>肘部法则</strong>：在使用聚类算法时，如果没有指定聚类的数量，即K值，则可以通过肘部法则来进行对K值得确定。肘部法则是通过成本函数来刻画的，其是通过将不同K值的<strong>成本函数</strong>刻画出来，随着K值的增大，平均畸变程度会不断减小且每个类包含的样本数会减少，于是样本离其重心会更近。但是，随着值继续增大，平均畸变程度的改善效果会不断减低。因此找出在K值增大的过程中，畸变程度下降幅度最大的位置所对应的K较为合理。</li> 
  </ul> 
  <h3><a id="112_Kmeans_SSEk_13"></a>1.1.2 K-means 成本函数（利用SSE选择k）</h3> 
  <p>  k-means是以最小化样本与质点平方误差作为目标函数，将<strong>每个簇的质点与簇内样本点的平方距离误差和称为畸变程度</strong>(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散。 畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点。<br>   指定一个i值，即可能的最大类簇数。然后将类簇数从1开始递增，一直到i，计算出i个<strong>簇内误方差(SSE)</strong>。根据数据的潜在模式，当设定的类簇数不断逼近真实类簇数时，SSE呈现快速下降态势，而当设定类簇数超过真实类簇数时，SSE也会继续下降，当下降会迅速趋于缓慢。通过画出<strong>K-SSE</strong>曲线，找出下降途中的拐点，即可较好的确定K值。</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730225319227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA5ODY3NTM=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>   从图中可以看出，k值从1到2时，平均畸变程度变化最大。超过2以后，平均畸变程度变化显著降低。因此最佳的k是2。</p> 
  <h2><a id="12__19"></a>1.2 层次聚类</h2> 
  <p>  尽管k-means的原理很简单，然而层次聚类法的原理更简单。它的基本过程如下：</p> 
  <ul> 
   <li>每一个样本点视为一个簇；</li> 
   <li>计算各个簇之间的距离，最近的两个簇聚合成一个新簇；</li> 
   <li>重复以上过程直至最后只有一簇。<br>   层次聚类不指定具体的簇数，而只关注簇之间的远近，最终会形成一个树形图。</li> 
  </ul> 
  <h2><a id="13_DBSCAN___26"></a>1.3 DBSCAN - 基于密度的聚类算法</h2> 
  <h3><a id="131__27"></a>1.3.1 简介</h3> 
  <p>  DBSCAN（Density-Based Spatial Clustering of Application with Noise） - 基于密度的聚类算法.<br>   基于密度的聚类方法与其他方法的一个根本区别是：它不是基于各种各样的距离度量的，而是基于密度的。因此它能克服基于距离的算法只能发现“类圆形”的聚类的缺点。<br>   <strong>DBSCAN的指导思想</strong>是：用一个点的∈邻域内的邻居点数衡量该点所在空间的密度，只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去</p> 
  <h3><a id="132__31"></a>1.3.2 具体步骤</h3> 
  <p>  与均值漂移聚类类似，DBSCAN也是基于密度的聚类算法。<br> <strong>具体步骤</strong>：</p> 
  <ol> 
   <li>首先确定半径r和minPoints. 从一个没有被访问过的任意数据点开始，以这个点为中心，r为半径的圆内包含的点的数量是否大于或等于minPoints，如果大于或等于minPoints则改点被标记为central point,反之则会被标记为noise point。</li> 
   <li>重复1的步骤，如果一个noise point存在于某个central point为半径的圆内，则这个点被标记为边缘点，反之仍为noise point。重复步骤1，知道所有的点都被访问过。<br> <strong>优点</strong>：不需要知道簇的数量<br> <strong>缺点</strong>：需要确定距离r和minPoints</li> 
  </ol> 
  <h2><a id="14__38"></a>1.4 均值漂移聚类</h2> 
  <h3><a id="141__39"></a>1.4.1 简介</h3> 
  <p>  均值漂移聚类是基于滑动窗口的算法，来找到数据点的密集区域。这是一个基于质心的算法，通过将中心点的候选点更新为滑动窗口内点的均值来完成，来定位每个组/类的中心点。然后对这些候选窗口进行相似窗口进行去除，最终形成中心点集及相应的分组。</p> 
  <h3><a id="142__41"></a>1.4.2 步骤</h3> 
  <p><strong>具体步骤</strong>：</p> 
  <ol> 
   <li>确定滑动窗口半径r，以随机选取的中心点C半径为r的圆形滑动窗口开始滑动。均值漂移类似一种爬山算法，在每一次迭代中向密度更高的区域移动，直到收敛。</li> 
   <li>每一次滑动到新的区域，计算滑动窗口内的均值来作为中心点，滑动窗口内的点的数量为窗口内的密度。在每一次移动中，窗口会想密度更高的区域移动。</li> 
   <li>移动窗口，计算窗口内的中心点以及窗口内的密度，知道没有方向在窗口内可以容纳更多的点，即一直移动到圆内密度不再增加为止。</li> 
   <li>步骤一到三会产生很多个滑动窗口，当多个滑动窗口重叠时，保留包含最多点的窗口，然后根据数据点所在的滑动窗口进行聚类。</li> 
  </ol> 
  <h1><a id="_48"></a>二、代码</h1> 
  <h2><a id="21__49"></a>2.1 原理推导</h2> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>io <span class="token keyword">import</span> loadmat
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sb
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token string">''</span>
<span class="token triple-quoted-string string">''' K-means是一个迭代的，无监督的聚类算法，将类似的实例组合成簇。 该算法通过猜测每个簇的 初始聚类中心开始，然后重复将实例分配给最近的簇，并重新计算该簇的聚类中心。 '''</span>


<span class="token keyword">def</span> <span class="token function">find_closest_centroids</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> centroids<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 初始质心centroids</span>
    m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># m是样本数</span>
    k <span class="token operator">=</span> centroids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    idx <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>m<span class="token punctuation">)</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        min_dist <span class="token operator">=</span> <span class="token number">1000000</span>  <span class="token comment"># 初始距离 样本与质心</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># k是聚类中心数</span>
            dist <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-</span> centroids<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'dist'</span><span class="token punctuation">,</span> dist<span class="token punctuation">)</span>  <span class="token comment"># 300个样本，输出900个dist（计算每个样本到质心的距离）</span>
            <span class="token keyword">if</span> dist <span class="token operator">&lt;</span> min_dist<span class="token punctuation">:</span>
                min_dist <span class="token operator">=</span> dist
                idx<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> j

    <span class="token keyword">return</span> idx

</code></pre> 
  <pre><code class="prism language-python">data <span class="token operator">=</span> loadmat<span class="token punctuation">(</span><span class="token string">'./data/ex7data2.mat'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'data.keys()'</span><span class="token punctuation">,</span> data<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># data.keys() dict_keys(['__header__', '__version__', '__globals__', 'X'])</span>
X <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'X'</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'X.shape'</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># X.shape (300, 2)</span>
initial_centroids <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

idx <span class="token operator">=</span> find_closest_centroids<span class="token punctuation">(</span>X<span class="token punctuation">,</span> initial_centroids<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'idx.shape'</span><span class="token punctuation">,</span> idx<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># idx.shape (300,),即预测的结果</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'idx[0:3]'</span><span class="token punctuation">,</span> idx<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># idx[0:3] [0. 2. 1.]</span>

<span class="token comment"># 输出与文本中的预期值匹配（记住我们的数组是从0开始索引的，而不是从1开始索引的，</span>
<span class="token comment">#接下来，我们需要一个函数来计算簇的聚类中心。 聚类中心只是当前分配给簇的所有样本的平均值。</span>

data2 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>data<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'X'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'X1'</span><span class="token punctuation">,</span> <span class="token string">'X2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'data2.head()'</span><span class="token punctuation">,</span>data2<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

sb<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span>context<span class="token operator">=</span><span class="token string">'notebook'</span><span class="token punctuation">,</span> style<span class="token operator">=</span><span class="token string">'white'</span><span class="token punctuation">)</span>
sb<span class="token punctuation">.</span>lmplot<span class="token punctuation">(</span><span class="token string">'X1'</span><span class="token punctuation">,</span> <span class="token string">'X2'</span><span class="token punctuation">,</span> data<span class="token operator">=</span>data2<span class="token punctuation">,</span> fit_reg<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment">#lmplot是用来绘制回归图的</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730223611674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA5ODY3NTM=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">compute_centroids</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#计算质心</span>
    m<span class="token punctuation">,</span> n <span class="token operator">=</span> X<span class="token punctuation">.</span>shape
    centroids <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>
        indices <span class="token operator">=</span> np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>idx <span class="token operator">==</span> i<span class="token punctuation">)</span>
        centroids<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>X<span class="token punctuation">[</span>indices<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>indices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> centroids

compute_centroids<span class="token punctuation">(</span>X<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">''' array([[2.42830111, 3.15792418], [5.81350331, 2.63365645], [7.11938687, 3.6166844 ]]) '''</span>


<span class="token keyword">def</span> <span class="token function">run_k_means</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> initial_centroids<span class="token punctuation">,</span> max_iters<span class="token punctuation">)</span><span class="token punctuation">:</span>
    m<span class="token punctuation">,</span> n <span class="token operator">=</span> X<span class="token punctuation">.</span>shape
    k <span class="token operator">=</span> initial_centroids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    idx <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>m<span class="token punctuation">)</span>
    centroids <span class="token operator">=</span> initial_centroids

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_iters<span class="token punctuation">)</span><span class="token punctuation">:</span>
        idx <span class="token operator">=</span> find_closest_centroids<span class="token punctuation">(</span>X<span class="token punctuation">,</span> centroids<span class="token punctuation">)</span>
        centroids <span class="token operator">=</span> compute_centroids<span class="token punctuation">(</span>X<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> k<span class="token punctuation">)</span>

    <span class="token keyword">return</span> idx<span class="token punctuation">,</span> centroids

idx<span class="token punctuation">,</span> centroids <span class="token operator">=</span> run_k_means<span class="token punctuation">(</span>X<span class="token punctuation">,</span> initial_centroids<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

cluster1 <span class="token operator">=</span> X<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>idx <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
cluster2 <span class="token operator">=</span> X<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>idx <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
cluster3 <span class="token operator">=</span> X<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>idx <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span>

fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>cluster1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cluster1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Cluster 1'</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>cluster2<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cluster2<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'g'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Cluster 2'</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>cluster3<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cluster3<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Cluster 3'</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730223622327.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA5ODY3NTM=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="22_make_blobs_152"></a>2.2 make_blobs</h2> 
  <pre><code class="prism language-python"><span class="token comment"># 数据构建。:创建的团状的数据集合，数据分布呈高斯分布状况。</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_blobs
N <span class="token operator">=</span> <span class="token number">1000</span>
centers <span class="token operator">=</span> <span class="token number">4</span>

X<span class="token punctuation">,</span> Y <span class="token operator">=</span> make_blobs<span class="token punctuation">(</span>n_samples<span class="token operator">=</span>N<span class="token punctuation">,</span> n_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> centers<span class="token operator">=</span>centers<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment"># 模型构建:导入K-Means算法包，其底层就是按照前面讲的算法步骤一步一步创建的</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cluster <span class="token keyword">import</span> KMeans

<span class="token comment"># 给出要划分的中心点树k，也可以给出算法中止条件，迭代次数，或者簇中心变化率。</span>

km <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span>centers<span class="token punctuation">,</span> init<span class="token operator">=</span><span class="token string">'random'</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">28</span><span class="token punctuation">)</span>

km<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">)</span>

<span class="token comment"># 模型的预测</span>
y_hat <span class="token operator">=</span> km<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>

<span class="token comment"># 求出模型的中心点坐标，并且得到，样本到中心点的总距离，也就是前面提到的损失函数。</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"所有样本距离所属簇中心点的总距离和为:%.5f"</span> <span class="token operator">%</span> km<span class="token punctuation">.</span>inertia_<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"所有的中心点聚类中心坐标:"</span><span class="token punctuation">)</span>
cluter_centers <span class="token operator">=</span> km<span class="token punctuation">.</span>cluster_centers_
<span class="token keyword">print</span><span class="token punctuation">(</span>cluter_centers<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"score其实就是所有样本点离所属簇中心点距离和的相反数:"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>km<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''运行结果 所有样本距离所属簇中心点的总距离和为:1734.01601 所有的中心点聚类中心坐标: [[ 1.99871335 0.79038817] [-1.31360134 7.86561393] [-1.51473374 2.8755229 ] [ 0.97790397 4.28661633]] score其实就是所有样本点离所属簇中心点距离和的相反数: -1734.0160089537276 '''</span>
</code></pre> 
  <pre><code class="prism language-python"><span class="token comment"># 画图，把原始数据和最终预测数据在图上表现出来</span>
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> matplotlib <span class="token keyword">as</span> mpl

cm <span class="token operator">=</span> mpl<span class="token punctuation">.</span>colors<span class="token punctuation">.</span>ListedColormap<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token string">'rgby'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'w'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">121</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span>u<span class="token string">'原始数据'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token operator">=</span>Y<span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span>cm<span class="token punctuation">,</span> edgecolors<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">122</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token operator">=</span>y_hat<span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span>cm<span class="token punctuation">,</span> edgecolors<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span>u<span class="token string">'K-Means算法聚类结果'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730223127853.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA5ODY3NTM=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h1><a id="_217"></a>三、总结</h1> 
  <h2><a id="31_KMeans__KNN_218"></a>3.1 K-Means 与 KNN</h2> 
  <ul> 
   <li>K-Means是<strong>无监督</strong>学习的聚类算法，没有样本输出；而KNN是<strong>监督</strong>学习的分类算法，有对应的类别输出。KNN基本不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。</li> 
   <li>当然，两者也有一些相似点，两个算法都包含一个过程，即找出和某一个点最近的点。两者都利用了最近邻(nearest neighbors)的思想。</li> 
  </ul> 
  <h2><a id="32_KMeans_221"></a>3.2 K-Means的主要优点有</h2> 
  <p> 　1）原理比较简单，实现也是很容易，收敛速度快。</p> 
  <p> 　2）聚类效果较优。</p> 
  <p> 　3）算法的可解释度比较强。</p> 
  <p> 　4）主要需要调参的参数仅仅是簇数k。</p> 
  <h2><a id="33_KMeans_229"></a>3.3 K-Means的主要缺点有</h2> 
  <p> 　1）K值的选取不好把握</p> 
  <p> 　2）对于不是凸的数据集比较难收敛</p> 
  <p> 　3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。</p> 
  <p> 　4） 采用迭代方法，得到的结果只是局部最优。</p> 
  <p> 　5） 对噪音和异常点比较的敏感。<br> 　　　<br> 　　　 　6）每一次迭代都要重新计算各个点与质心的距离，然后排序，时间成本较高。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
