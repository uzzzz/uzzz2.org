<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>大数据学习笔记之项目（三）：离线项目拓展youtube | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="大数据学习笔记之项目（三）：离线项目拓展youtube" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 一、需求描述 二、知识储备梳理 2.1、order by，sort by，distribute by，cluster by 背景表结构 2.1.1、order by 2.1.2、sort by 2.1.3、distribute by 2.1.4、cluster by 2.2、行转列、列转行（UDAF与UDTF） 2.2.1、行转列 2.2.2、列转行 在这里插入图片描述 2.3、数组操作 2.4、orc存储 2.5、Hive分桶 2.5.1、直接分桶 2.5.2、在分区中分桶 三、项目 3.1、数据结构 3.1.1、视频表 3.1.2、用户表 3.2原始数据存放地 在这里插入图片描述 3.3、技术选型 3.3.1、数据清洗 3.3.2、数据分析 3.4、ETL原始数据 3.6.1、ETL之ETLUtil 3.6.2、ETL之Mapper 3.6.3、ETL之Runner **3.6.4**、执行ETL 在这里插入图片描述 3.5、准备工作 3.5.1、创建表 3.5.2、导入ETL后的数据 3.5.3、向ORC表插入数据 3.6、业务分析 3.6.1、统计视频观看数Top10 3.6.2、统计视频类别热度Top10 3.6.3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数 3.6.4、统计视频观看数Top50所关联视频的所属类别Rank 3.6.5、统计每个类别中的视频热度Top10，以Music为例 3.6.6、统计每个类别中视频流量Top10，以Music为例 3.6.7、统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 3.6.8、统计每个类别视频观看数Top10 四、可能出现的问题 4.1、JVM堆内存溢出 五、总结及作业 一、需求描述 统计Youtube视频网站的常规指标，各种TopN指标： –统计视频观看数Top10 –统计视频类别热度Top10 –统计视频观看数Top20所属类别 –统计视频观看数Top50所关联视频的所属类别Rank –统计每个类别中的视频热度Top10，以Music为例 –统计每个类别中视频流量Top10，以Music为例 –统计上传视频最多的用户Top10以及他们上传的视频 –统计每个类别视频观看数Top10 数据格式： 第一列视频的id，唯一的表示某一个视频的 ，比如复制id在youtube上进查询 第二列是作者 第三列是视频的年龄 第四列是视频的类别 第五列是视频的长度 第六列是视频的观看次数 第七列是视频评分 第八列是流量 ps：下面有关于视频的的数结构 二、知识储备梳理 2.1、order by，sort by，distribute by，cluster by Order by 和sort by有什么区别？orderby是一个全局排序，如果数据量特别巨大 ,sort by是局部的，什么是局部的呢？比如说分区、分桶，distribute by是把数据划分成不同的区域，一般情况下sort by和sidtribute by是可以连用的，cluster by 和前面两个其实是一样的，还有一个作用是分桶 背景表结构 在讲解中我们需要贯串一个 例子，所以需要设计一个情景，对应 还要有一个表结构和填充数据。如下：有3个字段，分别为personId标识某一个人，company标识一家公司名称，money标识该公司每年盈利收入（单位：万元人民币） personId company money personId p1 公司1 100 p1 p2 公司2 200 p2 p1 公司3 150 p1 p3 公司4 300 p3 建表导入数据： create table company_info( ​ personId string, ​ company string, ​ money float )row format delimited fields terminated by “\t” load data local inpath “company_info.txt” into table company_info; 2.1.1、order by hive中的order by语句会对查询结果做一次全局排序，即，所有的mapper产生的结果都会交给一个reducer去处理，无论数据量大小，job任务只会启动一个reducer，如果数据量巨大，则会耗费大量的时间。 尖叫提示：如果在严格模式下，order by需要指定limit数据条数，不然数据量巨大的情况下会造成崩溃无输出结果。涉及属性：set hive.mapred.mode=nonstrict/strict 如果设置为了strict 严格模式， 就必须在后面通过limit关键字来设置条数，如果不设置的话，很可能数据量特别大的时候机器就崩溃了 如果nonstrict 非严格模式，就不用指定了，因为是默认的大小100 例如：按照money排序的例子 select * from company_info order by money desc; 2.1.2、sort by hive中的sort by语句会对每一块局部数据进行局部排序，即，每一个reducer处理的数据都是有序的，但是不能保证全局有序。如果想保证全局有序，可以在sort by的基础之上做一次全局的归并排序。 2.1.3、distribute by hive中的distribute by一般要和sort by一起使用，即将某一块数据归给(distribute by)某一个reducer处理，然后在指定的reducer中进行sort by排序。 单用dsidtribute是没有任何意义的，但是语法上是没有错误的 ，只是单纯的把数据划分到了不同的分区里面，但是没有做其他的事情 sort by按照 distribute by分好的分区进行排序 尖叫提示：distribute by必须写在sort by之前 例如：不同的人（personId）分为不同的组，每组按照money排序。 select \* from company\_info distribute by personId sort by personId, money desc; 如果不同组中的money值有相同的，还是需要全局排序的 2.1.4、cluster by hive中的cluster by在distribute by和sort by排序字段一致的情况下是等价的。同时，cluster by指定的列只能是降序，即默认的descend，而不能是ascend。 例如：写一个等价于distribute by 与sort by的例子 select * from company_info distribute by personId sort by personId; 等价于 select * from compnay_info cluster by personId; 2.2、行转列、列转行（UDAF与UDTF） 2.2.1、行转列 表结构： name constellation blood_type 孙悟空 白羊座 A 大海 射手座 A 宋宋 白羊座 B 猪八戒 白羊座 A 凤姐 射手座 A 孙悟空、猪八戒是分布在多行上面的，我们把多行上的数据转到一行上面，叫做行转列 创建表及数据导入：** create table person_info( name string, constellation string, blood_type string) row format delimited fields terminated by &quot;\t&quot;; load data local inpath “person_info.txt” into table person_info; 例如：把星座和血型一样的人归类到一起** select ​ t1.base, concat_ws(&#39;|&#39;, collect_set(t1.name)) name from ​ (select ​ name, ​ concat(constellation, &quot;,&quot;, blood_type) base ​ from ​ person_info) t1 group by t1.base; 分析： select name, concat(constellation, “,”, blood_type) base concat 就是把白羊座和A拼到一起了，”白羊座,A“ 命名为base 其实就是 concat_ws(’|’, collect_set(t1.name)) name collect_set聚合name，每个name之间用”|“分割，在mysql中有一个group_concat 最后的结果 如果 还是按照星座和血型聚合的，但是出来的不只是人名，并且修改了上面的表名， 结果 concat_ws(’|’, collect_set(t1.name)) name 意思是吧t1.name表里面的值全部用|拼接起来，group by是聚合的意思，根据concat进行聚合，然后通过collect_set 函数收集起来，通过concat_ws进行拼接 2.2.2、列转行 表结构： movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 《Lie to me》 悬疑,警匪,动作,心理,剧情 《战狼2》 战争,动作,灾难 movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 例如： 创建表及导入数据： create table movie_info( ​ movie string, ​ category array&lt;string&gt;) row format delimited fields terminated by &quot;\t&quot; collection items terminated by &quot;,&quot;; load data local inpath &quot;movie.txt&quot; into table movie_info; 解释： 电影名称 string类型 类别 String类型，可以有多个类别，所以是array的， hive中是支持array的 collection items terminated by “,”;当前表中的数据所有的数组类型的字段他们之间的元素用什么来分割，这里是用“，”来分割、 但是这里注意 如图，这里只能分割其中的一个list，这也是一个弊端 执行上面的建表操作 然后将其展开，但是这里注意列转行，只能对list等集合进行操作 当然也可以是map如下图 例如：将电影分类中的数组数据展开 select ​ movie, ​ category_name from ​ movie_info lateral view explode(category) table_tmp as category_name; 将上面的list展开 select movie,category_name from movie_info lateral view explide(category) table_tmp as category_name; 拓展： UDAF：聚合函数多行输入，一行输出 UDTF：一行输入，多行输出 explode这个词的意思是爆裂、炸开的意思，这里就是把一行的数据拆散的意思 也可以单独用 select explode(category) as category from movie_info; 这个时候的结果就是把category炸开了 上面的函数中latera是侧写的意思，就是将上面爆炸的数据进行一次侧写，生成一个新的表，标的名字叫做table_tmp ，这个表里面炸开的字段叫做category_name 2.3、数组操作 “fields terminated by”：字段与字段之间的分隔符。 “collection items terminated by”：一个字段中各个子元素item的分隔符。 2.4、orc存储 orc即Optimized Row Columnar (ORC) file，在RCFile的基础上演化而来，可以提供一种高效的方法在Hive中存储数据，提升了读、写、处理数据的效率。 他是一种默认的存储格式，还有一种存储格式是textFile，textFile可读性比较好，但是效率比较低。orc主要是为了提升了读、写、处理数据的效率。 orc会把每一列的数据转化成行去存储 一行存储不同格式的效率会比较低，怎样变高呢？name就应该一行存储相同格式的 一行数据存的全是某一列里面的内容，相当于把表做了一个90度的旋转 比如第一个索引存的是p1 p2 p3 p4 p5，蒂格尔索引存的是float 100 200 300 400 500 用这样的格式存储，压缩效率很高，数据处理也会高很多，甚至可以实现跳行，比如我就想访问某一列的数据，对于orc来讲，我只需要拿到某一类对应的index，就访问那一行 2.5、Hive分桶 Hive可以将表或者表的分区进一步组织成桶，以达到： 1、数据取样效率更高 2、数据处理效率更高 桶通过对指定列进行哈希来实现，将一个列名下的数据切分为&quot;一组桶&quot;，每个桶都对应了一个该列名下的一个存储文件。 之前说过hive是有分区的，建表的时候可以通过pertition by 分区的目的就是为了在访问数据的时候更快，在mysql中也有索引也有分区 那么hive的分桶是干嘛的？就是在分区的基础上在进行划分，可以理解为子分区，还有一种方式是在表里面直接进行分桶，其实表示有分区的，没有自定义分区的话，默认就是一个分区。所以在宏观上理解的话，可以分为两种，在分区上进行分桶，或者直接在表上进行分桶。严格上来讲都是在分区上进行分桶，只不过后者是只有一个分区而已。 2.5.1、直接分桶 开始操作之前，需要将hive.enforce.bucketing属性设置为true，以标识Hive可以识别桶。 create table music( ​ id int, ​ name string, ​ size float) row format delimited fields terminated by &quot;\t&quot; clustered by (id) into 4 buckets; 该代码的意思是将music表按照id将数据分成了4个桶，插入数据时，会对应4个 reduce操作，输出4个文件。 id 进来进行一种hash算法，然后得到一个数字，这个数字对桶的个数进行求余，就想当前的个数是四，最后得到的余数就是0123 ，最后都会依次的放进去，上面的clustered by 里面的id就是在hash归桶的时候要通过那个字段进行归桶 2.5.2、在分区中分桶 当数据量过大，需要庞大发分区数量时，可以考虑桶，因为分区数量太大的情况可能会导致文件系统挂掉，而且桶比分区有更高的查询效率。数据最终落在哪一个桶里，取决于clustered by的那个列的值的hash数与桶的个数求余来决定。虽然有一定离散行，但不能保证每个桶中的数据量是一样的。 create table music2( ​ id int, ​ name string, ​ size float) partitioned by (date string) clustered by (id) sorted by(size) into 4 bucket row format delimited fields terminated by &quot;\t&quot;; load data local inpath &#39;demo/music.txt&#39; into table music2 partition(date=&#39;2017-08-30&#39;); sorted by(size) into 4 bucket ，可以在某一个桶里面，按照某一个顺序进行排列顺序，即这个桶里面的数据都是有序的，按照size进行排序的。 partition(date=‘2017-08-30’); 指定导入到哪个分区里面 hbase的数据清洗的概念是什么？region，按照某一个范围，扔到某一个region里面，如果在rowkey是递增的，然后在某一个时段里面特别密集，就会导致某一个region特别大，某一个region特别大，就会导致数据处理的速度特别慢，某一台集群的压力特别大 hive中，加入第一个桶里面都是数据，第三个第四个桶里面没有数据，这样会导致怎样的现象呢？hive中的数据本身还是存在hdfs上面还是那个txt文件，在hbase里面有存储的倾斜，也有处理的倾斜，但是在hive、中只有数据处理的倾斜，在hive中，一个桶里面的数据会交给一个reducer处理，不可能出现两个桶对应于一个reducer，一个reducer就对应一个文件的输出，所以直白的说就是一个桶会对应一个文件的输出，两个桶会产生两个。现在问题就出来了，如果一个桶的数据特别多，剩下的桶没有数据，会导致第一个桶的reducer在第一台集齐的nodemanager执行很慢，然后第二个第三个第四个的reducer里面没数据，这就会导致负载不均衡了，所以分桶不是一定会导致数据不发生倾斜，但不是绝对的。 三、项目 3.1、数据结构 3.1.1、视频表 字段 备注 详细描述 video id 视频唯一id 11位字符串 uploader 视频上传者 上传视频的用户名String age 视频年龄 视频上传日期和2007年2月15日之间的整数天（Youtube的独特设定） category 视频类别 上传视频指定的视频分类 length 视频长度 整形数字标识的视频长度 views 观看次数 视频被浏览的次数 rate 视频评分 满分5分 ratings 流量 视频的流量，整型数字 conments 评论数 一个视频的整数评论数 related ids 相关视频id 相关视频的id，最多20个 3.1.2、用户表 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int 注意仔细观察上图中，我们上面说到这里有的是按照&amp;分割的，有的是按照tab分割的，这样上面只能分割一个list，所以我们需要进行清洗数据 3.2原始数据存放地 HDFS目录： 视频数据集：/youtube/video/2008 用户数据集：/youtube/users/2008 先把数据方法hdfs上面 3.3、技术选型 * CDH5.3.6-Hadoop2.5.0 * CDH5.3.6-Hive0.13.1 * Mysql 3.3.1、数据清洗 MapReduce 3.3.2、数据分析 MapReduce or Hive 3.4、ETL原始数据 通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用&quot;\t&quot;进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用&quot;&amp;“分割，同时去掉两边空格，多个相关视频id也使用”&amp;&quot;进行分割。 3.6.1、ETL之ETLUtil package com.z.youtube.util; public class ETLUtil { public static String oriString2ETLString(String ori){ StringBuilder etlString = new StringBuilder(); String[] splits = ori.split(&quot;\t&quot;); if(splits.length &lt; 9) return null; splits[3] = splits[3].replace(&quot; &quot;, &quot;&quot;); for(int i = 0; i &lt; splits.length; i++){ if(i &lt; 9){ if(i == splits.length - 1){ etlString.append(splits[i]); }else{ etlString.append(splits[i] + &quot;\t&quot;); } }else{ if(i == splits.length - 1){ etlString.append(splits[i]); }else{ etlString.append(splits[i] + &quot;&amp;&quot;); } } } return etlString.toString(); } } 完了之后可以写个main行数，随便粘贴上面的一行数据进行测试 3.6.2、ETL之Mapper package com.z.youtube.mr.etl; import java.io.IOException; import org.apache.commons.lang.StringUtils; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import com.z.youtube.util.ETLUtil; public class VideoETLMapper extends Mapper&lt;Object, Text, NullWritable, Text&gt;{ Text text = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException { String etlString = ETLUtil.oriString2ETLString(value.toString()); if(StringUtils.isBlank(etlString)) return; text.set(etlString); context.write(NullWritable.get(), text); } } 3.6.3、ETL之Runner package com.z.youtube.mr.etl; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; public class VideoETLRunner implements Tool { private Configuration conf = null; @Override public void setConf(Configuration conf) { this.conf = conf; } @Override public Configuration getConf() { return this.conf; } @Override public int run(String[] args) throws Exception { conf = this.getConf(); //需要传入两个参数，第一个参数是你要清洗的 //数据的所在目录。 //第二个参数是清洗完之后你的输出目录 conf.set(&quot;inpath&quot;, args[0]); conf.set(&quot;outpath&quot;, args[1]); Job job = Job.getInstance(conf, &quot;youtube-video-etl&quot;); job.setJarByClass(VideoETLRunner.class); job.setMapperClass(VideoETLMapper.class); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); job.setNumReduceTasks(0); this.initJobInputPath(job); this.initJobOutputPath(job); return job.waitForCompletion(true) ? 0 : 1; } private void initJobOutputPath(Job job) throws IOException { Configuration conf = job.getConfiguration(); //通过这个键把刚才传到conf里面的值取出来 String outPathString = conf.get(&quot;outpath&quot;); FileSystem fs = FileSystem.get(conf); Path outPath = new Path(outPathString); if(fs.exists(outPath)){ fs.delete(outPath, true); } FileOutputFormat.setOutputPath(job, outPath); } private void initJobInputPath(Job job) throws IOException { Configuration conf = job.getConfiguration(); String inPathString = conf.get(&quot;inpath&quot;); FileSystem fs = FileSystem.get(conf); Path inPath = new Path(inPathString); if(fs.exists(inPath)){ FileInputFormat.addInputPath(job, inPath); }else{ throw new RuntimeException(&quot;HDFS中该文件目录不存在：&quot; + inPathString); } } public static void main(String[] args) { try { int resultCode = ToolRunner.run(new VideoETLRunner(), args); if(resultCode == 0){ System.out.println(&quot;Success!&quot;); }else{ System.out.println(&quot;Fail!&quot;); } System.exit(resultCode); } catch (Exception e) { e.printStackTrace(); System.exit(1); } } } 3.6.4、执行ETL $ bin/yarn jar ~/Desktop/youtube-0.0.1-SNAPSHOT.jar \ com.z.youtube.mr.etl.VideoETLRunner \ /youtube/video/2008/0222 \ /youtube/output/video/2008/0222 3.5、准备工作 3.5.1、创建表 创建表：youtube_ori（原始视频数据表），youtube_user_ori（原始用户表）， 创建表：youtube_orc，youtube_user_orc 为什么要有原始表？因为想让数据的存储为orc形式，如果是orc形式是不能够直接使用load data这样的命令插入数据的，必须通过查询的方式把数据插入到orc格式的表里面，所以需要先见两个表，把清洗后的数据导进去，到入之后，对表进行查询，插入到orc表里面 youtube_ori： create table youtube_ori( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;) row format delimited fields terminated by &quot;\t&quot; collection items terminated by &quot;&amp;&quot; stored as textfile; 然后把原始数据插入到orc表中 youtube_orc： create table youtube_orc( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;) clustered by (uploader) into 8 buckets row format delimited fields terminated by &quot;\t&quot; collection items terminated by &quot;&amp;&quot; stored as orc; youtube_user_orc： create table youtube_user_orc( uploader string, videos int, friends int) clustered by (uploader) into 24 buckets row format delimited fields terminated by &quot;\t&quot; stored as orc; 注意不同的地方是stored as orc 然后开始把数据导入到orc表里面 过程可能比较慢 查一下orc表的数据 3.5.2、导入ETL后的数据 youtube_ori： load data inpath “/youtube/output/video/2008/0222” into table youtube_ori; youtube_user_ori： load data inpath “/youtube/user/2008/0903” into table youtube_user_ori; 3.5.3、向ORC表插入数据 youtube_orc： insert into table youtube_orc select * from youtube_ori; youtube_user_orc： insert into table youtube_user_orc select * from youtube_user_ori; 3.6、业务分析 3.6.1、统计视频观看数Top10 思路： 1) 使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。 最终代码： select videoId, uploader, age, category, length, views, rate, ratings, comments from youtube_orc order by views desc limit 10; 3.6.2、统计视频类别热度Top10 思路： 1) 即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。 2) 我们需要按照类别group by聚合，然后count组内的videoId个数即可。 3) 因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。 4) 最后按照热度排序，显示前10条。 最终代码： select category_name as category, count(t1.videoId) as hot from ( select videoId, category_name from youtube_orc lateral view explode(category) t_catetory as category_name) t1 group by t1.category_name order by hot desc limit 10; 3.6.3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数 思路： 1) 先找到观看数最高的20个视频所属条目的所有信息，降序排列 2) 把这20条信息中的category分裂出来(列转行) 3) 最后查询视频分类名称和该分类下有多少个Top20的视频 最终代码： select category_name as category, count(t2.videoId) as hot_with_views from ( select videoId, category_name from ( select * from youtube_orc order by views desc limit 20) t1 lateral view explode(category) t_catetory as category_name) t2 group by category_name order by hot_with_views desc; 执行结果 3.6.4、统计视频观看数Top50所关联视频的所属类别Rank rank排名的意思 思路： 查询出观看数最多的前50个视频的所有信息(当然包含了每个视频对应的关联视频)，记为临时表t1 t1:观看数前50的视频 select * from youtube_orc order by views desc limit 50; 2) 将找到的50条视频信息的相关视频relatedId列转行，记为临时表t2 t2:将相关视频的id进行列转行操作 select explode(relatedId) as videoId from t1; 3) 将相关视频的id和youtube_orc表进行inner join操作 t5:得到两列数据，一列是category，一列是之前查询出来的相关视频id (select distinct(t2.videoId), t3.category from t2 inner join youtube_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name; 4) 按照视频类别进行分组，统计每组视频个数，然后排行 最终代码： select category_name as category, count(t5.videoId) as hot from ( select videoId, category_name from ( select distinct(t2.videoId), t3.category from ( select explode(relatedId) as videoId from ( select * from youtube_orc order by views desc limit 50) t1) t2 inner join youtube_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name) t5 group by category_name order by hot desc; t4 lateral view explode(category) t_catetory 这句话的意思是对t4表进行侧写，侧写的内容是，把category这一例炸开，炸开的结果生成一张新的表叫做t_catetory，炸开的这一列有一列的别名叫做category_name ， 然后distinct去重，然后distinct上面的select整个完成之后生成一个新的表叫做t5表 3.6.5、统计每个类别中的视频热度Top10，以Music为例 思路： 要想统计Music类别中的视频热度Top10，需要先找到Music类别，所以需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。 2) 向category展开的表中插入数据。 3) 统计对应类别（Music）中的视频热度。 最终代码： 创建表类别表： create table youtube_category( videoId string, uploader string, age int, categoryId string, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;) row format delimited fields terminated by &quot;\t&quot; collection items terminated by &quot;&amp;&quot; stored as orc; 向类别表中插入数据： insert into table youtube_category select videoId, uploader, age, categoryId, length, views, rate, ratings, comments, relatedId from youtube_orc lateral view explode(category) catetory as categoryId; 将类别炸开 统计Music类别的Top10（也可以统计其他） select videoId, views from youtube_category where categoryId = &quot;Music&quot; order by views desc limit 10; 3.6.6、统计每个类别中视频流量Top10，以Music为例 思路： 1) 创建视频类别展开表（categoryId列转行后的表） 2) 按照ratings排序即可 最终代码： select videoId, views, ratings from youtube_category where categoryId = &quot;Music&quot; order by ratings desc limit 10; 3.6.7、统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 思路： 1) 先找到上传视频最多的10个用户的用户信息 select * from youtube_user_orc order by videos desc limit 10; 通过uploader字段与youtube_orc表进行join，得到的信息按照views观看次数进行排序即可。 最终代码： select t2.videoId, t2.views, t2.ratings, t1.videos, t1.friends from ( select * from youtube_user_orc order by videos desc limit 10) t1 join youtube_orc t2 on t1.uploader = t2.uploader order by views desc limit 20; 3.6.8、统计每个类别视频观看数Top10 思路： 1) 先得到categoryId展开的表数据 子查询按照categoryId进行分区，然后分区内排序，并生成递增数字，该递增数字这一列起名为rank列 3) 通过子查询产生的临时表，查询rank值小于等于10的数据行即可。 最终代码： select t1.* from ( select videoId, categoryId, views, row_number() over(partition by categoryId order by views desc) rank from youtube_category) t1 where rank &lt;= 10; partition by categoryId 相同的分类在一个分区里面 row_number 这个函数的意思是对后面每一分区里面的数据进行一个升序的序列号的生成 四、可能出现的问题 4.1、JVM堆内存溢出 描述：java.lang.OutOfMemoryError: Java heap space 解决：在yarn-site.xml中加入如下代码 &lt;property&gt; //yarn允许的最大执行任务的调度的内存是多少， //一定不要超过实际的电脑内存，比如电脑是2g，这里就分配2g， //这里默认是8g &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; //物理内存和虚拟内存的关系， //比如上面分配的物理内存是2g //2.1的意思是上面的2*2.1就是虚拟内存的大小 &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; //在执行mapreduce任务的时候，每一个jvm实例， //所允许的最大堆内存的大小是1024个，默认是200个 &lt;name&gt;mapred.child.java.opts&lt;/name&gt; &lt;value&gt;-Xmx1024m&lt;/value&gt; &lt;/property&gt; 五、总结及作业 请大家自行使用Java-MapReduce来实现上述需求。" />
<meta property="og:description" content="文章目录 一、需求描述 二、知识储备梳理 2.1、order by，sort by，distribute by，cluster by 背景表结构 2.1.1、order by 2.1.2、sort by 2.1.3、distribute by 2.1.4、cluster by 2.2、行转列、列转行（UDAF与UDTF） 2.2.1、行转列 2.2.2、列转行 在这里插入图片描述 2.3、数组操作 2.4、orc存储 2.5、Hive分桶 2.5.1、直接分桶 2.5.2、在分区中分桶 三、项目 3.1、数据结构 3.1.1、视频表 3.1.2、用户表 3.2原始数据存放地 在这里插入图片描述 3.3、技术选型 3.3.1、数据清洗 3.3.2、数据分析 3.4、ETL原始数据 3.6.1、ETL之ETLUtil 3.6.2、ETL之Mapper 3.6.3、ETL之Runner **3.6.4**、执行ETL 在这里插入图片描述 3.5、准备工作 3.5.1、创建表 3.5.2、导入ETL后的数据 3.5.3、向ORC表插入数据 3.6、业务分析 3.6.1、统计视频观看数Top10 3.6.2、统计视频类别热度Top10 3.6.3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数 3.6.4、统计视频观看数Top50所关联视频的所属类别Rank 3.6.5、统计每个类别中的视频热度Top10，以Music为例 3.6.6、统计每个类别中视频流量Top10，以Music为例 3.6.7、统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 3.6.8、统计每个类别视频观看数Top10 四、可能出现的问题 4.1、JVM堆内存溢出 五、总结及作业 一、需求描述 统计Youtube视频网站的常规指标，各种TopN指标： –统计视频观看数Top10 –统计视频类别热度Top10 –统计视频观看数Top20所属类别 –统计视频观看数Top50所关联视频的所属类别Rank –统计每个类别中的视频热度Top10，以Music为例 –统计每个类别中视频流量Top10，以Music为例 –统计上传视频最多的用户Top10以及他们上传的视频 –统计每个类别视频观看数Top10 数据格式： 第一列视频的id，唯一的表示某一个视频的 ，比如复制id在youtube上进查询 第二列是作者 第三列是视频的年龄 第四列是视频的类别 第五列是视频的长度 第六列是视频的观看次数 第七列是视频评分 第八列是流量 ps：下面有关于视频的的数结构 二、知识储备梳理 2.1、order by，sort by，distribute by，cluster by Order by 和sort by有什么区别？orderby是一个全局排序，如果数据量特别巨大 ,sort by是局部的，什么是局部的呢？比如说分区、分桶，distribute by是把数据划分成不同的区域，一般情况下sort by和sidtribute by是可以连用的，cluster by 和前面两个其实是一样的，还有一个作用是分桶 背景表结构 在讲解中我们需要贯串一个 例子，所以需要设计一个情景，对应 还要有一个表结构和填充数据。如下：有3个字段，分别为personId标识某一个人，company标识一家公司名称，money标识该公司每年盈利收入（单位：万元人民币） personId company money personId p1 公司1 100 p1 p2 公司2 200 p2 p1 公司3 150 p1 p3 公司4 300 p3 建表导入数据： create table company_info( ​ personId string, ​ company string, ​ money float )row format delimited fields terminated by “\t” load data local inpath “company_info.txt” into table company_info; 2.1.1、order by hive中的order by语句会对查询结果做一次全局排序，即，所有的mapper产生的结果都会交给一个reducer去处理，无论数据量大小，job任务只会启动一个reducer，如果数据量巨大，则会耗费大量的时间。 尖叫提示：如果在严格模式下，order by需要指定limit数据条数，不然数据量巨大的情况下会造成崩溃无输出结果。涉及属性：set hive.mapred.mode=nonstrict/strict 如果设置为了strict 严格模式， 就必须在后面通过limit关键字来设置条数，如果不设置的话，很可能数据量特别大的时候机器就崩溃了 如果nonstrict 非严格模式，就不用指定了，因为是默认的大小100 例如：按照money排序的例子 select * from company_info order by money desc; 2.1.2、sort by hive中的sort by语句会对每一块局部数据进行局部排序，即，每一个reducer处理的数据都是有序的，但是不能保证全局有序。如果想保证全局有序，可以在sort by的基础之上做一次全局的归并排序。 2.1.3、distribute by hive中的distribute by一般要和sort by一起使用，即将某一块数据归给(distribute by)某一个reducer处理，然后在指定的reducer中进行sort by排序。 单用dsidtribute是没有任何意义的，但是语法上是没有错误的 ，只是单纯的把数据划分到了不同的分区里面，但是没有做其他的事情 sort by按照 distribute by分好的分区进行排序 尖叫提示：distribute by必须写在sort by之前 例如：不同的人（personId）分为不同的组，每组按照money排序。 select \* from company\_info distribute by personId sort by personId, money desc; 如果不同组中的money值有相同的，还是需要全局排序的 2.1.4、cluster by hive中的cluster by在distribute by和sort by排序字段一致的情况下是等价的。同时，cluster by指定的列只能是降序，即默认的descend，而不能是ascend。 例如：写一个等价于distribute by 与sort by的例子 select * from company_info distribute by personId sort by personId; 等价于 select * from compnay_info cluster by personId; 2.2、行转列、列转行（UDAF与UDTF） 2.2.1、行转列 表结构： name constellation blood_type 孙悟空 白羊座 A 大海 射手座 A 宋宋 白羊座 B 猪八戒 白羊座 A 凤姐 射手座 A 孙悟空、猪八戒是分布在多行上面的，我们把多行上的数据转到一行上面，叫做行转列 创建表及数据导入：** create table person_info( name string, constellation string, blood_type string) row format delimited fields terminated by &quot;\t&quot;; load data local inpath “person_info.txt” into table person_info; 例如：把星座和血型一样的人归类到一起** select ​ t1.base, concat_ws(&#39;|&#39;, collect_set(t1.name)) name from ​ (select ​ name, ​ concat(constellation, &quot;,&quot;, blood_type) base ​ from ​ person_info) t1 group by t1.base; 分析： select name, concat(constellation, “,”, blood_type) base concat 就是把白羊座和A拼到一起了，”白羊座,A“ 命名为base 其实就是 concat_ws(’|’, collect_set(t1.name)) name collect_set聚合name，每个name之间用”|“分割，在mysql中有一个group_concat 最后的结果 如果 还是按照星座和血型聚合的，但是出来的不只是人名，并且修改了上面的表名， 结果 concat_ws(’|’, collect_set(t1.name)) name 意思是吧t1.name表里面的值全部用|拼接起来，group by是聚合的意思，根据concat进行聚合，然后通过collect_set 函数收集起来，通过concat_ws进行拼接 2.2.2、列转行 表结构： movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 《Lie to me》 悬疑,警匪,动作,心理,剧情 《战狼2》 战争,动作,灾难 movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 例如： 创建表及导入数据： create table movie_info( ​ movie string, ​ category array&lt;string&gt;) row format delimited fields terminated by &quot;\t&quot; collection items terminated by &quot;,&quot;; load data local inpath &quot;movie.txt&quot; into table movie_info; 解释： 电影名称 string类型 类别 String类型，可以有多个类别，所以是array的， hive中是支持array的 collection items terminated by “,”;当前表中的数据所有的数组类型的字段他们之间的元素用什么来分割，这里是用“，”来分割、 但是这里注意 如图，这里只能分割其中的一个list，这也是一个弊端 执行上面的建表操作 然后将其展开，但是这里注意列转行，只能对list等集合进行操作 当然也可以是map如下图 例如：将电影分类中的数组数据展开 select ​ movie, ​ category_name from ​ movie_info lateral view explode(category) table_tmp as category_name; 将上面的list展开 select movie,category_name from movie_info lateral view explide(category) table_tmp as category_name; 拓展： UDAF：聚合函数多行输入，一行输出 UDTF：一行输入，多行输出 explode这个词的意思是爆裂、炸开的意思，这里就是把一行的数据拆散的意思 也可以单独用 select explode(category) as category from movie_info; 这个时候的结果就是把category炸开了 上面的函数中latera是侧写的意思，就是将上面爆炸的数据进行一次侧写，生成一个新的表，标的名字叫做table_tmp ，这个表里面炸开的字段叫做category_name 2.3、数组操作 “fields terminated by”：字段与字段之间的分隔符。 “collection items terminated by”：一个字段中各个子元素item的分隔符。 2.4、orc存储 orc即Optimized Row Columnar (ORC) file，在RCFile的基础上演化而来，可以提供一种高效的方法在Hive中存储数据，提升了读、写、处理数据的效率。 他是一种默认的存储格式，还有一种存储格式是textFile，textFile可读性比较好，但是效率比较低。orc主要是为了提升了读、写、处理数据的效率。 orc会把每一列的数据转化成行去存储 一行存储不同格式的效率会比较低，怎样变高呢？name就应该一行存储相同格式的 一行数据存的全是某一列里面的内容，相当于把表做了一个90度的旋转 比如第一个索引存的是p1 p2 p3 p4 p5，蒂格尔索引存的是float 100 200 300 400 500 用这样的格式存储，压缩效率很高，数据处理也会高很多，甚至可以实现跳行，比如我就想访问某一列的数据，对于orc来讲，我只需要拿到某一类对应的index，就访问那一行 2.5、Hive分桶 Hive可以将表或者表的分区进一步组织成桶，以达到： 1、数据取样效率更高 2、数据处理效率更高 桶通过对指定列进行哈希来实现，将一个列名下的数据切分为&quot;一组桶&quot;，每个桶都对应了一个该列名下的一个存储文件。 之前说过hive是有分区的，建表的时候可以通过pertition by 分区的目的就是为了在访问数据的时候更快，在mysql中也有索引也有分区 那么hive的分桶是干嘛的？就是在分区的基础上在进行划分，可以理解为子分区，还有一种方式是在表里面直接进行分桶，其实表示有分区的，没有自定义分区的话，默认就是一个分区。所以在宏观上理解的话，可以分为两种，在分区上进行分桶，或者直接在表上进行分桶。严格上来讲都是在分区上进行分桶，只不过后者是只有一个分区而已。 2.5.1、直接分桶 开始操作之前，需要将hive.enforce.bucketing属性设置为true，以标识Hive可以识别桶。 create table music( ​ id int, ​ name string, ​ size float) row format delimited fields terminated by &quot;\t&quot; clustered by (id) into 4 buckets; 该代码的意思是将music表按照id将数据分成了4个桶，插入数据时，会对应4个 reduce操作，输出4个文件。 id 进来进行一种hash算法，然后得到一个数字，这个数字对桶的个数进行求余，就想当前的个数是四，最后得到的余数就是0123 ，最后都会依次的放进去，上面的clustered by 里面的id就是在hash归桶的时候要通过那个字段进行归桶 2.5.2、在分区中分桶 当数据量过大，需要庞大发分区数量时，可以考虑桶，因为分区数量太大的情况可能会导致文件系统挂掉，而且桶比分区有更高的查询效率。数据最终落在哪一个桶里，取决于clustered by的那个列的值的hash数与桶的个数求余来决定。虽然有一定离散行，但不能保证每个桶中的数据量是一样的。 create table music2( ​ id int, ​ name string, ​ size float) partitioned by (date string) clustered by (id) sorted by(size) into 4 bucket row format delimited fields terminated by &quot;\t&quot;; load data local inpath &#39;demo/music.txt&#39; into table music2 partition(date=&#39;2017-08-30&#39;); sorted by(size) into 4 bucket ，可以在某一个桶里面，按照某一个顺序进行排列顺序，即这个桶里面的数据都是有序的，按照size进行排序的。 partition(date=‘2017-08-30’); 指定导入到哪个分区里面 hbase的数据清洗的概念是什么？region，按照某一个范围，扔到某一个region里面，如果在rowkey是递增的，然后在某一个时段里面特别密集，就会导致某一个region特别大，某一个region特别大，就会导致数据处理的速度特别慢，某一台集群的压力特别大 hive中，加入第一个桶里面都是数据，第三个第四个桶里面没有数据，这样会导致怎样的现象呢？hive中的数据本身还是存在hdfs上面还是那个txt文件，在hbase里面有存储的倾斜，也有处理的倾斜，但是在hive、中只有数据处理的倾斜，在hive中，一个桶里面的数据会交给一个reducer处理，不可能出现两个桶对应于一个reducer，一个reducer就对应一个文件的输出，所以直白的说就是一个桶会对应一个文件的输出，两个桶会产生两个。现在问题就出来了，如果一个桶的数据特别多，剩下的桶没有数据，会导致第一个桶的reducer在第一台集齐的nodemanager执行很慢，然后第二个第三个第四个的reducer里面没数据，这就会导致负载不均衡了，所以分桶不是一定会导致数据不发生倾斜，但不是绝对的。 三、项目 3.1、数据结构 3.1.1、视频表 字段 备注 详细描述 video id 视频唯一id 11位字符串 uploader 视频上传者 上传视频的用户名String age 视频年龄 视频上传日期和2007年2月15日之间的整数天（Youtube的独特设定） category 视频类别 上传视频指定的视频分类 length 视频长度 整形数字标识的视频长度 views 观看次数 视频被浏览的次数 rate 视频评分 满分5分 ratings 流量 视频的流量，整型数字 conments 评论数 一个视频的整数评论数 related ids 相关视频id 相关视频的id，最多20个 3.1.2、用户表 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int 注意仔细观察上图中，我们上面说到这里有的是按照&amp;分割的，有的是按照tab分割的，这样上面只能分割一个list，所以我们需要进行清洗数据 3.2原始数据存放地 HDFS目录： 视频数据集：/youtube/video/2008 用户数据集：/youtube/users/2008 先把数据方法hdfs上面 3.3、技术选型 * CDH5.3.6-Hadoop2.5.0 * CDH5.3.6-Hive0.13.1 * Mysql 3.3.1、数据清洗 MapReduce 3.3.2、数据分析 MapReduce or Hive 3.4、ETL原始数据 通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用&quot;\t&quot;进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用&quot;&amp;“分割，同时去掉两边空格，多个相关视频id也使用”&amp;&quot;进行分割。 3.6.1、ETL之ETLUtil package com.z.youtube.util; public class ETLUtil { public static String oriString2ETLString(String ori){ StringBuilder etlString = new StringBuilder(); String[] splits = ori.split(&quot;\t&quot;); if(splits.length &lt; 9) return null; splits[3] = splits[3].replace(&quot; &quot;, &quot;&quot;); for(int i = 0; i &lt; splits.length; i++){ if(i &lt; 9){ if(i == splits.length - 1){ etlString.append(splits[i]); }else{ etlString.append(splits[i] + &quot;\t&quot;); } }else{ if(i == splits.length - 1){ etlString.append(splits[i]); }else{ etlString.append(splits[i] + &quot;&amp;&quot;); } } } return etlString.toString(); } } 完了之后可以写个main行数，随便粘贴上面的一行数据进行测试 3.6.2、ETL之Mapper package com.z.youtube.mr.etl; import java.io.IOException; import org.apache.commons.lang.StringUtils; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import com.z.youtube.util.ETLUtil; public class VideoETLMapper extends Mapper&lt;Object, Text, NullWritable, Text&gt;{ Text text = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException { String etlString = ETLUtil.oriString2ETLString(value.toString()); if(StringUtils.isBlank(etlString)) return; text.set(etlString); context.write(NullWritable.get(), text); } } 3.6.3、ETL之Runner package com.z.youtube.mr.etl; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; public class VideoETLRunner implements Tool { private Configuration conf = null; @Override public void setConf(Configuration conf) { this.conf = conf; } @Override public Configuration getConf() { return this.conf; } @Override public int run(String[] args) throws Exception { conf = this.getConf(); //需要传入两个参数，第一个参数是你要清洗的 //数据的所在目录。 //第二个参数是清洗完之后你的输出目录 conf.set(&quot;inpath&quot;, args[0]); conf.set(&quot;outpath&quot;, args[1]); Job job = Job.getInstance(conf, &quot;youtube-video-etl&quot;); job.setJarByClass(VideoETLRunner.class); job.setMapperClass(VideoETLMapper.class); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); job.setNumReduceTasks(0); this.initJobInputPath(job); this.initJobOutputPath(job); return job.waitForCompletion(true) ? 0 : 1; } private void initJobOutputPath(Job job) throws IOException { Configuration conf = job.getConfiguration(); //通过这个键把刚才传到conf里面的值取出来 String outPathString = conf.get(&quot;outpath&quot;); FileSystem fs = FileSystem.get(conf); Path outPath = new Path(outPathString); if(fs.exists(outPath)){ fs.delete(outPath, true); } FileOutputFormat.setOutputPath(job, outPath); } private void initJobInputPath(Job job) throws IOException { Configuration conf = job.getConfiguration(); String inPathString = conf.get(&quot;inpath&quot;); FileSystem fs = FileSystem.get(conf); Path inPath = new Path(inPathString); if(fs.exists(inPath)){ FileInputFormat.addInputPath(job, inPath); }else{ throw new RuntimeException(&quot;HDFS中该文件目录不存在：&quot; + inPathString); } } public static void main(String[] args) { try { int resultCode = ToolRunner.run(new VideoETLRunner(), args); if(resultCode == 0){ System.out.println(&quot;Success!&quot;); }else{ System.out.println(&quot;Fail!&quot;); } System.exit(resultCode); } catch (Exception e) { e.printStackTrace(); System.exit(1); } } } 3.6.4、执行ETL $ bin/yarn jar ~/Desktop/youtube-0.0.1-SNAPSHOT.jar \ com.z.youtube.mr.etl.VideoETLRunner \ /youtube/video/2008/0222 \ /youtube/output/video/2008/0222 3.5、准备工作 3.5.1、创建表 创建表：youtube_ori（原始视频数据表），youtube_user_ori（原始用户表）， 创建表：youtube_orc，youtube_user_orc 为什么要有原始表？因为想让数据的存储为orc形式，如果是orc形式是不能够直接使用load data这样的命令插入数据的，必须通过查询的方式把数据插入到orc格式的表里面，所以需要先见两个表，把清洗后的数据导进去，到入之后，对表进行查询，插入到orc表里面 youtube_ori： create table youtube_ori( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;) row format delimited fields terminated by &quot;\t&quot; collection items terminated by &quot;&amp;&quot; stored as textfile; 然后把原始数据插入到orc表中 youtube_orc： create table youtube_orc( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;) clustered by (uploader) into 8 buckets row format delimited fields terminated by &quot;\t&quot; collection items terminated by &quot;&amp;&quot; stored as orc; youtube_user_orc： create table youtube_user_orc( uploader string, videos int, friends int) clustered by (uploader) into 24 buckets row format delimited fields terminated by &quot;\t&quot; stored as orc; 注意不同的地方是stored as orc 然后开始把数据导入到orc表里面 过程可能比较慢 查一下orc表的数据 3.5.2、导入ETL后的数据 youtube_ori： load data inpath “/youtube/output/video/2008/0222” into table youtube_ori; youtube_user_ori： load data inpath “/youtube/user/2008/0903” into table youtube_user_ori; 3.5.3、向ORC表插入数据 youtube_orc： insert into table youtube_orc select * from youtube_ori; youtube_user_orc： insert into table youtube_user_orc select * from youtube_user_ori; 3.6、业务分析 3.6.1、统计视频观看数Top10 思路： 1) 使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。 最终代码： select videoId, uploader, age, category, length, views, rate, ratings, comments from youtube_orc order by views desc limit 10; 3.6.2、统计视频类别热度Top10 思路： 1) 即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。 2) 我们需要按照类别group by聚合，然后count组内的videoId个数即可。 3) 因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。 4) 最后按照热度排序，显示前10条。 最终代码： select category_name as category, count(t1.videoId) as hot from ( select videoId, category_name from youtube_orc lateral view explode(category) t_catetory as category_name) t1 group by t1.category_name order by hot desc limit 10; 3.6.3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数 思路： 1) 先找到观看数最高的20个视频所属条目的所有信息，降序排列 2) 把这20条信息中的category分裂出来(列转行) 3) 最后查询视频分类名称和该分类下有多少个Top20的视频 最终代码： select category_name as category, count(t2.videoId) as hot_with_views from ( select videoId, category_name from ( select * from youtube_orc order by views desc limit 20) t1 lateral view explode(category) t_catetory as category_name) t2 group by category_name order by hot_with_views desc; 执行结果 3.6.4、统计视频观看数Top50所关联视频的所属类别Rank rank排名的意思 思路： 查询出观看数最多的前50个视频的所有信息(当然包含了每个视频对应的关联视频)，记为临时表t1 t1:观看数前50的视频 select * from youtube_orc order by views desc limit 50; 2) 将找到的50条视频信息的相关视频relatedId列转行，记为临时表t2 t2:将相关视频的id进行列转行操作 select explode(relatedId) as videoId from t1; 3) 将相关视频的id和youtube_orc表进行inner join操作 t5:得到两列数据，一列是category，一列是之前查询出来的相关视频id (select distinct(t2.videoId), t3.category from t2 inner join youtube_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name; 4) 按照视频类别进行分组，统计每组视频个数，然后排行 最终代码： select category_name as category, count(t5.videoId) as hot from ( select videoId, category_name from ( select distinct(t2.videoId), t3.category from ( select explode(relatedId) as videoId from ( select * from youtube_orc order by views desc limit 50) t1) t2 inner join youtube_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name) t5 group by category_name order by hot desc; t4 lateral view explode(category) t_catetory 这句话的意思是对t4表进行侧写，侧写的内容是，把category这一例炸开，炸开的结果生成一张新的表叫做t_catetory，炸开的这一列有一列的别名叫做category_name ， 然后distinct去重，然后distinct上面的select整个完成之后生成一个新的表叫做t5表 3.6.5、统计每个类别中的视频热度Top10，以Music为例 思路： 要想统计Music类别中的视频热度Top10，需要先找到Music类别，所以需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。 2) 向category展开的表中插入数据。 3) 统计对应类别（Music）中的视频热度。 最终代码： 创建表类别表： create table youtube_category( videoId string, uploader string, age int, categoryId string, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;) row format delimited fields terminated by &quot;\t&quot; collection items terminated by &quot;&amp;&quot; stored as orc; 向类别表中插入数据： insert into table youtube_category select videoId, uploader, age, categoryId, length, views, rate, ratings, comments, relatedId from youtube_orc lateral view explode(category) catetory as categoryId; 将类别炸开 统计Music类别的Top10（也可以统计其他） select videoId, views from youtube_category where categoryId = &quot;Music&quot; order by views desc limit 10; 3.6.6、统计每个类别中视频流量Top10，以Music为例 思路： 1) 创建视频类别展开表（categoryId列转行后的表） 2) 按照ratings排序即可 最终代码： select videoId, views, ratings from youtube_category where categoryId = &quot;Music&quot; order by ratings desc limit 10; 3.6.7、统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 思路： 1) 先找到上传视频最多的10个用户的用户信息 select * from youtube_user_orc order by videos desc limit 10; 通过uploader字段与youtube_orc表进行join，得到的信息按照views观看次数进行排序即可。 最终代码： select t2.videoId, t2.views, t2.ratings, t1.videos, t1.friends from ( select * from youtube_user_orc order by videos desc limit 10) t1 join youtube_orc t2 on t1.uploader = t2.uploader order by views desc limit 20; 3.6.8、统计每个类别视频观看数Top10 思路： 1) 先得到categoryId展开的表数据 子查询按照categoryId进行分区，然后分区内排序，并生成递增数字，该递增数字这一列起名为rank列 3) 通过子查询产生的临时表，查询rank值小于等于10的数据行即可。 最终代码： select t1.* from ( select videoId, categoryId, views, row_number() over(partition by categoryId order by views desc) rank from youtube_category) t1 where rank &lt;= 10; partition by categoryId 相同的分类在一个分区里面 row_number 这个函数的意思是对后面每一分区里面的数据进行一个升序的序列号的生成 四、可能出现的问题 4.1、JVM堆内存溢出 描述：java.lang.OutOfMemoryError: Java heap space 解决：在yarn-site.xml中加入如下代码 &lt;property&gt; //yarn允许的最大执行任务的调度的内存是多少， //一定不要超过实际的电脑内存，比如电脑是2g，这里就分配2g， //这里默认是8g &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; //物理内存和虚拟内存的关系， //比如上面分配的物理内存是2g //2.1的意思是上面的2*2.1就是虚拟内存的大小 &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; //在执行mapreduce任务的时候，每一个jvm实例， //所允许的最大堆内存的大小是1024个，默认是200个 &lt;name&gt;mapred.child.java.opts&lt;/name&gt; &lt;value&gt;-Xmx1024m&lt;/value&gt; &lt;/property&gt; 五、总结及作业 请大家自行使用Java-MapReduce来实现上述需求。" />
<link rel="canonical" href="https://uzzz.org/2019/07/30/792984.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/30/792984.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-30T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 一、需求描述 二、知识储备梳理 2.1、order by，sort by，distribute by，cluster by 背景表结构 2.1.1、order by 2.1.2、sort by 2.1.3、distribute by 2.1.4、cluster by 2.2、行转列、列转行（UDAF与UDTF） 2.2.1、行转列 2.2.2、列转行 在这里插入图片描述 2.3、数组操作 2.4、orc存储 2.5、Hive分桶 2.5.1、直接分桶 2.5.2、在分区中分桶 三、项目 3.1、数据结构 3.1.1、视频表 3.1.2、用户表 3.2原始数据存放地 在这里插入图片描述 3.3、技术选型 3.3.1、数据清洗 3.3.2、数据分析 3.4、ETL原始数据 3.6.1、ETL之ETLUtil 3.6.2、ETL之Mapper 3.6.3、ETL之Runner **3.6.4**、执行ETL 在这里插入图片描述 3.5、准备工作 3.5.1、创建表 3.5.2、导入ETL后的数据 3.5.3、向ORC表插入数据 3.6、业务分析 3.6.1、统计视频观看数Top10 3.6.2、统计视频类别热度Top10 3.6.3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数 3.6.4、统计视频观看数Top50所关联视频的所属类别Rank 3.6.5、统计每个类别中的视频热度Top10，以Music为例 3.6.6、统计每个类别中视频流量Top10，以Music为例 3.6.7、统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 3.6.8、统计每个类别视频观看数Top10 四、可能出现的问题 4.1、JVM堆内存溢出 五、总结及作业 一、需求描述 统计Youtube视频网站的常规指标，各种TopN指标： –统计视频观看数Top10 –统计视频类别热度Top10 –统计视频观看数Top20所属类别 –统计视频观看数Top50所关联视频的所属类别Rank –统计每个类别中的视频热度Top10，以Music为例 –统计每个类别中视频流量Top10，以Music为例 –统计上传视频最多的用户Top10以及他们上传的视频 –统计每个类别视频观看数Top10 数据格式： 第一列视频的id，唯一的表示某一个视频的 ，比如复制id在youtube上进查询 第二列是作者 第三列是视频的年龄 第四列是视频的类别 第五列是视频的长度 第六列是视频的观看次数 第七列是视频评分 第八列是流量 ps：下面有关于视频的的数结构 二、知识储备梳理 2.1、order by，sort by，distribute by，cluster by Order by 和sort by有什么区别？orderby是一个全局排序，如果数据量特别巨大 ,sort by是局部的，什么是局部的呢？比如说分区、分桶，distribute by是把数据划分成不同的区域，一般情况下sort by和sidtribute by是可以连用的，cluster by 和前面两个其实是一样的，还有一个作用是分桶 背景表结构 在讲解中我们需要贯串一个 例子，所以需要设计一个情景，对应 还要有一个表结构和填充数据。如下：有3个字段，分别为personId标识某一个人，company标识一家公司名称，money标识该公司每年盈利收入（单位：万元人民币） personId company money personId p1 公司1 100 p1 p2 公司2 200 p2 p1 公司3 150 p1 p3 公司4 300 p3 建表导入数据： create table company_info( ​ personId string, ​ company string, ​ money float )row format delimited fields terminated by “\\t” load data local inpath “company_info.txt” into table company_info; 2.1.1、order by hive中的order by语句会对查询结果做一次全局排序，即，所有的mapper产生的结果都会交给一个reducer去处理，无论数据量大小，job任务只会启动一个reducer，如果数据量巨大，则会耗费大量的时间。 尖叫提示：如果在严格模式下，order by需要指定limit数据条数，不然数据量巨大的情况下会造成崩溃无输出结果。涉及属性：set hive.mapred.mode=nonstrict/strict 如果设置为了strict 严格模式， 就必须在后面通过limit关键字来设置条数，如果不设置的话，很可能数据量特别大的时候机器就崩溃了 如果nonstrict 非严格模式，就不用指定了，因为是默认的大小100 例如：按照money排序的例子 select * from company_info order by money desc; 2.1.2、sort by hive中的sort by语句会对每一块局部数据进行局部排序，即，每一个reducer处理的数据都是有序的，但是不能保证全局有序。如果想保证全局有序，可以在sort by的基础之上做一次全局的归并排序。 2.1.3、distribute by hive中的distribute by一般要和sort by一起使用，即将某一块数据归给(distribute by)某一个reducer处理，然后在指定的reducer中进行sort by排序。 单用dsidtribute是没有任何意义的，但是语法上是没有错误的 ，只是单纯的把数据划分到了不同的分区里面，但是没有做其他的事情 sort by按照 distribute by分好的分区进行排序 尖叫提示：distribute by必须写在sort by之前 例如：不同的人（personId）分为不同的组，每组按照money排序。 select \\* from company\\_info distribute by personId sort by personId, money desc; 如果不同组中的money值有相同的，还是需要全局排序的 2.1.4、cluster by hive中的cluster by在distribute by和sort by排序字段一致的情况下是等价的。同时，cluster by指定的列只能是降序，即默认的descend，而不能是ascend。 例如：写一个等价于distribute by 与sort by的例子 select * from company_info distribute by personId sort by personId; 等价于 select * from compnay_info cluster by personId; 2.2、行转列、列转行（UDAF与UDTF） 2.2.1、行转列 表结构： name constellation blood_type 孙悟空 白羊座 A 大海 射手座 A 宋宋 白羊座 B 猪八戒 白羊座 A 凤姐 射手座 A 孙悟空、猪八戒是分布在多行上面的，我们把多行上的数据转到一行上面，叫做行转列 创建表及数据导入：** create table person_info( name string, constellation string, blood_type string) row format delimited fields terminated by &quot;\\t&quot;; load data local inpath “person_info.txt” into table person_info; 例如：把星座和血型一样的人归类到一起** select ​ t1.base, concat_ws(&#39;|&#39;, collect_set(t1.name)) name from ​ (select ​ name, ​ concat(constellation, &quot;,&quot;, blood_type) base ​ from ​ person_info) t1 group by t1.base; 分析： select name, concat(constellation, “,”, blood_type) base concat 就是把白羊座和A拼到一起了，”白羊座,A“ 命名为base 其实就是 concat_ws(’|’, collect_set(t1.name)) name collect_set聚合name，每个name之间用”|“分割，在mysql中有一个group_concat 最后的结果 如果 还是按照星座和血型聚合的，但是出来的不只是人名，并且修改了上面的表名， 结果 concat_ws(’|’, collect_set(t1.name)) name 意思是吧t1.name表里面的值全部用|拼接起来，group by是聚合的意思，根据concat进行聚合，然后通过collect_set 函数收集起来，通过concat_ws进行拼接 2.2.2、列转行 表结构： movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 《Lie to me》 悬疑,警匪,动作,心理,剧情 《战狼2》 战争,动作,灾难 movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 例如： 创建表及导入数据： create table movie_info( ​ movie string, ​ category array&lt;string&gt;) row format delimited fields terminated by &quot;\\t&quot; collection items terminated by &quot;,&quot;; load data local inpath &quot;movie.txt&quot; into table movie_info; 解释： 电影名称 string类型 类别 String类型，可以有多个类别，所以是array的， hive中是支持array的 collection items terminated by “,”;当前表中的数据所有的数组类型的字段他们之间的元素用什么来分割，这里是用“，”来分割、 但是这里注意 如图，这里只能分割其中的一个list，这也是一个弊端 执行上面的建表操作 然后将其展开，但是这里注意列转行，只能对list等集合进行操作 当然也可以是map如下图 例如：将电影分类中的数组数据展开 select ​ movie, ​ category_name from ​ movie_info lateral view explode(category) table_tmp as category_name; 将上面的list展开 select movie,category_name from movie_info lateral view explide(category) table_tmp as category_name; 拓展： UDAF：聚合函数多行输入，一行输出 UDTF：一行输入，多行输出 explode这个词的意思是爆裂、炸开的意思，这里就是把一行的数据拆散的意思 也可以单独用 select explode(category) as category from movie_info; 这个时候的结果就是把category炸开了 上面的函数中latera是侧写的意思，就是将上面爆炸的数据进行一次侧写，生成一个新的表，标的名字叫做table_tmp ，这个表里面炸开的字段叫做category_name 2.3、数组操作 “fields terminated by”：字段与字段之间的分隔符。 “collection items terminated by”：一个字段中各个子元素item的分隔符。 2.4、orc存储 orc即Optimized Row Columnar (ORC) file，在RCFile的基础上演化而来，可以提供一种高效的方法在Hive中存储数据，提升了读、写、处理数据的效率。 他是一种默认的存储格式，还有一种存储格式是textFile，textFile可读性比较好，但是效率比较低。orc主要是为了提升了读、写、处理数据的效率。 orc会把每一列的数据转化成行去存储 一行存储不同格式的效率会比较低，怎样变高呢？name就应该一行存储相同格式的 一行数据存的全是某一列里面的内容，相当于把表做了一个90度的旋转 比如第一个索引存的是p1 p2 p3 p4 p5，蒂格尔索引存的是float 100 200 300 400 500 用这样的格式存储，压缩效率很高，数据处理也会高很多，甚至可以实现跳行，比如我就想访问某一列的数据，对于orc来讲，我只需要拿到某一类对应的index，就访问那一行 2.5、Hive分桶 Hive可以将表或者表的分区进一步组织成桶，以达到： 1、数据取样效率更高 2、数据处理效率更高 桶通过对指定列进行哈希来实现，将一个列名下的数据切分为&quot;一组桶&quot;，每个桶都对应了一个该列名下的一个存储文件。 之前说过hive是有分区的，建表的时候可以通过pertition by 分区的目的就是为了在访问数据的时候更快，在mysql中也有索引也有分区 那么hive的分桶是干嘛的？就是在分区的基础上在进行划分，可以理解为子分区，还有一种方式是在表里面直接进行分桶，其实表示有分区的，没有自定义分区的话，默认就是一个分区。所以在宏观上理解的话，可以分为两种，在分区上进行分桶，或者直接在表上进行分桶。严格上来讲都是在分区上进行分桶，只不过后者是只有一个分区而已。 2.5.1、直接分桶 开始操作之前，需要将hive.enforce.bucketing属性设置为true，以标识Hive可以识别桶。 create table music( ​ id int, ​ name string, ​ size float) row format delimited fields terminated by &quot;\\t&quot; clustered by (id) into 4 buckets; 该代码的意思是将music表按照id将数据分成了4个桶，插入数据时，会对应4个 reduce操作，输出4个文件。 id 进来进行一种hash算法，然后得到一个数字，这个数字对桶的个数进行求余，就想当前的个数是四，最后得到的余数就是0123 ，最后都会依次的放进去，上面的clustered by 里面的id就是在hash归桶的时候要通过那个字段进行归桶 2.5.2、在分区中分桶 当数据量过大，需要庞大发分区数量时，可以考虑桶，因为分区数量太大的情况可能会导致文件系统挂掉，而且桶比分区有更高的查询效率。数据最终落在哪一个桶里，取决于clustered by的那个列的值的hash数与桶的个数求余来决定。虽然有一定离散行，但不能保证每个桶中的数据量是一样的。 create table music2( ​ id int, ​ name string, ​ size float) partitioned by (date string) clustered by (id) sorted by(size) into 4 bucket row format delimited fields terminated by &quot;\\t&quot;; load data local inpath &#39;demo/music.txt&#39; into table music2 partition(date=&#39;2017-08-30&#39;); sorted by(size) into 4 bucket ，可以在某一个桶里面，按照某一个顺序进行排列顺序，即这个桶里面的数据都是有序的，按照size进行排序的。 partition(date=‘2017-08-30’); 指定导入到哪个分区里面 hbase的数据清洗的概念是什么？region，按照某一个范围，扔到某一个region里面，如果在rowkey是递增的，然后在某一个时段里面特别密集，就会导致某一个region特别大，某一个region特别大，就会导致数据处理的速度特别慢，某一台集群的压力特别大 hive中，加入第一个桶里面都是数据，第三个第四个桶里面没有数据，这样会导致怎样的现象呢？hive中的数据本身还是存在hdfs上面还是那个txt文件，在hbase里面有存储的倾斜，也有处理的倾斜，但是在hive、中只有数据处理的倾斜，在hive中，一个桶里面的数据会交给一个reducer处理，不可能出现两个桶对应于一个reducer，一个reducer就对应一个文件的输出，所以直白的说就是一个桶会对应一个文件的输出，两个桶会产生两个。现在问题就出来了，如果一个桶的数据特别多，剩下的桶没有数据，会导致第一个桶的reducer在第一台集齐的nodemanager执行很慢，然后第二个第三个第四个的reducer里面没数据，这就会导致负载不均衡了，所以分桶不是一定会导致数据不发生倾斜，但不是绝对的。 三、项目 3.1、数据结构 3.1.1、视频表 字段 备注 详细描述 video id 视频唯一id 11位字符串 uploader 视频上传者 上传视频的用户名String age 视频年龄 视频上传日期和2007年2月15日之间的整数天（Youtube的独特设定） category 视频类别 上传视频指定的视频分类 length 视频长度 整形数字标识的视频长度 views 观看次数 视频被浏览的次数 rate 视频评分 满分5分 ratings 流量 视频的流量，整型数字 conments 评论数 一个视频的整数评论数 related ids 相关视频id 相关视频的id，最多20个 3.1.2、用户表 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int 字段 备注 字段类型 uploader 上传者用户名 string videos 上传视频数 int 注意仔细观察上图中，我们上面说到这里有的是按照&amp;分割的，有的是按照tab分割的，这样上面只能分割一个list，所以我们需要进行清洗数据 3.2原始数据存放地 HDFS目录： 视频数据集：/youtube/video/2008 用户数据集：/youtube/users/2008 先把数据方法hdfs上面 3.3、技术选型 * CDH5.3.6-Hadoop2.5.0 * CDH5.3.6-Hive0.13.1 * Mysql 3.3.1、数据清洗 MapReduce 3.3.2、数据分析 MapReduce or Hive 3.4、ETL原始数据 通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用&quot;\\t&quot;进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用&quot;&amp;“分割，同时去掉两边空格，多个相关视频id也使用”&amp;&quot;进行分割。 3.6.1、ETL之ETLUtil package com.z.youtube.util; public class ETLUtil { public static String oriString2ETLString(String ori){ StringBuilder etlString = new StringBuilder(); String[] splits = ori.split(&quot;\\t&quot;); if(splits.length &lt; 9) return null; splits[3] = splits[3].replace(&quot; &quot;, &quot;&quot;); for(int i = 0; i &lt; splits.length; i++){ if(i &lt; 9){ if(i == splits.length - 1){ etlString.append(splits[i]); }else{ etlString.append(splits[i] + &quot;\\t&quot;); } }else{ if(i == splits.length - 1){ etlString.append(splits[i]); }else{ etlString.append(splits[i] + &quot;&amp;&quot;); } } } return etlString.toString(); } } 完了之后可以写个main行数，随便粘贴上面的一行数据进行测试 3.6.2、ETL之Mapper package com.z.youtube.mr.etl; import java.io.IOException; import org.apache.commons.lang.StringUtils; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import com.z.youtube.util.ETLUtil; public class VideoETLMapper extends Mapper&lt;Object, Text, NullWritable, Text&gt;{ Text text = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException { String etlString = ETLUtil.oriString2ETLString(value.toString()); if(StringUtils.isBlank(etlString)) return; text.set(etlString); context.write(NullWritable.get(), text); } } 3.6.3、ETL之Runner package com.z.youtube.mr.etl; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; public class VideoETLRunner implements Tool { private Configuration conf = null; @Override public void setConf(Configuration conf) { this.conf = conf; } @Override public Configuration getConf() { return this.conf; } @Override public int run(String[] args) throws Exception { conf = this.getConf(); //需要传入两个参数，第一个参数是你要清洗的 //数据的所在目录。 //第二个参数是清洗完之后你的输出目录 conf.set(&quot;inpath&quot;, args[0]); conf.set(&quot;outpath&quot;, args[1]); Job job = Job.getInstance(conf, &quot;youtube-video-etl&quot;); job.setJarByClass(VideoETLRunner.class); job.setMapperClass(VideoETLMapper.class); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); job.setNumReduceTasks(0); this.initJobInputPath(job); this.initJobOutputPath(job); return job.waitForCompletion(true) ? 0 : 1; } private void initJobOutputPath(Job job) throws IOException { Configuration conf = job.getConfiguration(); //通过这个键把刚才传到conf里面的值取出来 String outPathString = conf.get(&quot;outpath&quot;); FileSystem fs = FileSystem.get(conf); Path outPath = new Path(outPathString); if(fs.exists(outPath)){ fs.delete(outPath, true); } FileOutputFormat.setOutputPath(job, outPath); } private void initJobInputPath(Job job) throws IOException { Configuration conf = job.getConfiguration(); String inPathString = conf.get(&quot;inpath&quot;); FileSystem fs = FileSystem.get(conf); Path inPath = new Path(inPathString); if(fs.exists(inPath)){ FileInputFormat.addInputPath(job, inPath); }else{ throw new RuntimeException(&quot;HDFS中该文件目录不存在：&quot; + inPathString); } } public static void main(String[] args) { try { int resultCode = ToolRunner.run(new VideoETLRunner(), args); if(resultCode == 0){ System.out.println(&quot;Success!&quot;); }else{ System.out.println(&quot;Fail!&quot;); } System.exit(resultCode); } catch (Exception e) { e.printStackTrace(); System.exit(1); } } } 3.6.4、执行ETL $ bin/yarn jar ~/Desktop/youtube-0.0.1-SNAPSHOT.jar \\ com.z.youtube.mr.etl.VideoETLRunner \\ /youtube/video/2008/0222 \\ /youtube/output/video/2008/0222 3.5、准备工作 3.5.1、创建表 创建表：youtube_ori（原始视频数据表），youtube_user_ori（原始用户表）， 创建表：youtube_orc，youtube_user_orc 为什么要有原始表？因为想让数据的存储为orc形式，如果是orc形式是不能够直接使用load data这样的命令插入数据的，必须通过查询的方式把数据插入到orc格式的表里面，所以需要先见两个表，把清洗后的数据导进去，到入之后，对表进行查询，插入到orc表里面 youtube_ori： create table youtube_ori( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;) row format delimited fields terminated by &quot;\\t&quot; collection items terminated by &quot;&amp;&quot; stored as textfile; 然后把原始数据插入到orc表中 youtube_orc： create table youtube_orc( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;) clustered by (uploader) into 8 buckets row format delimited fields terminated by &quot;\\t&quot; collection items terminated by &quot;&amp;&quot; stored as orc; youtube_user_orc： create table youtube_user_orc( uploader string, videos int, friends int) clustered by (uploader) into 24 buckets row format delimited fields terminated by &quot;\\t&quot; stored as orc; 注意不同的地方是stored as orc 然后开始把数据导入到orc表里面 过程可能比较慢 查一下orc表的数据 3.5.2、导入ETL后的数据 youtube_ori： load data inpath “/youtube/output/video/2008/0222” into table youtube_ori; youtube_user_ori： load data inpath “/youtube/user/2008/0903” into table youtube_user_ori; 3.5.3、向ORC表插入数据 youtube_orc： insert into table youtube_orc select * from youtube_ori; youtube_user_orc： insert into table youtube_user_orc select * from youtube_user_ori; 3.6、业务分析 3.6.1、统计视频观看数Top10 思路： 1) 使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。 最终代码： select videoId, uploader, age, category, length, views, rate, ratings, comments from youtube_orc order by views desc limit 10; 3.6.2、统计视频类别热度Top10 思路： 1) 即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。 2) 我们需要按照类别group by聚合，然后count组内的videoId个数即可。 3) 因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。 4) 最后按照热度排序，显示前10条。 最终代码： select category_name as category, count(t1.videoId) as hot from ( select videoId, category_name from youtube_orc lateral view explode(category) t_catetory as category_name) t1 group by t1.category_name order by hot desc limit 10; 3.6.3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数 思路： 1) 先找到观看数最高的20个视频所属条目的所有信息，降序排列 2) 把这20条信息中的category分裂出来(列转行) 3) 最后查询视频分类名称和该分类下有多少个Top20的视频 最终代码： select category_name as category, count(t2.videoId) as hot_with_views from ( select videoId, category_name from ( select * from youtube_orc order by views desc limit 20) t1 lateral view explode(category) t_catetory as category_name) t2 group by category_name order by hot_with_views desc; 执行结果 3.6.4、统计视频观看数Top50所关联视频的所属类别Rank rank排名的意思 思路： 查询出观看数最多的前50个视频的所有信息(当然包含了每个视频对应的关联视频)，记为临时表t1 t1:观看数前50的视频 select * from youtube_orc order by views desc limit 50; 2) 将找到的50条视频信息的相关视频relatedId列转行，记为临时表t2 t2:将相关视频的id进行列转行操作 select explode(relatedId) as videoId from t1; 3) 将相关视频的id和youtube_orc表进行inner join操作 t5:得到两列数据，一列是category，一列是之前查询出来的相关视频id (select distinct(t2.videoId), t3.category from t2 inner join youtube_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name; 4) 按照视频类别进行分组，统计每组视频个数，然后排行 最终代码： select category_name as category, count(t5.videoId) as hot from ( select videoId, category_name from ( select distinct(t2.videoId), t3.category from ( select explode(relatedId) as videoId from ( select * from youtube_orc order by views desc limit 50) t1) t2 inner join youtube_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name) t5 group by category_name order by hot desc; t4 lateral view explode(category) t_catetory 这句话的意思是对t4表进行侧写，侧写的内容是，把category这一例炸开，炸开的结果生成一张新的表叫做t_catetory，炸开的这一列有一列的别名叫做category_name ， 然后distinct去重，然后distinct上面的select整个完成之后生成一个新的表叫做t5表 3.6.5、统计每个类别中的视频热度Top10，以Music为例 思路： 要想统计Music类别中的视频热度Top10，需要先找到Music类别，所以需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。 2) 向category展开的表中插入数据。 3) 统计对应类别（Music）中的视频热度。 最终代码： 创建表类别表： create table youtube_category( videoId string, uploader string, age int, categoryId string, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;) row format delimited fields terminated by &quot;\\t&quot; collection items terminated by &quot;&amp;&quot; stored as orc; 向类别表中插入数据： insert into table youtube_category select videoId, uploader, age, categoryId, length, views, rate, ratings, comments, relatedId from youtube_orc lateral view explode(category) catetory as categoryId; 将类别炸开 统计Music类别的Top10（也可以统计其他） select videoId, views from youtube_category where categoryId = &quot;Music&quot; order by views desc limit 10; 3.6.6、统计每个类别中视频流量Top10，以Music为例 思路： 1) 创建视频类别展开表（categoryId列转行后的表） 2) 按照ratings排序即可 最终代码： select videoId, views, ratings from youtube_category where categoryId = &quot;Music&quot; order by ratings desc limit 10; 3.6.7、统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 思路： 1) 先找到上传视频最多的10个用户的用户信息 select * from youtube_user_orc order by videos desc limit 10; 通过uploader字段与youtube_orc表进行join，得到的信息按照views观看次数进行排序即可。 最终代码： select t2.videoId, t2.views, t2.ratings, t1.videos, t1.friends from ( select * from youtube_user_orc order by videos desc limit 10) t1 join youtube_orc t2 on t1.uploader = t2.uploader order by views desc limit 20; 3.6.8、统计每个类别视频观看数Top10 思路： 1) 先得到categoryId展开的表数据 子查询按照categoryId进行分区，然后分区内排序，并生成递增数字，该递增数字这一列起名为rank列 3) 通过子查询产生的临时表，查询rank值小于等于10的数据行即可。 最终代码： select t1.* from ( select videoId, categoryId, views, row_number() over(partition by categoryId order by views desc) rank from youtube_category) t1 where rank &lt;= 10; partition by categoryId 相同的分类在一个分区里面 row_number 这个函数的意思是对后面每一分区里面的数据进行一个升序的序列号的生成 四、可能出现的问题 4.1、JVM堆内存溢出 描述：java.lang.OutOfMemoryError: Java heap space 解决：在yarn-site.xml中加入如下代码 &lt;property&gt; //yarn允许的最大执行任务的调度的内存是多少， //一定不要超过实际的电脑内存，比如电脑是2g，这里就分配2g， //这里默认是8g &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; //物理内存和虚拟内存的关系， //比如上面分配的物理内存是2g //2.1的意思是上面的2*2.1就是虚拟内存的大小 &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; //在执行mapreduce任务的时候，每一个jvm实例， //所允许的最大堆内存的大小是1024个，默认是200个 &lt;name&gt;mapred.child.java.opts&lt;/name&gt; &lt;value&gt;-Xmx1024m&lt;/value&gt; &lt;/property&gt; 五、总结及作业 请大家自行使用Java-MapReduce来实现上述需求。","@type":"BlogPosting","url":"https://uzzz.org/2019/07/30/792984.html","headline":"大数据学习笔记之项目（三）：离线项目拓展youtube","dateModified":"2019-07-30T00:00:00+08:00","datePublished":"2019-07-30T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/30/792984.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>大数据学习笔记之项目（三）：离线项目拓展youtube</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <li><a href="#_5" rel="nofollow" data-token="8c9a9d70137d460f3e89cd33f746dc8c">一、需求描述</a></li>
    <li><a href="#_52" rel="nofollow" data-token="14ea15b59c107c780366179ebd8763ce">二、知识储备梳理</a></li>
    <ul>
     <li><a href="#21order_bysort_bydistribute_bycluster_by_55" rel="nofollow" data-token="9e08fe009e82b0f1599d50407c0eae5a">2.1、order by，sort by，distribute by，cluster by</a></li>
     <ul>
      <li><a href="#_62" rel="nofollow" data-token="27e16f0899373440575792883d6054a6">背景表结构</a></li>
      <li><a href="#211order_by_90" rel="nofollow" data-token="1a16a709a775f10d259f879ac2d57262">2.1.1、order by</a></li>
      <li><a href="#212sort_by_108" rel="nofollow" data-token="fb0a944368be8d8c4a2cc39960dc7864">2.1.2、sort by</a></li>
      <li><a href="#213distribute_by_114" rel="nofollow" data-token="eb2f026bcf6c30efbd3d70373c2a2116">2.1.3、distribute by</a></li>
      <li><a href="#214cluster_by_146" rel="nofollow" data-token="336b77fd54d3efd0d0abcf1873c48291">2.1.4、cluster by</a></li>
     </ul>
     <li><a href="#22UDAFUDTF_164" rel="nofollow" data-token="821be4984ec5d751686b6282c3b2c73a">2.2、行转列、列转行（UDAF与UDTF）</a></li>
     <ul>
      <li><a href="#221_167" rel="nofollow" data-token="634651b9b2c12a3b0b835c8e749ec9d0">2.2.1、行转列</a></li>
      <li><a href="#222_266" rel="nofollow" data-token="980ef0bc65111c4ec7133353ac3c833c">2.2.2、列转行</a></li>
     </ul>
     <li><a href="#httpsimgblogcsdnimgcn20190730082224535pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3Usize_16color_FFFFFFt_70
23_369" rel="nofollow">在这里插入图片描述 2.3、数组操作</a></li>
     <li><a href="#24orc_377" rel="nofollow" data-token="0e2507608269dd14a43a675bf5ea657b">2.4、orc存储</a></li>
     <li><a href="#25Hive_398" rel="nofollow" data-token="ef9303b3b26422372f4c2fc4be87dbd7">2.5、Hive分桶</a></li>
     <ul>
      <li><a href="#251_416" rel="nofollow" data-token="b9493f5c76c45b867e9faf9c872ae2ba">2.5.1、直接分桶</a></li>
      <li><a href="#252_445" rel="nofollow" data-token="c5e5d6c3aee8f218cf3dc4d4cda35dfb">2.5.2、在分区中分桶</a></li>
     </ul>
    </ul>
    <li><a href="#_480" rel="nofollow" data-token="307d0cf3243ff1b4c5f33f5b31e5d315">三、项目</a></li>
    <ul>
     <li><a href="#31_483" rel="nofollow" data-token="e92350f34dae780e6f63e81b11c178df">3.1、数据结构</a></li>
     <ul>
      <li><a href="#311_486" rel="nofollow" data-token="c43b2782c3a9539aaaf744bc664ad1de">3.1.1、视频表</a></li>
      <li><a href="#312_503" rel="nofollow" data-token="cc03450dd4f4df533ef5eeb5dea4d0d0">3.1.2、用户表</a></li>
     </ul>
     <li><a href="#32_524" rel="nofollow" data-token="17894b9490cfd6303207d82bcda0a06e">3.2原始数据存放地</a></li>
     <li><a href="#httpsimgblogcsdnimgcn201907300823513pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3Usize_16color_FFFFFFt_70
33_537" rel="nofollow">在这里插入图片描述 3.3、技术选型</a></li>
     <ul>
      <li><a href="#331_547" rel="nofollow" data-token="712a917336706cc6029bb4f3d69b95fe">3.3.1、数据清洗</a></li>
      <li><a href="#332_551" rel="nofollow" data-token="c34f70078c6d0c966e4a21e03a246cfb">3.3.2、数据分析</a></li>
     </ul>
     <li><a href="#34ETL_555" rel="nofollow" data-token="ca0334c0cdf4c5fa61419deb1b87d7f8">3.4、ETL原始数据</a></li>
     <ul>
      <li><a href="#361ETLETLUtil_560" rel="nofollow" data-token="bcf964f1c7a61f134f5f1f6ca2b28fb8">3.6.1、ETL之ETLUtil</a></li>
      <li><a href="#362ETLMapper_594" rel="nofollow" data-token="7611d4c86558f038d021960da76fc3ca">3.6.2、ETL之Mapper</a></li>
      <li><a href="#363ETLRunner_627" rel="nofollow" data-token="b7472b4fdc4c932efbd5ec878fd0319b">3.6.3、ETL之Runner</a></li>
      <li><a href="#364ETL_730" rel="nofollow" data-token="6abeb74c0eb9321351bb2739ae0ce439">**3.6.4**、执行ETL</a></li>
     </ul>
     <li><a href="#httpsimgblogcsdnimgcn2019073008240961pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3Usize_16color_FFFFFFt_70
35_743" rel="nofollow">在这里插入图片描述 3.5、准备工作</a></li>
     <ul>
      <li><a href="#351_747" rel="nofollow" data-token="459b112734fd95f6ed3a1501237fae26">3.5.1、创建表</a></li>
      <li><a href="#352ETL_854" rel="nofollow" data-token="50f14a3455b45853261a31720f41f617">3.5.2、导入ETL后的数据</a></li>
      <li><a href="#353ORC_866" rel="nofollow" data-token="3c54d211ffc8a87b2dc406e434c8e464">3.5.3、向ORC表插入数据</a></li>
     </ul>
     <li><a href="#36_878" rel="nofollow" data-token="1833a1d976c2a1de655704a9d47ddc40">3.6、业务分析</a></li>
     <ul>
      <li><a href="#361Top10_881" rel="nofollow" data-token="1a6e59d875652d0f242ff28aeda6f76c">3.6.1、统计视频观看数Top10</a></li>
      <li><a href="#362Top10_915" rel="nofollow" data-token="3b863a346eceae978293e697ab5f2ec6">3.6.2、统计视频类别热度Top10</a></li>
      <li><a href="#36320Top20_954" rel="nofollow" data-token="2ce8c2df94076759152ebb818f39be2d">3.6.3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</a></li>
      <li><a href="#364Top50Rank_998" rel="nofollow" data-token="cce449cbaecea6e7a1114918dbaf0117">3.6.4、统计视频观看数Top50所关联视频的所属类别Rank</a></li>
      <li><a href="#365Top10Music_1089" rel="nofollow" data-token="7a9ba954b7a1d7e6e7c059a2496138cc">3.6.5、统计每个类别中的视频热度Top10，以Music为例</a></li>
      <li><a href="#366Top10Music_1165" rel="nofollow" data-token="09b8ced88869b15f858abbcaa00df6f3">3.6.6、统计每个类别中视频流量Top10，以Music为例</a></li>
      <li><a href="#367Top1020_1192" rel="nofollow" data-token="46d4f380b2fa884b60201bd6231d863a">3.6.7、统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频</a></li>
      <li><a href="#_1215" rel="nofollow"></a></li>
      <li><a href="#368Top10_1245" rel="nofollow" data-token="f38e2a5d0ced35ec9cb0362d9992e881">3.6.8、统计每个类别视频观看数Top10</a></li>
     </ul>
    </ul>
    <li><a href="#_1279" rel="nofollow" data-token="17e0d5b85ccb83f1491ac42a1ff11394">四、可能出现的问题</a></li>
    <ul>
     <li><a href="#41JVM_1282" rel="nofollow" data-token="3f2e1f8c320c3dd36f36942752e903f8">4.1、JVM堆内存溢出</a></li>
    </ul>
    <li><a href="#_1317" rel="nofollow" data-token="d82e9c12405c08c783075c1d720d2053">五、总结及作业</a></li>
   </ul>
  </div>
  <p></p> 
  <h1><a id="_5"></a>一、需求描述</h1> 
  <p>统计Youtube视频网站的常规指标，各种TopN指标：</p> 
  <p>–统计视频观看数Top10</p> 
  <p>–统计视频类别热度Top10</p> 
  <p>–统计视频观看数Top20所属类别</p> 
  <p>–统计视频观看数Top50所关联视频的所属类别Rank</p> 
  <p>–统计每个类别中的视频热度Top10，以Music为例</p> 
  <p>–统计每个类别中视频流量Top10，以Music为例</p> 
  <p>–统计上传视频最多的用户Top10以及他们上传的视频</p> 
  <p>–统计每个类别视频观看数Top10</p> 
  <p>数据格式：</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730081858579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>第一列视频的id，唯一的表示某一个视频的 ，比如复制id在youtube上进查询</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730081908411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败(img-jOBWcStB-1564445328542)(images/image-20190729125004918.png)]"></p> 
  <p>第二列是作者<br> 第三列是视频的年龄<br> 第四列是视频的类别<br> 第五列是视频的长度<br> 第六列是视频的观看次数<br> 第七列是视频评分<br> 第八列是流量</p> 
  <p>ps：下面有关于视频的的数结构</p> 
  <h1><a id="_52"></a>二、知识储备梳理</h1> 
  <h2><a id="21order_bysort_bydistribute_bycluster_by_55"></a>2.1、order by，sort by，distribute by，cluster by</h2> 
  <p>Order by 和sort by有什么区别？orderby是一个全局排序，如果数据量特别巨大 ,sort by是局部的，什么是局部的呢？比如说分区、分桶，distribute by是把数据划分成不同的区域，一般情况下sort by和sidtribute by是可以连用的，cluster by 和前面两个其实是一样的，还有一个作用是分桶</p> 
  <h3><a id="_62"></a>背景表结构</h3> 
  <p>在讲解中我们需要贯串一个 例子，所以需要设计一个情景，对应<br> 还要有一个表结构和填充数据。如下：有3个字段，分别为personId标识某一个人，company标识一家公司名称，money标识该公司每年盈利收入（单位：万元人民币）</p> 
  <hr> 
  <table> 
   <thead> 
    <tr> 
     <th>personId</th> 
     <th>company</th> 
     <th>money</th> 
     <th>personId</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>p1</td> 
     <td>公司1</td> 
     <td>100</td> 
     <td>p1</td> 
    </tr> 
    <tr> 
     <td>p2</td> 
     <td>公司2</td> 
     <td>200</td> 
     <td>p2</td> 
    </tr> 
    <tr> 
     <td>p1</td> 
     <td>公司3</td> 
     <td>150</td> 
     <td>p1</td> 
    </tr> 
    <tr> 
     <td>p3</td> 
     <td>公司4</td> 
     <td>300</td> 
     <td>p3</td> 
    </tr> 
   </tbody> 
  </table>
  <hr> 
  <p>建表导入数据：</p> 
  <p>create table company_info(</p> 
  <p>​ personId string,</p> 
  <p>​ company string,</p> 
  <p>​ money float</p> 
  <p>)row format delimited fields terminated by “\t”</p> 
  <p>load data local inpath “company_info.txt” into table company_info;</p> 
  <h3><a id="211order_by_90"></a>2.1.1、order by</h3> 
  <p>hive中的order<br> by语句会对查询结果做一次全局排序，即，所有的mapper产生的结果都会交给一个reducer去处理，无论数据量大小，job任务只会启动一个reducer，如果数据量巨大，则会耗费大量的时间。</p> 
  <p>尖叫提示：如果在严格模式下，order<br> by需要指定limit数据条数，不然数据量巨大的情况下会造成崩溃无输出结果。涉及属性：set<br> hive.mapred.mode=nonstrict/strict</p> 
  <p>如果设置为了strict 严格模式， 就必须在后面通过limit关键字来设置条数，如果不设置的话，很可能数据量特别大的时候机器就崩溃了</p> 
  <p>如果nonstrict 非严格模式，就不用指定了，因为是默认的大小100</p> 
  <p>例如：按照money排序的例子</p> 
  <p>select * from company_info order by money desc;</p> 
  <h3><a id="212sort_by_108"></a>2.1.2、sort by</h3> 
  <p>hive中的sort<br> by语句会对每一块局部数据进行局部排序，即，每一个reducer处理的数据都是有序的，但是不能保证全局有序。如果想保证全局有序，可以在sort<br> by的基础之上做一次全局的归并排序。</p> 
  <h3><a id="213distribute_by_114"></a>2.1.3、distribute by</h3> 
  <p>hive中的distribute by一般要和sort<br> by一起使用，即将某一块数据归给(distribute<br> by)某一个reducer处理，然后在指定的reducer中进行sort by排序。</p> 
  <p>单用dsidtribute是没有任何意义的，但是语法上是没有错误的 ，只是单纯的把数据划分到了不同的分区里面，但是没有做其他的事情</p> 
  <p>sort by按照 distribute by分好的分区进行排序</p> 
  <p>尖叫提示：distribute by必须写在sort by之前</p> 
  <p>例如：不同的人（personId）分为不同的组，每组按照money排序。</p> 
  <hr> 
  <pre><code>select \* from company\_info distribute by personId sort by personId, money desc;
</code></pre> 
  <p>如果不同组中的money值有相同的，还是需要全局排序的</p> 
  <h3><a id="214cluster_by_146"></a>2.1.4、cluster by</h3> 
  <p>hive中的cluster by在distribute by和sort<br> by排序字段一致的情况下是等价的。<strong>同时，cluster</strong><br> <strong>by指定的列只能是降序，即默认的descend，而不能是ascend。</strong></p> 
  <p>例如：写一个等价于distribute by 与sort by的例子</p> 
  <pre><code>select * from company_info distribute by personId sort by personId;

等价于

select * from compnay_info cluster by personId; 
</code></pre> 
  <h2><a id="22UDAFUDTF_164"></a>2.2、行转列、列转行（UDAF与UDTF）</h2> 
  <h3><a id="221_167"></a>2.2.1、行转列</h3> 
  <p><strong>表结构：</strong></p> 
  <table> 
   <thead> 
    <tr> 
     <th>name</th> 
     <th>constellation</th> 
     <th>blood_type</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>孙悟空</td> 
     <td>白羊座</td> 
     <td>A</td> 
    </tr> 
    <tr> 
     <td>大海</td> 
     <td>射手座</td> 
     <td>A</td> 
    </tr> 
    <tr> 
     <td>宋宋</td> 
     <td>白羊座</td> 
     <td>B</td> 
    </tr> 
    <tr> 
     <td>猪八戒</td> 
     <td>白羊座</td> 
     <td>A</td> 
    </tr> 
    <tr> 
     <td>凤姐</td> 
     <td>射手座</td> 
     <td>A</td> 
    </tr> 
   </tbody> 
  </table>
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730081940195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>孙悟空、猪八戒是分布在多行上面的，我们把多行上的数据转到一行上面，叫做行转列</p> 
  <p>创建表及数据导入：**</p> 
  <pre><code>create table person_info(

name string, 

constellation string, 

blood_type string) 

row format delimited fields terminated by "\t";

load data local inpath “person_info.txt” into table person_info;
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730081950753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>例如：把星座和血型一样的人归类到一起**</p> 
  <pre><code>select

​    t1.base,
 concat_ws('|', collect_set(t1.name)) name

from

​    (select

​        name,

​        concat(constellation, ",", blood_type) base

​    from

​        person_info) t1

group by

 t1.base;
</code></pre> 
  <p>分析：</p> 
  <p>select name, concat(constellation, “,”, blood_type) base</p> 
  <p>concat 就是把白羊座和A拼到一起了，”白羊座,A“ 命名为base<br> 其实就是</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082000392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> concat_ws(’|’, collect_set(<a href="http://t1.name" rel="nofollow" data-token="6583585224198b9d05f0bceada7c10fd">t1.name</a>)) name<br> collect_set聚合name，每个name之间用”|“分割，在mysql中有一个group_concat</p> 
  <p>最后的结果</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082010357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>如果</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082018177.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 还是按照星座和血型聚合的，但是出来的不只是人名，并且修改了上面的表名，</p> 
  <p>结果</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082026410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>concat_ws(’|’, collect_set(<a href="http://t1.name" rel="nofollow" data-token="6583585224198b9d05f0bceada7c10fd">t1.name</a>)) name 意思是吧t1.name表里面的值全部用|拼接起来，group by是聚合的意思，根据concat进行聚合，然后通过collect_set 函数收集起来，通过concat_ws进行拼接</p> 
  <h3><a id="222_266"></a>2.2.2、列转行</h3> 
  <p><strong>表结构：</strong></p> 
  <table> 
   <thead> 
    <tr> 
     <th>movie</th> 
     <th>category</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>《疑犯追踪》</td> 
     <td>悬疑,动作,科幻,剧情</td> 
    </tr> 
    <tr> 
     <td>《Lie to me》</td> 
     <td>悬疑,警匪,动作,心理,剧情</td> 
    </tr> 
    <tr> 
     <td>《战狼2》</td> 
     <td>战争,动作,灾难</td> 
    </tr> 
    <tr> 
     <td>movie</td> 
     <td>category</td> 
    </tr> 
    <tr> 
     <td>《疑犯追踪》</td> 
     <td>悬疑,动作,科幻,剧情</td> 
    </tr> 
   </tbody> 
  </table>
  <p>例如：</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082039755.png" alt="在这里插入图片描述"></p> 
  <p><strong>创建表及导入数据：</strong></p> 
  <pre><code>create table movie_info(

​    movie string, 

​    category array&lt;string&gt;) 

row format delimited fields terminated by "\t"

collection items terminated by ",";

load data local inpath "movie.txt" into table movie_info;
</code></pre> 
  <p>解释：<br> 电影名称 string类型<br> 类别 String类型，可以有多个类别，所以是array的，</p> 
  <p>hive中是支持array的</p> 
  <p>collection items terminated by “,”;当前表中的数据所有的数组类型的字段他们之间的元素用什么来分割，这里是用“，”来分割、</p> 
  <p>但是这里注意</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082049836.png" alt="在这里插入图片描述"><br> 如图，这里只能分割其中的一个list，这也是一个弊端</p> 
  <p>执行上面的建表操作<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082102399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 然后将其展开，但是这里注意列转行，只能对list等集合进行操作</p> 
  <p>当然也可以是map如下图</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082108895.png" alt="在这里插入图片描述"></p> 
  <p><strong>例如：将电影分类中的数组数据展开</strong></p> 
  <p>select</p> 
  <p>​ movie,</p> 
  <p>​ category_name</p> 
  <p>from</p> 
  <p>​ movie_info lateral view explode(category) table_tmp as category_name;</p> 
  <p>将上面的list展开</p> 
  <pre><code>select
	movie,category_name
from 
	movie_info lateral view explide(category) table_tmp as category_name;
</code></pre> 
  <p>拓展：</p> 
  <p>UDAF：聚合函数多行输入，一行输出<br> UDTF：一行输入，多行输出</p> 
  <p>explode这个词的意思是爆裂、炸开的意思，这里就是把一行的数据拆散的意思</p> 
  <p>也可以单独用</p> 
  <pre><code>select 
	explode(category) as category
from 
	movie_info;
</code></pre> 
  <p>这个时候的结果就是把category炸开了<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082131281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082236762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>上面的函数中latera是侧写的意思，就是将上面爆炸的数据进行一次侧写，生成一个新的表，标的名字叫做table_tmp ，这个表里面炸开的字段叫做category_name</p> 
  <h2><a id="httpsimgblogcsdnimgcn20190730082224535pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3Usize_16color_FFFFFFt_70
23_369"></a><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082224535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 2.3、数组操作</h2> 
  <p>“fields terminated by”：字段与字段之间的分隔符。</p> 
  <p>“collection items terminated by”：一个字段中各个子元素item的分隔符。</p> 
  <h2><a id="24orc_377"></a>2.4、orc存储</h2> 
  <p>orc即Optimized Row Columnar (ORC)<br> file，在RCFile的基础上演化而来，可以提供一种高效的方法在Hive中存储数据，提升了读、写、处理数据的效率。</p> 
  <p>他是一种默认的存储格式，还有一种存储格式是textFile，textFile可读性比较好，但是效率比较低。orc主要是为了提升了读、写、处理数据的效率。</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082254159.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>orc会把每一列的数据转化成行去存储<br> 一行存储不同格式的效率会比较低，怎样变高呢？name就应该一行存储相同格式的</p> 
  <p>一行数据存的全是某一列里面的内容，相当于把表做了一个90度的旋转</p> 
  <p>比如第一个索引存的是p1 p2 p3 p4 p5，蒂格尔索引存的是float 100 200 300 400 500</p> 
  <p>用这样的格式存储，压缩效率很高，数据处理也会高很多，甚至可以实现跳行，比如我就想访问某一列的数据，对于orc来讲，我只需要拿到某一类对应的index，就访问那一行</p> 
  <h2><a id="25Hive_398"></a>2.5、Hive分桶</h2> 
  <p>Hive可以将表或者表的分区进一步组织成桶，以达到：</p> 
  <p>1、数据取样效率更高</p> 
  <p>2、数据处理效率更高</p> 
  <p>桶通过对指定列进行哈希来实现，将一个列名下的数据切分为"一组桶"，每个桶都对应了一个该列名下的一个存储文件。</p> 
  <p>之前说过hive是有分区的，建表的时候可以通过pertition by<br> 分区的目的就是为了在访问数据的时候更快，在mysql中也有索引也有分区</p> 
  <p>那么hive的分桶是干嘛的？就是在分区的基础上在进行划分，可以理解为子分区，还有一种方式是在表里面直接进行分桶，其实表示有分区的，没有自定义分区的话，默认就是一个分区。所以在宏观上理解的话，可以分为两种，在分区上进行分桶，或者直接在表上进行分桶。严格上来讲都是在分区上进行分桶，只不过后者是只有一个分区而已。</p> 
  <h3><a id="251_416"></a>2.5.1、直接分桶</h3> 
  <p>开始操作之前，需要将hive.enforce.bucketing属性设置为true，以标识Hive可以识别桶。</p> 
  <pre><code>create table music(

​    id int,

​    name string,

​    size float)

row format delimited 

fields terminated by "\t"

clustered by (id) into 4 buckets;
</code></pre> 
  <p>该代码的意思是将music表按照id将数据分成了4个桶，插入数据时，会对应4个<br> reduce操作，输出4个文件。</p> 
  <p>id 进来进行一种hash算法，然后得到一个数字，这个数字对桶的个数进行求余，就想当前的个数是四，最后得到的余数就是0123 ，最后都会依次的放进去，上面的clustered by 里面的id就是在hash归桶的时候要通过那个字段进行归桶</p> 
  <h3><a id="252_445"></a>2.5.2、在分区中分桶</h3> 
  <p>当数据量过大，需要庞大发分区数量时，可以考虑桶，因为分区数量太大的情况可能会导致文件系统挂掉，而且桶比分区有更高的查询效率。数据最终落在哪一个桶里，取决于clustered<br> by的那个列的值的hash数与桶的个数求余来决定。虽然有一定离散行，但不能保证每个桶中的数据量是一样的。</p> 
  <pre><code>create table music2(

​    id int,

​    name string,

​    size float)

partitioned by (date string)

clustered by (id) sorted by(size) into 4 bucket

row format delimited 

fields terminated by "\t";

 

load data local inpath 'demo/music.txt' into table music2 partition(date='2017-08-30');
</code></pre> 
  <p>sorted by(size) into 4 bucket ，可以在某一个桶里面，按照某一个顺序进行排列顺序，即这个桶里面的数据都是有序的，按照size进行排序的。</p> 
  <p>partition(date=‘2017-08-30’); 指定导入到哪个分区里面</p> 
  <p>hbase的数据清洗的概念是什么？region，按照某一个范围，扔到某一个region里面，如果在rowkey是递增的，然后在某一个时段里面特别密集，就会导致某一个region特别大，某一个region特别大，就会导致数据处理的速度特别慢，某一台集群的压力特别大</p> 
  <p>hive中，加入第一个桶里面都是数据，第三个第四个桶里面没有数据，这样会导致怎样的现象呢？hive中的数据本身还是存在hdfs上面还是那个txt文件，在hbase里面有存储的倾斜，也有处理的倾斜，但是在hive、中只有数据处理的倾斜，在hive中，一个桶里面的数据会交给一个reducer处理，不可能出现两个桶对应于一个reducer，一个reducer就对应一个文件的输出，所以直白的说就是一个桶会对应一个文件的输出，两个桶会产生两个。现在问题就出来了，如果一个桶的数据特别多，剩下的桶没有数据，会导致第一个桶的reducer在第一台集齐的nodemanager执行很慢，然后第二个第三个第四个的reducer里面没数据，这就会导致负载不均衡了，<strong>所以分桶不是一定会导致数据不发生倾斜，但不是绝对的。</strong></p> 
  <h1><a id="_480"></a>三、项目</h1> 
  <h2><a id="31_483"></a>3.1、数据结构</h2> 
  <h3><a id="311_486"></a>3.1.1、视频表</h3> 
  <hr> 
  <table> 
   <thead> 
    <tr> 
     <th>字段</th> 
     <th>备注</th> 
     <th>详细描述</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>video id</td> 
     <td>视频唯一id</td> 
     <td>11位字符串</td> 
    </tr> 
    <tr> 
     <td>uploader</td> 
     <td>视频上传者</td> 
     <td>上传视频的用户名String</td> 
    </tr> 
    <tr> 
     <td>age</td> 
     <td>视频年龄</td> 
     <td>视频上传日期和2007年2月15日之间的整数天（Youtube的独特设定）</td> 
    </tr> 
    <tr> 
     <td>category</td> 
     <td>视频类别</td> 
     <td>上传视频指定的视频分类</td> 
    </tr> 
    <tr> 
     <td>length</td> 
     <td>视频长度</td> 
     <td>整形数字标识的视频长度</td> 
    </tr> 
    <tr> 
     <td>views</td> 
     <td>观看次数</td> 
     <td>视频被浏览的次数</td> 
    </tr> 
    <tr> 
     <td>rate</td> 
     <td>视频评分</td> 
     <td>满分5分</td> 
    </tr> 
    <tr> 
     <td>ratings</td> 
     <td>流量</td> 
     <td>视频的流量，整型数字</td> 
    </tr> 
    <tr> 
     <td>conments</td> 
     <td>评论数</td> 
     <td>一个视频的整数评论数</td> 
    </tr> 
    <tr> 
     <td>related ids</td> 
     <td>相关视频id</td> 
     <td>相关视频的id，最多20个</td> 
    </tr> 
   </tbody> 
  </table>
  <hr> 
  <h3><a id="312_503"></a>3.1.2、用户表</h3> 
  <hr> 
  <table> 
   <thead> 
    <tr> 
     <th>字段</th> 
     <th>备注</th> 
     <th>字段类型</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>uploader</td> 
     <td>上传者用户名</td> 
     <td>string</td> 
    </tr> 
    <tr> 
     <td>videos</td> 
     <td>上传视频数</td> 
     <td>int</td> 
    </tr> 
    <tr> 
     <td>friends</td> 
     <td>朋友数量</td> 
     <td>int</td> 
    </tr> 
    <tr> 
     <td>字段</td> 
     <td>备注</td> 
     <td>字段类型</td> 
    </tr> 
    <tr> 
     <td>uploader</td> 
     <td>上传者用户名</td> 
     <td>string</td> 
    </tr> 
    <tr> 
     <td>videos</td> 
     <td>上传视频数</td> 
     <td>int</td> 
    </tr> 
    <tr> 
     <td>friends</td> 
     <td>朋友数量</td> 
     <td>int</td> 
    </tr> 
    <tr> 
     <td>字段</td> 
     <td>备注</td> 
     <td>字段类型</td> 
    </tr> 
    <tr> 
     <td>uploader</td> 
     <td>上传者用户名</td> 
     <td>string</td> 
    </tr> 
    <tr> 
     <td>videos</td> 
     <td>上传视频数</td> 
     <td>int</td> 
    </tr> 
   </tbody> 
  </table>
  <hr> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082328874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082342165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>注意仔细观察上图中，我们上面说到这里有的是按照&amp;分割的，有的是按照tab分割的，这样上面只能分割一个list，所以我们需要进行清洗数据</p> 
  <h2><a id="32_524"></a>3.2原始数据存放地</h2> 
  <p>HDFS目录：</p> 
  <p>视频数据集：/youtube/video/2008</p> 
  <p>用户数据集：/youtube/users/2008</p> 
  <p>先把数据方法hdfs上面</p> 
  <h2><a id="httpsimgblogcsdnimgcn201907300823513pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3Usize_16color_FFFFFFt_70
33_537"></a><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/201907300823513.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 3.3、技术选型</h2> 
  <p>* CDH5.3.6-Hadoop2.5.0</p> 
  <p>* CDH5.3.6-Hive0.13.1</p> 
  <p>* Mysql</p> 
  <h3><a id="331_547"></a>3.3.1、数据清洗</h3> 
  <p>MapReduce</p> 
  <h3><a id="332_551"></a>3.3.2、数据分析</h3> 
  <p>MapReduce or Hive</p> 
  <h2><a id="34ETL_555"></a>3.4、ETL原始数据</h2> 
  <p>通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用"\t"进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用"&amp;“分割，同时去掉两边空格，多个相关视频id也使用”&amp;"进行分割。</p> 
  <h3><a id="361ETLETLUtil_560"></a>3.6.1、ETL之ETLUtil</h3> 
  <pre><code class="prism language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>z<span class="token punctuation">.</span>youtube<span class="token punctuation">.</span>util<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">ETLUtil</span> <span class="token punctuation">{</span>
	<span class="token keyword">public</span> <span class="token keyword">static</span> String <span class="token function">oriString2ETLString</span><span class="token punctuation">(</span>String ori<span class="token punctuation">)</span><span class="token punctuation">{</span>
		StringBuilder etlString <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">StringBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		String<span class="token punctuation">[</span><span class="token punctuation">]</span> splits <span class="token operator">=</span> ori<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">if</span><span class="token punctuation">(</span>splits<span class="token punctuation">.</span>length <span class="token operator">&lt;</span> <span class="token number">9</span><span class="token punctuation">)</span> <span class="token keyword">return</span> null<span class="token punctuation">;</span>
		splits<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> splits<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">replace</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> splits<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
			<span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">&lt;</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
				<span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">==</span> splits<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
					etlString<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>splits<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>					
				<span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>
					etlString<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>splits<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">"\t"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>	
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>
				<span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">==</span> splits<span class="token punctuation">.</span>length <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
					etlString<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>splits<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
				<span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>
					etlString<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>splits<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">"&amp;"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
		
		<span class="token keyword">return</span> etlString<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <p>完了之后可以写个main行数，随便粘贴上面的一行数据进行测试</p> 
  <h3><a id="362ETLMapper_594"></a>3.6.2、ETL之Mapper</h3> 
  <pre><code class="prism language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>z<span class="token punctuation">.</span>youtube<span class="token punctuation">.</span>mr<span class="token punctuation">.</span>etl<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>

<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>commons<span class="token punctuation">.</span>lang<span class="token punctuation">.</span>StringUtils<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>NullWritable<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Mapper<span class="token punctuation">;</span>

<span class="token keyword">import</span> com<span class="token punctuation">.</span>z<span class="token punctuation">.</span>youtube<span class="token punctuation">.</span>util<span class="token punctuation">.</span>ETLUtil<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">VideoETLMapper</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token generics function"><span class="token punctuation">&lt;</span>Object<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> NullWritable<span class="token punctuation">,</span> Text<span class="token punctuation">&gt;</span></span><span class="token punctuation">{</span>
	Text text <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	
	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>Object key<span class="token punctuation">,</span> Text value<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>
		String etlString <span class="token operator">=</span> ETLUtil<span class="token punctuation">.</span><span class="token function">oriString2ETLString</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token keyword">if</span><span class="token punctuation">(</span>StringUtils<span class="token punctuation">.</span><span class="token function">isBlank</span><span class="token punctuation">(</span>etlString<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">return</span><span class="token punctuation">;</span>
		
		text<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>etlString<span class="token punctuation">)</span><span class="token punctuation">;</span>
		context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>NullWritable<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>
	

<span class="token punctuation">}</span>
</code></pre> 
  <h3><a id="363ETLRunner_627"></a>3.6.3、ETL之Runner</h3> 
  <pre><code class="prism language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>z<span class="token punctuation">.</span>youtube<span class="token punctuation">.</span>mr<span class="token punctuation">.</span>etl<span class="token punctuation">;</span>

<span class="token keyword">import</span> java<span class="token punctuation">.</span>io<span class="token punctuation">.</span>IOException<span class="token punctuation">;</span>

<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span>Configuration<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span>FileSystem<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span>Path<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>NullWritable<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>Text<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>Job<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span>FileInputFormat<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>output<span class="token punctuation">.</span>FileOutputFormat<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Tool<span class="token punctuation">;</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>util<span class="token punctuation">.</span>ToolRunner<span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">VideoETLRunner</span> <span class="token keyword">implements</span> <span class="token class-name">Tool</span> <span class="token punctuation">{</span>
	<span class="token keyword">private</span> Configuration conf <span class="token operator">=</span> null<span class="token punctuation">;</span>

	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">setConf</span><span class="token punctuation">(</span>Configuration conf<span class="token punctuation">)</span> <span class="token punctuation">{</span>
		<span class="token keyword">this</span><span class="token punctuation">.</span>conf <span class="token operator">=</span> conf<span class="token punctuation">;</span>
	<span class="token punctuation">}</span>

	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">public</span> Configuration <span class="token function">getConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>

		<span class="token keyword">return</span> <span class="token keyword">this</span><span class="token punctuation">.</span>conf<span class="token punctuation">;</span>
	<span class="token punctuation">}</span>

	<span class="token annotation punctuation">@Override</span>
	<span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">run</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
		conf <span class="token operator">=</span> <span class="token keyword">this</span><span class="token punctuation">.</span><span class="token function">getConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//需要传入两个参数，第一个参数是你要清洗的</span>
        <span class="token comment">//数据的所在目录。</span>
        <span class="token comment">//第二个参数是清洗完之后你的输出目录</span>
		conf<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"inpath"</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		conf<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"outpath"</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

		Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>conf<span class="token punctuation">,</span> <span class="token string">"youtube-video-etl"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>VideoETLRunner<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>VideoETLMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>NullWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		job<span class="token punctuation">.</span><span class="token function">setNumReduceTasks</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token keyword">this</span><span class="token punctuation">.</span><span class="token function">initJobInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">this</span><span class="token punctuation">.</span><span class="token function">initJobOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token keyword">return</span> job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>

	<span class="token keyword">private</span> <span class="token keyword">void</span> <span class="token function">initJobOutputPath</span><span class="token punctuation">(</span>Job job<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException <span class="token punctuation">{</span>
		Configuration conf <span class="token operator">=</span> job<span class="token punctuation">.</span><span class="token function">getConfiguration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//通过这个键把刚才传到conf里面的值取出来</span>
		String outPathString <span class="token operator">=</span> conf<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token string">"outpath"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		Path outPath <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>outPathString<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">if</span><span class="token punctuation">(</span>fs<span class="token punctuation">.</span><span class="token function">exists</span><span class="token punctuation">(</span>outPath<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
			fs<span class="token punctuation">.</span><span class="token function">delete</span><span class="token punctuation">(</span>outPath<span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span>
		
		FileOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> outPath<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token punctuation">}</span>

	<span class="token keyword">private</span> <span class="token keyword">void</span> <span class="token function">initJobInputPath</span><span class="token punctuation">(</span>Job job<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException <span class="token punctuation">{</span>
		Configuration conf <span class="token operator">=</span> job<span class="token punctuation">.</span><span class="token function">getConfiguration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		String inPathString <span class="token operator">=</span> conf<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token string">"inpath"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		Path inPath <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>inPathString<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">if</span><span class="token punctuation">(</span>fs<span class="token punctuation">.</span><span class="token function">exists</span><span class="token punctuation">(</span>inPath<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
			FileInputFormat<span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> inPath<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>
			<span class="token keyword">throw</span> <span class="token keyword">new</span> <span class="token class-name">RuntimeException</span><span class="token punctuation">(</span><span class="token string">"HDFS中该文件目录不存在："</span> <span class="token operator">+</span> inPathString<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>

	<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>
		<span class="token keyword">try</span> <span class="token punctuation">{</span>
			<span class="token keyword">int</span> resultCode <span class="token operator">=</span> ToolRunner<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">VideoETLRunner</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token keyword">if</span><span class="token punctuation">(</span>resultCode <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
				System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Success!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>
				System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Fail!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span>
			System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>resultCode<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">Exception</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>
			e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <h3><a id="364ETL_730"></a><strong>3.6.4</strong>、执行ETL</h3> 
  <pre><code class="prism language-java">$ bin<span class="token operator">/</span>yarn jar <span class="token operator">~</span><span class="token operator">/</span>Desktop<span class="token operator">/</span>youtube<span class="token operator">-</span><span class="token number">0.0</span><span class="token number">.1</span><span class="token operator">-</span>SNAPSHOT<span class="token punctuation">.</span>jar \
com<span class="token punctuation">.</span>z<span class="token punctuation">.</span>youtube<span class="token punctuation">.</span>mr<span class="token punctuation">.</span>etl<span class="token punctuation">.</span>VideoETLRunner \
<span class="token operator">/</span>youtube<span class="token operator">/</span>video<span class="token operator">/</span><span class="token number">2008</span><span class="token operator">/</span><span class="token number">0222</span> \
<span class="token operator">/</span>youtube<span class="token operator">/</span>output<span class="token operator">/</span>video<span class="token operator">/</span><span class="token number">2008</span><span class="token operator">/</span><span class="token number">0222</span>
</code></pre> 
  <h2><a id="httpsimgblogcsdnimgcn2019073008240961pngxossprocessimagewatermarktype_ZmFuZ3poZW5naGVpdGkshadow_10text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3Usize_16color_FFFFFFt_70
35_743"></a><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019073008240961.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 3.5、准备工作</h2> 
  <h3><a id="351_747"></a>3.5.1、创建表</h3> 
  <p>创建表：youtube_ori（原始视频数据表），youtube_user_ori（原始用户表），</p> 
  <p>创建表：youtube_orc，youtube_user_orc</p> 
  <p>为什么要有原始表？因为想让数据的存储为orc形式，如果是orc形式是不能够直接使用load data这样的命令插入数据的，必须通过查询的方式把数据插入到orc格式的表里面，所以需要先见两个表，把清洗后的数据导进去，到入之后，对表进行查询，插入到orc表里面</p> 
  <p>youtube_ori：</p> 
  <pre><code>create table youtube_ori(

videoId string, 

uploader string, 

age int, 

category array&lt;string&gt;, 

length int, 

views int, 

rate float, 

ratings int, 

comments int,

relatedId array&lt;string&gt;)
row format delimited 
fields terminated by "\t"
collection items terminated by
"&amp;"
stored as textfile;
</code></pre> 
  <p>然后把原始数据插入到orc表中</p> 
  <p>youtube_orc：</p> 
  <pre><code>create table youtube_orc(

videoId string, 

uploader string, 

age int, 

category array&lt;string&gt;, 

length int, 

views int, 

rate float, 

ratings int, 

comments int,

relatedId array&lt;string&gt;)
clustered by (uploader) into 8 buckets 
row format delimited fields terminated by
"\t" 
collection items terminated by
"&amp;" 
stored as orc;
</code></pre> 
  <p>youtube_user_orc：</p> 
  <pre><code>create table youtube_user_orc(
    uploader string,
    videos int,
    friends int)
clustered by (uploader) into 24 buckets 
row format delimited 
fields terminated by "\t" 
stored as orc;
</code></pre> 
  <p>注意不同的地方是stored as orc<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082442472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019073008244611.png" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082452262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>然后开始把数据导入到orc表里面</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082500952.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 过程可能比较慢</p> 
  <p>查一下orc表的数据</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082515165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="352ETL_854"></a>3.5.2、导入ETL后的数据</h3> 
  <p>youtube_ori：</p> 
  <hr> 
  <p>load data inpath “/youtube/output/video/2008/0222” into table youtube_ori;</p> 
  <p>youtube_user_ori：</p> 
  <hr> 
  <p>load data inpath “/youtube/user/2008/0903” into table youtube_user_ori;</p> 
  <h3><a id="353ORC_866"></a>3.5.3、向ORC表插入数据</h3> 
  <p>youtube_orc：</p> 
  <hr> 
  <p>insert into table youtube_orc select * from youtube_ori;</p> 
  <p>youtube_user_orc：</p> 
  <hr> 
  <p>insert into table youtube_user_orc select * from youtube_user_ori;</p> 
  <h2><a id="36_878"></a>3.6、业务分析</h2> 
  <h3><a id="361Top10_881"></a>3.6.1、统计视频观看数Top10</h3> 
  <p><strong>思路：</strong></p> 
  <p>1) 使用order<br> by按照views字段做一个全局排序即可，同时我们设置只显示前10条。</p> 
  <p><strong>最终代码：</strong></p> 
  <pre><code>select 
    videoId, 
    uploader, 
    age, 
    category, 
    length, 
    views, 
    rate, 
    ratings, 
    comments 
from 
    youtube_orc 
order by 
views desc 
limit 
    10;

</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082526946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="362Top10_915"></a>3.6.2、统计视频类别热度Top10</h3> 
  <p><strong>思路：</strong></p> 
  <p>1) 即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。</p> 
  <p>2) 我们需要按照类别group by聚合，然后count组内的videoId个数即可。</p> 
  <p>3) 因为当前表结构为：一个视频对应一个或多个类别。所以如果要group<br> by类别，需要先将类别进行列转行(展开)，然后再进行count即可。</p> 
  <p>4) 最后按照热度排序，显示前10条。</p> 
  <p><strong>最终代码：</strong></p> 
  <pre><code>select 
    category_name as category, 
    count(t1.videoId) as hot 
from (
    select 
        videoId,
        category_name 
    from 
        youtube_orc lateral view explode(category) t_catetory as category_name) t1 
group by 
    t1.category_name 
order by 
    hot 
desc limit 
    10;

</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082547236.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="36320Top20_954"></a>3.6.3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</h3> 
  <p><strong>思路：</strong></p> 
  <p>1) 先找到观看数最高的20个视频所属条目的所有信息，降序排列</p> 
  <p>2) 把这20条信息中的category分裂出来(列转行)</p> 
  <p>3) 最后查询视频分类名称和该分类下有多少个Top20的视频</p> 
  <p><strong>最终代码：</strong></p> 
  <pre><code>select 
    category_name as category, 
    count(t2.videoId) as hot_with_views 
from (
    select 
        videoId, 
        category_name 
    from (
        select 
            * 
        from 
            youtube_orc 
        order by 
            views 
        desc limit 
            20) t1 lateral view explode(category) t_catetory as category_name) t2 
group by 
    category_name 
order by 
    hot_with_views 
desc;

</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082608377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082614561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 执行结果</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082625137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="364Top50Rank_998"></a>3.6.4、统计视频观看数Top50所关联视频的所属类别Rank</h3> 
  <p>rank排名的意思</p> 
  <p><strong>思路：</strong></p> 
  <ol> 
   <li></li> 
  </ol> 
  <p>查询出观看数最多的前50个视频的所有信息(当然包含了每个视频对应的关联视频)，记为临时表t1</p> 
  <pre><code>t1:观看数前50的视频
select 
    * 
from 
    youtube_orc 
order by 
    views 
desc limit 
    50;

</code></pre> 
  <p>2) 将找到的50条视频信息的相关视频relatedId列转行，记为临时表t2</p> 
  <pre><code>t2:将相关视频的id进行列转行操作
select 
    explode(relatedId) as videoId 
from 
	t1;
</code></pre> 
  <p>3) 将相关视频的id和youtube_orc表进行inner join操作</p> 
  <pre><code>t5:得到两列数据，一列是category，一列是之前查询出来的相关视频id
(select 
    distinct(t2.videoId), 
    t3.category 
from 
    t2
inner join 
    youtube_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name;

</code></pre> 
  <p>4) 按照视频类别进行分组，统计每组视频个数，然后排行</p> 
  <p><strong>最终代码：</strong></p> 
  <pre><code>select 
    category_name as category, 
    count(t5.videoId) as hot 
from (
    select 
        videoId, 
        category_name 
    from (
        select 
            distinct(t2.videoId), 
            t3.category 
        from (
            select 
                explode(relatedId) as videoId 
            from (
                select 
                    * 
                from 
                    youtube_orc 
                order by 
                    views 
                desc limit 
                    50) t1) t2 
        inner join 
            youtube_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name) t5
group by 
    category_name 
order by 
    hot 
desc;

</code></pre> 
  <p>t4 lateral view explode(category) t_catetory</p> 
  <p>这句话的意思是对t4表进行侧写，侧写的内容是，把category这一例炸开，炸开的结果生成一张新的表叫做t_catetory，炸开的这一列有一列的别名叫做category_name ，</p> 
  <p>然后distinct去重，然后distinct上面的select整个完成之后生成一个新的表叫做t5表</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019073008263753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="365Top10Music_1089"></a>3.6.5、统计每个类别中的视频热度Top10，以Music为例</h3> 
  <p><strong>思路：</strong></p> 
  <ol> 
   <li></li> 
  </ol> 
  <p>要想统计Music类别中的视频热度Top10，需要先找到Music类别，所以需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。</p> 
  <p>2) 向category展开的表中插入数据。</p> 
  <p>3) 统计对应类别（Music）中的视频热度。</p> 
  <p><strong>最终代码：</strong></p> 
  <p>创建表类别表：</p> 
  <pre><code>create table youtube_category(
    videoId string, 
    uploader string, 
    age int, 
    categoryId string, 
    length int, 
    views int, 
    rate float, 
    ratings int, 
    comments int, 
    relatedId array&lt;string&gt;)
row format delimited 
fields terminated by "\t" 
collection items terminated by "&amp;" 
stored as orc;

</code></pre> 
  <p>向类别表中插入数据：</p> 
  <pre><code>insert into table youtube_category  
    select 
        videoId,
        uploader,
        age,
        categoryId,
        length,
        views,
        rate,
        ratings,
        comments,
        relatedId 
    from 
        youtube_orc lateral view explode(category) catetory as categoryId;

</code></pre> 
  <p>将类别炸开</p> 
  <p>统计Music类别的Top10（也可以统计其他）</p> 
  <pre><code>select 
    videoId, 
    views
from 
    youtube_category 
where 
    categoryId = "Music" 
order by 
    views 
desc limit
    10;

</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082648462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="366Top10Music_1165"></a>3.6.6、统计每个类别中视频流量Top10，以Music为例</h3> 
  <p><strong>思路：</strong></p> 
  <p>1) 创建视频类别展开表（categoryId列转行后的表）</p> 
  <p>2) 按照ratings排序即可</p> 
  <p><strong>最终代码：</strong></p> 
  <pre><code>select 
    videoId,
    views,
    ratings 
from 
    youtube_category 
where 
    categoryId = "Music" 
order by 
    ratings 
desc limit 
    10;

</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082657588.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="367Top1020_1192"></a>3.6.7、统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频</h3> 
  <p><strong>思路：</strong></p> 
  <p>1) 先找到上传视频最多的10个用户的用户信息</p> 
  <pre><code>select 
    * 
from 
    youtube_user_orc 
order by 
    videos 
desc limit 
    10;

</code></pre> 
  <ol start="2"> 
   <li></li> 
  </ol> 
  <p>通过uploader字段与youtube_orc表进行join，得到的信息按照views观看次数进行排序即可。</p> 
  <p><strong>最终代码：</strong></p> 
  <h3><a id="_1215"></a></h3> 
  <pre><code>select 
    t2.videoId, 
    t2.views,
    t2.ratings,
    t1.videos,
    t1.friends 
from (
    select 
        * 
    from 
        youtube_user_orc 
    order by 
        videos desc 
    limit 
        10) t1 
join 
    youtube_orc t2
on 
    t1.uploader = t2.uploader 
order by 
    views desc 
limit 
    20;

</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082714603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="368Top10_1245"></a>3.6.8、统计每个类别视频观看数Top10</h3> 
  <p><strong>思路：</strong></p> 
  <p>1) 先得到categoryId展开的表数据</p> 
  <ol start="2"> 
   <li></li> 
  </ol> 
  <p>子查询按照categoryId进行分区，然后分区内排序，并生成递增数字，该递增数字这一列起名为rank列</p> 
  <p>3) 通过子查询产生的临时表，查询rank值小于等于10的数据行即可。</p> 
  <p><strong>最终代码</strong>：</p> 
  <pre><code>select 
    t1.* 
from (
    select 
        videoId,
        categoryId,
        views,
        row_number() over(partition by categoryId order by views desc) rank from youtube_category) t1 
where 
    rank &lt;= 10;

</code></pre> 
  <p>partition by categoryId 相同的分类在一个分区里面</p> 
  <p>row_number 这个函数的意思是对后面每一分区里面的数据进行一个升序的序列号的生成</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730082722994.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h1><a id="_1279"></a>四、可能出现的问题</h1> 
  <h2><a id="41JVM_1282"></a>4.1、JVM堆内存溢出</h2> 
  <p>描述：java.lang.OutOfMemoryError: Java heap space</p> 
  <p>解决：在yarn-site.xml中加入如下代码</p> 
  <pre><code>&lt;property&gt;
//yarn允许的最大执行任务的调度的内存是多少，
//一定不要超过实际的电脑内存，比如电脑是2g，这里就分配2g，
//这里默认是8g
	&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;
	&lt;value&gt;2048&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
//物理内存和虚拟内存的关系，
//比如上面分配的物理内存是2g
//2.1的意思是上面的2*2.1就是虚拟内存的大小
	&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;
	&lt;value&gt;2.1&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
//在执行mapreduce任务的时候，每一个jvm实例，
//所允许的最大堆内存的大小是1024个，默认是200个
&lt;name&gt;mapred.child.java.opts&lt;/name&gt;
	&lt;value&gt;-Xmx1024m&lt;/value&gt;
&lt;/property&gt;

</code></pre> 
  <h1><a id="_1317"></a>五、总结及作业</h1> 
  <p>请大家自行使用Java-MapReduce来实现上述需求。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
