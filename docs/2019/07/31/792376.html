<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>六、ResNet网络详细解析（超详细哦） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="六、ResNet网络详细解析（超详细哦）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1、 RestNet网络 1.1、 RestNet网络结构 ResNet在2015年被提出，在ImageNet比赛classification任务上获得第一名，因为它“简单与实用”并存，之后很多方法都建立在ResNet50或者ResNet101的基础上完成的，检测，分割，识别等领域里得到广泛的应用。它使用了一种连接方式叫做“shortcut connection”，顾名思义，shortcut就是“抄近道”的意思，下面是这个resnet的网络结构： 它对每层的输入做一个reference（X）, 学习形成残差函数， 而不是学习一些没有reference（X）的函数。这种残差函数更容易优化，能使网络层数大大加深。在上图的残差块中它有二层，如下表达式， 其中σ代表非线性函数ReLU。 然后通过一个shortcut，和第2个ReLU，获得输出y。 当需要对输入和输出维数进行变化时（如改变通道数目），可以在shortcut时对x做一个线性变换Ws，如下式。 然而实验证明x已经足够了，不需要再搞个维度变换，除非需求是某个特定维度的输出，如是将通道数翻倍，如下图所示： 由上图，我们可以清楚的看到“实线”和“虚线”两种连接方式， 实线的Connection部分 (第一个粉色矩形和第三个粉色矩形) 都是执行3x3x64的卷积，他们的channel个数一致，所以采用计算方式： Y = F(x) + x，虚线的Connection部分 (第一个绿色矩形和第三个绿色矩形) 分别是3x3x64和3x3x128的卷积操作，他们的channel个数不同(64和128)，所以采用计算方式： y=F(x)+Wx 。其中W是卷积操作，用来调整x的channel维度。 在计算机视觉里，网络的深度是实现网络好的效果的重要因素，输入特征的“等级”随增网络深度的加深而变高。然而在网络深度不断加深的情况下，梯度弥散/爆炸成为训练深层次的网络的障碍，导致导致网络无法收敛。虽然，归一初始化，各层输入归一化，使得可以收敛的网络的深度提升为原来的十倍。虽然网络收敛了，但网络却开始退化 （增加网络层数却导致更大的误差）， 如下图所示： 由上图可知，在一个浅层网络的基础上叠加y=x的层（称identity mappings，恒等映射），可以让网络随深度增加而不退化。这反映了多层非线性网络无法逼近恒等映射网络。 但是，在深度学习中我们希望有更好性能的网络，而网络不退化则不是我们的目的。 在 RestNet网络中学习的残差函数是F(x) = H(x) - x, 这里如果F(x) = 0, 那么就是上面提到的恒等映射（H(x) = x）。事实上，RestNet是“shortcut connections”的在connections是在恒等映射下的特殊情况，它没有引入额外的参数和计算的复杂度。 假如优化目标函数是逼近一个恒等映射, 而不是0映射（F(x) = 0）或者说恒等映射，那么学习找到对恒等映射的扰动会比重新学习一个映射函数要容易。 1.2、残差块的两种结构 这是文章里面的图，我们可以看到一个“弯弯的弧线“这个就是所谓的”shortcut connection“，也是文中提到identity mapping，这张图也诠释了ResNet的真谛，当然大家可以放心，真正在使用的ResNet模块并不是这么单一，文章中就提出了两种方式： 这两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图），一般称整个结构为一个“building block” 。其中右图又称为“bottleneck design”，目的就是为了降低参数的数目，实际中，考虑计算的成本，对残差块做了计算优化，即将两个3x3的卷积层替换为1x1 + 3x3 + 1x1，如右图所示。新结构中的中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量。第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。 对于常规ResNet，可以用于34层或者更少的网络中，对于Bottleneck Design的ResNet通常用于更深的如101这样的网络中，目的是减少计算和参数量。 1.3、ResNet50和ResNet101简单讲解 这里把ResNet50和ResNet101特别提出，主要因为它们的使用率很高，所以需要做特别的说明。给出了它们具体的结构： 上表是Resnet不同的结构，上表一共提出了5中深度的ResNet，分别是18，34，50，101和152，首先看表的最左侧，我们发现所有的网络都分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x，之后的其他论文也会专门用这个称呼指代ResNet50或者101的每部分。 例如：101-layer那列，101-layer指的是101层网络，首先有个输入7x7x64的卷积，然后经过3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，最后有个fc层(用于分类)，所以1 + 99 + 1 = 101层，确实有101层网络； 注：101层网络仅仅指卷积或者全连接层，而激活层或者Pooling层并没有计算在内；我们关注50-layer和101-layer这两列，可以发现，它们唯一的不同在于conv4_x，ResNet50有6个block，而ResNet101有23个block，两者之间差了17个block，也就是17 x 3 = 51层。 ！！！写博客不容易，请君给个赞在离开！！！！" />
<meta property="og:description" content="1、 RestNet网络 1.1、 RestNet网络结构 ResNet在2015年被提出，在ImageNet比赛classification任务上获得第一名，因为它“简单与实用”并存，之后很多方法都建立在ResNet50或者ResNet101的基础上完成的，检测，分割，识别等领域里得到广泛的应用。它使用了一种连接方式叫做“shortcut connection”，顾名思义，shortcut就是“抄近道”的意思，下面是这个resnet的网络结构： 它对每层的输入做一个reference（X）, 学习形成残差函数， 而不是学习一些没有reference（X）的函数。这种残差函数更容易优化，能使网络层数大大加深。在上图的残差块中它有二层，如下表达式， 其中σ代表非线性函数ReLU。 然后通过一个shortcut，和第2个ReLU，获得输出y。 当需要对输入和输出维数进行变化时（如改变通道数目），可以在shortcut时对x做一个线性变换Ws，如下式。 然而实验证明x已经足够了，不需要再搞个维度变换，除非需求是某个特定维度的输出，如是将通道数翻倍，如下图所示： 由上图，我们可以清楚的看到“实线”和“虚线”两种连接方式， 实线的Connection部分 (第一个粉色矩形和第三个粉色矩形) 都是执行3x3x64的卷积，他们的channel个数一致，所以采用计算方式： Y = F(x) + x，虚线的Connection部分 (第一个绿色矩形和第三个绿色矩形) 分别是3x3x64和3x3x128的卷积操作，他们的channel个数不同(64和128)，所以采用计算方式： y=F(x)+Wx 。其中W是卷积操作，用来调整x的channel维度。 在计算机视觉里，网络的深度是实现网络好的效果的重要因素，输入特征的“等级”随增网络深度的加深而变高。然而在网络深度不断加深的情况下，梯度弥散/爆炸成为训练深层次的网络的障碍，导致导致网络无法收敛。虽然，归一初始化，各层输入归一化，使得可以收敛的网络的深度提升为原来的十倍。虽然网络收敛了，但网络却开始退化 （增加网络层数却导致更大的误差）， 如下图所示： 由上图可知，在一个浅层网络的基础上叠加y=x的层（称identity mappings，恒等映射），可以让网络随深度增加而不退化。这反映了多层非线性网络无法逼近恒等映射网络。 但是，在深度学习中我们希望有更好性能的网络，而网络不退化则不是我们的目的。 在 RestNet网络中学习的残差函数是F(x) = H(x) - x, 这里如果F(x) = 0, 那么就是上面提到的恒等映射（H(x) = x）。事实上，RestNet是“shortcut connections”的在connections是在恒等映射下的特殊情况，它没有引入额外的参数和计算的复杂度。 假如优化目标函数是逼近一个恒等映射, 而不是0映射（F(x) = 0）或者说恒等映射，那么学习找到对恒等映射的扰动会比重新学习一个映射函数要容易。 1.2、残差块的两种结构 这是文章里面的图，我们可以看到一个“弯弯的弧线“这个就是所谓的”shortcut connection“，也是文中提到identity mapping，这张图也诠释了ResNet的真谛，当然大家可以放心，真正在使用的ResNet模块并不是这么单一，文章中就提出了两种方式： 这两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图），一般称整个结构为一个“building block” 。其中右图又称为“bottleneck design”，目的就是为了降低参数的数目，实际中，考虑计算的成本，对残差块做了计算优化，即将两个3x3的卷积层替换为1x1 + 3x3 + 1x1，如右图所示。新结构中的中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量。第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。 对于常规ResNet，可以用于34层或者更少的网络中，对于Bottleneck Design的ResNet通常用于更深的如101这样的网络中，目的是减少计算和参数量。 1.3、ResNet50和ResNet101简单讲解 这里把ResNet50和ResNet101特别提出，主要因为它们的使用率很高，所以需要做特别的说明。给出了它们具体的结构： 上表是Resnet不同的结构，上表一共提出了5中深度的ResNet，分别是18，34，50，101和152，首先看表的最左侧，我们发现所有的网络都分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x，之后的其他论文也会专门用这个称呼指代ResNet50或者101的每部分。 例如：101-layer那列，101-layer指的是101层网络，首先有个输入7x7x64的卷积，然后经过3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，最后有个fc层(用于分类)，所以1 + 99 + 1 = 101层，确实有101层网络； 注：101层网络仅仅指卷积或者全连接层，而激活层或者Pooling层并没有计算在内；我们关注50-layer和101-layer这两列，可以发现，它们唯一的不同在于conv4_x，ResNet50有6个block，而ResNet101有23个block，两者之间差了17个block，也就是17 x 3 = 51层。 ！！！写博客不容易，请君给个赞在离开！！！！" />
<link rel="canonical" href="https://uzzz.org/2019/07/31/792376.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/31/792376.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-31T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"1、 RestNet网络 1.1、 RestNet网络结构 ResNet在2015年被提出，在ImageNet比赛classification任务上获得第一名，因为它“简单与实用”并存，之后很多方法都建立在ResNet50或者ResNet101的基础上完成的，检测，分割，识别等领域里得到广泛的应用。它使用了一种连接方式叫做“shortcut connection”，顾名思义，shortcut就是“抄近道”的意思，下面是这个resnet的网络结构： 它对每层的输入做一个reference（X）, 学习形成残差函数， 而不是学习一些没有reference（X）的函数。这种残差函数更容易优化，能使网络层数大大加深。在上图的残差块中它有二层，如下表达式， 其中σ代表非线性函数ReLU。 然后通过一个shortcut，和第2个ReLU，获得输出y。 当需要对输入和输出维数进行变化时（如改变通道数目），可以在shortcut时对x做一个线性变换Ws，如下式。 然而实验证明x已经足够了，不需要再搞个维度变换，除非需求是某个特定维度的输出，如是将通道数翻倍，如下图所示： 由上图，我们可以清楚的看到“实线”和“虚线”两种连接方式， 实线的Connection部分 (第一个粉色矩形和第三个粉色矩形) 都是执行3x3x64的卷积，他们的channel个数一致，所以采用计算方式： Y = F(x) + x，虚线的Connection部分 (第一个绿色矩形和第三个绿色矩形) 分别是3x3x64和3x3x128的卷积操作，他们的channel个数不同(64和128)，所以采用计算方式： y=F(x)+Wx 。其中W是卷积操作，用来调整x的channel维度。 在计算机视觉里，网络的深度是实现网络好的效果的重要因素，输入特征的“等级”随增网络深度的加深而变高。然而在网络深度不断加深的情况下，梯度弥散/爆炸成为训练深层次的网络的障碍，导致导致网络无法收敛。虽然，归一初始化，各层输入归一化，使得可以收敛的网络的深度提升为原来的十倍。虽然网络收敛了，但网络却开始退化 （增加网络层数却导致更大的误差）， 如下图所示： 由上图可知，在一个浅层网络的基础上叠加y=x的层（称identity mappings，恒等映射），可以让网络随深度增加而不退化。这反映了多层非线性网络无法逼近恒等映射网络。 但是，在深度学习中我们希望有更好性能的网络，而网络不退化则不是我们的目的。 在 RestNet网络中学习的残差函数是F(x) = H(x) - x, 这里如果F(x) = 0, 那么就是上面提到的恒等映射（H(x) = x）。事实上，RestNet是“shortcut connections”的在connections是在恒等映射下的特殊情况，它没有引入额外的参数和计算的复杂度。 假如优化目标函数是逼近一个恒等映射, 而不是0映射（F(x) = 0）或者说恒等映射，那么学习找到对恒等映射的扰动会比重新学习一个映射函数要容易。 1.2、残差块的两种结构 这是文章里面的图，我们可以看到一个“弯弯的弧线“这个就是所谓的”shortcut connection“，也是文中提到identity mapping，这张图也诠释了ResNet的真谛，当然大家可以放心，真正在使用的ResNet模块并不是这么单一，文章中就提出了两种方式： 这两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图），一般称整个结构为一个“building block” 。其中右图又称为“bottleneck design”，目的就是为了降低参数的数目，实际中，考虑计算的成本，对残差块做了计算优化，即将两个3x3的卷积层替换为1x1 + 3x3 + 1x1，如右图所示。新结构中的中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量。第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。 对于常规ResNet，可以用于34层或者更少的网络中，对于Bottleneck Design的ResNet通常用于更深的如101这样的网络中，目的是减少计算和参数量。 1.3、ResNet50和ResNet101简单讲解 这里把ResNet50和ResNet101特别提出，主要因为它们的使用率很高，所以需要做特别的说明。给出了它们具体的结构： 上表是Resnet不同的结构，上表一共提出了5中深度的ResNet，分别是18，34，50，101和152，首先看表的最左侧，我们发现所有的网络都分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x，之后的其他论文也会专门用这个称呼指代ResNet50或者101的每部分。 例如：101-layer那列，101-layer指的是101层网络，首先有个输入7x7x64的卷积，然后经过3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，最后有个fc层(用于分类)，所以1 + 99 + 1 = 101层，确实有101层网络； 注：101层网络仅仅指卷积或者全连接层，而激活层或者Pooling层并没有计算在内；我们关注50-layer和101-layer这两列，可以发现，它们唯一的不同在于conv4_x，ResNet50有6个block，而ResNet101有23个block，两者之间差了17个block，也就是17 x 3 = 51层。 ！！！写博客不容易，请君给个赞在离开！！！！","@type":"BlogPosting","url":"https://uzzz.org/2019/07/31/792376.html","headline":"六、ResNet网络详细解析（超详细哦）","dateModified":"2019-07-31T00:00:00+08:00","datePublished":"2019-07-31T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/31/792376.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>六、ResNet网络详细解析（超详细哦）</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h2><a id="1	RestNet_0"></a>1、 RestNet网络</h2> 
  <h3><a id="11	RestNet_1"></a>1.1、 RestNet网络结构</h3> 
  <p>ResNet在2015年被提出，在ImageNet比赛classification任务上获得第一名，因为它“简单与实用”并存，之后很多方法都建立在ResNet50或者ResNet101的基础上完成的，检测，分割，识别等领域里得到广泛的应用。它使用了一种连接方式叫做“shortcut connection”，顾名思义，shortcut就是“抄近道”的意思，下面是这个resnet的网络结构：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731144208639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzYwNzY3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 它对每层的输入做一个reference（X）, 学习形成残差函数， 而不是学习一些没有reference（X）的函数。这种残差函数更容易优化，能使网络层数大大加深。在上图的残差块中它有二层，如下表达式，<br> 其中σ代表非线性函数ReLU。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731144237339.png" alt="在这里插入图片描述"><br> 然后通过一个shortcut，和第2个ReLU，获得输出y。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731144304151.png" alt="在这里插入图片描述"><br> 当需要对输入和输出维数进行变化时（如改变通道数目），可以在shortcut时对x做一个线性变换Ws，如下式。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731144342584.png" alt="在这里插入图片描述"><br> 然而实验证明x已经足够了，不需要再搞个维度变换，除非需求是某个特定维度的输出，如是将通道数翻倍，如下图所示：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731144535537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzYwNzY3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 由上图，我们可以清楚的看到“实线”和“虚线”两种连接方式， 实线的Connection部分 <strong>(第一个粉色矩形和第三个粉色矩形)</strong> 都是执行3x3x64的卷积，他们的channel个数一致，所以采用计算方式：<br> Y = F(x) + x，虚线的Connection部分 <strong>(第一个绿色矩形和第三个绿色矩形)</strong> 分别是3x3x64和3x3x128的卷积操作，他们的channel个数不同(64和128)，所以采用计算方式： y=F(x)+Wx 。其中W是卷积操作，用来调整x的channel维度。<br> 在计算机视觉里，网络的深度是实现网络好的效果的重要因素，输入特征的“等级”随增网络深度的加深而变高。然而在网络深度不断加深的情况下，梯度弥散/爆炸成为训练深层次的网络的障碍，导致导致网络无法收敛。虽然，归一初始化，各层输入归一化，使得可以收敛的网络的深度提升为原来的十倍。虽然网络收敛了，但网络却开始退化 <strong>（增加网络层数却导致更大的误差）</strong>， 如下图所示：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731144748820.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzYwNzY3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 由上图可知，在一个浅层网络的基础上叠加y=x的层（称identity mappings，恒等映射），可以让网络随深度增加而不退化。这反映了多层非线性网络无法逼近恒等映射网络。<br> 但是，在深度学习中我们希望有更好性能的网络，而网络不退化则不是我们的目的。 在 RestNet网络中学习的残差函数是F(x) = H(x) - x, 这里如果F(x) = 0, 那么就是上面提到的恒等映射（H(x) = x）。事实上，RestNet是“shortcut connections”的在connections是在恒等映射下的特殊情况，它没有引入额外的参数和计算的复杂度。 假如优化目标函数是逼近一个恒等映射, 而不是0映射（F(x) = 0）或者说恒等映射，那么学习找到对恒等映射的扰动会比重新学习一个映射函数要容易。</p> 
  <h3><a id="12_20"></a>1.2、残差块的两种结构</h3> 
  <p>这是文章里面的图，我们可以看到一个“弯弯的弧线“这个就是所谓的”shortcut connection“，也是文中提到identity mapping，这张图也诠释了ResNet的真谛，当然大家可以放心，真正在使用的ResNet模块并不是这么单一，文章中就提出了两种方式：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731144828635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzYwNzY3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 这两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图），一般称整个结构为一个“building block” 。其中右图又称为“bottleneck design”，目的就是为了降低参数的数目，实际中，考虑计算的成本，对残差块做了计算优化，即将两个3x3的卷积层替换为1x1 + 3x3 + 1x1，如右图所示。新结构中的中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量。第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。<br> 对于常规ResNet，可以用于34层或者更少的网络中，对于Bottleneck Design的ResNet通常用于更深的如101这样的网络中，目的是减少计算和参数量。</p> 
  <h3><a id="13ResNet50ResNet101_25"></a>1.3、ResNet50和ResNet101简单讲解</h3> 
  <p>这里把ResNet50和ResNet101特别提出，主要因为它们的使用率很高，所以需要做特别的说明。给出了它们具体的结构：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731144927857.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzYwNzY3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 上表是Resnet不同的结构，上表一共提出了5中深度的ResNet，分别是18，34，50，101和152，首先看表的最左侧，我们发现所有的网络都分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x，之后的其他论文也会专门用这个称呼指代ResNet50或者101的每部分。 例如：101-layer那列，101-layer指的是101层网络，首先有个输入7x7x64的卷积，然后经过3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，最后有个fc层(用于分类)，所以1 + 99 + 1 = 101层，确实有101层网络； 注：101层网络仅仅指卷积或者全连接层，而激活层或者Pooling层并没有计算在内；我们关注50-layer和101-layer这两列，可以发现，它们唯一的不同在于conv4_x，ResNet50有6个block，而ResNet101有23个block，两者之间差了17个block，也就是17 x 3 = 51层。</p> 
  <h2><a id="_30"></a>！！！写博客不容易，请君给个赞在离开！！！！</h2> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
