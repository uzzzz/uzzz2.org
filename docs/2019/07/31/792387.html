<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>聚类总结（二）聚类性能评估、肘部法则、轮廓系数 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="聚类总结（二）聚类性能评估、肘部法则、轮廓系数" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 一、聚类K的选择规则 1.1 肘部法则–Elbow Method 1.2 轮廓系数–Silhouette Coefficient 二、聚类性能评估 2.1 外部评估（external evaluation） 2.1.1 Adjusted Rand Index(兰德指数) 2.1.2 Entropy 熵 2.1.3 purity（纯度） 2.1.4 Accuracy（AC） 2.2 内部评估（internal evaluation） 2.2.1 Silhouette coefficient（轮廓系数） 2.2.2 Calinski-Harabaz（CH） 2.3 其它内部评估方法（others） 2.3.1 Davies-Bouldin Index（DBI） 2.3.2 Dunn Index（DI） 有趣的事，Python永远不会缺席 培训说明 聚类（一） https://blog.csdn.net/u010986753/article/details/97821890 聚类（二） https://blog.csdn.net/u010986753/article/details/97885955 一、聚类K的选择规则 1.1 肘部法则–Elbow Method   我们知道k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散。 畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点。 基于这个指标，我们可以重复训练多个k-means模型，选取不同的k值，来得到相对合适的聚类类别（簇内误方差(SSE)）   如上图所示，在k=3时，畸变程度得到大幅改善，可以考虑选取k=3作为聚类数量，附简单代码： from sklearn.cluster import KMeans model = KMeans(n_clusters=k) model.fit(vector_points) md = model.inertia_ / vector_points.shape[0] 1.2 轮廓系数–Silhouette Coefficient   对于一个聚类任务，我们希望得到的类别簇中，簇内尽量紧密，簇间尽量远离，轮廓系数便是类的密集与分散程度的评价指标，公式表达如下                   s=(b−a)/max(a,b) a簇样本到彼此间距离的均值 b代表样本到除自身所在簇外的最近簇的样本的均值 s取值在[-1, 1]之间。 如果s接近1，代表样本所在簇合理，若s接近-1代表s更应该分到其他簇中。 同样，利用上述指标，训练多个模型，对比选取合适的聚类类别：   图， 当k=3时，轮廓系数最大，代表此时聚类的效果相对合理，简单代码如下： from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score model = KMeans(n_clusters=k) model.fit(vector_points) s = silhouette_score(vector_points, model.labels_) 二、聚类性能评估   说到聚类性能比较好，就是说同一簇的样本尽可能的相似，不同簇的样本尽可能不同，即是说聚类结果簇内相似度（intra-cluster similarity）高，而簇间相似度（inter-cluster similarity）低。   有监督的分类算法的评价指标通常是accuracy, precision, recall, etc；由于聚类算法是无监督的学习算法，评价指标则没有那么简单了。因为聚类算法得到的类别实际上不能说明任何问题，除非这些类别的分布和样本的真实类别分布相似，或者聚类的结果满足某种假设，即同一类别中样本间的相似性高于不同类别间样本的相似性。聚类模型的评价指标如下： 2.1 外部评估（external evaluation）   将结果与某个“参考模型”（reference model）进行比较。 2.1.1 Adjusted Rand Index(兰德指数) ARI的优点: 随机均匀的标签分布的ARI值接近0，这点与raw Rand Index和 V-measure指标不同; ARI值的范围是[-1,1]，负的结果都是较差的，说明标签是独立分布的，相似分布；ARI结果是正的，1是最佳结果，说明两种标签的分布完全一致; 不用对聚类结果做任何假设，可以用来比较任意聚类算法的聚类结果间的相似性。 ARI的缺点：   ARI指标需要事先知道样本的真实标签，这和有监督学习的先决条件是一样的。然而ARI也可以作为一个通用的指标，用来评估不同的聚类模型的性能。 数学公式：   如果C是真实类别，K是聚类结果，我们定义a和b分别是： a: 在C和K中都是同一类别的样本对数 b: 在C和K中都是不同类别的样本对数 C 2 n C^n_2 C2n​samples 是样本所有的可能组合对. raw Rand Index 的公式如下 2.1.2 Entropy 熵   对于一个聚类i，首先计算。指的是聚类 i 中的成员（member）属于类（class）j 的概率，。其中是在聚类 i 中所有成员的个数，是聚类 i 中的成员属于类 j 的个数。每个聚类的entropy可以表示为，其中L是类（class）的个数。整个聚类划分的entropy为，其中K是聚类（cluster）的数目，m是整个聚类划分所涉及到的成员个数。 2.1.3 purity（纯度）   使用上述Entropy中的定义，我们将聚类 i 的purity定义为。整个聚类划分的purity为，其中K是聚类（cluster）的数目，m是整个聚类划分所涉及到的成员个数。 def purity(cluster, labels, k, label_set): p = np.zeros((k, len(label_set))) purity = 0 for i in range(len(cluster)): p[int(cluster[i]), label_set.index(labels[i])] += 1 purity = sum(np.max(p, axis=1))/len(labels) return purity 2.1.4 Accuracy（AC）   Accuracy, （Accuracy 里可以包含了precision, recall, f-measure.），AC是目前最流行的聚类评价指标。在很多文献里面，都将AC作为聚类结果的评价指标。   map(pi) 是一个排列映射函数，将聚类得到的标签映射到与之等价的真实标签，聚类标签与真实标签之间是1-1映射(不一定是满的)。 CA计算 聚类正确的百分比 CA越大证明聚类效果越好 def f_score(cluster, labels, label_set): TP, TN, FP, FN = 0, 0, 0, 0 n = len(labels) # a lookup table for i in range(n): if i not in cluster: continue for j in range(i + 1, n): if j not in cluster: continue same_label = (labels[i] == labels[j]) same_cluster = (cluster[i] == cluster[j]) if same_cluster: if same_label: TP += 1 else: FP += 1 elif same_label: FN += 1 else: TN += 1 precision = TP / (TP + FP) recall = TP / (TP + FN) fscore = 2 * precision * recall / (precision + recall) return fscore, precision, recall, TP + FP + FN + TN 在二分类中： TP(true positive):分类正确，把原本属于正类的样本分成正类。 TN(true negative):分类正确，把原本属于负类的样本分成负类。 FP(false positive):分类错误，把原本属于负类的错分成了正类。 FN(false negative):分类错误，把原本属于正类的错分成了负类。 2.2 内部评估（internal evaluation）   直接考虑聚类结果而不利用任何参考模型。 2.2.1 Silhouette coefficient（轮廓系数）   轮廓系数–Silhouette Coefficient 1.2 已讲。 2.2.2 Calinski-Harabaz（CH）   CH也适用于实际类别信息未知的情况，以下以K-means为例，给定聚类数目K 2.3 其它内部评估方法（others） 2.3.1 Davies-Bouldin Index（DBI） σi：本簇中到其它所有样本点的距离的平均； cici ：簇的中心； d(ci,cj)：样本间距。 DBI越小越好。 2.3.2 Dunn Index（DI） d(i,j) ：样本间距； d′(k) ：本簇内样本对间的最远距离 DI越大越好。 加油 有趣的事，Python永远不会缺席 欢迎关注小婷儿的博客 &nbsp;&nbsp;&nbsp;&nbsp;文章内容来源于小婷儿的学习笔记，部分整理自网络，若有侵权或不当之处还请谅解 &nbsp;&nbsp;&nbsp;&nbsp;如需转发，请注明出处：小婷儿的博客python&nbsp;&nbsp;&nbsp;&nbsp;https://www.cnblogs.com/xxtalhr/ 博客园 https://www.cnblogs.com/xxtalhr/ CSDN https://blog.csdn.net/u010986753 有问题请在博客下留言或加作者： &nbsp;&nbsp;&nbsp;&nbsp; 微信：tinghai87605025 联系我加微信群 &nbsp;&nbsp;&nbsp;&nbsp; QQ ：87605025 &nbsp;&nbsp;&nbsp;&nbsp; python QQ交流群：py_data 483766429 培训说明 OCP培训说明连接 https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA OCM培训说明连接 https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA &nbsp;&nbsp;&nbsp;&nbsp; 小婷儿的python正在成长中，其中还有很多不足之处，随着学习和工作的深入，会对以往的博客内容逐步改进和完善哒。重要的事多说几遍。。。。。。" />
<meta property="og:description" content="文章目录 一、聚类K的选择规则 1.1 肘部法则–Elbow Method 1.2 轮廓系数–Silhouette Coefficient 二、聚类性能评估 2.1 外部评估（external evaluation） 2.1.1 Adjusted Rand Index(兰德指数) 2.1.2 Entropy 熵 2.1.3 purity（纯度） 2.1.4 Accuracy（AC） 2.2 内部评估（internal evaluation） 2.2.1 Silhouette coefficient（轮廓系数） 2.2.2 Calinski-Harabaz（CH） 2.3 其它内部评估方法（others） 2.3.1 Davies-Bouldin Index（DBI） 2.3.2 Dunn Index（DI） 有趣的事，Python永远不会缺席 培训说明 聚类（一） https://blog.csdn.net/u010986753/article/details/97821890 聚类（二） https://blog.csdn.net/u010986753/article/details/97885955 一、聚类K的选择规则 1.1 肘部法则–Elbow Method   我们知道k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散。 畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点。 基于这个指标，我们可以重复训练多个k-means模型，选取不同的k值，来得到相对合适的聚类类别（簇内误方差(SSE)）   如上图所示，在k=3时，畸变程度得到大幅改善，可以考虑选取k=3作为聚类数量，附简单代码： from sklearn.cluster import KMeans model = KMeans(n_clusters=k) model.fit(vector_points) md = model.inertia_ / vector_points.shape[0] 1.2 轮廓系数–Silhouette Coefficient   对于一个聚类任务，我们希望得到的类别簇中，簇内尽量紧密，簇间尽量远离，轮廓系数便是类的密集与分散程度的评价指标，公式表达如下                   s=(b−a)/max(a,b) a簇样本到彼此间距离的均值 b代表样本到除自身所在簇外的最近簇的样本的均值 s取值在[-1, 1]之间。 如果s接近1，代表样本所在簇合理，若s接近-1代表s更应该分到其他簇中。 同样，利用上述指标，训练多个模型，对比选取合适的聚类类别：   图， 当k=3时，轮廓系数最大，代表此时聚类的效果相对合理，简单代码如下： from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score model = KMeans(n_clusters=k) model.fit(vector_points) s = silhouette_score(vector_points, model.labels_) 二、聚类性能评估   说到聚类性能比较好，就是说同一簇的样本尽可能的相似，不同簇的样本尽可能不同，即是说聚类结果簇内相似度（intra-cluster similarity）高，而簇间相似度（inter-cluster similarity）低。   有监督的分类算法的评价指标通常是accuracy, precision, recall, etc；由于聚类算法是无监督的学习算法，评价指标则没有那么简单了。因为聚类算法得到的类别实际上不能说明任何问题，除非这些类别的分布和样本的真实类别分布相似，或者聚类的结果满足某种假设，即同一类别中样本间的相似性高于不同类别间样本的相似性。聚类模型的评价指标如下： 2.1 外部评估（external evaluation）   将结果与某个“参考模型”（reference model）进行比较。 2.1.1 Adjusted Rand Index(兰德指数) ARI的优点: 随机均匀的标签分布的ARI值接近0，这点与raw Rand Index和 V-measure指标不同; ARI值的范围是[-1,1]，负的结果都是较差的，说明标签是独立分布的，相似分布；ARI结果是正的，1是最佳结果，说明两种标签的分布完全一致; 不用对聚类结果做任何假设，可以用来比较任意聚类算法的聚类结果间的相似性。 ARI的缺点：   ARI指标需要事先知道样本的真实标签，这和有监督学习的先决条件是一样的。然而ARI也可以作为一个通用的指标，用来评估不同的聚类模型的性能。 数学公式：   如果C是真实类别，K是聚类结果，我们定义a和b分别是： a: 在C和K中都是同一类别的样本对数 b: 在C和K中都是不同类别的样本对数 C 2 n C^n_2 C2n​samples 是样本所有的可能组合对. raw Rand Index 的公式如下 2.1.2 Entropy 熵   对于一个聚类i，首先计算。指的是聚类 i 中的成员（member）属于类（class）j 的概率，。其中是在聚类 i 中所有成员的个数，是聚类 i 中的成员属于类 j 的个数。每个聚类的entropy可以表示为，其中L是类（class）的个数。整个聚类划分的entropy为，其中K是聚类（cluster）的数目，m是整个聚类划分所涉及到的成员个数。 2.1.3 purity（纯度）   使用上述Entropy中的定义，我们将聚类 i 的purity定义为。整个聚类划分的purity为，其中K是聚类（cluster）的数目，m是整个聚类划分所涉及到的成员个数。 def purity(cluster, labels, k, label_set): p = np.zeros((k, len(label_set))) purity = 0 for i in range(len(cluster)): p[int(cluster[i]), label_set.index(labels[i])] += 1 purity = sum(np.max(p, axis=1))/len(labels) return purity 2.1.4 Accuracy（AC）   Accuracy, （Accuracy 里可以包含了precision, recall, f-measure.），AC是目前最流行的聚类评价指标。在很多文献里面，都将AC作为聚类结果的评价指标。   map(pi) 是一个排列映射函数，将聚类得到的标签映射到与之等价的真实标签，聚类标签与真实标签之间是1-1映射(不一定是满的)。 CA计算 聚类正确的百分比 CA越大证明聚类效果越好 def f_score(cluster, labels, label_set): TP, TN, FP, FN = 0, 0, 0, 0 n = len(labels) # a lookup table for i in range(n): if i not in cluster: continue for j in range(i + 1, n): if j not in cluster: continue same_label = (labels[i] == labels[j]) same_cluster = (cluster[i] == cluster[j]) if same_cluster: if same_label: TP += 1 else: FP += 1 elif same_label: FN += 1 else: TN += 1 precision = TP / (TP + FP) recall = TP / (TP + FN) fscore = 2 * precision * recall / (precision + recall) return fscore, precision, recall, TP + FP + FN + TN 在二分类中： TP(true positive):分类正确，把原本属于正类的样本分成正类。 TN(true negative):分类正确，把原本属于负类的样本分成负类。 FP(false positive):分类错误，把原本属于负类的错分成了正类。 FN(false negative):分类错误，把原本属于正类的错分成了负类。 2.2 内部评估（internal evaluation）   直接考虑聚类结果而不利用任何参考模型。 2.2.1 Silhouette coefficient（轮廓系数）   轮廓系数–Silhouette Coefficient 1.2 已讲。 2.2.2 Calinski-Harabaz（CH）   CH也适用于实际类别信息未知的情况，以下以K-means为例，给定聚类数目K 2.3 其它内部评估方法（others） 2.3.1 Davies-Bouldin Index（DBI） σi：本簇中到其它所有样本点的距离的平均； cici ：簇的中心； d(ci,cj)：样本间距。 DBI越小越好。 2.3.2 Dunn Index（DI） d(i,j) ：样本间距； d′(k) ：本簇内样本对间的最远距离 DI越大越好。 加油 有趣的事，Python永远不会缺席 欢迎关注小婷儿的博客 &nbsp;&nbsp;&nbsp;&nbsp;文章内容来源于小婷儿的学习笔记，部分整理自网络，若有侵权或不当之处还请谅解 &nbsp;&nbsp;&nbsp;&nbsp;如需转发，请注明出处：小婷儿的博客python&nbsp;&nbsp;&nbsp;&nbsp;https://www.cnblogs.com/xxtalhr/ 博客园 https://www.cnblogs.com/xxtalhr/ CSDN https://blog.csdn.net/u010986753 有问题请在博客下留言或加作者： &nbsp;&nbsp;&nbsp;&nbsp; 微信：tinghai87605025 联系我加微信群 &nbsp;&nbsp;&nbsp;&nbsp; QQ ：87605025 &nbsp;&nbsp;&nbsp;&nbsp; python QQ交流群：py_data 483766429 培训说明 OCP培训说明连接 https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA OCM培训说明连接 https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA &nbsp;&nbsp;&nbsp;&nbsp; 小婷儿的python正在成长中，其中还有很多不足之处，随着学习和工作的深入，会对以往的博客内容逐步改进和完善哒。重要的事多说几遍。。。。。。" />
<link rel="canonical" href="https://uzzz.org/2019/07/31/792387.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/31/792387.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-31T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 一、聚类K的选择规则 1.1 肘部法则–Elbow Method 1.2 轮廓系数–Silhouette Coefficient 二、聚类性能评估 2.1 外部评估（external evaluation） 2.1.1 Adjusted Rand Index(兰德指数) 2.1.2 Entropy 熵 2.1.3 purity（纯度） 2.1.4 Accuracy（AC） 2.2 内部评估（internal evaluation） 2.2.1 Silhouette coefficient（轮廓系数） 2.2.2 Calinski-Harabaz（CH） 2.3 其它内部评估方法（others） 2.3.1 Davies-Bouldin Index（DBI） 2.3.2 Dunn Index（DI） 有趣的事，Python永远不会缺席 培训说明 聚类（一） https://blog.csdn.net/u010986753/article/details/97821890 聚类（二） https://blog.csdn.net/u010986753/article/details/97885955 一、聚类K的选择规则 1.1 肘部法则–Elbow Method   我们知道k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散。 畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点。 基于这个指标，我们可以重复训练多个k-means模型，选取不同的k值，来得到相对合适的聚类类别（簇内误方差(SSE)）   如上图所示，在k=3时，畸变程度得到大幅改善，可以考虑选取k=3作为聚类数量，附简单代码： from sklearn.cluster import KMeans model = KMeans(n_clusters=k) model.fit(vector_points) md = model.inertia_ / vector_points.shape[0] 1.2 轮廓系数–Silhouette Coefficient   对于一个聚类任务，我们希望得到的类别簇中，簇内尽量紧密，簇间尽量远离，轮廓系数便是类的密集与分散程度的评价指标，公式表达如下                   s=(b−a)/max(a,b) a簇样本到彼此间距离的均值 b代表样本到除自身所在簇外的最近簇的样本的均值 s取值在[-1, 1]之间。 如果s接近1，代表样本所在簇合理，若s接近-1代表s更应该分到其他簇中。 同样，利用上述指标，训练多个模型，对比选取合适的聚类类别：   图， 当k=3时，轮廓系数最大，代表此时聚类的效果相对合理，简单代码如下： from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score model = KMeans(n_clusters=k) model.fit(vector_points) s = silhouette_score(vector_points, model.labels_) 二、聚类性能评估   说到聚类性能比较好，就是说同一簇的样本尽可能的相似，不同簇的样本尽可能不同，即是说聚类结果簇内相似度（intra-cluster similarity）高，而簇间相似度（inter-cluster similarity）低。   有监督的分类算法的评价指标通常是accuracy, precision, recall, etc；由于聚类算法是无监督的学习算法，评价指标则没有那么简单了。因为聚类算法得到的类别实际上不能说明任何问题，除非这些类别的分布和样本的真实类别分布相似，或者聚类的结果满足某种假设，即同一类别中样本间的相似性高于不同类别间样本的相似性。聚类模型的评价指标如下： 2.1 外部评估（external evaluation）   将结果与某个“参考模型”（reference model）进行比较。 2.1.1 Adjusted Rand Index(兰德指数) ARI的优点: 随机均匀的标签分布的ARI值接近0，这点与raw Rand Index和 V-measure指标不同; ARI值的范围是[-1,1]，负的结果都是较差的，说明标签是独立分布的，相似分布；ARI结果是正的，1是最佳结果，说明两种标签的分布完全一致; 不用对聚类结果做任何假设，可以用来比较任意聚类算法的聚类结果间的相似性。 ARI的缺点：   ARI指标需要事先知道样本的真实标签，这和有监督学习的先决条件是一样的。然而ARI也可以作为一个通用的指标，用来评估不同的聚类模型的性能。 数学公式：   如果C是真实类别，K是聚类结果，我们定义a和b分别是： a: 在C和K中都是同一类别的样本对数 b: 在C和K中都是不同类别的样本对数 C 2 n C^n_2 C2n​samples 是样本所有的可能组合对. raw Rand Index 的公式如下 2.1.2 Entropy 熵   对于一个聚类i，首先计算。指的是聚类 i 中的成员（member）属于类（class）j 的概率，。其中是在聚类 i 中所有成员的个数，是聚类 i 中的成员属于类 j 的个数。每个聚类的entropy可以表示为，其中L是类（class）的个数。整个聚类划分的entropy为，其中K是聚类（cluster）的数目，m是整个聚类划分所涉及到的成员个数。 2.1.3 purity（纯度）   使用上述Entropy中的定义，我们将聚类 i 的purity定义为。整个聚类划分的purity为，其中K是聚类（cluster）的数目，m是整个聚类划分所涉及到的成员个数。 def purity(cluster, labels, k, label_set): p = np.zeros((k, len(label_set))) purity = 0 for i in range(len(cluster)): p[int(cluster[i]), label_set.index(labels[i])] += 1 purity = sum(np.max(p, axis=1))/len(labels) return purity 2.1.4 Accuracy（AC）   Accuracy, （Accuracy 里可以包含了precision, recall, f-measure.），AC是目前最流行的聚类评价指标。在很多文献里面，都将AC作为聚类结果的评价指标。   map(pi) 是一个排列映射函数，将聚类得到的标签映射到与之等价的真实标签，聚类标签与真实标签之间是1-1映射(不一定是满的)。 CA计算 聚类正确的百分比 CA越大证明聚类效果越好 def f_score(cluster, labels, label_set): TP, TN, FP, FN = 0, 0, 0, 0 n = len(labels) # a lookup table for i in range(n): if i not in cluster: continue for j in range(i + 1, n): if j not in cluster: continue same_label = (labels[i] == labels[j]) same_cluster = (cluster[i] == cluster[j]) if same_cluster: if same_label: TP += 1 else: FP += 1 elif same_label: FN += 1 else: TN += 1 precision = TP / (TP + FP) recall = TP / (TP + FN) fscore = 2 * precision * recall / (precision + recall) return fscore, precision, recall, TP + FP + FN + TN 在二分类中： TP(true positive):分类正确，把原本属于正类的样本分成正类。 TN(true negative):分类正确，把原本属于负类的样本分成负类。 FP(false positive):分类错误，把原本属于负类的错分成了正类。 FN(false negative):分类错误，把原本属于正类的错分成了负类。 2.2 内部评估（internal evaluation）   直接考虑聚类结果而不利用任何参考模型。 2.2.1 Silhouette coefficient（轮廓系数）   轮廓系数–Silhouette Coefficient 1.2 已讲。 2.2.2 Calinski-Harabaz（CH）   CH也适用于实际类别信息未知的情况，以下以K-means为例，给定聚类数目K 2.3 其它内部评估方法（others） 2.3.1 Davies-Bouldin Index（DBI） σi：本簇中到其它所有样本点的距离的平均； cici ：簇的中心； d(ci,cj)：样本间距。 DBI越小越好。 2.3.2 Dunn Index（DI） d(i,j) ：样本间距； d′(k) ：本簇内样本对间的最远距离 DI越大越好。 加油 有趣的事，Python永远不会缺席 欢迎关注小婷儿的博客 &nbsp;&nbsp;&nbsp;&nbsp;文章内容来源于小婷儿的学习笔记，部分整理自网络，若有侵权或不当之处还请谅解 &nbsp;&nbsp;&nbsp;&nbsp;如需转发，请注明出处：小婷儿的博客python&nbsp;&nbsp;&nbsp;&nbsp;https://www.cnblogs.com/xxtalhr/ 博客园 https://www.cnblogs.com/xxtalhr/ CSDN https://blog.csdn.net/u010986753 有问题请在博客下留言或加作者： &nbsp;&nbsp;&nbsp;&nbsp; 微信：tinghai87605025 联系我加微信群 &nbsp;&nbsp;&nbsp;&nbsp; QQ ：87605025 &nbsp;&nbsp;&nbsp;&nbsp; python QQ交流群：py_data 483766429 培训说明 OCP培训说明连接 https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA OCM培训说明连接 https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA &nbsp;&nbsp;&nbsp;&nbsp; 小婷儿的python正在成长中，其中还有很多不足之处，随着学习和工作的深入，会对以往的博客内容逐步改进和完善哒。重要的事多说几遍。。。。。。","@type":"BlogPosting","url":"https://uzzz.org/2019/07/31/792387.html","headline":"聚类总结（二）聚类性能评估、肘部法则、轮廓系数","dateModified":"2019-07-31T00:00:00+08:00","datePublished":"2019-07-31T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/31/792387.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>聚类总结（二）聚类性能评估、肘部法则、轮廓系数</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <li><a href="#K_3" rel="nofollow" data-token="e38a70745d901cd886f82f544af6f3a9">一、聚类K的选择规则</a></li>
    <ul>
     <li><a href="#11_Elbow_Method_4" rel="nofollow" data-token="ad3bdec541eda6675b572b68ac6cedfd">1.1 肘部法则–Elbow Method</a></li>
     <li><a href="#12_Silhouette_Coefficient_16" rel="nofollow" data-token="01639aae8e5da1f8a080fd5c72ba9e54">1.2 轮廓系数–Silhouette Coefficient</a></li>
    </ul>
    <li><a href="#_34" rel="nofollow" data-token="dce845a8c1f1e4fa3948cea6226513ce">二、聚类性能评估</a></li>
    <ul>
     <li><a href="#21_external_evaluation_38" rel="nofollow" data-token="804a57d6385cecde8d37fec957447a7c">2.1 外部评估（external evaluation）</a></li>
     <ul>
      <li><a href="#211_Adjusted_Rand_Index_40" rel="nofollow" data-token="fae58471bd946047cbcde139d634960a">2.1.1 Adjusted Rand Index(兰德指数)</a></li>
      <li><a href="#212_Entropy__61" rel="nofollow" data-token="74ae6235ab34c0a71e1087012a58f7f0">2.1.2 Entropy 熵</a></li>
      <li><a href="#213_purity_67" rel="nofollow" data-token="a9a5b69fc838868cadf9fef59ab47c1d">2.1.3 purity（纯度）</a></li>
      <li><a href="#214_AccuracyAC_85" rel="nofollow" data-token="627351d8f7d6ef8194e46f2bf4ca677e">2.1.4 Accuracy（AC）</a></li>
     </ul>
     <li><a href="#22_internal_evaluation_131" rel="nofollow" data-token="36189bebe5a4145bd0a282c98599c099">2.2 内部评估（internal evaluation）</a></li>
     <ul>
      <li><a href="#221_Silhouette_coefficient_133" rel="nofollow" data-token="eecc8fc573ea48c56b5e010a90448881">2.2.1 Silhouette coefficient（轮廓系数）</a></li>
      <li><a href="#222_CalinskiHarabazCH_135" rel="nofollow" data-token="336b50280528e96353eeba3e1b1d5b7e">2.2.2 Calinski-Harabaz（CH）</a></li>
     </ul>
     <li><a href="#23_others_140" rel="nofollow" data-token="44de373d3a3d74b8d089451a031aa452">2.3 其它内部评估方法（others）</a></li>
     <ul>
      <li><a href="#231_DaviesBouldin_IndexDBI_141" rel="nofollow" data-token="47ab34439ab9f3da9d75cdfeb2803800">2.3.1 Davies-Bouldin Index（DBI）</a></li>
      <li><a href="#232_Dunn_IndexDI_150" rel="nofollow" data-token="57cf5be51aeb4572827e6dfa33f4b29e">2.3.2 Dunn Index（DI）</a></li>
     </ul>
    </ul>
    <li><a href="#Python_165" rel="nofollow" data-token="4ee3ad1f1ade38535e030d0a934a9afa">有趣的事，Python永远不会缺席</a></li>
    <li><a href="#strongstrongbr_179" rel="nofollow"><strong>培训说明</strong><br></a></li>
   </ul>
  </div>
  <br> 聚类（一）
  <a href="https://blog.csdn.net/u010986753/article/details/97821890" rel="nofollow" data-token="60c205cf643f5981b3b83115a10ad6da">https://blog.csdn.net/u010986753/article/details/97821890</a>
  <br> 聚类（二）
  <a href="https://blog.csdn.net/u010986753/article/details/97885955" rel="nofollow" data-token="7bd706f33483c3696cd7844feef61509">https://blog.csdn.net/u010986753/article/details/97885955</a>
  <p></p> 
  <h1><a id="K_3"></a>一、聚类K的选择规则</h1> 
  <h2><a id="11_Elbow_Method_4"></a>1.1 肘部法则–Elbow Method</h2> 
  <p>  我们知道k-means是以最小化样本与质点平方误差作为目标函数，将每个簇的质点与簇内样本点的平方距离误差和称为畸变程度(distortions)，那么，对于一个簇，它的畸变程度越低，代表簇内成员越紧密，畸变程度越高，代表簇内结构越松散。 畸变程度会随着类别的增加而降低，但对于有一定区分度的数据，在达到某个临界点时畸变程度会得到极大改善，之后缓慢下降，这个临界点就可以考虑为聚类性能较好的点。 基于这个指标，我们可以重复训练多个k-means模型，选取不同的k值，来得到相对合适的聚类类别（簇内误方差(SSE)）<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731093143223.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA5ODY3NTM=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>   如上图所示，在k=3时，畸变程度得到大幅改善，可以考虑选取k=3作为聚类数量，附简单代码：</p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cluster <span class="token keyword">import</span> KMeans
model <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span>k<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>vector_points<span class="token punctuation">)</span>
md <span class="token operator">=</span> model<span class="token punctuation">.</span>inertia_ <span class="token operator">/</span> vector_points<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
</code></pre> 
  <h2><a id="12_Silhouette_Coefficient_16"></a>1.2 轮廓系数–Silhouette Coefficient</h2> 
  <p>  对于一个聚类任务，我们希望得到的类别簇中，簇内尽量紧密，簇间尽量远离，轮廓系数便是类的密集与分散程度的评价指标，公式表达如下<br>                   <strong>s=(b−a)/max(a,b)</strong></p> 
  <ul> 
   <li>a簇样本到彼此间距离的均值</li> 
   <li>b代表样本到除自身所在簇外的最近簇的样本的均值</li> 
   <li>s取值在[-1, 1]之间。</li> 
   <li>如果s接近1，代表样本所在簇合理，若s接近-1代表s更应该分到其他簇中。 同样，利用上述指标，训练多个模型，对比选取合适的聚类类别：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731093330158.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA5ODY3NTM=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>   图， 当k=3时，轮廓系数最大，代表此时聚类的效果相对合理，简单代码如下：</li> 
  </ul> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cluster <span class="token keyword">import</span> KMeans
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> silhouette_score
model <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span>k<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>vector_points<span class="token punctuation">)</span>
s <span class="token operator">=</span> silhouette_score<span class="token punctuation">(</span>vector_points<span class="token punctuation">,</span> model<span class="token punctuation">.</span>labels_<span class="token punctuation">)</span>
 
</code></pre> 
  <h1><a id="_34"></a>二、聚类性能评估</h1> 
  <p>  说到聚类性能比较好，就是说同一簇的样本尽可能的相似，不同簇的样本尽可能不同，即是说聚类结果<strong>簇内相似度</strong>（intra-cluster similarity）<strong>高</strong>，而<strong>簇间相似度</strong>（inter-cluster similarity）<strong>低</strong>。<br>   有监督的分类算法的评价指标通常是accuracy, precision, recall, etc；由于聚类算法是无监督的学习算法，评价指标则没有那么简单了。因为聚类算法得到的类别实际上不能说明任何问题，除非这些类别的分布和样本的真实类别分布相似，或者聚类的结果满足某种假设，即同一类别中样本间的相似性高于不同类别间样本的相似性。聚类模型的评价指标如下：</p> 
  <h2><a id="21_external_evaluation_38"></a>2.1 外部评估（external evaluation）</h2> 
  <p>  将结果与某个“参考模型”（reference model）进行比较。</p> 
  <h3><a id="211_Adjusted_Rand_Index_40"></a>2.1.1 Adjusted Rand Index(兰德指数)</h3> 
  <p><strong>ARI的优点:</strong></p> 
  <ul> 
   <li>随机均匀的标签分布的ARI值接近0，这点与raw Rand Index和 V-measure指标不同;</li> 
   <li>ARI值的范围是[-1,1]，负的结果都是较差的，说明标签是独立分布的，相似分布；ARI结果是正的，1是最佳结果，说明两种标签的分布完全一致;</li> 
   <li>不用对聚类结果做任何假设，可以用来比较任意聚类算法的聚类结果间的相似性。<br> <strong>ARI的缺点：</strong><br>   ARI指标需要<strong>事先知道样本的真实标签</strong>，这和有监督学习的先决条件是一样的。然而ARI也可以作为一个通用的指标，用来评估不同的聚类模型的性能。</li> 
  </ul> 
  <p><strong>数学公式：</strong><br>   如果C是真实类别，K是聚类结果，我们定义a和b分别是：</p> 
  <ul> 
   <li> <p>a: 在C和K中都是同一类别的样本对数</p> </li> 
   <li> <p>b: 在C和K中都是不同类别的样本对数</p> </li> 
   <li> <p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
        <math>
         <semantics>
          <mrow>
           <msubsup>
            <mi>
             C
            </mi>
            <mn>
             2
            </mn>
            <mi>
             n
            </mi>
           </msubsup>
          </mrow>
          <annotation encoding="application/x-tex">
           C^n_2
          </annotation>
         </semantics>
        </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.931438em; vertical-align: -0.248108em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -2.45189em; margin-left: -0.07153em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.248108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>samples 是样本所有的可能组合对.</p> </li> 
  </ul> 
  <p><strong>raw Rand Index 的公式如下</strong></p> 
  <div align="center"> 
   <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731101410266.png"> 
   <div align="center"> 
    <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019073110194049.png"> 
   </div>
  </div>
  <h3><a id="212_Entropy__61"></a>2.1.2 Entropy 熵</h3> 
  <p>  对于一个聚类i，首先计算。指的是聚类 i 中的成员（member）属于类（class）j 的概率，。其中是在聚类 i 中所有成员的个数，是聚类 i 中的成员属于类 j 的个数。每个聚类的entropy可以表示为，其中L是类（class）的个数。整个聚类划分的entropy为，其中K是聚类（cluster）的数目，m是整个聚类划分所涉及到的成员个数。</p> 
  <div align="center"> 
   <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731105829583.png"> 
  </div>
  <h3><a id="213_purity_67"></a>2.1.3 purity（纯度）</h3> 
  <p>  使用上述Entropy中的定义，我们将聚类 i 的purity定义为。整个聚类划分的purity为，其中K是聚类（cluster）的数目，m是整个聚类划分所涉及到的成员个数。</p> 
  <div align="center"> 
   <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731110003813.png"> 
  </div>
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">purity</span><span class="token punctuation">(</span>cluster<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> k<span class="token punctuation">,</span> label_set<span class="token punctuation">)</span><span class="token punctuation">:</span>
    p <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>label_set<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    purity <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>cluster<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        p<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>cluster<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label_set<span class="token punctuation">.</span>index<span class="token punctuation">(</span>labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>

    purity <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>p<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>labels<span class="token punctuation">)</span>

    <span class="token keyword">return</span> purity

</code></pre> 
  <h3><a id="214_AccuracyAC_85"></a>2.1.4 Accuracy（AC）</h3> 
  <p>  Accuracy, （Accuracy 里可以包含了precision, recall, f-measure.），AC是目前最流行的聚类评价指标。在很多文献里面，都将AC作为聚类结果的评价指标。</p> 
  <div align="center"> 
   <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731110529263.png"> 
  </div>
  <p>  map(pi) 是一个排列映射函数，将聚类得到的标签映射到与之等价的真实标签，聚类标签与真实标签之间是1-1映射(不一定是满的)。</p> 
  <div align="center"> 
   <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731110740358.png"> 
  </div>
  <ul> 
   <li>CA计算 聚类正确的百分比</li> 
   <li>CA越大证明聚类效果越好</li> 
  </ul> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">f_score</span><span class="token punctuation">(</span>cluster<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> label_set<span class="token punctuation">)</span><span class="token punctuation">:</span>
    TP<span class="token punctuation">,</span> TN<span class="token punctuation">,</span> FP<span class="token punctuation">,</span> FN <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
    n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>labels<span class="token punctuation">)</span>
    <span class="token comment"># a lookup table</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> i <span class="token operator">not</span> <span class="token keyword">in</span> cluster<span class="token punctuation">:</span>
            <span class="token keyword">continue</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> j <span class="token operator">not</span> <span class="token keyword">in</span> cluster<span class="token punctuation">:</span>
                <span class="token keyword">continue</span>
            same_label <span class="token operator">=</span> <span class="token punctuation">(</span>labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>
            same_cluster <span class="token operator">=</span> <span class="token punctuation">(</span>cluster<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> cluster<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> same_cluster<span class="token punctuation">:</span>
                <span class="token keyword">if</span> same_label<span class="token punctuation">:</span>
                    TP <span class="token operator">+=</span> <span class="token number">1</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    FP <span class="token operator">+=</span> <span class="token number">1</span>
            <span class="token keyword">elif</span> same_label<span class="token punctuation">:</span>
                FN <span class="token operator">+=</span> <span class="token number">1</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                TN <span class="token operator">+=</span> <span class="token number">1</span>
    precision <span class="token operator">=</span> TP <span class="token operator">/</span> <span class="token punctuation">(</span>TP <span class="token operator">+</span> FP<span class="token punctuation">)</span>
    recall <span class="token operator">=</span> TP <span class="token operator">/</span> <span class="token punctuation">(</span>TP <span class="token operator">+</span> FN<span class="token punctuation">)</span>
    fscore <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> precision <span class="token operator">*</span> recall <span class="token operator">/</span> <span class="token punctuation">(</span>precision <span class="token operator">+</span> recall<span class="token punctuation">)</span>
    <span class="token keyword">return</span> fscore<span class="token punctuation">,</span> precision<span class="token punctuation">,</span> recall<span class="token punctuation">,</span> TP <span class="token operator">+</span> FP <span class="token operator">+</span> FN <span class="token operator">+</span> TN
</code></pre> 
  <p><strong>在二分类中：</strong></p> 
  <ul> 
   <li>TP(true positive):分类正确，把原本属于正类的样本分成正类。</li> 
   <li>TN(true negative):分类正确，把原本属于负类的样本分成负类。</li> 
   <li>FP(false positive):分类错误，把原本属于负类的错分成了正类。</li> 
   <li>FN(false negative):分类错误，把原本属于正类的错分成了负类。</li> 
  </ul> 
  <h2><a id="22_internal_evaluation_131"></a>2.2 内部评估（internal evaluation）</h2> 
  <p>  直接考虑聚类结果而不利用任何参考模型。</p> 
  <h3><a id="221_Silhouette_coefficient_133"></a>2.2.1 Silhouette coefficient（轮廓系数）</h3> 
  <p>  轮廓系数–Silhouette Coefficient 1.2 已讲。</p> 
  <h3><a id="222_CalinskiHarabazCH_135"></a>2.2.2 Calinski-Harabaz（CH）</h3> 
  <p>  CH也适用于实际类别信息未知的情况，以下以K-means为例，给定聚类数目K</p> 
  <div align="center"> 
   <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731104805413.png"> 
  </div>
  <h2><a id="23_others_140"></a>2.3 其它内部评估方法（others）</h2> 
  <h3><a id="231_DaviesBouldin_IndexDBI_141"></a>2.3.1 Davies-Bouldin Index（DBI）</h3> 
  <ul> 
   <li>σi：本簇中到其它所有样本点的距离的平均；</li> 
   <li>cici ：簇的中心；</li> 
   <li>d(ci,cj)：样本间距。</li> 
  </ul> 
  <div align="center"> 
   <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731104419101.png"> 
  </div>
  <p><strong>DBI越小越好。</strong></p> 
  <h3><a id="232_Dunn_IndexDI_150"></a>2.3.2 Dunn Index（DI）</h3> 
  <ul> 
   <li>d(i,j) ：样本间距；</li> 
   <li>d′(k) ：本簇内样本对间的最远距离</li> 
  </ul> 
  <div align="center"> 
   <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731104605969.png"> 
  </div>
  <p><strong>DI越大越好。</strong></p> 
  <pre><code class="prism language-python">加油
</code></pre> 
  <h1><a id="Python_165"></a>有趣的事，Python永远不会缺席</h1> 
  <p><strong>欢迎关注小婷儿的博客</strong><br><br> &nbsp;&nbsp;&nbsp;&nbsp;文章内容来源于小婷儿的学习笔记，部分整理自网络，若有侵权或不当之处还请谅解</p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;如需转发，请注明出处：小婷儿的博客python&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.cnblogs.com/xxtalhr/" rel="nofollow" data-token="d94eb1c1159960fbd6f06ab4f585e25c">https://www.cnblogs.com/xxtalhr/</a><br><br> <strong>博客园</strong> <a href="https://www.cnblogs.com/xxtalhr/" rel="nofollow" data-token="d94eb1c1159960fbd6f06ab4f585e25c">https://www.cnblogs.com/xxtalhr/</a><br><br> <strong>CSDN</strong> <a href="https://blog.csdn.net/u010986753" rel="nofollow" data-token="c4eb70158827de9419e004033949f780">https://blog.csdn.net/u010986753</a><br></p> 
  <p><strong>有问题请在博客下留言或加作者：</strong><br> &nbsp;&nbsp;&nbsp;&nbsp; 微信：tinghai87605025 联系我加微信群<br> &nbsp;&nbsp;&nbsp;&nbsp; QQ ：87605025<br> &nbsp;&nbsp;&nbsp;&nbsp; python QQ交流群：py_data 483766429</p> 
  <center>
   <img src="https://img2018.cnblogs.com/blog/1400528/201905/1400528-20190519213410020-442219723.png">
   <center> 
   </center>
   <h1><a id="strongstrongbr_179"></a><strong>培训说明</strong><br></h1> 
   <p><strong>OCP培训说明连接</strong> <a href="https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA" rel="nofollow" data-token="3609dec6b7ee5591e0a0ebd1966a6fee">https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA</a><br><br> <strong>OCM培训说明连接</strong> <a href="https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA" rel="nofollow" data-token="2d0153e05f4578015e24d26bed7ade64">https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA</a><br></p> 
   <p>&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://blog.csdn.net/u010986753" rel="nofollow" data-token="c4eb70158827de9419e004033949f780">小婷儿的python</a>正在成长中，其中还有很多不足之处，随着学习和工作的深入，会对以往的博客内容逐步改进和完善哒。重要的事多说几遍。。。。。。</p> 
  </center> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
