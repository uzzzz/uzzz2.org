<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Spark2.4.2源码编译集成hadoop-2.6.0-cdh5.7.0 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Spark2.4.2源码编译集成hadoop-2.6.0-cdh5.7.0" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 1、编译环境 2、解压并配置环境变量 3、配置mvn仓库地址 4、配置Spark源码下pom.xml文件 5、配置Spark源码下`make-distribution.sh` 文件 6、编译Spark 7、验证 8、配置Spark客户端 注意：因为我本地网络速度慢，不稳定，所以在阿里云上租了一台服务器编译的，编译完成后下载到了本地机器上。 最后配置Spark客户端，是在我虚拟机上进行的。 1、编译环境 参考官网上的步骤 打开older versions and other resources，找到对应的版本，我这里是2.4.2 首先创建以下目录 [root@hadoop001 ~]# ll total 24 drwxr-xr-x 5 root root 4096 Jul 30 15:47 app drwxr-xr-x 2 root root 4096 Jul 30 15:42 data drwxr-xr-x 2 root root 4096 Jul 30 15:42 lib drwxr-xr-x 7 root root 4096 Jul 30 16:27 maven_repo drwxr-xr-x 2 root root 4096 Jul 30 15:45 soft drwxr-xr-x 3 root root 4096 Jul 30 15:47 source 安装包 [root@hadoop001 ~]# cd soft/ [root@hadoop001 soft]# ll total 213916 -rw-r--r-- 1 root root 9136463 Jun 20 19:08 apache-maven-3.6.1-bin.tar.gz -rw-r--r-- 1 root root 173271626 Jul 10 19:41 jdk-8u45-linux-x64.gz -rw-r--r-- 1 root root 20467943 Jun 20 22:19 scala-2.12.8.tgz -rw-r--r-- 1 root root 16165557 Jul 30 07:32 spark-2.4.2.tgz spark下载地址 https://archive.apache.org/dist/spark/spark-2.4.2/ 以下Scala和Maven版本也可以，下载地址 https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz https://www.apache.org/dyn/closer.lua?action=download&amp;filename=/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz 2、解压并配置环境变量 [root@hadoop001 soft]# yum install git #安装git [root@hadoop001 soft]# tar -zxvf apache-maven-3.6.1-bin.tar.gz -C ../app [root@hadoop001 soft]# tar -zxvf jdk-8u45-linux-x64.gz -C ../app [root@hadoop001 soft]# tar -zxvf scala-2.12.8.tgz -C ../app [root@hadoop001 soft]# tar -zxvf spark-2.4.2.tgz -C ../source/ 配置环境变量 [root@hadoop001 soft]# cat /etc/profile #追加以下配置 export JAVA_HOME=/root/app/jdk1.8.0_45 export PATH=$JAVA_HOME/bin:$PATH export MAVEN_HOME=/root/app/apache-maven-3.6.1 export PATH=$MAVEN_HOME/bin:$PATH export MAVEN_OPTS=&quot;-Xmx2g -XX:ReservedCodeCacheSize=2048m&quot; #这个是内存大小，加快编译速度 export SCALA_HOME=/root/app/scala-2.12.8 export PATH=$SCALA_HOME/bin:$PATH [root@hadoop001 soft]# source /etc/profile 3、配置mvn仓库地址 [root@hadoop001 conf]# pwd /root/app/apache-maven-3.6.1/conf [root@hadoop001 conf]# vi settings.xml &lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: ${user.home}/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; #在这个位置添加 &lt;localRepository&gt;/root/maven_repo&lt;/localRepository&gt; 4、配置Spark源码下pom.xml文件 阿里云主机的配置 [root@hadoop001 spark-2.4.2]# pwd /root/source/spark-2.4.2 [root@hadoop001 spark-2.4.2]# vi pom.xml &lt;repositories&gt; #注释这段，记得调整下面这句话的位置，和注释符号 &lt;!-- This should be at top, it makes maven try the central repo first and then others and hence faster dep resolution &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Maven Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; --&gt; #添加这段配置 &lt;repository&gt; &lt;id&gt;maven-ali&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public//&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 如果是本地虚拟机 &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;name&gt;cloudera repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; 5、配置Spark源码下make-distribution.sh 文件 [root@hadoop001 dev]# pwd /root/source/spark-2.4.2/dev [root@hadoop001 dev]# vi make-distribution.sh #注释下面这段 #VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\ # | grep -v &quot;INFO&quot;\ # | grep -v &quot;WARNING&quot;\ # | tail -n 1) #SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\ # | grep -v &quot;INFO&quot;\ # | grep -v &quot;WARNING&quot;\ # | tail -n 1) #SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\ # | grep -v &quot;INFO&quot;\ # | grep -v &quot;WARNING&quot;\ # | tail -n 1) #SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\ # | grep -v &quot;INFO&quot;\ # | grep -v &quot;WARNING&quot;\ # | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\ # # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\ # # because we use &quot;set -o pipefail&quot; # echo -n) #添加以下配置 VERSION=2.4.2 #spark版本 SCALA_VERSION=2.12 #scala版本 SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0 #hadoop版本 SPARK_HIVE=1 #支持hive 6、编译Spark [root@hadoop001 spark-2.4.2]# pwd /root/source/spark-2.4.2 [root@hadoop001 spark-2.4.2]#./dev/make-distribution.sh \ --name cdh5.16.1 \ --tgz \ -Dhadoop.version=2.6.0-cdh5.16.1 \ -Phadoop-2.6 \ -Phive \ -Phive-thriftserver \ -Pyarn 然后开始漫长的等待~~ 7、验证 编译完成后，会生成spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz安装包，表示成功 [root@hadoop001 spark-2.4.2]# ll ... -rw-r--r-- 1 root root 214594144 Jul 30 17:00 spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz ... 然后看看其他，maven_repo仓库地址，会生成非常多的依赖文件 [root@hadoop001 maven_repo]# pwd /root/maven_repo [root@hadoop001 maven_repo]# ll total 184 drwxr-xr-x 3 root root 4096 Jul 30 16:37 antlr drwxr-xr-x 3 root root 4096 Jul 30 16:40 aopalliance drwxr-xr-x 4 root root 4096 Jul 30 16:51 asm drwxr-xr-x 3 root root 4096 Jul 30 16:37 avalon-framework drwxr-xr-x 3 root root 4096 Jul 30 16:37 backport-util-concurrent drwxr-xr-x 3 root root 4096 Jul 30 16:47 cglib drwxr-xr-x 3 root root 4096 Jul 30 16:28 classworlds drwxr-xr-x 26 root root 4096 Jul 30 16:53 com drwxr-xr-x 4 root root 4096 Jul 30 16:40 commons-beanutils drwxr-xr-x 3 root root 4096 Jul 30 16:37 commons-chain drwxr-xr-x 3 root root 4096 Jul 30 16:30 commons-cli drwxr-xr-x 3 root root 4096 Jul 30 16:28 commons-codec drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-collections drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-configuration drwxr-xr-x 3 root root 4096 Jul 30 16:48 commons-dbcp drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-digester drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-el drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-httpclient drwxr-xr-x 3 root root 4096 Jul 30 16:27 commons-io drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-lang drwxr-xr-x 4 root root 4096 Jul 30 16:37 commons-logging drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-net drwxr-xr-x 3 root root 4096 Jul 30 16:47 commons-pool drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-validator drwxr-xr-x 3 root root 4096 Jul 30 16:37 dom4j drwxr-xr-x 5 root root 4096 Jul 30 16:53 io drwxr-xr-x 3 root root 4096 Jul 30 16:53 it drwxr-xr-x 11 root root 4096 Jul 30 16:48 javax drwxr-xr-x 3 root root 4096 Jul 30 16:47 javolution drwxr-xr-x 3 root root 4096 Jul 30 16:58 jline drwxr-xr-x 3 root root 4096 Jul 30 16:53 joda-time drwxr-xr-x 3 root root 4096 Jul 30 16:27 junit drwxr-xr-x 4 root root 4096 Jul 30 16:47 log4j drwxr-xr-x 3 root root 4096 Jul 30 16:37 logkit drwxr-xr-x 3 root root 4096 Jul 30 16:53 mysql drwxr-xr-x 8 root root 4096 Jul 30 16:56 net drwxr-xr-x 46 root root 4096 Jul 30 16:56 org drwxr-xr-x 3 root root 4096 Jul 30 16:36 oro drwxr-xr-x 3 root root 4096 Jul 30 16:37 sslext drwxr-xr-x 3 root root 4096 Jul 30 16:47 stax drwxr-xr-x 5 root root 4096 Jul 30 16:40 tomcat drwxr-xr-x 4 root root 4096 Jul 30 16:47 xalan drwxr-xr-x 3 root root 4096 Jul 30 16:36 xerces drwxr-xr-x 3 root root 4096 Jul 30 16:36 xml-apis drwxr-xr-x 3 root root 4096 Jul 30 16:40 xmlenc drwxr-xr-x 3 root root 4096 Jul 30 16:38 xmlunit 8、配置Spark客户端 [root@vm01 ~]# su - hadoop [hadoop@vm01 software]$ tar -zxvf spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz -C ~/app/ [hadoop@vm01 software]$ vim ~/.bash_profile export SCALA_HOME=/home/hadoop/app/scala-2.12.8 export PATH=$SCALA_HOME/bin:$PATH export JAVA_HOME=/usr/java/jdk1.8.0_45 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JER_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JER_HOME/bin:$PATH export MAVEN_HOME=/home/hadoop/app/apache-maven-3.6.1 export PATH=$MAVEN_HOME/bin:$PATH export MAVEN_OPTS=&quot;-Xmx2g -XX:ReservedCodeCacheSize=512m&quot; export SPARK_HOME=/home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0 export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH [hadoop@vm01 ~]$ source ~/.bash_profile [hadoop@vm01 bin]$ pwd /home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0/bin [hadoop@vm01 bin]$ rm -fr *.cmd [hadoop@vm01 bin]$ ./spark-shell 19/07/30 22:12:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to &quot;WARN&quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Spark context Web UI available at http://vm01:4040 Spark context available as &#39;sc&#39; (master = local[*], app id = local-1564549938818). Spark session available as &#39;spark&#39;. Welcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ &#39;_/ /___/ .__/\_,_/_/ /_/\_\ version 2.4.2 /_/ Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45) Type in expressions to have them evaluated. Type :help for more information. scala&gt; scala&gt; :quit" />
<meta property="og:description" content="文章目录 1、编译环境 2、解压并配置环境变量 3、配置mvn仓库地址 4、配置Spark源码下pom.xml文件 5、配置Spark源码下`make-distribution.sh` 文件 6、编译Spark 7、验证 8、配置Spark客户端 注意：因为我本地网络速度慢，不稳定，所以在阿里云上租了一台服务器编译的，编译完成后下载到了本地机器上。 最后配置Spark客户端，是在我虚拟机上进行的。 1、编译环境 参考官网上的步骤 打开older versions and other resources，找到对应的版本，我这里是2.4.2 首先创建以下目录 [root@hadoop001 ~]# ll total 24 drwxr-xr-x 5 root root 4096 Jul 30 15:47 app drwxr-xr-x 2 root root 4096 Jul 30 15:42 data drwxr-xr-x 2 root root 4096 Jul 30 15:42 lib drwxr-xr-x 7 root root 4096 Jul 30 16:27 maven_repo drwxr-xr-x 2 root root 4096 Jul 30 15:45 soft drwxr-xr-x 3 root root 4096 Jul 30 15:47 source 安装包 [root@hadoop001 ~]# cd soft/ [root@hadoop001 soft]# ll total 213916 -rw-r--r-- 1 root root 9136463 Jun 20 19:08 apache-maven-3.6.1-bin.tar.gz -rw-r--r-- 1 root root 173271626 Jul 10 19:41 jdk-8u45-linux-x64.gz -rw-r--r-- 1 root root 20467943 Jun 20 22:19 scala-2.12.8.tgz -rw-r--r-- 1 root root 16165557 Jul 30 07:32 spark-2.4.2.tgz spark下载地址 https://archive.apache.org/dist/spark/spark-2.4.2/ 以下Scala和Maven版本也可以，下载地址 https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz https://www.apache.org/dyn/closer.lua?action=download&amp;filename=/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz 2、解压并配置环境变量 [root@hadoop001 soft]# yum install git #安装git [root@hadoop001 soft]# tar -zxvf apache-maven-3.6.1-bin.tar.gz -C ../app [root@hadoop001 soft]# tar -zxvf jdk-8u45-linux-x64.gz -C ../app [root@hadoop001 soft]# tar -zxvf scala-2.12.8.tgz -C ../app [root@hadoop001 soft]# tar -zxvf spark-2.4.2.tgz -C ../source/ 配置环境变量 [root@hadoop001 soft]# cat /etc/profile #追加以下配置 export JAVA_HOME=/root/app/jdk1.8.0_45 export PATH=$JAVA_HOME/bin:$PATH export MAVEN_HOME=/root/app/apache-maven-3.6.1 export PATH=$MAVEN_HOME/bin:$PATH export MAVEN_OPTS=&quot;-Xmx2g -XX:ReservedCodeCacheSize=2048m&quot; #这个是内存大小，加快编译速度 export SCALA_HOME=/root/app/scala-2.12.8 export PATH=$SCALA_HOME/bin:$PATH [root@hadoop001 soft]# source /etc/profile 3、配置mvn仓库地址 [root@hadoop001 conf]# pwd /root/app/apache-maven-3.6.1/conf [root@hadoop001 conf]# vi settings.xml &lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: ${user.home}/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; #在这个位置添加 &lt;localRepository&gt;/root/maven_repo&lt;/localRepository&gt; 4、配置Spark源码下pom.xml文件 阿里云主机的配置 [root@hadoop001 spark-2.4.2]# pwd /root/source/spark-2.4.2 [root@hadoop001 spark-2.4.2]# vi pom.xml &lt;repositories&gt; #注释这段，记得调整下面这句话的位置，和注释符号 &lt;!-- This should be at top, it makes maven try the central repo first and then others and hence faster dep resolution &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Maven Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; --&gt; #添加这段配置 &lt;repository&gt; &lt;id&gt;maven-ali&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public//&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 如果是本地虚拟机 &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;name&gt;cloudera repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; 5、配置Spark源码下make-distribution.sh 文件 [root@hadoop001 dev]# pwd /root/source/spark-2.4.2/dev [root@hadoop001 dev]# vi make-distribution.sh #注释下面这段 #VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\ # | grep -v &quot;INFO&quot;\ # | grep -v &quot;WARNING&quot;\ # | tail -n 1) #SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\ # | grep -v &quot;INFO&quot;\ # | grep -v &quot;WARNING&quot;\ # | tail -n 1) #SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\ # | grep -v &quot;INFO&quot;\ # | grep -v &quot;WARNING&quot;\ # | tail -n 1) #SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\ # | grep -v &quot;INFO&quot;\ # | grep -v &quot;WARNING&quot;\ # | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\ # # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\ # # because we use &quot;set -o pipefail&quot; # echo -n) #添加以下配置 VERSION=2.4.2 #spark版本 SCALA_VERSION=2.12 #scala版本 SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0 #hadoop版本 SPARK_HIVE=1 #支持hive 6、编译Spark [root@hadoop001 spark-2.4.2]# pwd /root/source/spark-2.4.2 [root@hadoop001 spark-2.4.2]#./dev/make-distribution.sh \ --name cdh5.16.1 \ --tgz \ -Dhadoop.version=2.6.0-cdh5.16.1 \ -Phadoop-2.6 \ -Phive \ -Phive-thriftserver \ -Pyarn 然后开始漫长的等待~~ 7、验证 编译完成后，会生成spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz安装包，表示成功 [root@hadoop001 spark-2.4.2]# ll ... -rw-r--r-- 1 root root 214594144 Jul 30 17:00 spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz ... 然后看看其他，maven_repo仓库地址，会生成非常多的依赖文件 [root@hadoop001 maven_repo]# pwd /root/maven_repo [root@hadoop001 maven_repo]# ll total 184 drwxr-xr-x 3 root root 4096 Jul 30 16:37 antlr drwxr-xr-x 3 root root 4096 Jul 30 16:40 aopalliance drwxr-xr-x 4 root root 4096 Jul 30 16:51 asm drwxr-xr-x 3 root root 4096 Jul 30 16:37 avalon-framework drwxr-xr-x 3 root root 4096 Jul 30 16:37 backport-util-concurrent drwxr-xr-x 3 root root 4096 Jul 30 16:47 cglib drwxr-xr-x 3 root root 4096 Jul 30 16:28 classworlds drwxr-xr-x 26 root root 4096 Jul 30 16:53 com drwxr-xr-x 4 root root 4096 Jul 30 16:40 commons-beanutils drwxr-xr-x 3 root root 4096 Jul 30 16:37 commons-chain drwxr-xr-x 3 root root 4096 Jul 30 16:30 commons-cli drwxr-xr-x 3 root root 4096 Jul 30 16:28 commons-codec drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-collections drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-configuration drwxr-xr-x 3 root root 4096 Jul 30 16:48 commons-dbcp drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-digester drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-el drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-httpclient drwxr-xr-x 3 root root 4096 Jul 30 16:27 commons-io drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-lang drwxr-xr-x 4 root root 4096 Jul 30 16:37 commons-logging drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-net drwxr-xr-x 3 root root 4096 Jul 30 16:47 commons-pool drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-validator drwxr-xr-x 3 root root 4096 Jul 30 16:37 dom4j drwxr-xr-x 5 root root 4096 Jul 30 16:53 io drwxr-xr-x 3 root root 4096 Jul 30 16:53 it drwxr-xr-x 11 root root 4096 Jul 30 16:48 javax drwxr-xr-x 3 root root 4096 Jul 30 16:47 javolution drwxr-xr-x 3 root root 4096 Jul 30 16:58 jline drwxr-xr-x 3 root root 4096 Jul 30 16:53 joda-time drwxr-xr-x 3 root root 4096 Jul 30 16:27 junit drwxr-xr-x 4 root root 4096 Jul 30 16:47 log4j drwxr-xr-x 3 root root 4096 Jul 30 16:37 logkit drwxr-xr-x 3 root root 4096 Jul 30 16:53 mysql drwxr-xr-x 8 root root 4096 Jul 30 16:56 net drwxr-xr-x 46 root root 4096 Jul 30 16:56 org drwxr-xr-x 3 root root 4096 Jul 30 16:36 oro drwxr-xr-x 3 root root 4096 Jul 30 16:37 sslext drwxr-xr-x 3 root root 4096 Jul 30 16:47 stax drwxr-xr-x 5 root root 4096 Jul 30 16:40 tomcat drwxr-xr-x 4 root root 4096 Jul 30 16:47 xalan drwxr-xr-x 3 root root 4096 Jul 30 16:36 xerces drwxr-xr-x 3 root root 4096 Jul 30 16:36 xml-apis drwxr-xr-x 3 root root 4096 Jul 30 16:40 xmlenc drwxr-xr-x 3 root root 4096 Jul 30 16:38 xmlunit 8、配置Spark客户端 [root@vm01 ~]# su - hadoop [hadoop@vm01 software]$ tar -zxvf spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz -C ~/app/ [hadoop@vm01 software]$ vim ~/.bash_profile export SCALA_HOME=/home/hadoop/app/scala-2.12.8 export PATH=$SCALA_HOME/bin:$PATH export JAVA_HOME=/usr/java/jdk1.8.0_45 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JER_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JER_HOME/bin:$PATH export MAVEN_HOME=/home/hadoop/app/apache-maven-3.6.1 export PATH=$MAVEN_HOME/bin:$PATH export MAVEN_OPTS=&quot;-Xmx2g -XX:ReservedCodeCacheSize=512m&quot; export SPARK_HOME=/home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0 export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH [hadoop@vm01 ~]$ source ~/.bash_profile [hadoop@vm01 bin]$ pwd /home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0/bin [hadoop@vm01 bin]$ rm -fr *.cmd [hadoop@vm01 bin]$ ./spark-shell 19/07/30 22:12:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to &quot;WARN&quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Spark context Web UI available at http://vm01:4040 Spark context available as &#39;sc&#39; (master = local[*], app id = local-1564549938818). Spark session available as &#39;spark&#39;. Welcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ &#39;_/ /___/ .__/\_,_/_/ /_/\_\ version 2.4.2 /_/ Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45) Type in expressions to have them evaluated. Type :help for more information. scala&gt; scala&gt; :quit" />
<link rel="canonical" href="https://uzzz.org/2019/07/31/792442.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/31/792442.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-31T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 1、编译环境 2、解压并配置环境变量 3、配置mvn仓库地址 4、配置Spark源码下pom.xml文件 5、配置Spark源码下`make-distribution.sh` 文件 6、编译Spark 7、验证 8、配置Spark客户端 注意：因为我本地网络速度慢，不稳定，所以在阿里云上租了一台服务器编译的，编译完成后下载到了本地机器上。 最后配置Spark客户端，是在我虚拟机上进行的。 1、编译环境 参考官网上的步骤 打开older versions and other resources，找到对应的版本，我这里是2.4.2 首先创建以下目录 [root@hadoop001 ~]# ll total 24 drwxr-xr-x 5 root root 4096 Jul 30 15:47 app drwxr-xr-x 2 root root 4096 Jul 30 15:42 data drwxr-xr-x 2 root root 4096 Jul 30 15:42 lib drwxr-xr-x 7 root root 4096 Jul 30 16:27 maven_repo drwxr-xr-x 2 root root 4096 Jul 30 15:45 soft drwxr-xr-x 3 root root 4096 Jul 30 15:47 source 安装包 [root@hadoop001 ~]# cd soft/ [root@hadoop001 soft]# ll total 213916 -rw-r--r-- 1 root root 9136463 Jun 20 19:08 apache-maven-3.6.1-bin.tar.gz -rw-r--r-- 1 root root 173271626 Jul 10 19:41 jdk-8u45-linux-x64.gz -rw-r--r-- 1 root root 20467943 Jun 20 22:19 scala-2.12.8.tgz -rw-r--r-- 1 root root 16165557 Jul 30 07:32 spark-2.4.2.tgz spark下载地址 https://archive.apache.org/dist/spark/spark-2.4.2/ 以下Scala和Maven版本也可以，下载地址 https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz https://www.apache.org/dyn/closer.lua?action=download&amp;filename=/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz 2、解压并配置环境变量 [root@hadoop001 soft]# yum install git #安装git [root@hadoop001 soft]# tar -zxvf apache-maven-3.6.1-bin.tar.gz -C ../app [root@hadoop001 soft]# tar -zxvf jdk-8u45-linux-x64.gz -C ../app [root@hadoop001 soft]# tar -zxvf scala-2.12.8.tgz -C ../app [root@hadoop001 soft]# tar -zxvf spark-2.4.2.tgz -C ../source/ 配置环境变量 [root@hadoop001 soft]# cat /etc/profile #追加以下配置 export JAVA_HOME=/root/app/jdk1.8.0_45 export PATH=$JAVA_HOME/bin:$PATH export MAVEN_HOME=/root/app/apache-maven-3.6.1 export PATH=$MAVEN_HOME/bin:$PATH export MAVEN_OPTS=&quot;-Xmx2g -XX:ReservedCodeCacheSize=2048m&quot; #这个是内存大小，加快编译速度 export SCALA_HOME=/root/app/scala-2.12.8 export PATH=$SCALA_HOME/bin:$PATH [root@hadoop001 soft]# source /etc/profile 3、配置mvn仓库地址 [root@hadoop001 conf]# pwd /root/app/apache-maven-3.6.1/conf [root@hadoop001 conf]# vi settings.xml &lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: ${user.home}/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; #在这个位置添加 &lt;localRepository&gt;/root/maven_repo&lt;/localRepository&gt; 4、配置Spark源码下pom.xml文件 阿里云主机的配置 [root@hadoop001 spark-2.4.2]# pwd /root/source/spark-2.4.2 [root@hadoop001 spark-2.4.2]# vi pom.xml &lt;repositories&gt; #注释这段，记得调整下面这句话的位置，和注释符号 &lt;!-- This should be at top, it makes maven try the central repo first and then others and hence faster dep resolution &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Maven Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; --&gt; #添加这段配置 &lt;repository&gt; &lt;id&gt;maven-ali&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public//&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 如果是本地虚拟机 &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;name&gt;cloudera repository&lt;/name&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; 5、配置Spark源码下make-distribution.sh 文件 [root@hadoop001 dev]# pwd /root/source/spark-2.4.2/dev [root@hadoop001 dev]# vi make-distribution.sh #注释下面这段 #VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\\ # | grep -v &quot;INFO&quot;\\ # | grep -v &quot;WARNING&quot;\\ # | tail -n 1) #SCALA_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\\ # | grep -v &quot;INFO&quot;\\ # | grep -v &quot;WARNING&quot;\\ # | tail -n 1) #SPARK_HADOOP_VERSION=$(&quot;$MVN&quot; help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\\ # | grep -v &quot;INFO&quot;\\ # | grep -v &quot;WARNING&quot;\\ # | tail -n 1) #SPARK_HIVE=$(&quot;$MVN&quot; help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\\ # | grep -v &quot;INFO&quot;\\ # | grep -v &quot;WARNING&quot;\\ # | fgrep --count &quot;&lt;id&gt;hive&lt;/id&gt;&quot;;\\ # # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\\ # # because we use &quot;set -o pipefail&quot; # echo -n) #添加以下配置 VERSION=2.4.2 #spark版本 SCALA_VERSION=2.12 #scala版本 SPARK_HADOOP_VERSION=2.6.0-cdh5.7.0 #hadoop版本 SPARK_HIVE=1 #支持hive 6、编译Spark [root@hadoop001 spark-2.4.2]# pwd /root/source/spark-2.4.2 [root@hadoop001 spark-2.4.2]#./dev/make-distribution.sh \\ --name cdh5.16.1 \\ --tgz \\ -Dhadoop.version=2.6.0-cdh5.16.1 \\ -Phadoop-2.6 \\ -Phive \\ -Phive-thriftserver \\ -Pyarn 然后开始漫长的等待~~ 7、验证 编译完成后，会生成spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz安装包，表示成功 [root@hadoop001 spark-2.4.2]# ll ... -rw-r--r-- 1 root root 214594144 Jul 30 17:00 spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz ... 然后看看其他，maven_repo仓库地址，会生成非常多的依赖文件 [root@hadoop001 maven_repo]# pwd /root/maven_repo [root@hadoop001 maven_repo]# ll total 184 drwxr-xr-x 3 root root 4096 Jul 30 16:37 antlr drwxr-xr-x 3 root root 4096 Jul 30 16:40 aopalliance drwxr-xr-x 4 root root 4096 Jul 30 16:51 asm drwxr-xr-x 3 root root 4096 Jul 30 16:37 avalon-framework drwxr-xr-x 3 root root 4096 Jul 30 16:37 backport-util-concurrent drwxr-xr-x 3 root root 4096 Jul 30 16:47 cglib drwxr-xr-x 3 root root 4096 Jul 30 16:28 classworlds drwxr-xr-x 26 root root 4096 Jul 30 16:53 com drwxr-xr-x 4 root root 4096 Jul 30 16:40 commons-beanutils drwxr-xr-x 3 root root 4096 Jul 30 16:37 commons-chain drwxr-xr-x 3 root root 4096 Jul 30 16:30 commons-cli drwxr-xr-x 3 root root 4096 Jul 30 16:28 commons-codec drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-collections drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-configuration drwxr-xr-x 3 root root 4096 Jul 30 16:48 commons-dbcp drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-digester drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-el drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-httpclient drwxr-xr-x 3 root root 4096 Jul 30 16:27 commons-io drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-lang drwxr-xr-x 4 root root 4096 Jul 30 16:37 commons-logging drwxr-xr-x 3 root root 4096 Jul 30 16:40 commons-net drwxr-xr-x 3 root root 4096 Jul 30 16:47 commons-pool drwxr-xr-x 3 root root 4096 Jul 30 16:36 commons-validator drwxr-xr-x 3 root root 4096 Jul 30 16:37 dom4j drwxr-xr-x 5 root root 4096 Jul 30 16:53 io drwxr-xr-x 3 root root 4096 Jul 30 16:53 it drwxr-xr-x 11 root root 4096 Jul 30 16:48 javax drwxr-xr-x 3 root root 4096 Jul 30 16:47 javolution drwxr-xr-x 3 root root 4096 Jul 30 16:58 jline drwxr-xr-x 3 root root 4096 Jul 30 16:53 joda-time drwxr-xr-x 3 root root 4096 Jul 30 16:27 junit drwxr-xr-x 4 root root 4096 Jul 30 16:47 log4j drwxr-xr-x 3 root root 4096 Jul 30 16:37 logkit drwxr-xr-x 3 root root 4096 Jul 30 16:53 mysql drwxr-xr-x 8 root root 4096 Jul 30 16:56 net drwxr-xr-x 46 root root 4096 Jul 30 16:56 org drwxr-xr-x 3 root root 4096 Jul 30 16:36 oro drwxr-xr-x 3 root root 4096 Jul 30 16:37 sslext drwxr-xr-x 3 root root 4096 Jul 30 16:47 stax drwxr-xr-x 5 root root 4096 Jul 30 16:40 tomcat drwxr-xr-x 4 root root 4096 Jul 30 16:47 xalan drwxr-xr-x 3 root root 4096 Jul 30 16:36 xerces drwxr-xr-x 3 root root 4096 Jul 30 16:36 xml-apis drwxr-xr-x 3 root root 4096 Jul 30 16:40 xmlenc drwxr-xr-x 3 root root 4096 Jul 30 16:38 xmlunit 8、配置Spark客户端 [root@vm01 ~]# su - hadoop [hadoop@vm01 software]$ tar -zxvf spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz -C ~/app/ [hadoop@vm01 software]$ vim ~/.bash_profile export SCALA_HOME=/home/hadoop/app/scala-2.12.8 export PATH=$SCALA_HOME/bin:$PATH export JAVA_HOME=/usr/java/jdk1.8.0_45 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JER_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JER_HOME/bin:$PATH export MAVEN_HOME=/home/hadoop/app/apache-maven-3.6.1 export PATH=$MAVEN_HOME/bin:$PATH export MAVEN_OPTS=&quot;-Xmx2g -XX:ReservedCodeCacheSize=512m&quot; export SPARK_HOME=/home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0 export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH [hadoop@vm01 ~]$ source ~/.bash_profile [hadoop@vm01 bin]$ pwd /home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0/bin [hadoop@vm01 bin]$ rm -fr *.cmd [hadoop@vm01 bin]$ ./spark-shell 19/07/30 22:12:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to &quot;WARN&quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Spark context Web UI available at http://vm01:4040 Spark context available as &#39;sc&#39; (master = local[*], app id = local-1564549938818). Spark session available as &#39;spark&#39;. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &#39;_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.4.2 /_/ Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45) Type in expressions to have them evaluated. Type :help for more information. scala&gt; scala&gt; :quit","@type":"BlogPosting","url":"https://uzzz.org/2019/07/31/792442.html","headline":"Spark2.4.2源码编译集成hadoop-2.6.0-cdh5.7.0","dateModified":"2019-07-31T00:00:00+08:00","datePublished":"2019-07-31T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/31/792442.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Spark2.4.2源码编译集成hadoop-2.6.0-cdh5.7.0</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <ul>
     <li><a href="#1_7" rel="nofollow" data-token="0652c165e2b534c71a81f7779559df18">1、编译环境</a></li>
     <li><a href="#2_40" rel="nofollow" data-token="3b8f895494e4246a83715f731a8ddf82">2、解压并配置环境变量</a></li>
     <li><a href="#3mvn_66" rel="nofollow" data-token="34ded2d7a0c2ae74651231c48fed41a5">3、配置mvn仓库地址</a></li>
     <li><a href="#4Sparkpomxml_81" rel="nofollow" data-token="fb5d4f7afdf9da8bddb180fb38bff998">4、配置Spark源码下pom.xml文件</a></li>
     <li><a href="#5Sparkmakedistributionsh__133" rel="nofollow" data-token="7ef2928f70842db42ea560105c08668f">5、配置Spark源码下`make-distribution.sh` 文件</a></li>
     <li><a href="#6Spark_167" rel="nofollow" data-token="fb017e5a25258474bc73631e04ea0017">6、编译Spark</a></li>
     <li><a href="#7_182" rel="nofollow" data-token="7d91b81f807552d7c899d2312d813f6a">7、验证</a></li>
     <li><a href="#8Spark_245" rel="nofollow" data-token="afc374eba5d3963a89951fd60d7866ed">8、配置Spark客户端</a></li>
    </ul>
   </ul>
  </div>
  <p></p> 
  <pre><code>注意：因为我本地网络速度慢，不稳定，所以在阿里云上租了一台服务器编译的，编译完成后下载到了本地机器上。
最后配置Spark客户端，是在我虚拟机上进行的。
</code></pre> 
  <h2><a id="1_7"></a>1、编译环境</h2> 
  <p>参考官网上的步骤<br> 打开older versions and other resources，找到对应的版本，我这里是2.4.2<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019073106421854.png" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731064355874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dyZWVucGx1bV94aWFvZmFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 首先创建以下目录</p> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 ~<span class="token punctuation">]</span><span class="token comment"># ll</span>
total 24
drwxr-xr-x 5 root root 4096 Jul 30 15:47 app
drwxr-xr-x 2 root root 4096 Jul 30 15:42 data
drwxr-xr-x 2 root root 4096 Jul 30 15:42 lib
drwxr-xr-x 7 root root 4096 Jul 30 16:27 maven_repo
drwxr-xr-x 2 root root 4096 Jul 30 15:45 soft
drwxr-xr-x 3 root root 4096 Jul 30 15:47 <span class="token function">source</span>
</code></pre> 
  <p>安装包</p> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 ~<span class="token punctuation">]</span><span class="token comment"># cd soft/</span>
<span class="token punctuation">[</span>root@hadoop001 soft<span class="token punctuation">]</span><span class="token comment"># ll</span>
total 213916
-rw-r--r-- 1 root root   9136463 Jun 20 19:08 apache-maven-3.6.1-bin.tar.gz
-rw-r--r-- 1 root root 173271626 Jul 10 19:41 jdk-8u45-linux-x64.gz
-rw-r--r-- 1 root root  20467943 Jun 20 22:19 scala-2.12.8.tgz
-rw-r--r-- 1 root root  16165557 Jul 30 07:32 spark-2.4.2.tgz
</code></pre> 
  <p>spark下载地址<br> <a href="https://archive.apache.org/dist/spark/spark-2.4.2/" rel="nofollow" data-token="77c76f3d0f213758fe48a5cf9174c259">https://archive.apache.org/dist/spark/spark-2.4.2/</a><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730163143413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dyZWVucGx1bV94aWFvZmFu,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 以下Scala和Maven版本也可以，下载地址<br> <a href="https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz" rel="nofollow" data-token="d27f5cb1977eff4988649d01af6f8389">https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz</a><br> <a href="https://www.apache.org/dyn/closer.lua?action=download&amp;filename=/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz" rel="nofollow" data-token="25ceac41ee196b61e7eea6744663525f">https://www.apache.org/dyn/closer.lua?action=download&amp;filename=/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz</a></p> 
  <h2><a id="2_40"></a>2、解压并配置环境变量</h2> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 soft<span class="token punctuation">]</span><span class="token comment"># yum install git #安装git</span>
<span class="token punctuation">[</span>root@hadoop001 soft<span class="token punctuation">]</span><span class="token comment"># tar -zxvf apache-maven-3.6.1-bin.tar.gz -C ../app</span>
<span class="token punctuation">[</span>root@hadoop001 soft<span class="token punctuation">]</span><span class="token comment"># tar -zxvf jdk-8u45-linux-x64.gz -C ../app</span>
<span class="token punctuation">[</span>root@hadoop001 soft<span class="token punctuation">]</span><span class="token comment"># tar -zxvf scala-2.12.8.tgz -C ../app</span>
<span class="token punctuation">[</span>root@hadoop001 soft<span class="token punctuation">]</span><span class="token comment"># tar -zxvf spark-2.4.2.tgz -C ../source/</span>
</code></pre> 
  <p>配置环境变量</p> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 soft<span class="token punctuation">]</span><span class="token comment"># cat /etc/profile</span>
<span class="token comment">#追加以下配置</span>
<span class="token function">export</span> JAVA_HOME<span class="token operator">=</span>/root/app/jdk1.8.0_45
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$JAVA_HOME</span>/bin:<span class="token variable">$PATH</span>

<span class="token function">export</span> MAVEN_HOME<span class="token operator">=</span>/root/app/apache-maven-3.6.1
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$MAVEN_HOME</span>/bin:<span class="token variable">$PATH</span>
<span class="token function">export</span> MAVEN_OPTS<span class="token operator">=</span><span class="token string">"-Xmx2g -XX:ReservedCodeCacheSize=2048m"</span>  <span class="token comment">#这个是内存大小，加快编译速度</span>

<span class="token function">export</span> SCALA_HOME<span class="token operator">=</span>/root/app/scala-2.12.8
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$SCALA_HOME</span>/bin:<span class="token variable">$PATH</span>

<span class="token punctuation">[</span>root@hadoop001 soft<span class="token punctuation">]</span><span class="token comment"># source /etc/profile</span>
</code></pre> 
  <h2><a id="3mvn_66"></a>3、配置mvn仓库地址</h2> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 conf<span class="token punctuation">]</span><span class="token comment"># pwd</span>
/root/app/apache-maven-3.6.1/conf
<span class="token punctuation">[</span>root@hadoop001 conf<span class="token punctuation">]</span><span class="token comment"># vi settings.xml </span>
 <span class="token operator">&lt;</span><span class="token operator">!</span>-- localRepository
   <span class="token operator">|</span> The path to the local repository maven will use to store artifacts.
   <span class="token operator">|</span>
   <span class="token operator">|</span> Default: <span class="token variable">${user.home}</span>/.m2/repository
  <span class="token operator">&lt;</span>localRepository<span class="token operator">&gt;</span>/path/to/local/repo<span class="token operator">&lt;</span>/localRepository<span class="token operator">&gt;</span>
  --<span class="token operator">&gt;</span>
  <span class="token comment">#在这个位置添加</span>
  <span class="token operator">&lt;</span>localRepository<span class="token operator">&gt;</span>/root/maven_repo<span class="token operator">&lt;</span>/localRepository<span class="token operator">&gt;</span>
</code></pre> 
  <h2><a id="4Sparkpomxml_81"></a>4、配置Spark源码下pom.xml文件</h2> 
  <p>阿里云主机的配置</p> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 spark-2.4.2<span class="token punctuation">]</span><span class="token comment"># pwd</span>
/root/source/spark-2.4.2
<span class="token punctuation">[</span>root@hadoop001 spark-2.4.2<span class="token punctuation">]</span><span class="token comment"># vi pom.xml </span>
 <span class="token operator">&lt;</span>repositories<span class="token operator">&gt;</span>
 	<span class="token comment">#注释这段，记得调整下面这句话的位置，和注释符号</span>
     <span class="token operator">&lt;</span><span class="token operator">!</span>-- This should be at top, it makes maven try the central repo first and <span class="token keyword">then</span> others and hence faster dep resolution
    <span class="token operator">&lt;</span>repository<span class="token operator">&gt;</span>
      <span class="token operator">&lt;</span>id<span class="token operator">&gt;</span>central<span class="token operator">&lt;</span>/id<span class="token operator">&gt;</span>
      <span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>Maven Repository<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
      <span class="token operator">&lt;</span>url<span class="token operator">&gt;</span>https://repo.maven.apache.org/maven2<span class="token operator">&lt;</span>/url<span class="token operator">&gt;</span>
      <span class="token operator">&lt;</span>releases<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>enabled<span class="token operator">&gt;</span>true<span class="token operator">&lt;</span>/enabled<span class="token operator">&gt;</span>
      <span class="token operator">&lt;</span>/releases<span class="token operator">&gt;</span>
      <span class="token operator">&lt;</span>snapshots<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>enabled<span class="token operator">&gt;</span>false<span class="token operator">&lt;</span>/enabled<span class="token operator">&gt;</span>
      <span class="token operator">&lt;</span>/snapshots<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>/repository<span class="token operator">&gt;</span>
     --<span class="token operator">&gt;</span>
     
   <span class="token comment">#添加这段配置</span>
    <span class="token operator">&lt;</span>repository<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>id<span class="token operator">&gt;</span>maven-ali<span class="token operator">&lt;</span>/id<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>url<span class="token operator">&gt;</span>http://maven.aliyun.com/nexus/content/groups/public//<span class="token operator">&lt;</span>/url<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>releases<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>enabled<span class="token operator">&gt;</span>true<span class="token operator">&lt;</span>/enabled<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>/releases<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>snapshots<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>enabled<span class="token operator">&gt;</span>true<span class="token operator">&lt;</span>/enabled<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>updatePolicy<span class="token operator">&gt;</span>always<span class="token operator">&lt;</span>/updatePolicy<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>checksumPolicy<span class="token operator">&gt;</span>fail<span class="token operator">&lt;</span>/checksumPolicy<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>/snapshots<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>/repository<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>repository<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>id<span class="token operator">&gt;</span>cloudera<span class="token operator">&lt;</span>/id<span class="token operator">&gt;</span>
        <span class="token operator">&lt;</span>url<span class="token operator">&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="token operator">&lt;</span>/url<span class="token operator">&gt;</span>
   <span class="token operator">&lt;</span>/repository<span class="token operator">&gt;</span>

  <span class="token operator">&lt;</span>/repositories<span class="token operator">&gt;</span>
</code></pre> 
  <p>如果是本地虚拟机</p> 
  <pre><code class="prism language-shell"><span class="token operator">&lt;</span>repository<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>id<span class="token operator">&gt;</span>cloudera<span class="token operator">&lt;</span>/id<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>name<span class="token operator">&gt;</span>cloudera repository<span class="token operator">&lt;</span>/name<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>url<span class="token operator">&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="token operator">&lt;</span>/url<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/repository<span class="token operator">&gt;</span>
</code></pre> 
  <h2><a id="5Sparkmakedistributionsh__133"></a>5、配置Spark源码下<code>make-distribution.sh</code> 文件</h2> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 dev<span class="token punctuation">]</span><span class="token comment"># pwd</span>
/root/source/spark-2.4.2/dev
<span class="token punctuation">[</span>root@hadoop001 dev<span class="token punctuation">]</span><span class="token comment"># vi make-distribution.sh </span>
<span class="token comment">#注释下面这段</span>
<span class="token comment">#VERSION=$("$MVN" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\</span>
<span class="token comment"># | grep -v "INFO"\</span>
<span class="token comment"># | grep -v "WARNING"\</span>
<span class="token comment"># | tail -n 1)</span>
<span class="token comment">#SCALA_VERSION=$("$MVN" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\</span>
<span class="token comment"># | grep -v "INFO"\</span>
<span class="token comment"># | grep -v "WARNING"\</span>
<span class="token comment"># | tail -n 1)</span>
<span class="token comment">#SPARK_HADOOP_VERSION=$("$MVN" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\</span>
<span class="token comment"># | grep -v "INFO"\</span>
<span class="token comment"># | grep -v "WARNING"\</span>
<span class="token comment"># | tail -n 1)</span>
<span class="token comment">#SPARK_HIVE=$("$MVN" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\</span>
<span class="token comment"># | grep -v "INFO"\</span>
<span class="token comment"># | grep -v "WARNING"\</span>
<span class="token comment"># | fgrep --count "&lt;id&gt;hive&lt;/id&gt;";\</span>
<span class="token comment"># # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\</span>
<span class="token comment"># # because we use "set -o pipefail"</span>
<span class="token comment"># echo -n)</span>

<span class="token comment">#添加以下配置</span>
VERSION<span class="token operator">=</span>2.4.2   <span class="token comment">#spark版本</span>
SCALA_VERSION<span class="token operator">=</span>2.12   <span class="token comment">#scala版本</span>
SPARK_HADOOP_VERSION<span class="token operator">=</span>2.6.0-cdh5.7.0   <span class="token comment">#hadoop版本</span>
SPARK_HIVE<span class="token operator">=</span>1  <span class="token comment">#支持hive</span>
</code></pre> 
  <h2><a id="6Spark_167"></a>6、编译Spark</h2> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 spark-2.4.2<span class="token punctuation">]</span><span class="token comment"># pwd</span>
/root/source/spark-2.4.2
<span class="token punctuation">[</span>root@hadoop001 spark-2.4.2<span class="token punctuation">]</span><span class="token comment">#./dev/make-distribution.sh \</span>
--name cdh5.16.1 \
--tgz \
-Dhadoop.version<span class="token operator">=</span>2.6.0-cdh5.16.1 \
-Phadoop-2.6 \
-Phive \
-Phive-thriftserver \
-Pyarn
</code></pre> 
  <p>然后开始漫长的等待~~</p> 
  <h2><a id="7_182"></a>7、验证</h2> 
  <p>编译完成后，会生成spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz安装包，表示成功</p> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 spark-2.4.2<span class="token punctuation">]</span><span class="token comment"># ll</span>
<span class="token punctuation">..</span>.
-rw-r--r--  1 root   root   214594144 Jul 30 17:00 spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz
<span class="token punctuation">..</span>.
</code></pre> 
  <p>然后看看其他，maven_repo仓库地址，会生成非常多的依赖文件</p> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@hadoop001 maven_repo<span class="token punctuation">]</span><span class="token comment"># pwd</span>
/root/maven_repo
<span class="token punctuation">[</span>root@hadoop001 maven_repo<span class="token punctuation">]</span><span class="token comment"># ll</span>
total 184
drwxr-xr-x  3 root root 4096 Jul 30 16:37 antlr
drwxr-xr-x  3 root root 4096 Jul 30 16:40 aopalliance
drwxr-xr-x  4 root root 4096 Jul 30 16:51 asm
drwxr-xr-x  3 root root 4096 Jul 30 16:37 avalon-framework
drwxr-xr-x  3 root root 4096 Jul 30 16:37 backport-util-concurrent
drwxr-xr-x  3 root root 4096 Jul 30 16:47 cglib
drwxr-xr-x  3 root root 4096 Jul 30 16:28 classworlds
drwxr-xr-x 26 root root 4096 Jul 30 16:53 com
drwxr-xr-x  4 root root 4096 Jul 30 16:40 commons-beanutils
drwxr-xr-x  3 root root 4096 Jul 30 16:37 commons-chain
drwxr-xr-x  3 root root 4096 Jul 30 16:30 commons-cli
drwxr-xr-x  3 root root 4096 Jul 30 16:28 commons-codec
drwxr-xr-x  3 root root 4096 Jul 30 16:36 commons-collections
drwxr-xr-x  3 root root 4096 Jul 30 16:40 commons-configuration
drwxr-xr-x  3 root root 4096 Jul 30 16:48 commons-dbcp
drwxr-xr-x  3 root root 4096 Jul 30 16:36 commons-digester
drwxr-xr-x  3 root root 4096 Jul 30 16:40 commons-el
drwxr-xr-x  3 root root 4096 Jul 30 16:40 commons-httpclient
drwxr-xr-x  3 root root 4096 Jul 30 16:27 commons-io
drwxr-xr-x  3 root root 4096 Jul 30 16:36 commons-lang
drwxr-xr-x  4 root root 4096 Jul 30 16:37 commons-logging
drwxr-xr-x  3 root root 4096 Jul 30 16:40 commons-net
drwxr-xr-x  3 root root 4096 Jul 30 16:47 commons-pool
drwxr-xr-x  3 root root 4096 Jul 30 16:36 commons-validator
drwxr-xr-x  3 root root 4096 Jul 30 16:37 dom4j
drwxr-xr-x  5 root root 4096 Jul 30 16:53 io
drwxr-xr-x  3 root root 4096 Jul 30 16:53 it
drwxr-xr-x 11 root root 4096 Jul 30 16:48 javax
drwxr-xr-x  3 root root 4096 Jul 30 16:47 javolution
drwxr-xr-x  3 root root 4096 Jul 30 16:58 jline
drwxr-xr-x  3 root root 4096 Jul 30 16:53 joda-time
drwxr-xr-x  3 root root 4096 Jul 30 16:27 junit
drwxr-xr-x  4 root root 4096 Jul 30 16:47 log4j
drwxr-xr-x  3 root root 4096 Jul 30 16:37 logkit
drwxr-xr-x  3 root root 4096 Jul 30 16:53 mysql
drwxr-xr-x  8 root root 4096 Jul 30 16:56 net
drwxr-xr-x 46 root root 4096 Jul 30 16:56 org
drwxr-xr-x  3 root root 4096 Jul 30 16:36 oro
drwxr-xr-x  3 root root 4096 Jul 30 16:37 sslext
drwxr-xr-x  3 root root 4096 Jul 30 16:47 stax
drwxr-xr-x  5 root root 4096 Jul 30 16:40 tomcat
drwxr-xr-x  4 root root 4096 Jul 30 16:47 xalan
drwxr-xr-x  3 root root 4096 Jul 30 16:36 xerces
drwxr-xr-x  3 root root 4096 Jul 30 16:36 xml-apis
drwxr-xr-x  3 root root 4096 Jul 30 16:40 xmlenc
drwxr-xr-x  3 root root 4096 Jul 30 16:38 xmlunit
</code></pre> 
  <h2><a id="8Spark_245"></a>8、配置Spark客户端</h2> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>root@vm01 ~<span class="token punctuation">]</span><span class="token comment"># su - hadoop</span>
<span class="token punctuation">[</span>hadoop@vm01 software<span class="token punctuation">]</span>$ <span class="token function">tar</span> -zxvf spark-2.4.2-bin-2.6.0-cdh5.7.0.tgz -C ~/app/
<span class="token punctuation">[</span>hadoop@vm01 software<span class="token punctuation">]</span>$ vim ~/.bash_profile
<span class="token function">export</span> SCALA_HOME<span class="token operator">=</span>/home/hadoop/app/scala-2.12.8
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$SCALA_HOME</span>/bin:<span class="token variable">$PATH</span>

<span class="token function">export</span> JAVA_HOME<span class="token operator">=</span>/usr/java/jdk1.8.0_45
<span class="token function">export</span> JRE_HOME<span class="token operator">=</span><span class="token variable">$JAVA_HOME</span>/jre
<span class="token function">export</span> CLASSPATH<span class="token operator">=</span>.:<span class="token variable">$JAVA_HOME</span>/lib:<span class="token variable">$JER_HOME</span>/lib:<span class="token variable">$CLASSPATH</span>
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$JAVA_HOME</span>/bin:<span class="token variable">$JER_HOME</span>/bin:<span class="token variable">$PATH</span>

<span class="token function">export</span> MAVEN_HOME<span class="token operator">=</span>/home/hadoop/app/apache-maven-3.6.1
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$MAVEN_HOME</span>/bin:<span class="token variable">$PATH</span>
<span class="token function">export</span> MAVEN_OPTS<span class="token operator">=</span><span class="token string">"-Xmx2g -XX:ReservedCodeCacheSize=512m"</span>

<span class="token function">export</span> SPARK_HOME<span class="token operator">=</span>/home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0
<span class="token function">export</span> PATH<span class="token operator">=</span><span class="token variable">$SPARK_HOME</span>/bin:<span class="token variable">$SPARK_HOME</span>/sbin:<span class="token variable">$PATH</span>

<span class="token punctuation">[</span>hadoop@vm01 ~<span class="token punctuation">]</span>$ <span class="token function">source</span> ~/.bash_profile
</code></pre> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>hadoop@vm01 bin<span class="token punctuation">]</span>$ <span class="token function">pwd</span>
/home/hadoop/app/spark-2.4.2-bin-2.6.0-cdh5.7.0/bin
<span class="token punctuation">[</span>hadoop@vm01 bin<span class="token punctuation">]</span>$ <span class="token function">rm</span> -fr *.cmd
</code></pre> 
  <pre><code class="prism language-shell"><span class="token punctuation">[</span>hadoop@vm01 bin<span class="token punctuation">]</span>$ ./spark-shell 
19/07/30 22:12:06 WARN NativeCodeLoader: Unable to load native-hadoop library <span class="token keyword">for</span> your platform<span class="token punctuation">..</span>. using builtin-java classes where applicable
Using Spark<span class="token string">'s default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to "WARN". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Spark context Web UI available at http://vm01:4040 Spark context available as '</span>sc<span class="token string">' (master = local[*], app id = local-1564549938818). Spark session available as '</span>spark<span class="token string">'. Welcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ '</span>_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.2
      /_/
         
Using Scala version 2.11.12 <span class="token punctuation">(</span>Java HotSpot<span class="token punctuation">(</span>TM<span class="token punctuation">)</span> 64-Bit Server VM, Java 1.8.0_45<span class="token punctuation">)</span>
Type <span class="token keyword">in</span> expressions to have them evaluated.
Type :help <span class="token keyword">for</span> <span class="token function">more</span> information.

scala<span class="token operator">&gt;</span> 
scala<span class="token operator">&gt;</span> :quit
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
