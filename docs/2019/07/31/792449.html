<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Hadoop(七)Hive基础 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Hadoop(七)Hive基础" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hive是基于Hadoop的数据仓库工具，提供了在Hadoop分布式存储上对大数据集使用SQL进行查询、修改、管理数据的功能。 Hive提供标准SQL功能，包括SQL：2003，SQL：2011和SQL：2016分析功能。Hive的SQL也可以通过用户自定义的函数（UDF），用户自定义聚合（UDAF）和用户自定义的表函数（UDTF）使用用户代码进行扩展。 Hive不要求数据存储为某种特定的格式，Hive自带连接器可以构建“,”或者“\t”分隔的CSV，TSV文本文件，Apache Parquet，Apache ORC和其他格式，用户可以为其他格式扩展Hive连接器。 一，概念 Hive是 Hive是基于Hadoop数据仓库工具，来处理结构化数据。Hadoop为Hive提供了大规模扩展和容错功能。 Hive不是 用来处理联机事务（OLTP） 不是关系数据库 不是实时查询和行级更新语言 基于Hadoop，Hive提供了以下功能。 通过SQL轻松访问数据的工具，从而实现数据仓库，如（extract/transform/load）ETL，报告，数据分析等。 将结构强加于各种数据结构的一种机制。 直接访问存储在Apache HDFS中的文件，或者在其他存储系统的文件，如：Hbase 通过Apache Tez，Apache Spark或者MapReduce执行查询 使用HPL-SQL 通过Hive LLAP，Apache Yarn和Apache Slider进行亚秒级查询 Hive最好用于传统的数据仓库。 按照粒度单位，Hive数据分为 Databases，命名空间，用于避免表，视图，分区，列等命名冲突。还用于用户或用户组权限。 Tables，具有相同模式的同类数据单元，例如page_views表，每一行包含以下列： timestmap，INT类型，对应于查看页面的Unix时间戳 userId，BIGINT类型，对应于查看页面的用户 page_url，String类型，页面的资源路径 referer_url，String类型，用户到达当前页面的页面位置 ip，String类型，发出请求的用户IP Partitions，每一张Table可以有一个或者多个Partition key，Partition key决定了数据是怎们存储的。Partitions，一是作为存储单元，另一个是通过Partition，用户可以识别满足标准的行。例如String类型的date_partition和String类型的county_partition。每一个Partition key唯一键定义了Table的一个partition，例如从2009-12-23开始所有的US数据是page_views表的一个partition。因此，如果仅仅对2009-12-23以后的US数据进行分析，则只在表的相关分区上运行查询，从而可以加快数据分析速度。注意，仅仅因为partition名称为2009-12-23，并不是他包含该日期的全部或者只包含该日期的数据。为了方便理解，partition使用日期命名。partition和数据的关系取决用用户，分区时虚拟的，不是数据的一部分，而是加载时派生的。 Buckets，每个分区的数据又可以分为Bucket。如page_views表可以根据userId分，userid是表的一个列，或者 结构 Hive组件包括HCatalog和WebHCat。 HCatalog是Hadoop的表和存储管理层，可以让用户使用不同的数据处理工具（Pig和MapReduce），可以更轻松的读写表格数据。 WebHCat，提供了一个用来执行Hadoop MapReduce或者YARN，Pig，Hive任务的服务，你也可以执行Hive元数据操作通过Rest风格Http接口。 Hive的结构如下图 &nbsp; 单元名称 操作 用户接口/界面 Hive是一个数据仓库基础工具软件，可以创建用户和HDFS之间互动。用户界面，Hive支持是Hive的Web UI，Hive命令行，HiveHD洞察（在Windows服务器）。 元存储 Hive选择各自的数据库服务器，用以储存表，数据库，列模式或元数据表，它们的数据类型和HDFS映射。 HiveQL处理引擎 HiveQL类似于SQL的查询上Metastore模式信息。这是传统的方式进行MapReduce程序的替代品之一。相反，使用Java编写的MapReduce程序，可以编写为MapReduce工作，并处理它的查询。 执行引擎 HiveQL处理引擎和MapReduce的结合部分是由Hive执行引擎。执行引擎处理查询并产生结果和MapReduce的结果一样。它采用MapReduce方法。 HDFS 或&nbsp;HBASE Hadoop的分布式文件系统或者HBASE数据存储技术是用于将数据存储到文件系统。 二，安装 2.1，选择版本 根据自己hadoop的版本选择正确的hive版本 在官网https://hive.apache.org/downloads.html页面查看hive版本，以及对应的hadoop版本。 hive2已经不再支持基于Map-Reduce的计算，所以选择1.x版本使用MapReduce进行计算。 2.2，下载解压 [root@ecs-7bc6-0001 hive]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz [root@ecs-7bc6-0001 hive]# tar -zxvf apache-hive-1.2.2-bin.tar.gz 重命名一下 [root@ecs-7bc6-0001 hive]# mv apache-hive-1.2.2-bin hive-1.2.2 2.3，添加环境变量 export HIVE_HOME=/home/work/docker/hive/hive-1.2.2 export PATH=$PATH:$HIVE_HOME/bin 使环境变量生效 source /etc/profile 2.4，运行 2.4.1，准备 必须添加hadoop在环境变量中 2.4.2，修改配置 进入hive安装目录的conf目录下，将配置模板复制一份，命名为hive-site.xml [root@master2 conf]# cp hive-default.xml.template hive-site.xml 修改配置，主要分为三个方面， 1)，metastore对应的数据库配置，包括URL，用户名，密码，JDBC驱动 2)，数据存储目录 3)，相关日志文件目录 根据自己的情况，修改相应的配置，比如mysql在172.16.0.5:3306，hdfs在172.16.0.2:9000 &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://172.16.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;mysql&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;hdfs://172.16.0.2:9000/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt; &lt;value&gt;/tmp/operation_log/operation_logs&lt;/value&gt; &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/tmp/querylog/&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/${hive.session.id}_resources&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt; &lt;/property&gt; 2.4.3，初始化metastore 连入mysql，创建hive数据库，初始化hive的metastor数据库如下 [root@master2 hive-1.2.2]# schematool -dbType mysql -initSchema Metastore connection URL: jdbc:mysql://172.16.0.5:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false Metastore Connection Driver : com.mysql.jdbc.Driver Metastore connection User: root Starting metastore schema initialization to 1.2.0 Initialization script hive-schema-1.2.0.mysql.sql Initialization script completed schemaTool completed 2.4.4，运行 执行hive [root@master2 hive-1.2.2]# hive Logging initialized using configuration in jar:file:/usr/hive-1.2.2/lib/hive-common-1.2.2.jar!/hive-log4j.properties hive&gt; &gt; 创建库 hive&gt; show databases; OK default Time taken: 0.012 seconds, Fetched: 1 row(s) hive&gt; create database hive1; OK Time taken: 0.103 seconds 创建表 hive&gt; use hive1; OK Time taken: 0.015 seconds hive&gt; create table test1(key string, value string); OK Time taken: 0.172 seconds 插入数据 hive&gt; insert into test1 values (&#39;weihao&#39;,&#39;27&#39;),(&#39;yangtian&#39;,&#39;26&#39;); 查询 hive&gt; select * from test1; OK weihao 27 yangtian 26 Time taken: 0.042 seconds, Fetched: 2 row(s) load hdfs数据 创建文件test2 [root@master2 work]# cat test2 1001 weihao 18111111111 1002 zhangsan 18888888888 1003 lisi 17777777777 1004 wangwu 15555555555 上传hdfs [root@master2 work]# hdfs dfs -copyFromLocal test2 /test2 创建表并加载数据 hive&gt; create table test2(user_id bigint, username string, phone string) row format DELIMITED FIELDS TERMINATED BY &#39;\t&#39;; OK Time taken: 0.335 seconds hive&gt; load data inpath &#39;hdfs://172.16.0.2:9000/test2&#39; overwrite into table hive1.test2; Loading data to table hive1.test2 Table hive1.test2 stats: [numFiles=1, numRows=0, totalSize=96, rawDataSize=0] OK Time taken: 0.443 seconds hive&gt; select * from hive1.test2; OK 1001 weihao 18111111111 1002 zhangsan 18888888888 1003 lisi 17777777777 1004 wangwu 15555555555 Time taken: 0.286 seconds, Fetched: 4 row(s) load本地数据， load data命令后添加local表示从本地文件系统加载数据，load data可以添加overwrite命令标识是否覆盖源表。 hive&gt; load data local inpath &quot;/data/work/text&quot; overwrite into table hive1.test1; Loading data to table hive1.test1 Table hive1.test1 stats: [numFiles=1, totalSize=58] OK Time taken: 0.167 seconds hive&gt; select * from test1; OK zhangsan 26 lisi 13 wangwu 45 zhaoliu 25 lanal 45 fdsa 34 Time taken: 0.039 seconds, Fetched: 6 row(s) (完)(^_^)" />
<meta property="og:description" content="Hive是基于Hadoop的数据仓库工具，提供了在Hadoop分布式存储上对大数据集使用SQL进行查询、修改、管理数据的功能。 Hive提供标准SQL功能，包括SQL：2003，SQL：2011和SQL：2016分析功能。Hive的SQL也可以通过用户自定义的函数（UDF），用户自定义聚合（UDAF）和用户自定义的表函数（UDTF）使用用户代码进行扩展。 Hive不要求数据存储为某种特定的格式，Hive自带连接器可以构建“,”或者“\t”分隔的CSV，TSV文本文件，Apache Parquet，Apache ORC和其他格式，用户可以为其他格式扩展Hive连接器。 一，概念 Hive是 Hive是基于Hadoop数据仓库工具，来处理结构化数据。Hadoop为Hive提供了大规模扩展和容错功能。 Hive不是 用来处理联机事务（OLTP） 不是关系数据库 不是实时查询和行级更新语言 基于Hadoop，Hive提供了以下功能。 通过SQL轻松访问数据的工具，从而实现数据仓库，如（extract/transform/load）ETL，报告，数据分析等。 将结构强加于各种数据结构的一种机制。 直接访问存储在Apache HDFS中的文件，或者在其他存储系统的文件，如：Hbase 通过Apache Tez，Apache Spark或者MapReduce执行查询 使用HPL-SQL 通过Hive LLAP，Apache Yarn和Apache Slider进行亚秒级查询 Hive最好用于传统的数据仓库。 按照粒度单位，Hive数据分为 Databases，命名空间，用于避免表，视图，分区，列等命名冲突。还用于用户或用户组权限。 Tables，具有相同模式的同类数据单元，例如page_views表，每一行包含以下列： timestmap，INT类型，对应于查看页面的Unix时间戳 userId，BIGINT类型，对应于查看页面的用户 page_url，String类型，页面的资源路径 referer_url，String类型，用户到达当前页面的页面位置 ip，String类型，发出请求的用户IP Partitions，每一张Table可以有一个或者多个Partition key，Partition key决定了数据是怎们存储的。Partitions，一是作为存储单元，另一个是通过Partition，用户可以识别满足标准的行。例如String类型的date_partition和String类型的county_partition。每一个Partition key唯一键定义了Table的一个partition，例如从2009-12-23开始所有的US数据是page_views表的一个partition。因此，如果仅仅对2009-12-23以后的US数据进行分析，则只在表的相关分区上运行查询，从而可以加快数据分析速度。注意，仅仅因为partition名称为2009-12-23，并不是他包含该日期的全部或者只包含该日期的数据。为了方便理解，partition使用日期命名。partition和数据的关系取决用用户，分区时虚拟的，不是数据的一部分，而是加载时派生的。 Buckets，每个分区的数据又可以分为Bucket。如page_views表可以根据userId分，userid是表的一个列，或者 结构 Hive组件包括HCatalog和WebHCat。 HCatalog是Hadoop的表和存储管理层，可以让用户使用不同的数据处理工具（Pig和MapReduce），可以更轻松的读写表格数据。 WebHCat，提供了一个用来执行Hadoop MapReduce或者YARN，Pig，Hive任务的服务，你也可以执行Hive元数据操作通过Rest风格Http接口。 Hive的结构如下图 &nbsp; 单元名称 操作 用户接口/界面 Hive是一个数据仓库基础工具软件，可以创建用户和HDFS之间互动。用户界面，Hive支持是Hive的Web UI，Hive命令行，HiveHD洞察（在Windows服务器）。 元存储 Hive选择各自的数据库服务器，用以储存表，数据库，列模式或元数据表，它们的数据类型和HDFS映射。 HiveQL处理引擎 HiveQL类似于SQL的查询上Metastore模式信息。这是传统的方式进行MapReduce程序的替代品之一。相反，使用Java编写的MapReduce程序，可以编写为MapReduce工作，并处理它的查询。 执行引擎 HiveQL处理引擎和MapReduce的结合部分是由Hive执行引擎。执行引擎处理查询并产生结果和MapReduce的结果一样。它采用MapReduce方法。 HDFS 或&nbsp;HBASE Hadoop的分布式文件系统或者HBASE数据存储技术是用于将数据存储到文件系统。 二，安装 2.1，选择版本 根据自己hadoop的版本选择正确的hive版本 在官网https://hive.apache.org/downloads.html页面查看hive版本，以及对应的hadoop版本。 hive2已经不再支持基于Map-Reduce的计算，所以选择1.x版本使用MapReduce进行计算。 2.2，下载解压 [root@ecs-7bc6-0001 hive]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz [root@ecs-7bc6-0001 hive]# tar -zxvf apache-hive-1.2.2-bin.tar.gz 重命名一下 [root@ecs-7bc6-0001 hive]# mv apache-hive-1.2.2-bin hive-1.2.2 2.3，添加环境变量 export HIVE_HOME=/home/work/docker/hive/hive-1.2.2 export PATH=$PATH:$HIVE_HOME/bin 使环境变量生效 source /etc/profile 2.4，运行 2.4.1，准备 必须添加hadoop在环境变量中 2.4.2，修改配置 进入hive安装目录的conf目录下，将配置模板复制一份，命名为hive-site.xml [root@master2 conf]# cp hive-default.xml.template hive-site.xml 修改配置，主要分为三个方面， 1)，metastore对应的数据库配置，包括URL，用户名，密码，JDBC驱动 2)，数据存储目录 3)，相关日志文件目录 根据自己的情况，修改相应的配置，比如mysql在172.16.0.5:3306，hdfs在172.16.0.2:9000 &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://172.16.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;mysql&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;hdfs://172.16.0.2:9000/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt; &lt;value&gt;/tmp/operation_log/operation_logs&lt;/value&gt; &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/tmp/querylog/&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/${hive.session.id}_resources&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt; &lt;/property&gt; 2.4.3，初始化metastore 连入mysql，创建hive数据库，初始化hive的metastor数据库如下 [root@master2 hive-1.2.2]# schematool -dbType mysql -initSchema Metastore connection URL: jdbc:mysql://172.16.0.5:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false Metastore Connection Driver : com.mysql.jdbc.Driver Metastore connection User: root Starting metastore schema initialization to 1.2.0 Initialization script hive-schema-1.2.0.mysql.sql Initialization script completed schemaTool completed 2.4.4，运行 执行hive [root@master2 hive-1.2.2]# hive Logging initialized using configuration in jar:file:/usr/hive-1.2.2/lib/hive-common-1.2.2.jar!/hive-log4j.properties hive&gt; &gt; 创建库 hive&gt; show databases; OK default Time taken: 0.012 seconds, Fetched: 1 row(s) hive&gt; create database hive1; OK Time taken: 0.103 seconds 创建表 hive&gt; use hive1; OK Time taken: 0.015 seconds hive&gt; create table test1(key string, value string); OK Time taken: 0.172 seconds 插入数据 hive&gt; insert into test1 values (&#39;weihao&#39;,&#39;27&#39;),(&#39;yangtian&#39;,&#39;26&#39;); 查询 hive&gt; select * from test1; OK weihao 27 yangtian 26 Time taken: 0.042 seconds, Fetched: 2 row(s) load hdfs数据 创建文件test2 [root@master2 work]# cat test2 1001 weihao 18111111111 1002 zhangsan 18888888888 1003 lisi 17777777777 1004 wangwu 15555555555 上传hdfs [root@master2 work]# hdfs dfs -copyFromLocal test2 /test2 创建表并加载数据 hive&gt; create table test2(user_id bigint, username string, phone string) row format DELIMITED FIELDS TERMINATED BY &#39;\t&#39;; OK Time taken: 0.335 seconds hive&gt; load data inpath &#39;hdfs://172.16.0.2:9000/test2&#39; overwrite into table hive1.test2; Loading data to table hive1.test2 Table hive1.test2 stats: [numFiles=1, numRows=0, totalSize=96, rawDataSize=0] OK Time taken: 0.443 seconds hive&gt; select * from hive1.test2; OK 1001 weihao 18111111111 1002 zhangsan 18888888888 1003 lisi 17777777777 1004 wangwu 15555555555 Time taken: 0.286 seconds, Fetched: 4 row(s) load本地数据， load data命令后添加local表示从本地文件系统加载数据，load data可以添加overwrite命令标识是否覆盖源表。 hive&gt; load data local inpath &quot;/data/work/text&quot; overwrite into table hive1.test1; Loading data to table hive1.test1 Table hive1.test1 stats: [numFiles=1, totalSize=58] OK Time taken: 0.167 seconds hive&gt; select * from test1; OK zhangsan 26 lisi 13 wangwu 45 zhaoliu 25 lanal 45 fdsa 34 Time taken: 0.039 seconds, Fetched: 6 row(s) (完)(^_^)" />
<link rel="canonical" href="https://uzzz.org/2019/07/31/792449.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/31/792449.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-31T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Hive是基于Hadoop的数据仓库工具，提供了在Hadoop分布式存储上对大数据集使用SQL进行查询、修改、管理数据的功能。 Hive提供标准SQL功能，包括SQL：2003，SQL：2011和SQL：2016分析功能。Hive的SQL也可以通过用户自定义的函数（UDF），用户自定义聚合（UDAF）和用户自定义的表函数（UDTF）使用用户代码进行扩展。 Hive不要求数据存储为某种特定的格式，Hive自带连接器可以构建“,”或者“\\t”分隔的CSV，TSV文本文件，Apache Parquet，Apache ORC和其他格式，用户可以为其他格式扩展Hive连接器。 一，概念 Hive是 Hive是基于Hadoop数据仓库工具，来处理结构化数据。Hadoop为Hive提供了大规模扩展和容错功能。 Hive不是 用来处理联机事务（OLTP） 不是关系数据库 不是实时查询和行级更新语言 基于Hadoop，Hive提供了以下功能。 通过SQL轻松访问数据的工具，从而实现数据仓库，如（extract/transform/load）ETL，报告，数据分析等。 将结构强加于各种数据结构的一种机制。 直接访问存储在Apache HDFS中的文件，或者在其他存储系统的文件，如：Hbase 通过Apache Tez，Apache Spark或者MapReduce执行查询 使用HPL-SQL 通过Hive LLAP，Apache Yarn和Apache Slider进行亚秒级查询 Hive最好用于传统的数据仓库。 按照粒度单位，Hive数据分为 Databases，命名空间，用于避免表，视图，分区，列等命名冲突。还用于用户或用户组权限。 Tables，具有相同模式的同类数据单元，例如page_views表，每一行包含以下列： timestmap，INT类型，对应于查看页面的Unix时间戳 userId，BIGINT类型，对应于查看页面的用户 page_url，String类型，页面的资源路径 referer_url，String类型，用户到达当前页面的页面位置 ip，String类型，发出请求的用户IP Partitions，每一张Table可以有一个或者多个Partition key，Partition key决定了数据是怎们存储的。Partitions，一是作为存储单元，另一个是通过Partition，用户可以识别满足标准的行。例如String类型的date_partition和String类型的county_partition。每一个Partition key唯一键定义了Table的一个partition，例如从2009-12-23开始所有的US数据是page_views表的一个partition。因此，如果仅仅对2009-12-23以后的US数据进行分析，则只在表的相关分区上运行查询，从而可以加快数据分析速度。注意，仅仅因为partition名称为2009-12-23，并不是他包含该日期的全部或者只包含该日期的数据。为了方便理解，partition使用日期命名。partition和数据的关系取决用用户，分区时虚拟的，不是数据的一部分，而是加载时派生的。 Buckets，每个分区的数据又可以分为Bucket。如page_views表可以根据userId分，userid是表的一个列，或者 结构 Hive组件包括HCatalog和WebHCat。 HCatalog是Hadoop的表和存储管理层，可以让用户使用不同的数据处理工具（Pig和MapReduce），可以更轻松的读写表格数据。 WebHCat，提供了一个用来执行Hadoop MapReduce或者YARN，Pig，Hive任务的服务，你也可以执行Hive元数据操作通过Rest风格Http接口。 Hive的结构如下图 &nbsp; 单元名称 操作 用户接口/界面 Hive是一个数据仓库基础工具软件，可以创建用户和HDFS之间互动。用户界面，Hive支持是Hive的Web UI，Hive命令行，HiveHD洞察（在Windows服务器）。 元存储 Hive选择各自的数据库服务器，用以储存表，数据库，列模式或元数据表，它们的数据类型和HDFS映射。 HiveQL处理引擎 HiveQL类似于SQL的查询上Metastore模式信息。这是传统的方式进行MapReduce程序的替代品之一。相反，使用Java编写的MapReduce程序，可以编写为MapReduce工作，并处理它的查询。 执行引擎 HiveQL处理引擎和MapReduce的结合部分是由Hive执行引擎。执行引擎处理查询并产生结果和MapReduce的结果一样。它采用MapReduce方法。 HDFS 或&nbsp;HBASE Hadoop的分布式文件系统或者HBASE数据存储技术是用于将数据存储到文件系统。 二，安装 2.1，选择版本 根据自己hadoop的版本选择正确的hive版本 在官网https://hive.apache.org/downloads.html页面查看hive版本，以及对应的hadoop版本。 hive2已经不再支持基于Map-Reduce的计算，所以选择1.x版本使用MapReduce进行计算。 2.2，下载解压 [root@ecs-7bc6-0001 hive]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz [root@ecs-7bc6-0001 hive]# tar -zxvf apache-hive-1.2.2-bin.tar.gz 重命名一下 [root@ecs-7bc6-0001 hive]# mv apache-hive-1.2.2-bin hive-1.2.2 2.3，添加环境变量 export HIVE_HOME=/home/work/docker/hive/hive-1.2.2 export PATH=$PATH:$HIVE_HOME/bin 使环境变量生效 source /etc/profile 2.4，运行 2.4.1，准备 必须添加hadoop在环境变量中 2.4.2，修改配置 进入hive安装目录的conf目录下，将配置模板复制一份，命名为hive-site.xml [root@master2 conf]# cp hive-default.xml.template hive-site.xml 修改配置，主要分为三个方面， 1)，metastore对应的数据库配置，包括URL，用户名，密码，JDBC驱动 2)，数据存储目录 3)，相关日志文件目录 根据自己的情况，修改相应的配置，比如mysql在172.16.0.5:3306，hdfs在172.16.0.2:9000 &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://172.16.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;mysql&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;hdfs://172.16.0.2:9000/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt; &lt;value&gt;/tmp/operation_log/operation_logs&lt;/value&gt; &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/tmp/querylog/&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/${hive.session.id}_resources&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt; &lt;/property&gt; 2.4.3，初始化metastore 连入mysql，创建hive数据库，初始化hive的metastor数据库如下 [root@master2 hive-1.2.2]# schematool -dbType mysql -initSchema Metastore connection URL: jdbc:mysql://172.16.0.5:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false Metastore Connection Driver : com.mysql.jdbc.Driver Metastore connection User: root Starting metastore schema initialization to 1.2.0 Initialization script hive-schema-1.2.0.mysql.sql Initialization script completed schemaTool completed 2.4.4，运行 执行hive [root@master2 hive-1.2.2]# hive Logging initialized using configuration in jar:file:/usr/hive-1.2.2/lib/hive-common-1.2.2.jar!/hive-log4j.properties hive&gt; &gt; 创建库 hive&gt; show databases; OK default Time taken: 0.012 seconds, Fetched: 1 row(s) hive&gt; create database hive1; OK Time taken: 0.103 seconds 创建表 hive&gt; use hive1; OK Time taken: 0.015 seconds hive&gt; create table test1(key string, value string); OK Time taken: 0.172 seconds 插入数据 hive&gt; insert into test1 values (&#39;weihao&#39;,&#39;27&#39;),(&#39;yangtian&#39;,&#39;26&#39;); 查询 hive&gt; select * from test1; OK weihao 27 yangtian 26 Time taken: 0.042 seconds, Fetched: 2 row(s) load hdfs数据 创建文件test2 [root@master2 work]# cat test2 1001 weihao 18111111111 1002 zhangsan 18888888888 1003 lisi 17777777777 1004 wangwu 15555555555 上传hdfs [root@master2 work]# hdfs dfs -copyFromLocal test2 /test2 创建表并加载数据 hive&gt; create table test2(user_id bigint, username string, phone string) row format DELIMITED FIELDS TERMINATED BY &#39;\\t&#39;; OK Time taken: 0.335 seconds hive&gt; load data inpath &#39;hdfs://172.16.0.2:9000/test2&#39; overwrite into table hive1.test2; Loading data to table hive1.test2 Table hive1.test2 stats: [numFiles=1, numRows=0, totalSize=96, rawDataSize=0] OK Time taken: 0.443 seconds hive&gt; select * from hive1.test2; OK 1001 weihao 18111111111 1002 zhangsan 18888888888 1003 lisi 17777777777 1004 wangwu 15555555555 Time taken: 0.286 seconds, Fetched: 4 row(s) load本地数据， load data命令后添加local表示从本地文件系统加载数据，load data可以添加overwrite命令标识是否覆盖源表。 hive&gt; load data local inpath &quot;/data/work/text&quot; overwrite into table hive1.test1; Loading data to table hive1.test1 Table hive1.test1 stats: [numFiles=1, totalSize=58] OK Time taken: 0.167 seconds hive&gt; select * from test1; OK zhangsan 26 lisi 13 wangwu 45 zhaoliu 25 lanal 45 fdsa 34 Time taken: 0.039 seconds, Fetched: 6 row(s) (完)(^_^)","@type":"BlogPosting","url":"https://uzzz.org/2019/07/31/792449.html","headline":"Hadoop(七)Hive基础","dateModified":"2019-07-31T00:00:00+08:00","datePublished":"2019-07-31T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/31/792449.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Hadoop(七)Hive基础</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p style="margin-left:0in;">Hive是基于Hadoop的数据仓库工具，提供了在Hadoop分布式存储上对大数据集使用SQL进行查询、修改、管理数据的功能。</p> 
  <p style="margin-left:0in;">Hive提供标准SQL功能，包括SQL：2003，SQL：2011和SQL：2016分析功能。Hive的SQL也可以通过用户自定义的函数（UDF），用户自定义聚合（UDAF）和用户自定义的表函数（UDTF）使用用户代码进行扩展。</p> 
  <p style="margin-left:0in;">Hive不要求数据存储为某种特定的格式，Hive自带连接器可以构建“,”或者“\t”分隔的CSV，TSV文本文件，Apache Parquet，Apache ORC和其他格式，用户可以为其他格式扩展Hive连接器。</p> 
  <h1 style="margin-left:0in;"><span style="color:#1e4e79;">一，概念</span></h1> 
  <p style="margin-left:0in;"><strong>Hive</strong><strong>是</strong></p> 
  <p style="margin-left:0in;">Hive是基于Hadoop数据仓库工具，来处理结构化数据。Hadoop为Hive提供了大规模扩展和容错功能。</p> 
  <p style="margin-left:0in;"><strong>Hive</strong><strong>不是</strong></p> 
  <ul style="margin-left:.75in;">
   <li>用来处理联机事务（OLTP）</li> 
   <li>不是关系数据库</li> 
   <li>不是实时查询和行级更新语言</li> 
  </ul>
  <p style="margin-left:0in;"><strong>基于Hadoop，Hive提供了以下功能。</strong></p> 
  <ul style="margin-left:.75in;">
   <li>通过SQL轻松访问数据的工具，从而实现数据仓库，如（extract/transform/load）ETL，报告，数据分析等。</li> 
   <li>将结构强加于各种数据结构的一种机制。</li> 
   <li>直接访问存储在Apache HDFS中的文件，或者在其他存储系统的文件，如：Hbase</li> 
   <li>通过Apache Tez，Apache Spark或者MapReduce执行查询</li> 
   <li>使用HPL-SQL</li> 
   <li>通过Hive LLAP，Apache Yarn和Apache Slider进行亚秒级查询</li> 
  </ul>
  <p style="margin-left:0in;">Hive最好用于传统的数据仓库。</p> 
  <p style="margin-left:0in;"><strong>按照粒度单位，Hive数据分为</strong></p> 
  <ul style="margin-left:.75in;">
   <li>Databases，命名空间，用于避免表，视图，分区，列等命名冲突。还用于用户或用户组权限。</li> 
   <li>Tables，具有相同模式的同类数据单元，例如page_views表，每一行包含以下列： 
    <ol style="margin-left:.375in;">
     <li>timestmap，INT类型，对应于查看页面的Unix时间戳</li> 
     <li>userId，BIGINT类型，对应于查看页面的用户</li> 
     <li>page_url，String类型，页面的资源路径</li> 
     <li>referer_url，String类型，用户到达当前页面的页面位置</li> 
     <li>ip，String类型，发出请求的用户IP</li> 
    </ol></li> 
   <li>Partitions，每一张Table可以有一个或者多个Partition key，Partition key决定了数据是怎们存储的。Partitions，一是作为存储单元，另一个是通过Partition，用户可以识别满足标准的行。例如String类型的date_partition和String类型的county_partition。每一个Partition key唯一键定义了Table的一个partition，例如从2009-12-23开始所有的US数据是page_views表的一个partition。因此，如果仅仅对2009-12-23以后的US数据进行分析，则只在表的相关分区上运行查询，从而可以加快数据分析速度。注意，仅仅因为partition名称为2009-12-23，并不是他包含该日期的全部或者只包含该日期的数据。为了方便理解，partition使用日期命名。partition和数据的关系取决用用户，分区时虚拟的，不是数据的一部分，而是加载时派生的。</li> 
   <li>Buckets，每个分区的数据又可以分为Bucket。如page_views表可以根据userId分，userid是表的一个列，或者</li> 
  </ul>
  <p style="margin-left:0in;"><strong>结构</strong></p> 
  <p style="margin-left:0in;">Hive组件包括HCatalog和WebHCat。</p> 
  <ul style="margin-left:.75in;">
   <li>HCatalog是Hadoop的表和存储管理层，可以让用户使用不同的数据处理工具（Pig和MapReduce），可以更轻松的读写表格数据。</li> 
   <li>WebHCat，提供了一个用来执行Hadoop MapReduce或者YARN，Pig，Hive任务的服务，你也可以执行Hive元数据操作通过Rest风格Http接口。</li> 
  </ul>
  <p style="margin-left:0in;">Hive的结构如下图</p> 
  <p style="margin-left:0in;">&nbsp;</p> 
  <p style="text-indent:50px;"><img alt="" class="has" height="330" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731113041101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaWhhb18=,size_16,color_FFFFFF,t_70" width="600"></p> 
  <table cellspacing="0" summary="">
   <tbody>
    <tr>
     <td style="border-color:#a3a3a3;vertical-align:top;width:1.4875in;"> <p style="margin-left:0in;"><strong>单元名称</strong></p> </td> 
     <td style="border-color:#a3a3a3;vertical-align:top;"> <p style="margin-left:0in;"><strong>操作</strong></p> </td> 
    </tr>
    <tr>
     <td style="border-color:#a3a3a3;vertical-align:top;width:1.4875in;"> <p style="margin-left:0in;">用户接口/界面</p> </td> 
     <td style="border-color:#a3a3a3;vertical-align:top;"> <p style="margin-left:0in;">Hive是一个数据仓库基础工具软件，可以创建用户和HDFS之间互动。用户界面，Hive支持是Hive的Web UI，Hive命令行，HiveHD洞察（在Windows服务器）。</p> </td> 
    </tr>
    <tr>
     <td style="border-color:#a3a3a3;vertical-align:top;width:1.4875in;"> <p style="margin-left:0in;">元存储</p> </td> 
     <td style="border-color:#a3a3a3;vertical-align:top;"> <p style="margin-left:0in;">Hive选择各自的数据库服务器，用以储存表，数据库，列模式或元数据表，它们的数据类型和HDFS映射。</p> </td> 
    </tr>
    <tr>
     <td style="border-color:#a3a3a3;vertical-align:top;width:1.4875in;"> <p style="margin-left:0in;">HiveQL处理引擎</p> </td> 
     <td style="border-color:#a3a3a3;vertical-align:top;"> <p style="margin-left:0in;">HiveQL类似于SQL的查询上Metastore模式信息。这是传统的方式进行MapReduce程序的替代品之一。相反，使用Java编写的MapReduce程序，可以编写为MapReduce工作，并处理它的查询。</p> </td> 
    </tr>
    <tr>
     <td style="border-color:#a3a3a3;vertical-align:top;width:1.4875in;"> <p style="margin-left:0in;">执行引擎</p> </td> 
     <td style="border-color:#a3a3a3;vertical-align:top;"> <p style="margin-left:0in;">HiveQL处理引擎和MapReduce的结合部分是由Hive执行引擎。执行引擎处理查询并产生结果和MapReduce的结果一样。它采用MapReduce方法。</p> </td> 
    </tr>
    <tr>
     <td style="border-color:#a3a3a3;vertical-align:top;width:1.4875in;"> <p style="margin-left:0in;">HDFS 或&nbsp;HBASE</p> </td> 
     <td style="border-color:#a3a3a3;vertical-align:top;"> <p style="margin-left:0in;">Hadoop的分布式文件系统或者HBASE数据存储技术是用于将数据存储到文件系统。</p> </td> 
    </tr>
   </tbody>
  </table>
  <h1 style="margin-left:0in;"><span style="color:#1e4e79;">二，安装</span></h1> 
  <p style="margin-left:0in;"><strong>2.1</strong><strong>，选择版本</strong></p> 
  <p style="margin-left:0in;">根据自己hadoop的版本选择正确的hive版本</p> 
  <p style="margin-left:0in;">在官网<a href="https://hive.apache.org/downloads.html" rel="nofollow" data-token="259c55566e8fe52c9817a5290c30fdfb">https://hive.apache.org/downloads.html</a>页面查看hive版本，以及对应的hadoop版本。</p> 
  <p style="margin-left:0in;">hive2已经不再支持基于Map-Reduce的计算，所以选择1.x版本使用MapReduce进行计算。</p> 
  <p style="margin-left:0in;"><strong>2.2</strong><strong>，下载解压</strong></p> 
  <pre class="has">
<code>[root@ecs-7bc6-0001 hive]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz
[root@ecs-7bc6-0001 hive]# tar -zxvf apache-hive-1.2.2-bin.tar.gz
</code></pre> 
  <p>重命名一下</p> 
  <pre class="has">
<code>[root@ecs-7bc6-0001 hive]# mv apache-hive-1.2.2-bin hive-1.2.2
</code></pre> 
  <p style="margin-left:0in;"><strong>2.3</strong><strong>，添加环境变量</strong></p> 
  <pre class="has">
<code>export HIVE_HOME=/home/work/docker/hive/hive-1.2.2
export PATH=$PATH:$HIVE_HOME/bin</code></pre> 
  <p style="margin-left:0in;">使环境变量生效</p> 
  <pre class="has">
<code>source /etc/profile
</code></pre> 
  <p style="margin-left:0in;"><strong>2.4</strong><strong>，运行</strong></p> 
  <p style="margin-left:0in;">2.4.1，准备</p> 
  <p style="margin-left:0in;">必须添加hadoop在环境变量中</p> 
  <p style="margin-left:0in;">2.4.2，修改配置</p> 
  <p style="margin-left:0in;">进入hive安装目录的conf目录下，将配置模板复制一份，命名为hive-site.xml</p> 
  <pre class="has">
<code>[root@master2 conf]# cp hive-default.xml.template hive-site.xml
</code></pre> 
  <p style="margin-left:0in;">修改配置，主要分为三个方面，</p> 
  <p style="margin-left:0in;text-indent:50px;">1)，metastore对应的数据库配置，包括URL，用户名，密码，JDBC驱动</p> 
  <p style="margin-left:0in;text-indent:50px;">2)，数据存储目录</p> 
  <p style="margin-left:0in;text-indent:50px;">3)，相关日志文件目录</p> 
  <p style="margin-left:0in;">根据自己的情况，修改相应的配置，比如mysql在172.16.0.5:3306，hdfs在172.16.0.2:9000</p> 
  <pre class="has">
<code>&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
    &lt;value&gt;jdbc:mysql://172.16.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;
    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
  &lt;/property&gt;
 &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
    &lt;value&gt;root&lt;/value&gt;
    &lt;description&gt;Username to use against metastore database&lt;/description&gt;
  &lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
    &lt;value&gt;mysql&lt;/value&gt;
    &lt;description&gt;password to use against metastore database&lt;/description&gt;
  &lt;/property&gt;
 &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
    &lt;value&gt;hdfs://172.16.0.2:9000/user/hive/warehouse&lt;/value&gt;
    &lt;description&gt;location of default database for the warehouse&lt;/description&gt;
  &lt;/property&gt;
 &lt;property&gt;
    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;
    &lt;value&gt;/tmp/operation_log/operation_logs&lt;/value&gt;
    &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt;
  &lt;/property&gt;
 &lt;property&gt;
    &lt;name&gt;hive.querylog.location&lt;/name&gt;
    &lt;value&gt;/tmp/querylog/&lt;/value&gt;
    &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;
  &lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;
    &lt;value&gt;/tmp/hive&lt;/value&gt;
    &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;
    &lt;value&gt;/tmp/hive&lt;/value&gt;
    &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;
    &lt;value&gt;/tmp/${hive.session.id}_resources&lt;/value&gt;
    &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;
  &lt;/property&gt;
</code></pre> 
  <p style="margin-left:0in;">2.4.3，初始化metastore</p> 
  <p style="margin-left:0in;">连入mysql，创建hive数据库，初始化hive的metastor数据库如下</p> 
  <pre class="has">
<code>[root@master2 hive-1.2.2]# schematool -dbType mysql -initSchema
Metastore connection URL:	 jdbc:mysql://172.16.0.5:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false
Metastore Connection Driver :	 com.mysql.jdbc.Driver
Metastore connection User:	 root
Starting metastore schema initialization to 1.2.0
Initialization script hive-schema-1.2.0.mysql.sql
Initialization script completed
schemaTool completed</code></pre> 
  <p style="margin-left:0in;">2.4.4，运行</p> 
  <p style="margin-left:0in;">执行hive</p> 
  <pre class="has">
<code>[root@master2 hive-1.2.2]# hive
Logging initialized using configuration in jar:file:/usr/hive-1.2.2/lib/hive-common-1.2.2.jar!/hive-log4j.properties
hive&gt; 
    &gt; 
</code></pre> 
  <p style="margin-left:0in;">创建库</p> 
  <pre class="has">
<code>hive&gt; show databases;
OK
default
Time taken: 0.012 seconds, Fetched: 1 row(s)
hive&gt; create database hive1;
OK
Time taken: 0.103 seconds
</code></pre> 
  <p>创建表</p> 
  <pre class="has">
<code>hive&gt; use hive1;
OK
Time taken: 0.015 seconds
hive&gt; create table test1(key string, value string);
OK
Time taken: 0.172 seconds
插入数据
hive&gt; insert into test1 values ('weihao','27'),('yangtian','26');
</code></pre> 
  <p>查询</p> 
  <pre class="has">
<code>hive&gt; select * from test1;
OK
weihao	27
yangtian	26
Time taken: 0.042 seconds, Fetched: 2 row(s)
</code></pre> 
  <p style="margin-left:0in;">load hdfs数据</p> 
  <p style="margin-left:0in;">创建文件test2</p> 
  <pre class="has">
<code>[root@master2 work]# cat test2
1001	weihao	18111111111
1002	zhangsan	18888888888
1003	lisi	17777777777
1004	wangwu	15555555555
</code></pre> 
  <p>上传hdfs</p> 
  <pre class="has">
<code>[root@master2 work]# hdfs dfs -copyFromLocal test2 /test2
</code></pre> 
  <p>创建表并加载数据</p> 
  <pre class="has">
<code>hive&gt; create table test2(user_id bigint, username string, phone string) row format DELIMITED FIELDS TERMINATED BY '\t';
OK
Time taken: 0.335 seconds
hive&gt; load data inpath 'hdfs://172.16.0.2:9000/test2' overwrite into table hive1.test2;
Loading data to table hive1.test2
Table hive1.test2 stats: [numFiles=1, numRows=0, totalSize=96, rawDataSize=0]
OK
Time taken: 0.443 seconds
hive&gt; select * from hive1.test2;
OK
1001	weihao	18111111111
1002	zhangsan	18888888888
1003	lisi	17777777777
1004	wangwu	15555555555
Time taken: 0.286 seconds, Fetched: 4 row(s)
</code></pre> 
  <p style="margin-left:0in;">load本地数据，</p> 
  <p style="margin-left:0in;">load data命令后添加local表示从本地文件系统加载数据，load data可以添加overwrite命令标识是否覆盖源表。</p> 
  <pre class="has">
<code>hive&gt; load data local inpath "/data/work/text" overwrite into table hive1.test1;
Loading data to table hive1.test1
Table hive1.test1 stats: [numFiles=1, totalSize=58]
OK
Time taken: 0.167 seconds
hive&gt; select * from test1;
OK
zhangsan	26
lisi	13
wangwu	45
zhaoliu	25
lanal	45
fdsa	34
Time taken: 0.039 seconds, Fetched: 6 row(s)
</code></pre> 
  <p style="margin-left:0in;">(完)(^_^)</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
