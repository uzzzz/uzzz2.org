<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>MapReduce任务运行慢问题排查 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="MapReduce任务运行慢问题排查" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 一、问题描述 二、问题分析 1、第一次失败（yarn的磁盘健康检查机制导致的任务失败） 2、第二次失败（map和reduce资源竞争导致的死锁） 三、总结 参考资料 一、问题描述 今天有业务反馈有个MapReduce任务运行很慢，于是看了下JobHIstory上任务的运行情况，发现任务就剩一个reduce还在执行，当时第一反应以为是出现了数据倾斜。但实际排查后发现不是，因为这个任务的reduce task平均执行时间就是2个小时那里（有几十个reduce task），还在运行的那个reduce task是因为出现了失败重试，所以过了一个多小时也一直没跑完： 二、问题分析 看了JobHIstory的UI，发现这个任务失败了两次，这两次失败的情况都不一样。下面我详细的分析一下这两个错误的产生原因 1、第一次失败（yarn的磁盘健康检查机制导致的任务失败） 其实JobHIstory的异常信息说的很明白了，是因为节点不可用，所以taskAttempt被kill。那为什么节点会变成不可用呢？后面排查发现，在那个时间点机器的某几块磁盘达到90%+，随即就想到了yarn的磁盘健康检查机制。 但是我们的节点有10块磁盘，node不健康的阈值配的是25%，也就是说只要保证有超过25%的磁盘是健康的，这个节点就认为是健康的（挂了8块磁盘才会导致node节点被认为是不健康的）。 后面在网上搜一些资料，才发现yarn磁盘健康检查机制的一个细节。就是yarn磁盘健康检查线程会检查两种类型的磁盘：local-dirs和log-dirs。 local-dirs是通过yarn.nodemanager.local-dirs配置的，一般是用来存放yarn任务的一些临时文件的。 log-dirs是通过yarn.nodemanager.log-dirs配置的，一般是用来存放任务运行时输出的相关日志的。 我们集群的local-dirs配置了10块磁盘，log-dirs却只有一块磁盘，因此刚好log-dirs的那块磁盘使用率达到了90%，所以该磁盘被认为是不健康的，继而yarn磁盘健康检查机制则判定该Node是不健康的。通过NodeManager的日志，我们也可以看出确实是这么一回事： 从日志1/1 log-dirs are bad可以看出， 所有的log-dirs都变成了不健康的，随即Node马上变成不健康的了。 yarn的Node变成不健康后，会kill该Node上所有的Container，因此任务的task失败。 2、第二次失败（map和reduce资源竞争导致的死锁） 在第一次因为磁盘健康的问题失败后，我们发现该reduce task的第二次运行也失败了。看错误描述也很简单，网上搜了下也发现很多该问题的解决方案。 这个问题主要是因为reduce提前启动导致的。在MapReduce的流程中，reduce启动后需要先copy map的相关shuffle数据，然后全部搜集完所有map的数据后才能处理(具体流程看mapreduce shuffle的细节，这里不详细展开)。hadoop为了加快MapReduce job的整体速度，会提前启动一些reduce，先去拉取那些已经完成的map的数据，这样就可以节省后面的一部分IO时间。但是这样设计就会有一个问题，就是当资源有限的情况下，reduce占用了map的资源，然后又在等待map的数据，map则在等待资源运行，这就形成了一个死锁。之后在发生死锁一定时间后，hadoop就会kill掉那个reduce task。 这种情况我们可以通过调整参数来推迟reduce task的执行： mapreduce.job.reduce.slowstart.completedmaps：默认是0.05，表示只要有5%的map task执行完，就可以开启reduce task开始copy数据了。我们调大这个值推迟reduce task的执行 三、总结 这次主要的原因还是因为刚好日志所在磁盘使用率达到了90%，导致整个节点不可用。虽然说是当时刚好有几个大任务写了很多数据到磁盘，导致磁盘使用率飚高。但是这也说明集群的健壮性还不够。 目前集群各个机器磁盘的平均使用率大概在74%，这其实不是一个比较安全的值，稍微几个大任务可能就会导致类似的情况再次发生。因此，后续需要对集群一些无用或者冷数据进行删除或者压缩（在无法扩容的情况下）来释放磁盘空间，提高集群的健壮性。 参考资料 https://blog.csdn.net/zpf336/article/details/80931629 https://stackoverflow.com/questions/31683536/yarn-error-taskattempt-killed-because-it-ran-on-unusable-node-container-rel" />
<meta property="og:description" content="文章目录 一、问题描述 二、问题分析 1、第一次失败（yarn的磁盘健康检查机制导致的任务失败） 2、第二次失败（map和reduce资源竞争导致的死锁） 三、总结 参考资料 一、问题描述 今天有业务反馈有个MapReduce任务运行很慢，于是看了下JobHIstory上任务的运行情况，发现任务就剩一个reduce还在执行，当时第一反应以为是出现了数据倾斜。但实际排查后发现不是，因为这个任务的reduce task平均执行时间就是2个小时那里（有几十个reduce task），还在运行的那个reduce task是因为出现了失败重试，所以过了一个多小时也一直没跑完： 二、问题分析 看了JobHIstory的UI，发现这个任务失败了两次，这两次失败的情况都不一样。下面我详细的分析一下这两个错误的产生原因 1、第一次失败（yarn的磁盘健康检查机制导致的任务失败） 其实JobHIstory的异常信息说的很明白了，是因为节点不可用，所以taskAttempt被kill。那为什么节点会变成不可用呢？后面排查发现，在那个时间点机器的某几块磁盘达到90%+，随即就想到了yarn的磁盘健康检查机制。 但是我们的节点有10块磁盘，node不健康的阈值配的是25%，也就是说只要保证有超过25%的磁盘是健康的，这个节点就认为是健康的（挂了8块磁盘才会导致node节点被认为是不健康的）。 后面在网上搜一些资料，才发现yarn磁盘健康检查机制的一个细节。就是yarn磁盘健康检查线程会检查两种类型的磁盘：local-dirs和log-dirs。 local-dirs是通过yarn.nodemanager.local-dirs配置的，一般是用来存放yarn任务的一些临时文件的。 log-dirs是通过yarn.nodemanager.log-dirs配置的，一般是用来存放任务运行时输出的相关日志的。 我们集群的local-dirs配置了10块磁盘，log-dirs却只有一块磁盘，因此刚好log-dirs的那块磁盘使用率达到了90%，所以该磁盘被认为是不健康的，继而yarn磁盘健康检查机制则判定该Node是不健康的。通过NodeManager的日志，我们也可以看出确实是这么一回事： 从日志1/1 log-dirs are bad可以看出， 所有的log-dirs都变成了不健康的，随即Node马上变成不健康的了。 yarn的Node变成不健康后，会kill该Node上所有的Container，因此任务的task失败。 2、第二次失败（map和reduce资源竞争导致的死锁） 在第一次因为磁盘健康的问题失败后，我们发现该reduce task的第二次运行也失败了。看错误描述也很简单，网上搜了下也发现很多该问题的解决方案。 这个问题主要是因为reduce提前启动导致的。在MapReduce的流程中，reduce启动后需要先copy map的相关shuffle数据，然后全部搜集完所有map的数据后才能处理(具体流程看mapreduce shuffle的细节，这里不详细展开)。hadoop为了加快MapReduce job的整体速度，会提前启动一些reduce，先去拉取那些已经完成的map的数据，这样就可以节省后面的一部分IO时间。但是这样设计就会有一个问题，就是当资源有限的情况下，reduce占用了map的资源，然后又在等待map的数据，map则在等待资源运行，这就形成了一个死锁。之后在发生死锁一定时间后，hadoop就会kill掉那个reduce task。 这种情况我们可以通过调整参数来推迟reduce task的执行： mapreduce.job.reduce.slowstart.completedmaps：默认是0.05，表示只要有5%的map task执行完，就可以开启reduce task开始copy数据了。我们调大这个值推迟reduce task的执行 三、总结 这次主要的原因还是因为刚好日志所在磁盘使用率达到了90%，导致整个节点不可用。虽然说是当时刚好有几个大任务写了很多数据到磁盘，导致磁盘使用率飚高。但是这也说明集群的健壮性还不够。 目前集群各个机器磁盘的平均使用率大概在74%，这其实不是一个比较安全的值，稍微几个大任务可能就会导致类似的情况再次发生。因此，后续需要对集群一些无用或者冷数据进行删除或者压缩（在无法扩容的情况下）来释放磁盘空间，提高集群的健壮性。 参考资料 https://blog.csdn.net/zpf336/article/details/80931629 https://stackoverflow.com/questions/31683536/yarn-error-taskattempt-killed-because-it-ran-on-unusable-node-container-rel" />
<link rel="canonical" href="https://uzzz.org/2019/07/31/792451.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/31/792451.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-31T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 一、问题描述 二、问题分析 1、第一次失败（yarn的磁盘健康检查机制导致的任务失败） 2、第二次失败（map和reduce资源竞争导致的死锁） 三、总结 参考资料 一、问题描述 今天有业务反馈有个MapReduce任务运行很慢，于是看了下JobHIstory上任务的运行情况，发现任务就剩一个reduce还在执行，当时第一反应以为是出现了数据倾斜。但实际排查后发现不是，因为这个任务的reduce task平均执行时间就是2个小时那里（有几十个reduce task），还在运行的那个reduce task是因为出现了失败重试，所以过了一个多小时也一直没跑完： 二、问题分析 看了JobHIstory的UI，发现这个任务失败了两次，这两次失败的情况都不一样。下面我详细的分析一下这两个错误的产生原因 1、第一次失败（yarn的磁盘健康检查机制导致的任务失败） 其实JobHIstory的异常信息说的很明白了，是因为节点不可用，所以taskAttempt被kill。那为什么节点会变成不可用呢？后面排查发现，在那个时间点机器的某几块磁盘达到90%+，随即就想到了yarn的磁盘健康检查机制。 但是我们的节点有10块磁盘，node不健康的阈值配的是25%，也就是说只要保证有超过25%的磁盘是健康的，这个节点就认为是健康的（挂了8块磁盘才会导致node节点被认为是不健康的）。 后面在网上搜一些资料，才发现yarn磁盘健康检查机制的一个细节。就是yarn磁盘健康检查线程会检查两种类型的磁盘：local-dirs和log-dirs。 local-dirs是通过yarn.nodemanager.local-dirs配置的，一般是用来存放yarn任务的一些临时文件的。 log-dirs是通过yarn.nodemanager.log-dirs配置的，一般是用来存放任务运行时输出的相关日志的。 我们集群的local-dirs配置了10块磁盘，log-dirs却只有一块磁盘，因此刚好log-dirs的那块磁盘使用率达到了90%，所以该磁盘被认为是不健康的，继而yarn磁盘健康检查机制则判定该Node是不健康的。通过NodeManager的日志，我们也可以看出确实是这么一回事： 从日志1/1 log-dirs are bad可以看出， 所有的log-dirs都变成了不健康的，随即Node马上变成不健康的了。 yarn的Node变成不健康后，会kill该Node上所有的Container，因此任务的task失败。 2、第二次失败（map和reduce资源竞争导致的死锁） 在第一次因为磁盘健康的问题失败后，我们发现该reduce task的第二次运行也失败了。看错误描述也很简单，网上搜了下也发现很多该问题的解决方案。 这个问题主要是因为reduce提前启动导致的。在MapReduce的流程中，reduce启动后需要先copy map的相关shuffle数据，然后全部搜集完所有map的数据后才能处理(具体流程看mapreduce shuffle的细节，这里不详细展开)。hadoop为了加快MapReduce job的整体速度，会提前启动一些reduce，先去拉取那些已经完成的map的数据，这样就可以节省后面的一部分IO时间。但是这样设计就会有一个问题，就是当资源有限的情况下，reduce占用了map的资源，然后又在等待map的数据，map则在等待资源运行，这就形成了一个死锁。之后在发生死锁一定时间后，hadoop就会kill掉那个reduce task。 这种情况我们可以通过调整参数来推迟reduce task的执行： mapreduce.job.reduce.slowstart.completedmaps：默认是0.05，表示只要有5%的map task执行完，就可以开启reduce task开始copy数据了。我们调大这个值推迟reduce task的执行 三、总结 这次主要的原因还是因为刚好日志所在磁盘使用率达到了90%，导致整个节点不可用。虽然说是当时刚好有几个大任务写了很多数据到磁盘，导致磁盘使用率飚高。但是这也说明集群的健壮性还不够。 目前集群各个机器磁盘的平均使用率大概在74%，这其实不是一个比较安全的值，稍微几个大任务可能就会导致类似的情况再次发生。因此，后续需要对集群一些无用或者冷数据进行删除或者压缩（在无法扩容的情况下）来释放磁盘空间，提高集群的健壮性。 参考资料 https://blog.csdn.net/zpf336/article/details/80931629 https://stackoverflow.com/questions/31683536/yarn-error-taskattempt-killed-because-it-ran-on-unusable-node-container-rel","@type":"BlogPosting","url":"https://uzzz.org/2019/07/31/792451.html","headline":"MapReduce任务运行慢问题排查","dateModified":"2019-07-31T00:00:00+08:00","datePublished":"2019-07-31T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/31/792451.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>MapReduce任务运行慢问题排查</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <ul>
     <li><a href="#_1" rel="nofollow" data-token="544d24628aac1f2110bc37d17d1536a3">一、问题描述</a></li>
     <li><a href="#_7" rel="nofollow" data-token="c3ce50809eb479346f949c8850925e4f">二、问题分析</a></li>
     <ul>
      <li><a href="#1yarn_11" rel="nofollow" data-token="cdfb18c97713679145073bad6935b614">1、第一次失败（yarn的磁盘健康检查机制导致的任务失败）</a></li>
      <li><a href="#2mapreduce_32" rel="nofollow" data-token="2650806834cdae86c4f2c8be69b9b4ad">2、第二次失败（map和reduce资源竞争导致的死锁）</a></li>
     </ul>
     <li><a href="#_42" rel="nofollow" data-token="2ed1e2cc9e2df0ac1510734d2cb470a7">三、总结</a></li>
     <li><a href="#_48" rel="nofollow" data-token="c91ff5eea6d882b540a8d7ce23a46b9a">参考资料</a></li>
    </ul>
   </ul>
  </div>
  <p></p> 
  <h2><a id="_1"></a>一、问题描述</h2> 
  <p>今天有业务反馈有个MapReduce任务运行很慢，于是看了下JobHIstory上任务的运行情况，发现任务就剩一个reduce还在执行，当时第一反应以为是出现了数据倾斜。但实际排查后发现不是，因为这个任务的reduce task平均执行时间就是2个小时那里（有几十个reduce task），还在运行的那个reduce task是因为出现了失败重试，所以过了一个多小时也一直没跑完：</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731180954302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMzMzIxMjQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="_7"></a>二、问题分析</h2> 
  <p>看了JobHIstory的UI，发现这个任务失败了两次，这两次失败的情况都不一样。下面我详细的分析一下这两个错误的产生原因</p> 
  <h3><a id="1yarn_11"></a>1、第一次失败（yarn的磁盘健康检查机制导致的任务失败）</h3> 
  <p>其实JobHIstory的异常信息说的很明白了，是因为节点不可用，所以taskAttempt被kill。那为什么节点会变成不可用呢？后面排查发现，在那个时间点机器的某几块磁盘达到90%+，随即就想到了yarn的磁盘健康检查机制。</p> 
  <p>但是我们的节点有10块磁盘，node不健康的阈值配的是25%，也就是说只要保证有超过25%的磁盘是健康的，这个节点就认为是健康的（挂了8块磁盘才会导致node节点被认为是不健康的）。</p> 
  <p>后面在网上搜一些资料，才发现yarn磁盘健康检查机制的一个细节。就是yarn磁盘健康检查线程会检查两种类型的磁盘：local-dirs和log-dirs。</p> 
  <ul> 
   <li>local-dirs是通过<code>yarn.nodemanager.local-dirs</code>配置的，一般是用来存放yarn任务的一些临时文件的。</li> 
   <li>log-dirs是通过<code>yarn.nodemanager.log-dirs</code>配置的，一般是用来存放任务运行时输出的相关日志的。</li> 
  </ul> 
  <p><strong>我们集群的local-dirs配置了10块磁盘，log-dirs却只有一块磁盘，因此刚好log-dirs的那块磁盘使用率达到了90%，所以该磁盘被认为是不健康的，继而yarn磁盘健康检查机制则判定该Node是不健康的</strong>。通过NodeManager的日志，我们也可以看出确实是这么一回事：</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731181003690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMzMzIxMjQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731181011993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMzMzIxMjQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>从日志<code>1/1 log-dirs are bad</code>可以看出， 所有的log-dirs都变成了不健康的，随即Node马上变成不健康的了。</p> 
  <p><strong>yarn的Node变成不健康后，会kill该Node上所有的Container，因此任务的task失败</strong>。</p> 
  <h3><a id="2mapreduce_32"></a>2、第二次失败（map和reduce资源竞争导致的死锁）</h3> 
  <p>在第一次因为磁盘健康的问题失败后，我们发现该reduce task的第二次运行也失败了。看错误描述也很简单，网上搜了下也发现很多该问题的解决方案。</p> 
  <p><strong>这个问题主要是因为reduce提前启动导致的</strong>。在MapReduce的流程中，reduce启动后需要先copy map的相关shuffle数据，然后全部搜集完所有map的数据后才能处理(<strong>具体流程看mapreduce shuffle的细节，这里不详细展开</strong>)。<strong>hadoop为了加快MapReduce job的整体速度，会提前启动一些reduce，先去拉取那些已经完成的map的数据</strong>，这样就可以节省后面的一部分IO时间。但是这样设计就会有一个问题，<strong>就是当资源有限的情况下，reduce占用了map的资源，然后又在等待map的数据，map则在等待资源运行，这就形成了一个死锁</strong>。之后在发生死锁一定时间后，hadoop就会kill掉那个reduce task。</p> 
  <p>这种情况我们可以通过调整参数来推迟reduce task的执行：</p> 
  <ul> 
   <li>mapreduce.job.reduce.slowstart.completedmaps：默认是0.05，表示只要有5%的map task执行完，就可以开启reduce task开始copy数据了。我们调大这个值推迟reduce task的执行</li> 
  </ul> 
  <h2><a id="_42"></a>三、总结</h2> 
  <p>这次主要的原因还是因为刚好日志所在磁盘使用率达到了90%，导致整个节点不可用。虽然说是当时刚好有几个大任务写了很多数据到磁盘，导致磁盘使用率飚高。但是这也说明集群的健壮性还不够。</p> 
  <p>目前集群各个机器磁盘的平均使用率大概在74%，这其实不是一个比较安全的值，稍微几个大任务可能就会导致类似的情况再次发生。因此，后续需要对集群一些无用或者冷数据进行删除或者压缩（在无法扩容的情况下）来释放磁盘空间，提高集群的健壮性。</p> 
  <h2><a id="_48"></a>参考资料</h2> 
  <p><a href="https://blog.csdn.net/zpf336/article/details/80931629" rel="nofollow" data-token="fed3da4e33dea81105d45bcebc5a3935">https://blog.csdn.net/zpf336/article/details/80931629</a></p> 
  <p><a href="https://stackoverflow.com/questions/31683536/yarn-error-taskattempt-killed-because-it-ran-on-unusable-node-container-rel" rel="nofollow" data-token="53ad2c0789674f01ab42fbbe9f05b032">https://stackoverflow.com/questions/31683536/yarn-error-taskattempt-killed-because-it-ran-on-unusable-node-container-rel</a></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
