<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Spark-RDD的基本操作(二) | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Spark-RDD的基本操作(二)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="一、上次课回顾 二、RDD常用算子再次实验 三、RDD中join使用深度详解 四、使用Spark-Core进行词频统计剖析 五、RDD中subtract &amp; intersection &amp; cartesian 使用详解 一、上次课回顾 https://blog.csdn.net/zhikanjiani/article/details/97833470 写代码的时候检查是否有action，没有action的话，即使有100个transformation，作业也不会执行。 Spark Application编程时的整个执行流程： 二、RDD常用算子再次实验 scala&gt; nums.map(x =&gt;(x*x)).collect res8: Array[Int] = Array(1, 4, 9, 16, 25, 36, 49, 64, 81) scala&gt; nums.flatMap(x =&gt;(1 to x)).collect res9: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9) scala&gt; nums.flatMap(x =&gt;(1 to x)).reduce(+) res10: Int = 165 作业1：Spark Core读取SequenceFIle文件 **由于某些历史原因：**Hive中有些表是采用SequenceFIle存储，现在你想要使用Spark Core来作为分布式计算框架；Spark SQL是能直接读取的。 三、RDD中join使用深度详解 1、scala&gt; val a = sc.parallelize(Array((&quot;A&quot;,&quot;a1&quot;),(&quot;C&quot;,&quot;c1&quot;),(&quot;D&quot;,&quot;d1&quot;),(&quot;F&quot;,&quot;f1&quot;))) a: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[13] at parallelize at &lt;console&gt;:24 2、scala&gt; val b = sc.parallelize(Array((&quot;A&quot;,&quot;a2&quot;),(&quot;C&quot;,&quot;c2&quot;),(&quot;C&quot;,&quot;c3&quot;),(&quot;E&quot;,&quot;e1&quot;))) b: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24 3、a.join(b).collect //相当于是innerJoin，只返回左右都匹配上的. scala&gt; a.join(b).collect res16: Array[(String, (String, String))] = Array((A,(a1,a2)), (C,(c1,c2)), (C,(c1,c3))) 4、a.leftOuterJoin(b).collect //看返回的数据结构，以a表为主表，去b表匹配；返回左表的所有 res30: Array[(String, (String, Option[String]))] = Array((F,(f1,None)), (D,(d1,None)), (A,(a1,Some(a2))), (C,(c1,Some(c2))), (C,(c1,Some(c3)))) 5、a.rightOuterJoin(b).collect //必然是返回右表的所有 res31: Array[(String, (Option[String], String))] = Array((A,(Some(a1),a2)), (C,(Some(c1),c2)), (C,(Some(c1),c3)), (E,(None,e1))) 6、a.fullOuterJoin(b).collect //全连接 res32: Array[(String, (Option[String], Option[String]))] = Array((F,(Some(f1),None)), (D,(Some(d1),None)), (A,(Some(a1),Some(a2))), (C,(Some(c1),Some(c2))), (C,(Some(c1),Some(c3))), (E,(None,Some(e1)))) 四、使用Spark-Core进行词频统计剖析 1、scala&gt; val log = sc.textFile(&quot;file:///home/hadoop/data/ruozeinput.txt&quot;) log: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/data/ruozeinput.txt MapPartitionsRDD[62] at textFile at &lt;console&gt;:24 第一次打印：Array[String] = Array(hello world john, hello world, hello) 2、scala&gt; log.map( x =&gt; x.split(&quot;\t&quot;)) res33: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[63] at map at &lt;console&gt;:26 第二次打印：Array(Array(hello, world, john), Array(hello, world), Array(hello)) 3、scala&gt; val splits = log.flatMap( x =&gt; x.split(&quot;\t&quot;)) splits: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[66] at flatMap at &lt;console&gt;:25 打印：Array[String] = Array(hello, world, john, hello, world, hello) 4、scala&gt; splits.map(x =&gt;(x,1)).reduceByKey(_+_).collect res41: Array[(String, Int)] = Array((hello,3), (world,2), (john,1)) //这个操作存在shuffle，把相同的key分发到同一个reduce中，把key相加. （hello,1）(hello,1) (hello,1) ==&gt;(hello,3) (world,1) (world,1) ==&gt;(world,2) (john,1) ==&gt;(john,1) 作业2：按照每个单词出现次数做降序排列/升序排列 五、RDD中subtract &amp; intersection &amp; cartesian 使用详解 1、subtract（减去、扣掉） 代码注释：Return an RDD with the elements from `this` that are not in `other`. 在RDD中，两个DF做减法是非常常见的： val a = sc.parallelize(1 to 5) val b = sc.parallelize(2 to 3) a.subtract(b).collect 输出：Array[Int] = Array(4, 1, 5) 2、intersection（交集） 概念：Return the intersection of this RDD and another one. The output will not contain any duplicate a.intersection(b).collect 输出： Array[Int] = Array(2, 3) 3、cartesian（笛卡尔积） 概念：Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in this and b is in other. a.cartesian(b).collect Array[(Int, Int)] = Array((1,2), (2,2), (1,3), (2,3), (3,2), (4,2), (5,2), (3,3), (4,3), (5,3)) Spark-shell适用于测试，开发环境：IDEA + Maven + Scala" />
<meta property="og:description" content="一、上次课回顾 二、RDD常用算子再次实验 三、RDD中join使用深度详解 四、使用Spark-Core进行词频统计剖析 五、RDD中subtract &amp; intersection &amp; cartesian 使用详解 一、上次课回顾 https://blog.csdn.net/zhikanjiani/article/details/97833470 写代码的时候检查是否有action，没有action的话，即使有100个transformation，作业也不会执行。 Spark Application编程时的整个执行流程： 二、RDD常用算子再次实验 scala&gt; nums.map(x =&gt;(x*x)).collect res8: Array[Int] = Array(1, 4, 9, 16, 25, 36, 49, 64, 81) scala&gt; nums.flatMap(x =&gt;(1 to x)).collect res9: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9) scala&gt; nums.flatMap(x =&gt;(1 to x)).reduce(+) res10: Int = 165 作业1：Spark Core读取SequenceFIle文件 **由于某些历史原因：**Hive中有些表是采用SequenceFIle存储，现在你想要使用Spark Core来作为分布式计算框架；Spark SQL是能直接读取的。 三、RDD中join使用深度详解 1、scala&gt; val a = sc.parallelize(Array((&quot;A&quot;,&quot;a1&quot;),(&quot;C&quot;,&quot;c1&quot;),(&quot;D&quot;,&quot;d1&quot;),(&quot;F&quot;,&quot;f1&quot;))) a: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[13] at parallelize at &lt;console&gt;:24 2、scala&gt; val b = sc.parallelize(Array((&quot;A&quot;,&quot;a2&quot;),(&quot;C&quot;,&quot;c2&quot;),(&quot;C&quot;,&quot;c3&quot;),(&quot;E&quot;,&quot;e1&quot;))) b: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24 3、a.join(b).collect //相当于是innerJoin，只返回左右都匹配上的. scala&gt; a.join(b).collect res16: Array[(String, (String, String))] = Array((A,(a1,a2)), (C,(c1,c2)), (C,(c1,c3))) 4、a.leftOuterJoin(b).collect //看返回的数据结构，以a表为主表，去b表匹配；返回左表的所有 res30: Array[(String, (String, Option[String]))] = Array((F,(f1,None)), (D,(d1,None)), (A,(a1,Some(a2))), (C,(c1,Some(c2))), (C,(c1,Some(c3)))) 5、a.rightOuterJoin(b).collect //必然是返回右表的所有 res31: Array[(String, (Option[String], String))] = Array((A,(Some(a1),a2)), (C,(Some(c1),c2)), (C,(Some(c1),c3)), (E,(None,e1))) 6、a.fullOuterJoin(b).collect //全连接 res32: Array[(String, (Option[String], Option[String]))] = Array((F,(Some(f1),None)), (D,(Some(d1),None)), (A,(Some(a1),Some(a2))), (C,(Some(c1),Some(c2))), (C,(Some(c1),Some(c3))), (E,(None,Some(e1)))) 四、使用Spark-Core进行词频统计剖析 1、scala&gt; val log = sc.textFile(&quot;file:///home/hadoop/data/ruozeinput.txt&quot;) log: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/data/ruozeinput.txt MapPartitionsRDD[62] at textFile at &lt;console&gt;:24 第一次打印：Array[String] = Array(hello world john, hello world, hello) 2、scala&gt; log.map( x =&gt; x.split(&quot;\t&quot;)) res33: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[63] at map at &lt;console&gt;:26 第二次打印：Array(Array(hello, world, john), Array(hello, world), Array(hello)) 3、scala&gt; val splits = log.flatMap( x =&gt; x.split(&quot;\t&quot;)) splits: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[66] at flatMap at &lt;console&gt;:25 打印：Array[String] = Array(hello, world, john, hello, world, hello) 4、scala&gt; splits.map(x =&gt;(x,1)).reduceByKey(_+_).collect res41: Array[(String, Int)] = Array((hello,3), (world,2), (john,1)) //这个操作存在shuffle，把相同的key分发到同一个reduce中，把key相加. （hello,1）(hello,1) (hello,1) ==&gt;(hello,3) (world,1) (world,1) ==&gt;(world,2) (john,1) ==&gt;(john,1) 作业2：按照每个单词出现次数做降序排列/升序排列 五、RDD中subtract &amp; intersection &amp; cartesian 使用详解 1、subtract（减去、扣掉） 代码注释：Return an RDD with the elements from `this` that are not in `other`. 在RDD中，两个DF做减法是非常常见的： val a = sc.parallelize(1 to 5) val b = sc.parallelize(2 to 3) a.subtract(b).collect 输出：Array[Int] = Array(4, 1, 5) 2、intersection（交集） 概念：Return the intersection of this RDD and another one. The output will not contain any duplicate a.intersection(b).collect 输出： Array[Int] = Array(2, 3) 3、cartesian（笛卡尔积） 概念：Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in this and b is in other. a.cartesian(b).collect Array[(Int, Int)] = Array((1,2), (2,2), (1,3), (2,3), (3,2), (4,2), (5,2), (3,3), (4,3), (5,3)) Spark-shell适用于测试，开发环境：IDEA + Maven + Scala" />
<link rel="canonical" href="https://uzzz.org/2019/07/31/792467.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/31/792467.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-31T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"一、上次课回顾 二、RDD常用算子再次实验 三、RDD中join使用深度详解 四、使用Spark-Core进行词频统计剖析 五、RDD中subtract &amp; intersection &amp; cartesian 使用详解 一、上次课回顾 https://blog.csdn.net/zhikanjiani/article/details/97833470 写代码的时候检查是否有action，没有action的话，即使有100个transformation，作业也不会执行。 Spark Application编程时的整个执行流程： 二、RDD常用算子再次实验 scala&gt; nums.map(x =&gt;(x*x)).collect res8: Array[Int] = Array(1, 4, 9, 16, 25, 36, 49, 64, 81) scala&gt; nums.flatMap(x =&gt;(1 to x)).collect res9: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9) scala&gt; nums.flatMap(x =&gt;(1 to x)).reduce(+) res10: Int = 165 作业1：Spark Core读取SequenceFIle文件 **由于某些历史原因：**Hive中有些表是采用SequenceFIle存储，现在你想要使用Spark Core来作为分布式计算框架；Spark SQL是能直接读取的。 三、RDD中join使用深度详解 1、scala&gt; val a = sc.parallelize(Array((&quot;A&quot;,&quot;a1&quot;),(&quot;C&quot;,&quot;c1&quot;),(&quot;D&quot;,&quot;d1&quot;),(&quot;F&quot;,&quot;f1&quot;))) a: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[13] at parallelize at &lt;console&gt;:24 2、scala&gt; val b = sc.parallelize(Array((&quot;A&quot;,&quot;a2&quot;),(&quot;C&quot;,&quot;c2&quot;),(&quot;C&quot;,&quot;c3&quot;),(&quot;E&quot;,&quot;e1&quot;))) b: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24 3、a.join(b).collect //相当于是innerJoin，只返回左右都匹配上的. scala&gt; a.join(b).collect res16: Array[(String, (String, String))] = Array((A,(a1,a2)), (C,(c1,c2)), (C,(c1,c3))) 4、a.leftOuterJoin(b).collect //看返回的数据结构，以a表为主表，去b表匹配；返回左表的所有 res30: Array[(String, (String, Option[String]))] = Array((F,(f1,None)), (D,(d1,None)), (A,(a1,Some(a2))), (C,(c1,Some(c2))), (C,(c1,Some(c3)))) 5、a.rightOuterJoin(b).collect //必然是返回右表的所有 res31: Array[(String, (Option[String], String))] = Array((A,(Some(a1),a2)), (C,(Some(c1),c2)), (C,(Some(c1),c3)), (E,(None,e1))) 6、a.fullOuterJoin(b).collect //全连接 res32: Array[(String, (Option[String], Option[String]))] = Array((F,(Some(f1),None)), (D,(Some(d1),None)), (A,(Some(a1),Some(a2))), (C,(Some(c1),Some(c2))), (C,(Some(c1),Some(c3))), (E,(None,Some(e1)))) 四、使用Spark-Core进行词频统计剖析 1、scala&gt; val log = sc.textFile(&quot;file:///home/hadoop/data/ruozeinput.txt&quot;) log: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/data/ruozeinput.txt MapPartitionsRDD[62] at textFile at &lt;console&gt;:24 第一次打印：Array[String] = Array(hello world john, hello world, hello) 2、scala&gt; log.map( x =&gt; x.split(&quot;\\t&quot;)) res33: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[63] at map at &lt;console&gt;:26 第二次打印：Array(Array(hello, world, john), Array(hello, world), Array(hello)) 3、scala&gt; val splits = log.flatMap( x =&gt; x.split(&quot;\\t&quot;)) splits: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[66] at flatMap at &lt;console&gt;:25 打印：Array[String] = Array(hello, world, john, hello, world, hello) 4、scala&gt; splits.map(x =&gt;(x,1)).reduceByKey(_+_).collect res41: Array[(String, Int)] = Array((hello,3), (world,2), (john,1)) //这个操作存在shuffle，把相同的key分发到同一个reduce中，把key相加. （hello,1）(hello,1) (hello,1) ==&gt;(hello,3) (world,1) (world,1) ==&gt;(world,2) (john,1) ==&gt;(john,1) 作业2：按照每个单词出现次数做降序排列/升序排列 五、RDD中subtract &amp; intersection &amp; cartesian 使用详解 1、subtract（减去、扣掉） 代码注释：Return an RDD with the elements from `this` that are not in `other`. 在RDD中，两个DF做减法是非常常见的： val a = sc.parallelize(1 to 5) val b = sc.parallelize(2 to 3) a.subtract(b).collect 输出：Array[Int] = Array(4, 1, 5) 2、intersection（交集） 概念：Return the intersection of this RDD and another one. The output will not contain any duplicate a.intersection(b).collect 输出： Array[Int] = Array(2, 3) 3、cartesian（笛卡尔积） 概念：Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in this and b is in other. a.cartesian(b).collect Array[(Int, Int)] = Array((1,2), (2,2), (1,3), (2,3), (3,2), (4,2), (5,2), (3,3), (4,3), (5,3)) Spark-shell适用于测试，开发环境：IDEA + Maven + Scala","@type":"BlogPosting","url":"https://uzzz.org/2019/07/31/792467.html","headline":"Spark-RDD的基本操作(二)","dateModified":"2019-07-31T00:00:00+08:00","datePublished":"2019-07-31T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/31/792467.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Spark-RDD的基本操作(二)</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p><a href="#id_1" rel="nofollow" target="_self" data-token="88db4525089afde45684f0145683a66f">一、上次课回顾</a><br> <a href="#id_2" rel="nofollow" target="_self" data-token="364fd55241b28164360fbccdfa799324">二、RDD常用算子再次实验</a><br> <a href="#id_3" rel="nofollow" target="_self" data-token="6e3552df7d20bd0d4465f4603eeeb83a">三、RDD中join使用深度详解</a><br> <a href="#id_4" rel="nofollow" target="_self" data-token="4035480e674fbc75e40ba3e01998ec2d">四、使用Spark-Core进行词频统计剖析</a><br> <a href="#id_5" rel="nofollow" target="_self" data-token="73f3b2d0820621a56ae3bc3085dd2780">五、RDD中subtract &amp; intersection &amp; cartesian 使用详解</a></p> 
  <h2><a id="h1_idid_1h1_8"></a></h2>
  <h1 id="id_1">一、上次课回顾</h1> 
  <p><a href="https://blog.csdn.net/zhikanjiani/article/details/97833470" rel="nofollow" data-token="35f546ae34b9103ce5915a1cc6788ec8">https://blog.csdn.net/zhikanjiani/article/details/97833470</a><br> 写代码的时候检查是否有action，没有action的话，即使有100个transformation，作业也不会执行。</p> 
  <p><strong>Spark Application编程时的整个执行流程：</strong><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731121212498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poaWthbmppYW5p,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="h2_idid_2RDDh2_17"></a></h2>
  <h2 id="id_2">二、RDD常用算子再次实验</h2> 
  <p>scala&gt; nums.map(x =&gt;(x*x)).collect<br> res8: Array[Int] = Array(1, 4, 9, 16, 25, 36, 49, 64, 81)</p> 
  <p>scala&gt; nums.flatMap(x =&gt;(1 to x)).collect<br> res9: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9)</p> 
  <p>scala&gt; nums.flatMap(x =&gt;(1 to x)).reduce(<em>+</em>)<br> res10: Int = 165</p> 
  <p><strong>作业1：Spark Core读取SequenceFIle文件</strong></p> 
  <p>**由于某些历史原因：**Hive中有些表是采用SequenceFIle存储，现在你想要使用Spark Core来作为分布式计算框架；Spark SQL是能直接读取的。</p> 
  <h2><a id="h3_idid_3RDDjoinh3_31"></a></h2>
  <h3 id="id_3">三、RDD中join使用深度详解</h3> 
  <pre><code>1、scala&gt; val a = sc.parallelize(Array(("A","a1"),("C","c1"),("D","d1"),("F","f1")))
a: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[13] at parallelize at &lt;console&gt;:24

2、scala&gt; val b = sc.parallelize(Array(("A","a2"),("C","c2"),("C","c3"),("E","e1")))
b: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24

3、a.join(b).collect			//相当于是innerJoin，只返回左右都匹配上的.
scala&gt; a.join(b).collect
res16: Array[(String, (String, String))] = Array((A,(a1,a2)), (C,(c1,c2)), (C,(c1,c3)))

4、a.leftOuterJoin(b).collect		//看返回的数据结构，以a表为主表，去b表匹配；返回左表的所有
res30: Array[(String, (String, Option[String]))] = Array((F,(f1,None)), (D,(d1,None)), (A,(a1,Some(a2))), (C,(c1,Some(c2))), (C,(c1,Some(c3))))

5、a.rightOuterJoin(b).collect			//必然是返回右表的所有
res31: Array[(String, (Option[String], String))] = Array((A,(Some(a1),a2)), (C,(Some(c1),c2)), (C,(Some(c1),c3)), (E,(None,e1)))

6、a.fullOuterJoin(b).collect			//全连接
res32: Array[(String, (Option[String], Option[String]))] = Array((F,(Some(f1),None)), (D,(Some(d1),None)), (A,(Some(a1),Some(a2))), (C,(Some(c1),Some(c2))), (C,(Some(c1),Some(c3))), (E,(None,Some(e1))))

</code></pre> 
  <h2><a id="h4_idid_4SparkCoreh4_55"></a></h2>
  <h4 id="id_4">四、使用Spark-Core进行词频统计剖析</h4> 
  <pre><code>1、scala&gt; val log = sc.textFile("file:///home/hadoop/data/ruozeinput.txt")
log: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/data/ruozeinput.txt MapPartitionsRDD[62] at textFile at &lt;console&gt;:24
第一次打印：Array[String] = Array(hello      world   john, hello     world, hello)

2、scala&gt; log.map( x =&gt; x.split("\t"))
res33: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[63] at map at &lt;console&gt;:26
第二次打印：Array(Array(hello, world, john), Array(hello, world), Array(hello))

3、scala&gt; val splits = log.flatMap( x =&gt; x.split("\t"))
splits: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[66] at flatMap at &lt;console&gt;:25
打印：Array[String] = Array(hello, world, john, hello, world, hello)

4、scala&gt; splits.map(x =&gt;(x,1)).reduceByKey(_+_).collect
res41: Array[(String, Int)] = Array((hello,3), (world,2), (john,1))
//这个操作存在shuffle，把相同的key分发到同一个reduce中，把key相加.

（hello,1）(hello,1) (hello,1)	==&gt;(hello,3)
(world,1) (world,1)		==&gt;(world,2)
(john,1)		==&gt;(john,1)
</code></pre> 
  <p><strong>作业2：按照每个单词出现次数做降序排列/升序排列</strong></p> 
  <h2><a id="h5_idid_5RDDsubtract__intersection__cartesian_h5_81"></a></h2>
  <h5 id="id_5">五、RDD中subtract &amp; intersection &amp; cartesian 使用详解</h5> 
  <p>1、subtract（减去、扣掉）</p> 
  <pre><code>代码注释：Return an RDD with the elements from `this` that are not in `other`.
在RDD中，两个DF做减法是非常常见的：
val a = sc.parallelize(1 to 5)
val b = sc.parallelize(2 to 3)
a.subtract(b).collect
输出：Array[Int] = Array(4, 1, 5)
</code></pre> 
  <p>2、intersection（交集）<br> 概念：Return the intersection of this RDD and another one. The output will not contain any duplicate</p> 
  <pre><code>a.intersection(b).collect
输出： Array[Int] = Array(2, 3)
</code></pre> 
  <p>3、cartesian（笛卡尔积）<br> 概念：Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in <code>this</code> and b is in <code>other</code>.</p> 
  <pre><code>a.cartesian(b).collect
Array[(Int, Int)] = Array((1,2), (2,2), (1,3), (2,3), (3,2), (4,2), (5,2), (3,3), (4,3), (5,3))
</code></pre> 
  <p>Spark-shell适用于测试，开发环境：IDEA + Maven + Scala</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
