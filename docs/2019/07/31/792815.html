<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>聚类（三）k-means++、k-means参数、Mini Batch K-Means | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="聚类（三）k-means++、k-means参数、Mini Batch K-Means" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 一 k-means 介绍 1.1 KMeans介绍 1.2 KMeans() 参数 二 k-means++ 介绍 2.1 算法步骤 三、Mini Batch K-Means 3.1 介绍 有趣的事，Python永远不会缺席 培训说明 聚类（一） https://blog.csdn.net/u010986753/article/details/97821890 聚类（二） https://blog.csdn.net/u010986753/article/details/97885955 聚类（三） https://blog.csdn.net/u010986753/article/details/97916856 一 k-means 介绍 1.1 KMeans介绍 k-means 优缺点： 　1.算法快速、简单; 　2.对大数据集有较高的效率并且是可伸缩性的; 　3.时间复杂度近于线性，而且适合挖掘大规模数据集。K-Means聚类算法的时间复杂度是O(n×k×t) ,其中n代表数据集中对象的数量，t代表着算法迭代的次数，k代表着簇的数目　。计算复杂度在最坏的情况下为 O(n^(k+2/p))，其中n是样本量，p是特征个数。 注 在实践中，k-means算法时非常快的，属于可实践的算法中最快的那一类。但是它的解只是由特定初始值所产生的局部解。所以为了让结果更准确真实，在实践中要用不同的初始值重复几次才可以。 k-means的缺点： 　　1、在 K-means 算法中 K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。 　　2、 在 K-means 算法中，首先需要根据初始聚类中心来确定一个初始划分，然后对初始划分进行优化。这个初始聚类中心的选择对聚类结果有较大的影响，一旦初始值选择的不好，可能无法得到有效的聚类结果，这也成为 K-means算法的一个主要问题。 　　3、从 K-means 算法框架可以看出，该算法需要不断地进行样本分类调整，不断地计算调整后的新的聚类中心，因此当数据量非常大时，算法的时间开销是非常大的。所以需要对算法的时间复杂度进行分析、改进，提高算法应用范围。   对于上述的初始聚类中心的选择可以用**k-means++**来解决 1.2 KMeans() 参数 KMeans( n_clusters=8, init=&#39;k-means++&#39;, n_init=10, max_iter=300, tol=0.0001, precompute_distances=&#39;auto&#39;, verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=&#39;auto&#39;, ) 参数： n_clusters：整形，缺省值=8 【生成的聚类数，即产生的质心（centroids）数。】 max_iter：整形，缺省值=300 ,执行一次k-means算法所进行的最大迭代数。 n_init：整形，缺省值=10 ,用不同的质心初始化值运行算法的次数，最终解是在inertia意义下选出的最优结果。 init：有三个可选值：’k-means++’， ‘random’，或者传递一个ndarray向量。 此参数指定初始化方法，默认值为 ‘k-means++’。 （１）‘k-means++’ 用一种特殊的方法选定初始质心从而能加速迭代过程的收敛（即上文中的k-means++介绍） （２）‘random’ 随机从训练数据中选取初始质心。 （３）如果传递的是一个ndarray，则应该形如 (n_clusters, n_features) 并给出初始质心。 precompute_distances：三个可选值，‘auto’，True 或者 False。 预计算距离，计算速度更快但占用更多内存。 （１）‘auto’：如果 样本数乘以聚类数大于 12million 的话则不预计算距离。This corresponds to about 100MB overhead per job using double precision. （２）True：总是预先计算距离。 （３）False：永远不预先计算距离。 tol：float形，默认值= 1e-4　与inertia结合来确定收敛条件。 n_jobs：整形数。　指定计算所用的进程数。内部原理是同时进行n_init指定次数的计算。 （１）若值为 -1，则用所有的CPU进行运算。若值为1，则不进行并行运算，这样的话方便调试。 （２）若值小于-1，则用到的CPU数为(n_cpus + 1 + n_jobs)。因此如果 n_jobs值为-2，则用到的CPU数为总CPU数减1。 random_state：整形或 numpy.RandomState 类型，可选  用于初始化质心的生成器（generator）。如果值为一个整数，则确定一个seed。此参数默认值为numpy的随机数生成器。 copy_x：布尔型，默认值=True   当我们precomputing distances时，将数据中心化会得到更准确的结果。如果把此参数值设为True，则原始数据不会被改变。如果是False，则会直接在原始数据 上做修改并在函数返回值时将其还原。但是在计算过程中由于有对数据均值的加减运算，所以数据返回后，原始数据和计算前可能会有细小差别。 属性： cluster_centers_：向量，[n_clusters, n_features] (聚类中心的坐标) Labels_: 每个点的分类 inertia_：float形 ,每个点到其簇的质心的距离之和。 Methods： fit(X[,y]): 计算k-means聚类。 fit_predictt(X[,y]): 计算簇质心并给每个样本预测类别。 fit_transform(X[,y])： 计算簇并 transform X to cluster-distance space。 get_params([deep])： 取得估计器的参数。 predict(X):predict(X) 给每个样本估计最接近的簇。 score(X[,y]): 计算聚类误差 set_params(params): 为这个估计器手动设定参数。 transform(X[,y]): 将X转换为群集距离空间。 在新空间中，每个维度都是到集群中心的距离。 请注意，即使X是稀疏的，转换返回的数组通常也是密集的。 二 k-means++ 介绍   k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。 2.1 算法步骤 （1）从输入的数据点集合中随机选择一个点作为第一个聚类中心 （2）对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) （3）选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 （4）重复2和3直到k个聚类中心被选出来 （5）利用这k个初始的聚类中心来运行标准的k-means算法 从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下： （1）先从我们的数据库随机挑个随机点当“种子点” （2）对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。 （3）然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。 （4）重复2和3直到k个聚类中心被选出来 （5）利用这k个初始的聚类中心来运行标准的k-means算法 可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。 三、Mini Batch K-Means 3.1 介绍   在统的K-Means算法中，要计算所有的样本点到所有的质心的距离。如果样本量非常大，比如达到10万以上，特征有100以上，此时用传统的K-Means算法非常的耗时，就算加上elkan K-Means优化也依旧。在大数据时代，这样的场景越来越多。此时Mini Batch K-Means应运而生。   顾名思义，Mini Batch，也就是用样本集中的一部分的样本来做传统的K-Means，这样可以避免样本量太大时的计算难题，算法收敛速度大大加快。当然此时的代价就是我们的聚类的精确度也会有一些降低。一般来说这个降低的幅度在可以接受的范围之内。   在Mini Batch K-Means中，我们会选择一个合适的批样本大小batch size，我们仅仅用batch size个样本来做K-Means聚类。那么这batch size个样本怎么来的？一般是通过无放回的随机采样得到的。   为了增加算法的准确性，我们一般会多跑几次Mini Batch K-Means算法，用得到不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。 有趣的事，Python永远不会缺席 欢迎关注小婷儿的博客 &nbsp;&nbsp;&nbsp;&nbsp;文章内容来源于小婷儿的学习笔记，部分整理自网络，若有侵权或不当之处还请谅解 &nbsp;&nbsp;&nbsp;&nbsp;如需转发，请注明出处：小婷儿的博客python&nbsp;&nbsp;&nbsp;&nbsp;https://www.cnblogs.com/xxtalhr/ 博客园 https://www.cnblogs.com/xxtalhr/ CSDN https://blog.csdn.net/u010986753 有问题请在博客下留言或加作者： &nbsp;&nbsp;&nbsp;&nbsp; 微信：tinghai87605025 联系我加微信群 &nbsp;&nbsp;&nbsp;&nbsp; QQ ：87605025 &nbsp;&nbsp;&nbsp;&nbsp; python QQ交流群：py_data 483766429 培训说明 OCP培训说明连接 https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA OCM培训说明连接 https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA &nbsp;&nbsp;&nbsp;&nbsp; 小婷儿的python正在成长中，其中还有很多不足之处，随着学习和工作的深入，会对以往的博客内容逐步改进和完善哒。重要的事多说几遍。。。。。。" />
<meta property="og:description" content="文章目录 一 k-means 介绍 1.1 KMeans介绍 1.2 KMeans() 参数 二 k-means++ 介绍 2.1 算法步骤 三、Mini Batch K-Means 3.1 介绍 有趣的事，Python永远不会缺席 培训说明 聚类（一） https://blog.csdn.net/u010986753/article/details/97821890 聚类（二） https://blog.csdn.net/u010986753/article/details/97885955 聚类（三） https://blog.csdn.net/u010986753/article/details/97916856 一 k-means 介绍 1.1 KMeans介绍 k-means 优缺点： 　1.算法快速、简单; 　2.对大数据集有较高的效率并且是可伸缩性的; 　3.时间复杂度近于线性，而且适合挖掘大规模数据集。K-Means聚类算法的时间复杂度是O(n×k×t) ,其中n代表数据集中对象的数量，t代表着算法迭代的次数，k代表着簇的数目　。计算复杂度在最坏的情况下为 O(n^(k+2/p))，其中n是样本量，p是特征个数。 注 在实践中，k-means算法时非常快的，属于可实践的算法中最快的那一类。但是它的解只是由特定初始值所产生的局部解。所以为了让结果更准确真实，在实践中要用不同的初始值重复几次才可以。 k-means的缺点： 　　1、在 K-means 算法中 K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。 　　2、 在 K-means 算法中，首先需要根据初始聚类中心来确定一个初始划分，然后对初始划分进行优化。这个初始聚类中心的选择对聚类结果有较大的影响，一旦初始值选择的不好，可能无法得到有效的聚类结果，这也成为 K-means算法的一个主要问题。 　　3、从 K-means 算法框架可以看出，该算法需要不断地进行样本分类调整，不断地计算调整后的新的聚类中心，因此当数据量非常大时，算法的时间开销是非常大的。所以需要对算法的时间复杂度进行分析、改进，提高算法应用范围。   对于上述的初始聚类中心的选择可以用**k-means++**来解决 1.2 KMeans() 参数 KMeans( n_clusters=8, init=&#39;k-means++&#39;, n_init=10, max_iter=300, tol=0.0001, precompute_distances=&#39;auto&#39;, verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=&#39;auto&#39;, ) 参数： n_clusters：整形，缺省值=8 【生成的聚类数，即产生的质心（centroids）数。】 max_iter：整形，缺省值=300 ,执行一次k-means算法所进行的最大迭代数。 n_init：整形，缺省值=10 ,用不同的质心初始化值运行算法的次数，最终解是在inertia意义下选出的最优结果。 init：有三个可选值：’k-means++’， ‘random’，或者传递一个ndarray向量。 此参数指定初始化方法，默认值为 ‘k-means++’。 （１）‘k-means++’ 用一种特殊的方法选定初始质心从而能加速迭代过程的收敛（即上文中的k-means++介绍） （２）‘random’ 随机从训练数据中选取初始质心。 （３）如果传递的是一个ndarray，则应该形如 (n_clusters, n_features) 并给出初始质心。 precompute_distances：三个可选值，‘auto’，True 或者 False。 预计算距离，计算速度更快但占用更多内存。 （１）‘auto’：如果 样本数乘以聚类数大于 12million 的话则不预计算距离。This corresponds to about 100MB overhead per job using double precision. （２）True：总是预先计算距离。 （３）False：永远不预先计算距离。 tol：float形，默认值= 1e-4　与inertia结合来确定收敛条件。 n_jobs：整形数。　指定计算所用的进程数。内部原理是同时进行n_init指定次数的计算。 （１）若值为 -1，则用所有的CPU进行运算。若值为1，则不进行并行运算，这样的话方便调试。 （２）若值小于-1，则用到的CPU数为(n_cpus + 1 + n_jobs)。因此如果 n_jobs值为-2，则用到的CPU数为总CPU数减1。 random_state：整形或 numpy.RandomState 类型，可选  用于初始化质心的生成器（generator）。如果值为一个整数，则确定一个seed。此参数默认值为numpy的随机数生成器。 copy_x：布尔型，默认值=True   当我们precomputing distances时，将数据中心化会得到更准确的结果。如果把此参数值设为True，则原始数据不会被改变。如果是False，则会直接在原始数据 上做修改并在函数返回值时将其还原。但是在计算过程中由于有对数据均值的加减运算，所以数据返回后，原始数据和计算前可能会有细小差别。 属性： cluster_centers_：向量，[n_clusters, n_features] (聚类中心的坐标) Labels_: 每个点的分类 inertia_：float形 ,每个点到其簇的质心的距离之和。 Methods： fit(X[,y]): 计算k-means聚类。 fit_predictt(X[,y]): 计算簇质心并给每个样本预测类别。 fit_transform(X[,y])： 计算簇并 transform X to cluster-distance space。 get_params([deep])： 取得估计器的参数。 predict(X):predict(X) 给每个样本估计最接近的簇。 score(X[,y]): 计算聚类误差 set_params(params): 为这个估计器手动设定参数。 transform(X[,y]): 将X转换为群集距离空间。 在新空间中，每个维度都是到集群中心的距离。 请注意，即使X是稀疏的，转换返回的数组通常也是密集的。 二 k-means++ 介绍   k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。 2.1 算法步骤 （1）从输入的数据点集合中随机选择一个点作为第一个聚类中心 （2）对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) （3）选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 （4）重复2和3直到k个聚类中心被选出来 （5）利用这k个初始的聚类中心来运行标准的k-means算法 从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下： （1）先从我们的数据库随机挑个随机点当“种子点” （2）对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。 （3）然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。 （4）重复2和3直到k个聚类中心被选出来 （5）利用这k个初始的聚类中心来运行标准的k-means算法 可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。 三、Mini Batch K-Means 3.1 介绍   在统的K-Means算法中，要计算所有的样本点到所有的质心的距离。如果样本量非常大，比如达到10万以上，特征有100以上，此时用传统的K-Means算法非常的耗时，就算加上elkan K-Means优化也依旧。在大数据时代，这样的场景越来越多。此时Mini Batch K-Means应运而生。   顾名思义，Mini Batch，也就是用样本集中的一部分的样本来做传统的K-Means，这样可以避免样本量太大时的计算难题，算法收敛速度大大加快。当然此时的代价就是我们的聚类的精确度也会有一些降低。一般来说这个降低的幅度在可以接受的范围之内。   在Mini Batch K-Means中，我们会选择一个合适的批样本大小batch size，我们仅仅用batch size个样本来做K-Means聚类。那么这batch size个样本怎么来的？一般是通过无放回的随机采样得到的。   为了增加算法的准确性，我们一般会多跑几次Mini Batch K-Means算法，用得到不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。 有趣的事，Python永远不会缺席 欢迎关注小婷儿的博客 &nbsp;&nbsp;&nbsp;&nbsp;文章内容来源于小婷儿的学习笔记，部分整理自网络，若有侵权或不当之处还请谅解 &nbsp;&nbsp;&nbsp;&nbsp;如需转发，请注明出处：小婷儿的博客python&nbsp;&nbsp;&nbsp;&nbsp;https://www.cnblogs.com/xxtalhr/ 博客园 https://www.cnblogs.com/xxtalhr/ CSDN https://blog.csdn.net/u010986753 有问题请在博客下留言或加作者： &nbsp;&nbsp;&nbsp;&nbsp; 微信：tinghai87605025 联系我加微信群 &nbsp;&nbsp;&nbsp;&nbsp; QQ ：87605025 &nbsp;&nbsp;&nbsp;&nbsp; python QQ交流群：py_data 483766429 培训说明 OCP培训说明连接 https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA OCM培训说明连接 https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA &nbsp;&nbsp;&nbsp;&nbsp; 小婷儿的python正在成长中，其中还有很多不足之处，随着学习和工作的深入，会对以往的博客内容逐步改进和完善哒。重要的事多说几遍。。。。。。" />
<link rel="canonical" href="https://uzzz.org/2019/07/31/792815.html" />
<meta property="og:url" content="https://uzzz.org/2019/07/31/792815.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-31T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 一 k-means 介绍 1.1 KMeans介绍 1.2 KMeans() 参数 二 k-means++ 介绍 2.1 算法步骤 三、Mini Batch K-Means 3.1 介绍 有趣的事，Python永远不会缺席 培训说明 聚类（一） https://blog.csdn.net/u010986753/article/details/97821890 聚类（二） https://blog.csdn.net/u010986753/article/details/97885955 聚类（三） https://blog.csdn.net/u010986753/article/details/97916856 一 k-means 介绍 1.1 KMeans介绍 k-means 优缺点： 　1.算法快速、简单; 　2.对大数据集有较高的效率并且是可伸缩性的; 　3.时间复杂度近于线性，而且适合挖掘大规模数据集。K-Means聚类算法的时间复杂度是O(n×k×t) ,其中n代表数据集中对象的数量，t代表着算法迭代的次数，k代表着簇的数目　。计算复杂度在最坏的情况下为 O(n^(k+2/p))，其中n是样本量，p是特征个数。 注 在实践中，k-means算法时非常快的，属于可实践的算法中最快的那一类。但是它的解只是由特定初始值所产生的局部解。所以为了让结果更准确真实，在实践中要用不同的初始值重复几次才可以。 k-means的缺点： 　　1、在 K-means 算法中 K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。 　　2、 在 K-means 算法中，首先需要根据初始聚类中心来确定一个初始划分，然后对初始划分进行优化。这个初始聚类中心的选择对聚类结果有较大的影响，一旦初始值选择的不好，可能无法得到有效的聚类结果，这也成为 K-means算法的一个主要问题。 　　3、从 K-means 算法框架可以看出，该算法需要不断地进行样本分类调整，不断地计算调整后的新的聚类中心，因此当数据量非常大时，算法的时间开销是非常大的。所以需要对算法的时间复杂度进行分析、改进，提高算法应用范围。   对于上述的初始聚类中心的选择可以用**k-means++**来解决 1.2 KMeans() 参数 KMeans( n_clusters=8, init=&#39;k-means++&#39;, n_init=10, max_iter=300, tol=0.0001, precompute_distances=&#39;auto&#39;, verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=&#39;auto&#39;, ) 参数： n_clusters：整形，缺省值=8 【生成的聚类数，即产生的质心（centroids）数。】 max_iter：整形，缺省值=300 ,执行一次k-means算法所进行的最大迭代数。 n_init：整形，缺省值=10 ,用不同的质心初始化值运行算法的次数，最终解是在inertia意义下选出的最优结果。 init：有三个可选值：’k-means++’， ‘random’，或者传递一个ndarray向量。 此参数指定初始化方法，默认值为 ‘k-means++’。 （１）‘k-means++’ 用一种特殊的方法选定初始质心从而能加速迭代过程的收敛（即上文中的k-means++介绍） （２）‘random’ 随机从训练数据中选取初始质心。 （３）如果传递的是一个ndarray，则应该形如 (n_clusters, n_features) 并给出初始质心。 precompute_distances：三个可选值，‘auto’，True 或者 False。 预计算距离，计算速度更快但占用更多内存。 （１）‘auto’：如果 样本数乘以聚类数大于 12million 的话则不预计算距离。This corresponds to about 100MB overhead per job using double precision. （２）True：总是预先计算距离。 （３）False：永远不预先计算距离。 tol：float形，默认值= 1e-4　与inertia结合来确定收敛条件。 n_jobs：整形数。　指定计算所用的进程数。内部原理是同时进行n_init指定次数的计算。 （１）若值为 -1，则用所有的CPU进行运算。若值为1，则不进行并行运算，这样的话方便调试。 （２）若值小于-1，则用到的CPU数为(n_cpus + 1 + n_jobs)。因此如果 n_jobs值为-2，则用到的CPU数为总CPU数减1。 random_state：整形或 numpy.RandomState 类型，可选  用于初始化质心的生成器（generator）。如果值为一个整数，则确定一个seed。此参数默认值为numpy的随机数生成器。 copy_x：布尔型，默认值=True   当我们precomputing distances时，将数据中心化会得到更准确的结果。如果把此参数值设为True，则原始数据不会被改变。如果是False，则会直接在原始数据 上做修改并在函数返回值时将其还原。但是在计算过程中由于有对数据均值的加减运算，所以数据返回后，原始数据和计算前可能会有细小差别。 属性： cluster_centers_：向量，[n_clusters, n_features] (聚类中心的坐标) Labels_: 每个点的分类 inertia_：float形 ,每个点到其簇的质心的距离之和。 Methods： fit(X[,y]): 计算k-means聚类。 fit_predictt(X[,y]): 计算簇质心并给每个样本预测类别。 fit_transform(X[,y])： 计算簇并 transform X to cluster-distance space。 get_params([deep])： 取得估计器的参数。 predict(X):predict(X) 给每个样本估计最接近的簇。 score(X[,y]): 计算聚类误差 set_params(params): 为这个估计器手动设定参数。 transform(X[,y]): 将X转换为群集距离空间。 在新空间中，每个维度都是到集群中心的距离。 请注意，即使X是稀疏的，转换返回的数组通常也是密集的。 二 k-means++ 介绍   k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。 2.1 算法步骤 （1）从输入的数据点集合中随机选择一个点作为第一个聚类中心 （2）对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) （3）选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 （4）重复2和3直到k个聚类中心被选出来 （5）利用这k个初始的聚类中心来运行标准的k-means算法 从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下： （1）先从我们的数据库随机挑个随机点当“种子点” （2）对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。 （3）然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。 （4）重复2和3直到k个聚类中心被选出来 （5）利用这k个初始的聚类中心来运行标准的k-means算法 可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。 三、Mini Batch K-Means 3.1 介绍   在统的K-Means算法中，要计算所有的样本点到所有的质心的距离。如果样本量非常大，比如达到10万以上，特征有100以上，此时用传统的K-Means算法非常的耗时，就算加上elkan K-Means优化也依旧。在大数据时代，这样的场景越来越多。此时Mini Batch K-Means应运而生。   顾名思义，Mini Batch，也就是用样本集中的一部分的样本来做传统的K-Means，这样可以避免样本量太大时的计算难题，算法收敛速度大大加快。当然此时的代价就是我们的聚类的精确度也会有一些降低。一般来说这个降低的幅度在可以接受的范围之内。   在Mini Batch K-Means中，我们会选择一个合适的批样本大小batch size，我们仅仅用batch size个样本来做K-Means聚类。那么这batch size个样本怎么来的？一般是通过无放回的随机采样得到的。   为了增加算法的准确性，我们一般会多跑几次Mini Batch K-Means算法，用得到不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。 有趣的事，Python永远不会缺席 欢迎关注小婷儿的博客 &nbsp;&nbsp;&nbsp;&nbsp;文章内容来源于小婷儿的学习笔记，部分整理自网络，若有侵权或不当之处还请谅解 &nbsp;&nbsp;&nbsp;&nbsp;如需转发，请注明出处：小婷儿的博客python&nbsp;&nbsp;&nbsp;&nbsp;https://www.cnblogs.com/xxtalhr/ 博客园 https://www.cnblogs.com/xxtalhr/ CSDN https://blog.csdn.net/u010986753 有问题请在博客下留言或加作者： &nbsp;&nbsp;&nbsp;&nbsp; 微信：tinghai87605025 联系我加微信群 &nbsp;&nbsp;&nbsp;&nbsp; QQ ：87605025 &nbsp;&nbsp;&nbsp;&nbsp; python QQ交流群：py_data 483766429 培训说明 OCP培训说明连接 https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA OCM培训说明连接 https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA &nbsp;&nbsp;&nbsp;&nbsp; 小婷儿的python正在成长中，其中还有很多不足之处，随着学习和工作的深入，会对以往的博客内容逐步改进和完善哒。重要的事多说几遍。。。。。。","@type":"BlogPosting","url":"https://uzzz.org/2019/07/31/792815.html","headline":"聚类（三）k-means++、k-means参数、Mini Batch K-Means","dateModified":"2019-07-31T00:00:00+08:00","datePublished":"2019-07-31T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/07/31/792815.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>聚类（三）k-means++、k-means参数、Mini Batch K-Means</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <li><a href="#_kmeans__4" rel="nofollow" data-token="74356e25680c588a21f95efd05b25b84">一 k-means 介绍</a></li>
    <ul>
     <li><a href="#11_KMeans_5" rel="nofollow" data-token="7a105f560c9256ae6ce2229473c6d960">1.1 KMeans介绍</a></li>
     <li><a href="#12_KMeans__16" rel="nofollow" data-token="dfaa85b12f22a7df5f961cc203c074c3">1.2 KMeans() 参数</a></li>
    </ul>
    <li><a href="#_kmeans__74" rel="nofollow" data-token="28f4d25092a3929130cfb6d2cccfd228">二 k-means++ 介绍</a></li>
    <ul>
     <li><a href="#21__76" rel="nofollow" data-token="01476021200bdcd3bb0ffaa0d4f82fdb">2.1 算法步骤</a></li>
    </ul>
    <li><a href="#Mini_Batch_KMeans_90" rel="nofollow" data-token="ca07b812c9397a95800e73a1168acebd">三、Mini Batch K-Means</a></li>
    <ul>
     <li><a href="#31__91" rel="nofollow" data-token="5e5c42d0cd8221fcf0f6943ec23c8198">3.1 介绍</a></li>
    </ul>
    <li><a href="#Python_102" rel="nofollow" data-token="a3b3053a177b078447343ee0f1284f11">有趣的事，Python永远不会缺席</a></li>
    <li><a href="#strongstrongbr_116" rel="nofollow" data-token="cd5c86689eb2a19de36bdfbd838c9fc8"><strong>培训说明</strong><br></a></li>
   </ul>
  </div>
  <br> 聚类（一）
  <a href="https://blog.csdn.net/u010986753/article/details/97821890" rel="nofollow" data-token="30320e91f391770d8e214b70ab4a47ac">https://blog.csdn.net/u010986753/article/details/97821890</a>
  <br> 聚类（二）
  <a href="https://blog.csdn.net/u010986753/article/details/97885955" rel="nofollow" data-token="114e297cf64a31ed687e9231ed6b702d">https://blog.csdn.net/u010986753/article/details/97885955</a>
  <br> 聚类（三）
  <a href="https://blog.csdn.net/u010986753/article/details/97916856" rel="nofollow" data-token="25fbbf31a7f1ffd6db752badc2ac7040">https://blog.csdn.net/u010986753/article/details/97916856</a>
  <p></p> 
  <h1><a id="_kmeans__4"></a>一 k-means 介绍</h1> 
  <h2><a id="11_KMeans_5"></a>1.1 KMeans介绍</h2> 
  <p><strong>k-means 优缺点：</strong><br> 　1.算法快速、简单;<br> 　2.对大数据集有较高的效率并且是可伸缩性的;<br> 　3.时间复杂度近于线性，而且适合挖掘大规模数据集。K-Means聚类算法的时间复杂度是O(n×k×t) ,其中n代表数据集中对象的数量，t代表着算法迭代的次数，k代表着簇的数目　。计算复杂度在最坏的情况下为 O(n^(k+2/p))，其中n是样本量，p是特征个数。<br> <strong>注</strong> 在实践中，k-means算法时非常快的，属于可实践的算法中最快的那一类。但是它的解只是由特定初始值所产生的局部解。所以为了让结果更准确真实，在实践中要用不同的初始值重复几次才可以。<br> <strong>k-means的缺点：</strong><br> 　　1、在 K-means 算法中 K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。<br> 　　2、 在 K-means 算法中，首先需要根据初始聚类中心来确定一个初始划分，然后对初始划分进行优化。这个初始聚类中心的选择对聚类结果有较大的影响，一旦初始值选择的不好，可能无法得到有效的聚类结果，这也成为 K-means算法的一个主要问题。<br> 　　3、从 K-means 算法框架可以看出，该算法需要不断地进行样本分类调整，不断地计算调整后的新的聚类中心，因此当数据量非常大时，算法的时间开销是非常大的。所以需要对算法的时间复杂度进行分析、改进，提高算法应用范围。<br>   对于上述的初始聚类中心的选择可以用**k-means++**来解决</p> 
  <h2><a id="12_KMeans__16"></a>1.2 KMeans() 参数</h2> 
  <pre><code class="prism language-python">KMeans<span class="token punctuation">(</span>
    n_clusters<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    init<span class="token operator">=</span><span class="token string">'k-means++'</span><span class="token punctuation">,</span>
    n_init<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
    max_iter<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span>
    tol<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span>
    precompute_distances<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    random_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    copy_x<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    n_jobs<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    algorithm<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre> 
  <p><strong>参数</strong>：</p> 
  <p><strong>n_clusters</strong>：整形，缺省值=8 【生成的聚类数，即产生的质心（centroids）数。】<br> <strong>max_iter</strong>：整形，缺省值=300 ,执行一次k-means算法所进行的最大迭代数。<br> <strong>n_init</strong>：整形，缺省值=10 ,用不同的质心初始化值运行算法的次数，最终解是在inertia意义下选出的最优结果。<br> <strong>init</strong>：有三个可选值：’k-means++’， ‘random’，或者传递一个ndarray向量。<br> 此参数指定初始化方法，默认值为 ‘k-means++’。<br> （１）‘k-means++’ 用一种特殊的方法选定初始质心从而能加速迭代过程的收敛（即上文中的k-means++介绍）<br> （２）‘random’ 随机从训练数据中选取初始质心。<br> （３）如果传递的是一个ndarray，则应该形如 (n_clusters, n_features) 并给出初始质心。<br> <strong>precompute_distances</strong>：三个可选值，‘auto’，True 或者 False。<br> 预计算距离，计算速度更快但占用更多内存。<br> （１）‘auto’：如果 样本数乘以聚类数大于 12million 的话则不预计算距离。This corresponds to about 100MB overhead per job using double precision.<br> （２）True：总是预先计算距离。<br> （３）False：永远不预先计算距离。<br> <strong>tol</strong>：float形，默认值= 1e-4　与inertia结合来确定收敛条件。<br> <strong>n_jobs</strong>：整形数。　指定计算所用的进程数。内部原理是同时进行n_init指定次数的计算。<br> （１）若值为 -1，则用所有的CPU进行运算。若值为1，则不进行并行运算，这样的话方便调试。<br> （２）若值小于-1，则用到的CPU数为(n_cpus + 1 + n_jobs)。因此如果 n_jobs值为-2，则用到的CPU数为总CPU数减1。<br> <strong>random_state</strong>：整形或 numpy.RandomState 类型，可选<br>  用于初始化质心的生成器（generator）。如果值为一个整数，则确定一个seed。此参数默认值为numpy的随机数生成器。<br> <strong>copy_x</strong>：布尔型，默认值=True<br>   当我们precomputing distances时，将数据中心化会得到更准确的结果。如果把此参数值设为True，则原始数据不会被改变。如果是False，则会直接在原始数据 上做修改并在函数返回值时将其还原。但是在计算过程中由于有对数据均值的加减运算，所以数据返回后，原始数据和计算前可能会有细小差别。</p> 
  <p><strong>属性：</strong></p> 
  <p><strong>cluster_centers_</strong>：向量，[n_clusters, n_features] (聚类中心的坐标)<br> <strong>Labels_</strong>: 每个点的分类<br> <strong>inertia_</strong>：float形 ,每个点到其簇的质心的距离之和。</p> 
  <p><strong>Methods：</strong></p> 
  <p><strong>fit(X[,y])</strong>: 计算k-means聚类。<br> <strong>fit_predictt(X[,y])</strong>: 计算簇质心并给每个样本预测类别。<br> <strong>fit_transform(X[,y])</strong>： 计算簇并 transform X to cluster-distance space。<br> <strong>get_params([deep])</strong>： 取得估计器的参数。<br> <strong>predict(X):predict(X)</strong> 给每个样本估计最接近的簇。<br> <strong>score(X[,y])</strong>: 计算聚类误差<br> <strong>set_params(params)</strong>: 为这个估计器手动设定参数。<br> <strong>transform(X[,y])</strong>: 将X转换为群集距离空间。 在新空间中，每个维度都是到集群中心的距离。 请注意，即使X是稀疏的，转换返回的数组通常也是密集的。</p> 
  <h1><a id="_kmeans__74"></a>二 k-means++ 介绍</h1> 
  <p>  k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。</p> 
  <h2><a id="21__76"></a>2.1 算法步骤</h2> 
  <p>（1）从输入的数据点集合中随机选择一个点作为第一个聚类中心<br> （2）对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)<br> （3）选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大<br> （4）重复2和3直到k个聚类中心被选出来<br> （5）利用这k个初始的聚类中心来运行标准的k-means算法<br> <strong>从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下：</strong><br> （1）先从我们的数据库随机挑个随机点当“种子点”<br> （2）对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。<br> （3）然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。<br> （4）重复2和3直到k个聚类中心被选出来<br> （5）利用这k个初始的聚类中心来运行标准的k-means算法<br> 可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。</p> 
  <h1><a id="Mini_Batch_KMeans_90"></a>三、Mini Batch K-Means</h1> 
  <h2><a id="31__91"></a>3.1 介绍</h2> 
  <p>  在统的K-Means算法中，要计算所有的样本点到所有的质心的距离。如果样本量非常大，比如达到10万以上，特征有100以上，此时用传统的K-Means算法非常的耗时，就算加上elkan K-Means优化也依旧。在大数据时代，这样的场景越来越多。此时Mini Batch K-Means应运而生。</p> 
  <p>  顾名思义，Mini Batch，也就是用样本集中的一部分的样本来做传统的K-Means，这样可以避免样本量太大时的计算难题，算法收敛速度大大加快。当然此时的代价就是我们的聚类的精确度也会有一些降低。一般来说这个降低的幅度在可以接受的范围之内。</p> 
  <p>  在Mini Batch K-Means中，我们会选择一个合适的批样本大小batch size，我们仅仅用batch size个样本来做K-Means聚类。那么这batch size个样本怎么来的？一般是通过无放回的随机采样得到的。</p> 
  <p>  为了增加算法的准确性，我们一般会多跑几次Mini Batch K-Means算法，用得到不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。</p> 
  <h1><a id="Python_102"></a>有趣的事，Python永远不会缺席</h1> 
  <p><strong>欢迎关注小婷儿的博客</strong><br><br> &nbsp;&nbsp;&nbsp;&nbsp;文章内容来源于小婷儿的学习笔记，部分整理自网络，若有侵权或不当之处还请谅解</p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;如需转发，请注明出处：小婷儿的博客python&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.cnblogs.com/xxtalhr/" rel="nofollow" data-token="b2d66365a5a05d702f64ceacc4a870f2">https://www.cnblogs.com/xxtalhr/</a><br><br> <strong>博客园</strong> <a href="https://www.cnblogs.com/xxtalhr/" rel="nofollow" data-token="b2d66365a5a05d702f64ceacc4a870f2">https://www.cnblogs.com/xxtalhr/</a><br><br> <strong>CSDN</strong> <a href="https://blog.csdn.net/u010986753" rel="nofollow" data-token="2ca348ec74b768eaa589944a275cbc1d">https://blog.csdn.net/u010986753</a><br></p> 
  <p><strong>有问题请在博客下留言或加作者：</strong><br> &nbsp;&nbsp;&nbsp;&nbsp; 微信：tinghai87605025 联系我加微信群<br> &nbsp;&nbsp;&nbsp;&nbsp; QQ ：87605025<br> &nbsp;&nbsp;&nbsp;&nbsp; python QQ交流群：py_data 483766429</p> 
  <center>
   <img src="https://img2018.cnblogs.com/blog/1400528/201905/1400528-20190519213410020-442219723.png"> 
  </center>
  <h1><a id="strongstrongbr_116"></a><strong>培训说明</strong><br></h1> 
  <p><strong>OCP培训说明连接</strong> <a href="https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA" rel="nofollow">https://mp.weixin.qq.com/s/2cymJ4xiBPtTaHu16HkiuA</a><br><br> <strong>OCM培训说明连接</strong> <a href="https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA" rel="nofollow" data-token="8e391eaa7d1f20236a12d1be6f0a645f">https://mp.weixin.qq.com/s/7-R6Cz8RcJKduVv6YlAxJA</a><br></p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://blog.csdn.net/u010986753" rel="nofollow" data-token="2ca348ec74b768eaa589944a275cbc1d">小婷儿的python</a>正在成长中，其中还有很多不足之处，随着学习和工作的深入，会对以往的博客内容逐步改进和完善哒。重要的事多说几遍。。。。。。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
