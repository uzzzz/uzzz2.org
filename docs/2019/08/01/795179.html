<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>kafka 相关笔记 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="kafka 相关笔记" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="kafka 设计概要 &nbsp;&nbsp; &nbsp;吞吐量/延时 &nbsp;&nbsp; &nbsp;消息持久化 &nbsp;&nbsp; &nbsp;负载均衡和故障转移 &nbsp;&nbsp; &nbsp;伸缩性 &nbsp; 一些常用命令通过GetOffsetShell 工具类查看 &nbsp;topic &nbsp; 分区消息 ./kafka-run-class.sh &nbsp;kafka.tools.GetOffsetShell &nbsp; --broker-list &nbsp;192.168.2.52:9092 &nbsp;--topic &nbsp;test查看当前topic 当前对应的信息&nbsp; ./kafka-topics.sh &nbsp;--zookeeper &nbsp;192.168.2.52:2181 &nbsp; --describe &nbsp; --topic &nbsp; test1 &nbsp;&nbsp; &nbsp;Topic: test1&nbsp;&nbsp; &nbsp;Partition: 0&nbsp;&nbsp; &nbsp;Leader: 0&nbsp;&nbsp; &nbsp;Replicas: 0&nbsp;&nbsp; &nbsp;Isr: 0 &nbsp;&nbsp; &nbsp;Topic: test1&nbsp;&nbsp; &nbsp;Partition: 1&nbsp;&nbsp; &nbsp;Leader: 0&nbsp;&nbsp; &nbsp;Replicas: 0&nbsp;&nbsp; &nbsp;Isr: 0修改kafka分区个数 ./kafka-topics.sh &nbsp;-alter --zookeeper &nbsp;192.168.2.52:2181 --partitions 2 &nbsp;--topic &nbsp;test给现有topic 添加新的配置 &nbsp; 添加压实配置 ./kafka-configs.sh &nbsp; --zookeeper 192.168.2.52:2181 --alter &nbsp;--entity-type topics --entity-name test &nbsp;--add-config cleanup.policy=compact查看当前topic 的配置&nbsp; ./kafka-configs.sh &nbsp;--zookeeper &nbsp;192.168.2.52:2181 &nbsp;--describe &nbsp;--entity-type topics &nbsp;--entity-name &nbsp;test对topic 进行生产者消息测试 ./kafka-producer-perf-test.sh &nbsp; --topic topic1 --throughput &nbsp;-1 &nbsp;--num-records 50000 &nbsp;--record-size 100 &nbsp;--producer-props &nbsp;bootstrap.servers=192.168.2.52:9092 &nbsp;acks=1查看当前存在的消费者组 ./kafka-consumer-groups.sh &nbsp; --bootstrap-server &nbsp;192.168.2.52:9092 &nbsp;--list查看当前消费者组的信息&nbsp; ./kafka-consumer-groups.sh &nbsp; &nbsp; &nbsp; --bootstrap-server &nbsp;192.168.2.52:9092 --describe --group &nbsp;topic1group重置当前消费者组的offset ./kafka-consumer-groups.sh &nbsp; &nbsp;--bootstrap-server &nbsp;192.168.2.52:9092 --group topic1group &nbsp;--reset-off sets --all-topics &nbsp;--to-earliest --execute&nbsp; &nbsp; kafka rebalance 触发条件 消费者新增或者 移除 topic &nbsp;分区的新增或减少 Kafka多线程消费两种方式 1 每个线程维护一个kafkaConsumer 2 单个kafkaConsumer 实例 一个线程对应一个分区 两种方式的优缺点 方法1 (每个线程维护专属 Ka fkaConsumer )&nbsp;&nbsp; &nbsp; 实现简单；速度较快，因为无线程 间交互开销：方便位移管理；易于 维护分区间的消息消费顺序&nbsp;&nbsp; &nbsp; Socket连接开销大；consumer数受限于topic分区 数，扩展性差；broker端处理负载高（因为发往 broker的请求数多）；rebalance可能性增大 方法2 (全局consumer多 worker 线程）&nbsp;&nbsp; &nbsp; 消息获取与处理解耦：可独立扩展 consumer数和woricer数，伸缩性 好&nbsp;&nbsp; &nbsp; 实现负载：难于维护分区内的消息顺序；处理链路 变长，导致位移管理困难；worker线程异常可能导 致消费数据丢失 写入消息的时候， 什么情况下 导致HW 会和LEO 不同步呢 follwer请求速度追不上，leader的接受消息的速度， 可能原因 &nbsp;follwer 副本所在的broker 的网络I/O开销过大导致备份消息的速度满于 leader处获取的消息速度 进程卡住： follwer在一段时间内无法向leader 请求数据， 比如之前提到的频繁gc 或者程序bug LEO &nbsp; log end offset &nbsp;日志末端唯一， 代表kafka 当前分区存了多少条数据， 12代表存在11条数据 HW &nbsp; 高水印 &nbsp; &nbsp; &nbsp; &nbsp; 代表 这个kafka &nbsp;consumer 可见的 ，当前能够读取的 &nbsp;最大offset ， &nbsp;8 代表kafka消费者最多可以获取到7条数据 HW &nbsp;必须小于或者等于 &nbsp;LEO &nbsp; 代表所有消息，已提交，或者已备份 分区中的HW &nbsp;为 ISR 中所有副本LEO 的最小值 log compaction &nbsp;日志压实(必须设置key) &nbsp; 确保kafka topic 每个分区中，具有相同的key的消息， 至少会保存最新的value 的消息&nbsp; _consumer_offsets 就使用了日志压实的策略， &nbsp; &nbsp; 最会保存(groupId + topic &nbsp;+ 分区号)最新的 &nbsp; broker 中的controller 的作用 更新集群元数据信息 创建topic &nbsp;删除topic &nbsp;分区重新分配 prefered leader 副本选举 topic 分区扩展， broker 加入集群 ， broker 崩溃&nbsp; 受控关闭， controller leader 选举&nbsp; 生产者如何实现，消息的精确一次处理的语义呢， 从kafka 0.11.0.0后 kafka提供了幂等性， enable.idempotence=true &nbsp; kafka调优方向 1 吞吐量 2 &nbsp;延时 3 &nbsp;持久性 4 可用性 OS 级别的错误 &nbsp; connection refused &nbsp; too &nbsp;many open &nbsp;files&nbsp; address in &nbsp;use connect &nbsp; 解决方案 ，基础设置 1 &nbsp;禁止atime(最新的访问时间) 更新 &nbsp; 可以使用 &nbsp;mount &nbsp;-o &nbsp;noatime 2 &nbsp;可以通过禁止记日志操作， 的方式来提高写的速度， commit=N_secs &nbsp;该选项是 &nbsp;没N秒 同步一次数据和元数据 ， &nbsp;默认是5秒。 如果该值设的比较小， 可以减少崩溃发生时带来的数据丢失， 但若设置的较大， 则会提升整体的吞吐量，以及降低延时， 生产环境 推荐用户设置一个较大的值 1-2分钟 swapiness 设置 2 设置swapiness &nbsp; linux 默认为 60 s &nbsp; 一般建议设置为 &nbsp;0 &nbsp;禁用来提升 内存的使用率，&nbsp; 这里存在一个问题， &nbsp;当内存用完后， linux &nbsp;OOM Killer &nbsp;回去杀掉一个进程， 这样可能会导致数据丢失， 坐着建议， 设置为 &nbsp; 1-10 之间，&nbsp; jvm &nbsp; 设置&nbsp; 由于kafka 并没有大量使用堆上内存 &nbsp;所以堆内存设置为 6G，差不多可以满足 -Xmx6g -Xms6g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize—128m -XX:+UseG1GC -XX:MaxGCPauseMillis— 20 -XX: InitiatingHeapOccupancyPercent 35 -XX: GlHeapRegionSize—16M -XX: MinMetaspaceFreeRatio—50 -XX: MaxMetaspaceFreeRatio=85 设置 单个jvm 最大支持的线程数 &nbsp;vm.max_map_count &nbsp; 65530调优吞吐量， 1producer &nbsp;设置 &nbsp;batch.size &nbsp;和linger.ms &nbsp;通常情况下， 会提升producer 端的TPS 会增加了 消息的延时性， 2合理设置分区个数， 分区的增大到某个数值，后由于锁竞争，和内存占用过多 降低tps, 在实际中， 可以用两倍数来逐步测试 ，直到性能拐点 3设置compression.type &nbsp; 对数据进行压缩，降低I/O的方式来提高tps ,压缩的话主要时针对batch的，&nbsp; kafka 支持的压缩格式又， gzip ,snappy,lz4 &nbsp; 相比来说，lz4组合的性能最好， 4 设置acks 参数， 当参数为1 代表leaderx写入底层文件系统即返回，无须等待follower 响应，acks=0, &nbsp; 表示 producer 端不理会 broker 端的响应， &nbsp;会提升tps 但是牺牲了消息的持久化，&nbsp; 5 &nbsp;当 producer 端 &nbsp;retries 参数设置的值，进行指定次数的重试，如果消息能够忍受偶尔的数据丢失， 可以将 retries 设置为0 ， tps 会有提升， 但是 消息发生失败后， 就不会重试，&nbsp; 6 &nbsp;max.block.ms &nbsp; 当producer 端的缓冲区被填满后， &nbsp;producer &nbsp;将会进入阻塞状态， 最大的阻塞时长， 一旦超过， producer 就会抛出 &nbsp;TimeoutException 因此，设置 buffer.memeory(默认32MB) 的值 能使 &nbsp; producer 使得阻塞的情况得到缓解， consumer 端 &nbsp;fetch.min.bytes 的作用， 设置leader 副本没错返回 consumer 的最小字节数， &nbsp; 通过增加该参数 fafka 会为每个fetch 请求的response 填入更多的数据， 减少网络开销， 提升TPS， 另一方面， 会导致延时的增加，&nbsp; broker &nbsp;，num。replica.fetchers 的值， 该值控制follower 从leader 副本中，获取的最大线程数， 默认使1 &nbsp;表示 follower 只有一个线程去实时拉去leader 的最新消息， 当 acks=all 的 producer 而言， 主要的延时可能都耽误在follower于leader 同步的过程，&nbsp; 增加 该值能够缩短同步的实际间隔， 提升producer的tps 参数总结 &nbsp; broker 端 •适当增加num.replica.fetchers,但不要超过CPU核数。 •调优GC避免经常性的Full GC。 producer 端 •适当增加batch.size，比如100-512KB。 •适当增加linger.ms，比如10-00毫秒。 •设置 compression.type=lz4。 •&nbsp;&nbsp; &nbsp;acks=0 或 1&nbsp; •&nbsp;&nbsp; &nbsp;retries=0 •若多线程共享producer或分区数很多，增加buffer.memory。 consumer 端 •采用多consumer实例。 •增加 fetch.min.bytes，比如 100000。调优延时， 大体和调优吞吐量相反，&nbsp; broker 端 • 适 度 增 加 num.replica.fetchers。 •避 免 创 建 过 多 topic分 区 。 producer 端 •设 置 linger.ms=0 • 设 置 compression.type=none •设 置 acks=l或 0 consumer 端 设 置 fetch.min.bytes=l调优持久性 broker 端 调优持久性， 有两个重要的参数可以调节，log.flush.interval.ms 和 log.flush.interval.message， 默认情况下， log.flush.interval.ms=0, log.flush.interval.message=long.max_value，一般不需要调节 broker 端 •设置 unclean.leader.election.enable=false (0.1 丨 .0.0之前版本）《 •设置 auto.create.topics.enable=false。 •设置 replicat丨 on.factor = 3， min.insync.replicas = replication.factor - 1» •设置 default.replication.factor = 3。 •设 置 brokcr.rack属性分散分区数据到不同机架。 •设置丨 og.flush.interval.message 和 丨 og.flush.interval.ms 为一个较小的值&nbsp; producer 端 •设置 acks=all。 •设 罝 retries为一个较大的值，比如 10-30。 •设置 max.in.flight.requesls.per.connection=l&nbsp; •设置 enabIe.idempotence=true 启用幂等性。 consumer 端 •设置 auto.commit.enable=false。 •消息消费成功后调用commitSync提交位移。调优可用性 broker 端 避 免 创 建 过 多 分 设 置 unclean.leader.election.enable=true。 设 置 min.insync.replicas=l。 设 置 num.recovery.threads.per.data.dir=brolcer 端 参 数 log.dirs 中 设 置 的 目 录 数 9 &nbsp;producer 端 &nbsp;•设 罝 acks=l，若一定要设置为all，则遵循h面broker端的min.insyn.replicas配置。 &nbsp;consumer 端 &nbsp;•设 置 session.timeout.ms为 较 低 的 值 ， 比 如 10000。 &nbsp;• (0.丨0.1.0及之后版本）设罝max.poll.imerva丨.ms为比消息平均处理时间稍大的值。 &nbsp;• (0.10.1.0 之前版本）设置 max.poll.records 和 max.partition.fetch.bytes 减少 consumer 处理 消 息 的 总 时 长 ， 避 免 频 繁 rebalance。 &nbsp; 笔记来源于 胡夕的apache kafka实战， 有兴趣的可以去看看那本书，&nbsp;" />
<meta property="og:description" content="kafka 设计概要 &nbsp;&nbsp; &nbsp;吞吐量/延时 &nbsp;&nbsp; &nbsp;消息持久化 &nbsp;&nbsp; &nbsp;负载均衡和故障转移 &nbsp;&nbsp; &nbsp;伸缩性 &nbsp; 一些常用命令通过GetOffsetShell 工具类查看 &nbsp;topic &nbsp; 分区消息 ./kafka-run-class.sh &nbsp;kafka.tools.GetOffsetShell &nbsp; --broker-list &nbsp;192.168.2.52:9092 &nbsp;--topic &nbsp;test查看当前topic 当前对应的信息&nbsp; ./kafka-topics.sh &nbsp;--zookeeper &nbsp;192.168.2.52:2181 &nbsp; --describe &nbsp; --topic &nbsp; test1 &nbsp;&nbsp; &nbsp;Topic: test1&nbsp;&nbsp; &nbsp;Partition: 0&nbsp;&nbsp; &nbsp;Leader: 0&nbsp;&nbsp; &nbsp;Replicas: 0&nbsp;&nbsp; &nbsp;Isr: 0 &nbsp;&nbsp; &nbsp;Topic: test1&nbsp;&nbsp; &nbsp;Partition: 1&nbsp;&nbsp; &nbsp;Leader: 0&nbsp;&nbsp; &nbsp;Replicas: 0&nbsp;&nbsp; &nbsp;Isr: 0修改kafka分区个数 ./kafka-topics.sh &nbsp;-alter --zookeeper &nbsp;192.168.2.52:2181 --partitions 2 &nbsp;--topic &nbsp;test给现有topic 添加新的配置 &nbsp; 添加压实配置 ./kafka-configs.sh &nbsp; --zookeeper 192.168.2.52:2181 --alter &nbsp;--entity-type topics --entity-name test &nbsp;--add-config cleanup.policy=compact查看当前topic 的配置&nbsp; ./kafka-configs.sh &nbsp;--zookeeper &nbsp;192.168.2.52:2181 &nbsp;--describe &nbsp;--entity-type topics &nbsp;--entity-name &nbsp;test对topic 进行生产者消息测试 ./kafka-producer-perf-test.sh &nbsp; --topic topic1 --throughput &nbsp;-1 &nbsp;--num-records 50000 &nbsp;--record-size 100 &nbsp;--producer-props &nbsp;bootstrap.servers=192.168.2.52:9092 &nbsp;acks=1查看当前存在的消费者组 ./kafka-consumer-groups.sh &nbsp; --bootstrap-server &nbsp;192.168.2.52:9092 &nbsp;--list查看当前消费者组的信息&nbsp; ./kafka-consumer-groups.sh &nbsp; &nbsp; &nbsp; --bootstrap-server &nbsp;192.168.2.52:9092 --describe --group &nbsp;topic1group重置当前消费者组的offset ./kafka-consumer-groups.sh &nbsp; &nbsp;--bootstrap-server &nbsp;192.168.2.52:9092 --group topic1group &nbsp;--reset-off sets --all-topics &nbsp;--to-earliest --execute&nbsp; &nbsp; kafka rebalance 触发条件 消费者新增或者 移除 topic &nbsp;分区的新增或减少 Kafka多线程消费两种方式 1 每个线程维护一个kafkaConsumer 2 单个kafkaConsumer 实例 一个线程对应一个分区 两种方式的优缺点 方法1 (每个线程维护专属 Ka fkaConsumer )&nbsp;&nbsp; &nbsp; 实现简单；速度较快，因为无线程 间交互开销：方便位移管理；易于 维护分区间的消息消费顺序&nbsp;&nbsp; &nbsp; Socket连接开销大；consumer数受限于topic分区 数，扩展性差；broker端处理负载高（因为发往 broker的请求数多）；rebalance可能性增大 方法2 (全局consumer多 worker 线程）&nbsp;&nbsp; &nbsp; 消息获取与处理解耦：可独立扩展 consumer数和woricer数，伸缩性 好&nbsp;&nbsp; &nbsp; 实现负载：难于维护分区内的消息顺序；处理链路 变长，导致位移管理困难；worker线程异常可能导 致消费数据丢失 写入消息的时候， 什么情况下 导致HW 会和LEO 不同步呢 follwer请求速度追不上，leader的接受消息的速度， 可能原因 &nbsp;follwer 副本所在的broker 的网络I/O开销过大导致备份消息的速度满于 leader处获取的消息速度 进程卡住： follwer在一段时间内无法向leader 请求数据， 比如之前提到的频繁gc 或者程序bug LEO &nbsp; log end offset &nbsp;日志末端唯一， 代表kafka 当前分区存了多少条数据， 12代表存在11条数据 HW &nbsp; 高水印 &nbsp; &nbsp; &nbsp; &nbsp; 代表 这个kafka &nbsp;consumer 可见的 ，当前能够读取的 &nbsp;最大offset ， &nbsp;8 代表kafka消费者最多可以获取到7条数据 HW &nbsp;必须小于或者等于 &nbsp;LEO &nbsp; 代表所有消息，已提交，或者已备份 分区中的HW &nbsp;为 ISR 中所有副本LEO 的最小值 log compaction &nbsp;日志压实(必须设置key) &nbsp; 确保kafka topic 每个分区中，具有相同的key的消息， 至少会保存最新的value 的消息&nbsp; _consumer_offsets 就使用了日志压实的策略， &nbsp; &nbsp; 最会保存(groupId + topic &nbsp;+ 分区号)最新的 &nbsp; broker 中的controller 的作用 更新集群元数据信息 创建topic &nbsp;删除topic &nbsp;分区重新分配 prefered leader 副本选举 topic 分区扩展， broker 加入集群 ， broker 崩溃&nbsp; 受控关闭， controller leader 选举&nbsp; 生产者如何实现，消息的精确一次处理的语义呢， 从kafka 0.11.0.0后 kafka提供了幂等性， enable.idempotence=true &nbsp; kafka调优方向 1 吞吐量 2 &nbsp;延时 3 &nbsp;持久性 4 可用性 OS 级别的错误 &nbsp; connection refused &nbsp; too &nbsp;many open &nbsp;files&nbsp; address in &nbsp;use connect &nbsp; 解决方案 ，基础设置 1 &nbsp;禁止atime(最新的访问时间) 更新 &nbsp; 可以使用 &nbsp;mount &nbsp;-o &nbsp;noatime 2 &nbsp;可以通过禁止记日志操作， 的方式来提高写的速度， commit=N_secs &nbsp;该选项是 &nbsp;没N秒 同步一次数据和元数据 ， &nbsp;默认是5秒。 如果该值设的比较小， 可以减少崩溃发生时带来的数据丢失， 但若设置的较大， 则会提升整体的吞吐量，以及降低延时， 生产环境 推荐用户设置一个较大的值 1-2分钟 swapiness 设置 2 设置swapiness &nbsp; linux 默认为 60 s &nbsp; 一般建议设置为 &nbsp;0 &nbsp;禁用来提升 内存的使用率，&nbsp; 这里存在一个问题， &nbsp;当内存用完后， linux &nbsp;OOM Killer &nbsp;回去杀掉一个进程， 这样可能会导致数据丢失， 坐着建议， 设置为 &nbsp; 1-10 之间，&nbsp; jvm &nbsp; 设置&nbsp; 由于kafka 并没有大量使用堆上内存 &nbsp;所以堆内存设置为 6G，差不多可以满足 -Xmx6g -Xms6g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize—128m -XX:+UseG1GC -XX:MaxGCPauseMillis— 20 -XX: InitiatingHeapOccupancyPercent 35 -XX: GlHeapRegionSize—16M -XX: MinMetaspaceFreeRatio—50 -XX: MaxMetaspaceFreeRatio=85 设置 单个jvm 最大支持的线程数 &nbsp;vm.max_map_count &nbsp; 65530调优吞吐量， 1producer &nbsp;设置 &nbsp;batch.size &nbsp;和linger.ms &nbsp;通常情况下， 会提升producer 端的TPS 会增加了 消息的延时性， 2合理设置分区个数， 分区的增大到某个数值，后由于锁竞争，和内存占用过多 降低tps, 在实际中， 可以用两倍数来逐步测试 ，直到性能拐点 3设置compression.type &nbsp; 对数据进行压缩，降低I/O的方式来提高tps ,压缩的话主要时针对batch的，&nbsp; kafka 支持的压缩格式又， gzip ,snappy,lz4 &nbsp; 相比来说，lz4组合的性能最好， 4 设置acks 参数， 当参数为1 代表leaderx写入底层文件系统即返回，无须等待follower 响应，acks=0, &nbsp; 表示 producer 端不理会 broker 端的响应， &nbsp;会提升tps 但是牺牲了消息的持久化，&nbsp; 5 &nbsp;当 producer 端 &nbsp;retries 参数设置的值，进行指定次数的重试，如果消息能够忍受偶尔的数据丢失， 可以将 retries 设置为0 ， tps 会有提升， 但是 消息发生失败后， 就不会重试，&nbsp; 6 &nbsp;max.block.ms &nbsp; 当producer 端的缓冲区被填满后， &nbsp;producer &nbsp;将会进入阻塞状态， 最大的阻塞时长， 一旦超过， producer 就会抛出 &nbsp;TimeoutException 因此，设置 buffer.memeory(默认32MB) 的值 能使 &nbsp; producer 使得阻塞的情况得到缓解， consumer 端 &nbsp;fetch.min.bytes 的作用， 设置leader 副本没错返回 consumer 的最小字节数， &nbsp; 通过增加该参数 fafka 会为每个fetch 请求的response 填入更多的数据， 减少网络开销， 提升TPS， 另一方面， 会导致延时的增加，&nbsp; broker &nbsp;，num。replica.fetchers 的值， 该值控制follower 从leader 副本中，获取的最大线程数， 默认使1 &nbsp;表示 follower 只有一个线程去实时拉去leader 的最新消息， 当 acks=all 的 producer 而言， 主要的延时可能都耽误在follower于leader 同步的过程，&nbsp; 增加 该值能够缩短同步的实际间隔， 提升producer的tps 参数总结 &nbsp; broker 端 •适当增加num.replica.fetchers,但不要超过CPU核数。 •调优GC避免经常性的Full GC。 producer 端 •适当增加batch.size，比如100-512KB。 •适当增加linger.ms，比如10-00毫秒。 •设置 compression.type=lz4。 •&nbsp;&nbsp; &nbsp;acks=0 或 1&nbsp; •&nbsp;&nbsp; &nbsp;retries=0 •若多线程共享producer或分区数很多，增加buffer.memory。 consumer 端 •采用多consumer实例。 •增加 fetch.min.bytes，比如 100000。调优延时， 大体和调优吞吐量相反，&nbsp; broker 端 • 适 度 增 加 num.replica.fetchers。 •避 免 创 建 过 多 topic分 区 。 producer 端 •设 置 linger.ms=0 • 设 置 compression.type=none •设 置 acks=l或 0 consumer 端 设 置 fetch.min.bytes=l调优持久性 broker 端 调优持久性， 有两个重要的参数可以调节，log.flush.interval.ms 和 log.flush.interval.message， 默认情况下， log.flush.interval.ms=0, log.flush.interval.message=long.max_value，一般不需要调节 broker 端 •设置 unclean.leader.election.enable=false (0.1 丨 .0.0之前版本）《 •设置 auto.create.topics.enable=false。 •设置 replicat丨 on.factor = 3， min.insync.replicas = replication.factor - 1» •设置 default.replication.factor = 3。 •设 置 brokcr.rack属性分散分区数据到不同机架。 •设置丨 og.flush.interval.message 和 丨 og.flush.interval.ms 为一个较小的值&nbsp; producer 端 •设置 acks=all。 •设 罝 retries为一个较大的值，比如 10-30。 •设置 max.in.flight.requesls.per.connection=l&nbsp; •设置 enabIe.idempotence=true 启用幂等性。 consumer 端 •设置 auto.commit.enable=false。 •消息消费成功后调用commitSync提交位移。调优可用性 broker 端 避 免 创 建 过 多 分 设 置 unclean.leader.election.enable=true。 设 置 min.insync.replicas=l。 设 置 num.recovery.threads.per.data.dir=brolcer 端 参 数 log.dirs 中 设 置 的 目 录 数 9 &nbsp;producer 端 &nbsp;•设 罝 acks=l，若一定要设置为all，则遵循h面broker端的min.insyn.replicas配置。 &nbsp;consumer 端 &nbsp;•设 置 session.timeout.ms为 较 低 的 值 ， 比 如 10000。 &nbsp;• (0.丨0.1.0及之后版本）设罝max.poll.imerva丨.ms为比消息平均处理时间稍大的值。 &nbsp;• (0.10.1.0 之前版本）设置 max.poll.records 和 max.partition.fetch.bytes 减少 consumer 处理 消 息 的 总 时 长 ， 避 免 频 繁 rebalance。 &nbsp; 笔记来源于 胡夕的apache kafka实战， 有兴趣的可以去看看那本书，&nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/08/01/795179.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/01/795179.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-01T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"kafka 设计概要 &nbsp;&nbsp; &nbsp;吞吐量/延时 &nbsp;&nbsp; &nbsp;消息持久化 &nbsp;&nbsp; &nbsp;负载均衡和故障转移 &nbsp;&nbsp; &nbsp;伸缩性 &nbsp; 一些常用命令通过GetOffsetShell 工具类查看 &nbsp;topic &nbsp; 分区消息 ./kafka-run-class.sh &nbsp;kafka.tools.GetOffsetShell &nbsp; --broker-list &nbsp;192.168.2.52:9092 &nbsp;--topic &nbsp;test查看当前topic 当前对应的信息&nbsp; ./kafka-topics.sh &nbsp;--zookeeper &nbsp;192.168.2.52:2181 &nbsp; --describe &nbsp; --topic &nbsp; test1 &nbsp;&nbsp; &nbsp;Topic: test1&nbsp;&nbsp; &nbsp;Partition: 0&nbsp;&nbsp; &nbsp;Leader: 0&nbsp;&nbsp; &nbsp;Replicas: 0&nbsp;&nbsp; &nbsp;Isr: 0 &nbsp;&nbsp; &nbsp;Topic: test1&nbsp;&nbsp; &nbsp;Partition: 1&nbsp;&nbsp; &nbsp;Leader: 0&nbsp;&nbsp; &nbsp;Replicas: 0&nbsp;&nbsp; &nbsp;Isr: 0修改kafka分区个数 ./kafka-topics.sh &nbsp;-alter --zookeeper &nbsp;192.168.2.52:2181 --partitions 2 &nbsp;--topic &nbsp;test给现有topic 添加新的配置 &nbsp; 添加压实配置 ./kafka-configs.sh &nbsp; --zookeeper 192.168.2.52:2181 --alter &nbsp;--entity-type topics --entity-name test &nbsp;--add-config cleanup.policy=compact查看当前topic 的配置&nbsp; ./kafka-configs.sh &nbsp;--zookeeper &nbsp;192.168.2.52:2181 &nbsp;--describe &nbsp;--entity-type topics &nbsp;--entity-name &nbsp;test对topic 进行生产者消息测试 ./kafka-producer-perf-test.sh &nbsp; --topic topic1 --throughput &nbsp;-1 &nbsp;--num-records 50000 &nbsp;--record-size 100 &nbsp;--producer-props &nbsp;bootstrap.servers=192.168.2.52:9092 &nbsp;acks=1查看当前存在的消费者组 ./kafka-consumer-groups.sh &nbsp; --bootstrap-server &nbsp;192.168.2.52:9092 &nbsp;--list查看当前消费者组的信息&nbsp; ./kafka-consumer-groups.sh &nbsp; &nbsp; &nbsp; --bootstrap-server &nbsp;192.168.2.52:9092 --describe --group &nbsp;topic1group重置当前消费者组的offset ./kafka-consumer-groups.sh &nbsp; &nbsp;--bootstrap-server &nbsp;192.168.2.52:9092 --group topic1group &nbsp;--reset-off sets --all-topics &nbsp;--to-earliest --execute&nbsp; &nbsp; kafka rebalance 触发条件 消费者新增或者 移除 topic &nbsp;分区的新增或减少 Kafka多线程消费两种方式 1 每个线程维护一个kafkaConsumer 2 单个kafkaConsumer 实例 一个线程对应一个分区 两种方式的优缺点 方法1 (每个线程维护专属 Ka fkaConsumer )&nbsp;&nbsp; &nbsp; 实现简单；速度较快，因为无线程 间交互开销：方便位移管理；易于 维护分区间的消息消费顺序&nbsp;&nbsp; &nbsp; Socket连接开销大；consumer数受限于topic分区 数，扩展性差；broker端处理负载高（因为发往 broker的请求数多）；rebalance可能性增大 方法2 (全局consumer多 worker 线程）&nbsp;&nbsp; &nbsp; 消息获取与处理解耦：可独立扩展 consumer数和woricer数，伸缩性 好&nbsp;&nbsp; &nbsp; 实现负载：难于维护分区内的消息顺序；处理链路 变长，导致位移管理困难；worker线程异常可能导 致消费数据丢失 写入消息的时候， 什么情况下 导致HW 会和LEO 不同步呢 follwer请求速度追不上，leader的接受消息的速度， 可能原因 &nbsp;follwer 副本所在的broker 的网络I/O开销过大导致备份消息的速度满于 leader处获取的消息速度 进程卡住： follwer在一段时间内无法向leader 请求数据， 比如之前提到的频繁gc 或者程序bug LEO &nbsp; log end offset &nbsp;日志末端唯一， 代表kafka 当前分区存了多少条数据， 12代表存在11条数据 HW &nbsp; 高水印 &nbsp; &nbsp; &nbsp; &nbsp; 代表 这个kafka &nbsp;consumer 可见的 ，当前能够读取的 &nbsp;最大offset ， &nbsp;8 代表kafka消费者最多可以获取到7条数据 HW &nbsp;必须小于或者等于 &nbsp;LEO &nbsp; 代表所有消息，已提交，或者已备份 分区中的HW &nbsp;为 ISR 中所有副本LEO 的最小值 log compaction &nbsp;日志压实(必须设置key) &nbsp; 确保kafka topic 每个分区中，具有相同的key的消息， 至少会保存最新的value 的消息&nbsp; _consumer_offsets 就使用了日志压实的策略， &nbsp; &nbsp; 最会保存(groupId + topic &nbsp;+ 分区号)最新的 &nbsp; broker 中的controller 的作用 更新集群元数据信息 创建topic &nbsp;删除topic &nbsp;分区重新分配 prefered leader 副本选举 topic 分区扩展， broker 加入集群 ， broker 崩溃&nbsp; 受控关闭， controller leader 选举&nbsp; 生产者如何实现，消息的精确一次处理的语义呢， 从kafka 0.11.0.0后 kafka提供了幂等性， enable.idempotence=true &nbsp; kafka调优方向 1 吞吐量 2 &nbsp;延时 3 &nbsp;持久性 4 可用性 OS 级别的错误 &nbsp; connection refused &nbsp; too &nbsp;many open &nbsp;files&nbsp; address in &nbsp;use connect &nbsp; 解决方案 ，基础设置 1 &nbsp;禁止atime(最新的访问时间) 更新 &nbsp; 可以使用 &nbsp;mount &nbsp;-o &nbsp;noatime 2 &nbsp;可以通过禁止记日志操作， 的方式来提高写的速度， commit=N_secs &nbsp;该选项是 &nbsp;没N秒 同步一次数据和元数据 ， &nbsp;默认是5秒。 如果该值设的比较小， 可以减少崩溃发生时带来的数据丢失， 但若设置的较大， 则会提升整体的吞吐量，以及降低延时， 生产环境 推荐用户设置一个较大的值 1-2分钟 swapiness 设置 2 设置swapiness &nbsp; linux 默认为 60 s &nbsp; 一般建议设置为 &nbsp;0 &nbsp;禁用来提升 内存的使用率，&nbsp; 这里存在一个问题， &nbsp;当内存用完后， linux &nbsp;OOM Killer &nbsp;回去杀掉一个进程， 这样可能会导致数据丢失， 坐着建议， 设置为 &nbsp; 1-10 之间，&nbsp; jvm &nbsp; 设置&nbsp; 由于kafka 并没有大量使用堆上内存 &nbsp;所以堆内存设置为 6G，差不多可以满足 -Xmx6g -Xms6g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize—128m -XX:+UseG1GC -XX:MaxGCPauseMillis— 20 -XX: InitiatingHeapOccupancyPercent 35 -XX: GlHeapRegionSize—16M -XX: MinMetaspaceFreeRatio—50 -XX: MaxMetaspaceFreeRatio=85 设置 单个jvm 最大支持的线程数 &nbsp;vm.max_map_count &nbsp; 65530调优吞吐量， 1producer &nbsp;设置 &nbsp;batch.size &nbsp;和linger.ms &nbsp;通常情况下， 会提升producer 端的TPS 会增加了 消息的延时性， 2合理设置分区个数， 分区的增大到某个数值，后由于锁竞争，和内存占用过多 降低tps, 在实际中， 可以用两倍数来逐步测试 ，直到性能拐点 3设置compression.type &nbsp; 对数据进行压缩，降低I/O的方式来提高tps ,压缩的话主要时针对batch的，&nbsp; kafka 支持的压缩格式又， gzip ,snappy,lz4 &nbsp; 相比来说，lz4组合的性能最好， 4 设置acks 参数， 当参数为1 代表leaderx写入底层文件系统即返回，无须等待follower 响应，acks=0, &nbsp; 表示 producer 端不理会 broker 端的响应， &nbsp;会提升tps 但是牺牲了消息的持久化，&nbsp; 5 &nbsp;当 producer 端 &nbsp;retries 参数设置的值，进行指定次数的重试，如果消息能够忍受偶尔的数据丢失， 可以将 retries 设置为0 ， tps 会有提升， 但是 消息发生失败后， 就不会重试，&nbsp; 6 &nbsp;max.block.ms &nbsp; 当producer 端的缓冲区被填满后， &nbsp;producer &nbsp;将会进入阻塞状态， 最大的阻塞时长， 一旦超过， producer 就会抛出 &nbsp;TimeoutException 因此，设置 buffer.memeory(默认32MB) 的值 能使 &nbsp; producer 使得阻塞的情况得到缓解， consumer 端 &nbsp;fetch.min.bytes 的作用， 设置leader 副本没错返回 consumer 的最小字节数， &nbsp; 通过增加该参数 fafka 会为每个fetch 请求的response 填入更多的数据， 减少网络开销， 提升TPS， 另一方面， 会导致延时的增加，&nbsp; broker &nbsp;，num。replica.fetchers 的值， 该值控制follower 从leader 副本中，获取的最大线程数， 默认使1 &nbsp;表示 follower 只有一个线程去实时拉去leader 的最新消息， 当 acks=all 的 producer 而言， 主要的延时可能都耽误在follower于leader 同步的过程，&nbsp; 增加 该值能够缩短同步的实际间隔， 提升producer的tps 参数总结 &nbsp; broker 端 •适当增加num.replica.fetchers,但不要超过CPU核数。 •调优GC避免经常性的Full GC。 producer 端 •适当增加batch.size，比如100-512KB。 •适当增加linger.ms，比如10-00毫秒。 •设置 compression.type=lz4。 •&nbsp;&nbsp; &nbsp;acks=0 或 1&nbsp; •&nbsp;&nbsp; &nbsp;retries=0 •若多线程共享producer或分区数很多，增加buffer.memory。 consumer 端 •采用多consumer实例。 •增加 fetch.min.bytes，比如 100000。调优延时， 大体和调优吞吐量相反，&nbsp; broker 端 • 适 度 增 加 num.replica.fetchers。 •避 免 创 建 过 多 topic分 区 。 producer 端 •设 置 linger.ms=0 • 设 置 compression.type=none •设 置 acks=l或 0 consumer 端 设 置 fetch.min.bytes=l调优持久性 broker 端 调优持久性， 有两个重要的参数可以调节，log.flush.interval.ms 和 log.flush.interval.message， 默认情况下， log.flush.interval.ms=0, log.flush.interval.message=long.max_value，一般不需要调节 broker 端 •设置 unclean.leader.election.enable=false (0.1 丨 .0.0之前版本）《 •设置 auto.create.topics.enable=false。 •设置 replicat丨 on.factor = 3， min.insync.replicas = replication.factor - 1» •设置 default.replication.factor = 3。 •设 置 brokcr.rack属性分散分区数据到不同机架。 •设置丨 og.flush.interval.message 和 丨 og.flush.interval.ms 为一个较小的值&nbsp; producer 端 •设置 acks=all。 •设 罝 retries为一个较大的值，比如 10-30。 •设置 max.in.flight.requesls.per.connection=l&nbsp; •设置 enabIe.idempotence=true 启用幂等性。 consumer 端 •设置 auto.commit.enable=false。 •消息消费成功后调用commitSync提交位移。调优可用性 broker 端 避 免 创 建 过 多 分 设 置 unclean.leader.election.enable=true。 设 置 min.insync.replicas=l。 设 置 num.recovery.threads.per.data.dir=brolcer 端 参 数 log.dirs 中 设 置 的 目 录 数 9 &nbsp;producer 端 &nbsp;•设 罝 acks=l，若一定要设置为all，则遵循h面broker端的min.insyn.replicas配置。 &nbsp;consumer 端 &nbsp;•设 置 session.timeout.ms为 较 低 的 值 ， 比 如 10000。 &nbsp;• (0.丨0.1.0及之后版本）设罝max.poll.imerva丨.ms为比消息平均处理时间稍大的值。 &nbsp;• (0.10.1.0 之前版本）设置 max.poll.records 和 max.partition.fetch.bytes 减少 consumer 处理 消 息 的 总 时 长 ， 避 免 频 繁 rebalance。 &nbsp; 笔记来源于 胡夕的apache kafka实战， 有兴趣的可以去看看那本书，&nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/08/01/795179.html","headline":"kafka 相关笔记","dateModified":"2019-08-01T00:00:00+08:00","datePublished":"2019-08-01T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/01/795179.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>kafka 相关笔记</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p><strong>kafka 设计概要</strong><br> &nbsp;&nbsp; &nbsp;吞吐量/延时<br> &nbsp;&nbsp; &nbsp;消息持久化<br> &nbsp;&nbsp; &nbsp;负载均衡和故障转移<br> &nbsp;&nbsp; &nbsp;伸缩性</p> 
  <p>&nbsp;</p> 
  <p><strong>一些常用命令</strong><br><strong>通过GetOffsetShell 工具类查看 &nbsp;topic &nbsp; 分区消息</strong><br> ./kafka-run-class.sh &nbsp;kafka.tools.GetOffsetShell &nbsp; --broker-list &nbsp;192.168.2.52:9092 &nbsp;--topic &nbsp;test<br><strong>查看当前topic 当前对应的信息&nbsp;</strong><br> ./kafka-topics.sh &nbsp;--zookeeper &nbsp;192.168.2.52:2181 &nbsp; --describe &nbsp; --topic &nbsp; test1<br> &nbsp;&nbsp; &nbsp;Topic: test1&nbsp;&nbsp; &nbsp;Partition: 0&nbsp;&nbsp; &nbsp;Leader: 0&nbsp;&nbsp; &nbsp;Replicas: 0&nbsp;&nbsp; &nbsp;Isr: 0<br> &nbsp;&nbsp; &nbsp;Topic: test1&nbsp;&nbsp; &nbsp;Partition: 1&nbsp;&nbsp; &nbsp;Leader: 0&nbsp;&nbsp; &nbsp;Replicas: 0&nbsp;&nbsp; &nbsp;Isr: 0<br><strong>修改kafka分区个数</strong><br> ./kafka-topics.sh &nbsp;-alter --zookeeper &nbsp;192.168.2.52:2181 --partitions 2 &nbsp;--topic &nbsp;test<br><strong>给现有topic 添加新的配置 &nbsp; 添加压实配置</strong><br> ./kafka-configs.sh &nbsp; --zookeeper 192.168.2.52:2181 --alter &nbsp;--entity-type topics --entity-name test &nbsp;--add-config cleanup.policy=compact<br><strong>查看当前topic 的配置&nbsp;</strong><br> ./kafka-configs.sh &nbsp;--zookeeper &nbsp;192.168.2.52:2181 &nbsp;--describe &nbsp;--entity-type topics &nbsp;--entity-name &nbsp;test<br><strong>对topic 进行生产者消息测试</strong><br> ./kafka-producer-perf-test.sh &nbsp; --topic topic1 --throughput &nbsp;-1 &nbsp;--num-records 50000 &nbsp;--record-size 100 &nbsp;--producer-props &nbsp;bootstrap.servers=192.168.2.52:9092 &nbsp;acks=1<br><strong>查看当前存在的消费者组</strong><br> ./kafka-consumer-groups.sh &nbsp; --bootstrap-server &nbsp;192.168.2.52:9092 &nbsp;--list<br><strong>查看当前消费者组的信息&nbsp;</strong><br> ./kafka-consumer-groups.sh &nbsp; &nbsp; &nbsp; --bootstrap-server &nbsp;192.168.2.52:9092 --describe --group &nbsp;topic1group<br><strong>重置当前消费者组的offset</strong><br> ./kafka-consumer-groups.sh &nbsp; &nbsp;--bootstrap-server &nbsp;192.168.2.52:9092 --group topic1group &nbsp;--reset-off sets --all-topics &nbsp;--to-earliest --execute&nbsp;<br> &nbsp;</p> 
  <p><strong>kafka rebalance 触发条件</strong><br> 消费者新增或者 移除<br> topic &nbsp;分区的新增或减少</p> 
  <p><strong>Kafka多线程消费两种方式</strong><br> 1 每个线程维护一个kafkaConsumer<br> 2 单个kafkaConsumer 实例 一个线程对应一个分区</p> 
  <p>两种方式的优缺点<br> 方法1 (每个线程维护专属 Ka fkaConsumer )&nbsp;&nbsp; &nbsp;<br> 实现简单；速度较快，因为无线程 间交互开销：方便位移管理；易于 维护分区间的消息消费顺序&nbsp;&nbsp; &nbsp;<br> Socket连接开销大；consumer数受限于topic分区 数，扩展性差；broker端处理负载高（因为发往 broker的请求数多）；rebalance可能性增大<br> 方法2 (全局consumer多 worker 线程）&nbsp;&nbsp; &nbsp;<br> 消息获取与处理解耦：可独立扩展 consumer数和woricer数，伸缩性 好&nbsp;&nbsp; &nbsp;<br> 实现负载：难于维护分区内的消息顺序；处理链路 变长，导致位移管理困难；worker线程异常可能导 致消费数据丢失</p> 
  <p><strong>写入消息的时候， 什么情况下 导致HW 会和LEO 不同步呢</strong><br> follwer请求速度追不上，leader的接受消息的速度， 可能原因 &nbsp;follwer 副本所在的broker 的网络I/O开销过大导致备份消息的速度满于 leader处获取的消息速度<br> 进程卡住： follwer在一段时间内无法向leader 请求数据， 比如之前提到的频繁gc 或者程序bug</p> 
  <p>LEO &nbsp; log end offset &nbsp;日志末端唯一， 代表kafka 当前分区存了多少条数据， 12代表存在11条数据<br> HW &nbsp; 高水印 &nbsp; &nbsp; &nbsp; &nbsp; 代表 这个kafka &nbsp;consumer 可见的 ，当前能够读取的 &nbsp;最大offset ， &nbsp;8 代表kafka消费者最多可以获取到7条数据<br> HW &nbsp;必须小于或者等于 &nbsp;LEO &nbsp; 代表所有消息，已提交，或者已备份<br> 分区中的HW &nbsp;为 ISR 中所有副本LEO 的最小值</p> 
  <p>log compaction &nbsp;日志压实(必须设置key) &nbsp; 确保kafka topic 每个分区中，具有相同的key的消息， 至少会保存最新的value 的消息&nbsp;<br> _consumer_offsets 就使用了日志压实的策略， &nbsp; &nbsp; 最会保存(groupId + topic &nbsp;+ 分区号)最新的<br> &nbsp;</p> 
  <p>broker 中的controller 的作用<br> 更新集群元数据信息<br> 创建topic &nbsp;删除topic &nbsp;分区重新分配<br> prefered leader 副本选举<br> topic 分区扩展， broker 加入集群 ， broker 崩溃&nbsp;<br> 受控关闭， controller leader 选举&nbsp;<br> 生产者如何实现，消息的精确一次处理的语义呢， 从kafka 0.11.0.0后 kafka提供了幂等性， enable.idempotence=true<br> &nbsp;</p> 
  <h3><strong>kafka调优方向</strong><br> 1 吞吐量<br> 2 &nbsp;延时<br> 3 &nbsp;持久性<br> 4 可用性</h3> 
  <p>OS 级别的错误 &nbsp;<br> connection refused &nbsp;<br> too &nbsp;many open &nbsp;files&nbsp;<br> address in &nbsp;use connect &nbsp;<br> 解决方案 ，基础设置<br> 1 &nbsp;禁止atime(最新的访问时间) 更新 &nbsp; 可以使用 &nbsp;mount &nbsp;-o &nbsp;noatime<br> 2 &nbsp;可以通过禁止记日志操作， 的方式来提高写的速度，<br> commit=N_secs &nbsp;该选项是 &nbsp;没N秒 同步一次数据和元数据 ， &nbsp;默认是5秒。 如果该值设的比较小，<br> 可以减少崩溃发生时带来的数据丢失， 但若设置的较大， 则会提升整体的吞吐量，以及降低延时， 生产环境 推荐用户设置一个较大的值 1-2分钟<br> swapiness 设置<br> 2 设置swapiness &nbsp; linux 默认为 60 s &nbsp; 一般建议设置为 &nbsp;0 &nbsp;禁用来提升 内存的使用率，&nbsp;<br> 这里存在一个问题， &nbsp;当内存用完后， linux &nbsp;OOM Killer &nbsp;回去杀掉一个进程， 这样可能会导致数据丢失，<br> 坐着建议， 设置为 &nbsp; 1-10 之间，&nbsp;<br> jvm &nbsp; 设置&nbsp;<br> 由于kafka 并没有大量使用堆上内存 &nbsp;所以堆内存设置为 6G，差不多可以满足<br> -Xmx6g -Xms6g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize—128m -XX:+UseG1GC<br> -XX:MaxGCPauseMillis— 20 -XX: InitiatingHeapOccupancyPercent 35 -XX:<br> GlHeapRegionSize—16M -XX: MinMetaspaceFreeRatio—50 -XX:<br> MaxMetaspaceFreeRatio=85<br> 设置 单个jvm 最大支持的线程数 &nbsp;vm.max_map_count &nbsp; 65530<br><strong>调优吞吐量，</strong><br> 1producer &nbsp;设置 &nbsp;batch.size &nbsp;和linger.ms &nbsp;通常情况下， 会提升producer 端的TPS<br> 会增加了 消息的延时性，<br> 2合理设置分区个数， 分区的增大到某个数值，后由于锁竞争，和内存占用过多 降低tps, 在实际中，<br> 可以用两倍数来逐步测试 ，直到性能拐点<br> 3设置compression.type &nbsp; 对数据进行压缩，降低I/O的方式来提高tps ,压缩的话主要时针对batch的，&nbsp;<br> kafka 支持的压缩格式又， gzip ,snappy,lz4 &nbsp; 相比来说，lz4组合的性能最好，<br> 4 设置acks 参数， 当参数为1 代表leaderx写入底层文件系统即返回，无须等待follower 响应，acks=0, &nbsp;<br> 表示 producer 端不理会 broker 端的响应， &nbsp;会提升tps 但是牺牲了消息的持久化，&nbsp;<br> 5 &nbsp;当 producer 端 &nbsp;retries 参数设置的值，进行指定次数的重试，如果消息能够忍受偶尔的数据丢失， 可以将<br> retries 设置为0 ， tps 会有提升， 但是 消息发生失败后， 就不会重试，&nbsp;<br> 6 &nbsp;max.block.ms &nbsp; 当producer 端的缓冲区被填满后， &nbsp;producer &nbsp;将会进入阻塞状态， 最大的阻塞时长，<br> 一旦超过， producer 就会抛出 &nbsp;TimeoutException 因此，设置 buffer.memeory(默认32MB) 的值 能使 &nbsp;<br> producer 使得阻塞的情况得到缓解，<br> consumer 端 &nbsp;fetch.min.bytes 的作用， 设置leader 副本没错返回 consumer 的最小字节数， &nbsp;<br> 通过增加该参数 fafka 会为每个fetch 请求的response 填入更多的数据， 减少网络开销， 提升TPS， 另一方面，<br> 会导致延时的增加，&nbsp;<br> broker &nbsp;，num。replica.fetchers 的值， 该值控制follower 从leader 副本中，获取的最大线程数， 默认使1 &nbsp;表示<br> follower 只有一个线程去实时拉去leader 的最新消息， 当 acks=all 的 producer 而言， 主要的延时可能都耽误在follower于leader 同步的过程，&nbsp;<br> 增加 该值能够缩短同步的实际间隔， 提升producer的tps<br> 参数总结 &nbsp;<br> broker 端<br> •适当增加num.replica.fetchers,但不要超过CPU核数。<br> •调优GC避免经常性的Full GC。<br> producer 端<br> •适当增加batch.size，比如100-512KB。<br> •适当增加linger.ms，比如10-00毫秒。<br> •设置 compression.type=lz4。<br> •&nbsp;&nbsp; &nbsp;acks=0 或 1&nbsp;<br> •&nbsp;&nbsp; &nbsp;retries=0<br> •若多线程共享producer或分区数很多，增加buffer.memory。 consumer 端<br> •采用多consumer实例。<br> •增加 fetch.min.bytes，比如 100000。<br><strong>调优延时， 大体和调优吞吐量相反，</strong>&nbsp;<br> broker 端<br> • 适 度 增 加 num.replica.fetchers。<br> •避 免 创 建 过 多 topic分 区 。<br> producer 端<br> •设 置 linger.ms=0<br> • 设 置 compression.type=none<br> •设 置 acks=l或 0<br> consumer 端<br> 设 置 fetch.min.bytes=l<br><strong>调优持久性</strong><br> broker 端 调优持久性， 有两个重要的参数可以调节，log.flush.interval.ms 和 log.flush.interval.message，<br> 默认情况下， log.flush.interval.ms=0, log.flush.interval.message=long.max_value，一般不需要调节<br> broker 端<br> •设置 unclean.leader.election.enable=false (0.1 丨 .0.0之前版本）《<br> •设置 auto.create.topics.enable=false。<br> •设置 replicat丨 on.factor = 3， min.insync.replicas = replication.factor - 1»<br> •设置 default.replication.factor = 3。<br> •设 置 brokcr.rack属性分散分区数据到不同机架。<br> •设置丨 og.flush.interval.message 和 丨 og.flush.interval.ms 为一个较小的值&nbsp;<br> producer 端<br> •设置 acks=all。<br> •设 罝 retries为一个较大的值，比如 10-30。<br> •设置 max.in.flight.requesls.per.connection=l&nbsp;<br> •设置 enabIe.idempotence=true 启用幂等性。<br> consumer 端<br> •设置 auto.commit.enable=false。<br> •消息消费成功后调用commitSync提交位移。<br><strong>调优可用性</strong><br> broker 端<br> 避 免 创 建 过 多 分<br> 设 置 unclean.leader.election.enable=true。<br> 设 置 min.insync.replicas=l。<br> 设 置 num.recovery.threads.per.data.dir=brolcer 端 参 数 log.dirs 中 设 置 的 目 录 数 9<br> &nbsp;producer 端<br> &nbsp;•设 罝 acks=l，若一定要设置为all，则遵循h面broker端的min.insyn.replicas配置。<br> &nbsp;consumer 端<br> &nbsp;•设 置 session.timeout.ms为 较 低 的 值 ， 比 如 10000。<br> &nbsp;• (0.丨0.1.0及之后版本）设罝max.poll.imerva丨.ms为比消息平均处理时间稍大的值。<br> &nbsp;• (0.10.1.0 之前版本）设置 max.poll.records 和 max.partition.fetch.bytes 减少 consumer 处理 消 息 的 总 时 长 ， 避 免 频 繁 rebalance。</p> 
  <p>&nbsp;</p> 
  <p>笔记来源于 胡夕的apache kafka实战， 有兴趣的可以去看看那本书，&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
