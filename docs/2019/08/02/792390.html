<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>深度学习之图像分类–VGG | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="深度学习之图像分类–VGG" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="目录 1 简介 2 网络结构 网络结构的特点 使用多个3*3叠加的原因： 使用1*1的卷积核 3 VGGNet网络的参数： 参数和计算量的对比 4 VGGNet的训练 超参数的设置 图片的处理 5 VGGNet的测试 将全连接换成全卷积 多重裁剪评估方式 5 VGGNet实验结果 5.1 单尺度评估 5.2 多尺度评估 5.3 多重裁剪评估 5.4 多个网络结合 5.5 多种方法的对比 1 简介 VGGNet由牛津大学的视觉几何组（Visual Geometry Group）提出，获得了2014年ILSVRC竞赛的分类任务第二名和定位任务第一名，主要贡献在于证明了使用3x3小卷积核，增加网络深度可以有效提升模型性能，并且对于其他数据集也有很好的泛化性能。 论文链接：Very deep convolutional networks for large-scale image recognition 2 网络结构 论文中一共提供了6种网络配置，层数从浅到深分别为11层、13层、16层和19层。其中11层时，主要比较了Local Response Normalisation（LRN）的作用，结果是LRN并没有提升网络性能。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 网络结构的特点 网络有5段卷积，每段卷积内分别有22333个卷积层，每段后面跟着个最大池化层 每段内的卷积核数量一样，越后边段内卷积核数量越多，依次为64-128-256-512-512 使用多个3*3叠加的原因： （两个3*3相当于5*5的感受野） 保持感受野相同的情况下，参数更少 增加了非线性，使网络对特征的学习能力更强 使用1*1的卷积核 在不影响感受野的情况下，增加非线性 用来调整特征图的channel的维度（这里没有使用） 3 VGGNet网络的参数： 现象：网络逐渐变深，参数量并没有增加很多。 原因：参数量主要消耗在最后的3个全连接层，前面的卷积层数虽多，但参数消耗量不大。（但卷积层训练比较耗时，因为计算量大） 参数和计算量的对比 在计算量这里，为了突出小卷积核的优势，我拿同样conv3x3、conv5x5、conv7x7、conv9x9和conv11x11，在224x224x3的RGB图上（output_channel=96）做卷积，卷积层的参数规模和得到的feature map的大小如下： kernel conv&nbsp;param:&nbsp;A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(与步长、padding无关) feature&nbsp;map:&nbsp;B &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 计算量 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; kernel_param * B *&nbsp;2 conv3x3 3x3x3x96&nbsp;&nbsp;（2592） 224x224x96&nbsp;(4816896) ‭260,112,384‬ conv5x5 5x5x3x96&nbsp;&nbsp;（7200） 224x224x96&nbsp;(4816896) ‭722,534,400‬ conv7x7 7x7x3x96&nbsp;&nbsp;（14112） 224x224x96&nbsp;(4816896) ‭1,416,167,424‬ conv9x9 9x9x3x96&nbsp;&nbsp;（23328） 224x224x96&nbsp;(4816896) ‭2,341,011,456‬ conv11x11 11x11x3x96&nbsp;&nbsp;（34848） 224x224x96&nbsp;(4816896) ‭3,497,066,496‬ &nbsp; &nbsp; &nbsp; &nbsp; 训练时网络的参数量:&nbsp;&nbsp;&nbsp;(A&nbsp;+&nbsp;B) 计算量:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_param * B *&nbsp;2 计算量：思考角度为特征图，特征图B上的每一个参数都需要经过kernel_param个计算。前项和后向共两次，所以再乘以2。 对于该例子：网络本身的参数占少数，网络每层出来的特征图参数400w；conv3*3的计算量达到2亿，conv11*11的计算量达到30亿。所以，使用两个conv3*3代替conv5*5，网络本身参数由7200--&gt;‭5184‬(2592+2592)，计算量由7亿--&gt;5亿(2.6亿+2.6亿） &nbsp; 4 VGGNet的训练 超参数的设置 训练使用加动量的小批基于反向传播的梯度下降法来优化多项逻辑回归目标 batch size为256，momentum设为0.9，权重衰减系数为0.0005 前两层的全连接后面跟着Dropout，设置为0.5 学习率初始为0.01，当验证集的准确率停止提高时，学习率除以10。最终学习率降低了三次以后，网络就收敛了 &nbsp;对于深度网络来说，网络的权重初始化十分重要。为此，论文中首先训练了一前程的网络结构A（第一个图中），训练折后网络时，随机初始化他的权重就足够得到比较好的结果。然后，当训练深层的网络时，前四层卷积核最后三个全连接层使用的是学习好的A网络的权重来进行初始化，其余层则随机初始化。（即某些层的预初始化） 图片的处理 （1）首先将图片保持高宽比进行缩放，使最短边缩放至S(S&gt;=224)。由于要求输入224*224大小，再对缩放后的图片进行crop。 S的取值方法： 固定的尺寸。对所有的图片，S都是固定的值。论文考察了S=256和S=384两种情况下分别训练得到的网络性能。为了加速，训练S=384的网络时，使用的训练好的S=256的网络来初始化权重，并且学习率更小，为0.001 可变的尺寸。设置一个范围[Smin, Smax]，每一张图片都从这个范围中随机选取一个数作为它的S。论文中使用的范围是[256, 512]。这个方法叫做尺度抖动scal jittering，有利于训练集增强，这使得训练在一个很大范围的图像尺度之上进行。为了加速，通过微调S=384的固定尺寸的网络来训练得到可变尺度的网络 （2）为了更进一步的增加训练集，对每张图片进行水平翻转以及进行随机RGB色差调整 &nbsp; 5 VGGNet的测试 将全连接换成全卷积 相同处： 两种方式，网络的参数是相同的。对于输入固定的清凉，效果也是相同的。 不同处： 全连接的多次输入时，必须保持输入维度不变。 需要保证网络的测试的图片保持相同的大小。 一张大图裁剪成多张224*224小图，重复的部分就会多次进入网络进行计算，产生很多冗余操作 卷积是可以接受不同尺寸的信息，当图片较大的情况下，网络输出的feature map尺寸就不是1*1*1000。这样： 图片输入时，不需要提前裁剪成224*224。 网络的输出还保留了得分的位置信息。在输出channel上求平均后，在输入softmax层。减少冗余操作（下图例子） 多重裁剪评估方式 在GoogleLeNet中描述的详细过程如下： 将图片缩放到不同的4种尺寸(纵横比不变，GoogleLeNet使用的4种尺寸为：缩放后的最短边长度分别为：256,288,320和352)。 对于得到的每个尺寸的图像，取左、中、右三个位置的正方形图像(边长就是最短边的长度。对于纵向图像来说，则取上、中、下三个位置)，因此每个尺寸的图像得到3个正方形图像； 然后再在每个正方形图像的4个crop顶点和中心位置处crop处224*224的图像，此外再加上将这个正方形图像缩放到224*224大小的图像，因此每个正方形图像得到6个224*224的图像； 最后，再将所有得到的224*224的图像水平翻转。因此，每个图像可以得到4*3*6*2=144个224*224大小的图像。 将这些图像分别输入神经网络进行分类，最后取平均，作为这个图像最终的分类结果。 而VGG中则使用的是3种尺寸，每个尺寸在5个位置处取正方形图像，每个正方形图像crop出5个224*224大小的图像，最后水平翻转，即3*5*5*2=150。 &nbsp; 5 VGGNet实验结果 &nbsp; &nbsp; 数据集 在本章，我们讲述了卷积神经网络在ILSVRC2012数据集上的分类结果。数据集包含1000个类别，被分为三部分：训练集（1.3M张图片），验证集（50K张图片），测试集（100K张图片，没有标签）。分类性能使用两个办法评估：top-1和top-5 error。前者是一个多类分类错误率，即错误分类图像的比例；后者是在ILSVRC上的主要评估标准，即真实类别不在top-5预测类别之中的图像的比例。 5.1 单尺度评估 对上面提出的模型进行测试。测试时，对于固定的S，去Q=S；对于可变的S，取Q=0.5*(Smin + Smax)。实验结果如下： A和A-LRN对比：后者性能并没有提升，LRN没有好的效果 B和C对比（C优）：C有多的1*1的卷积。说明额外的非线性可以提升网络的性能 C和D对比（D优）：C有些1*1的卷积，D相应的是3*3卷积。说明抓取空间局部信息同样重要 总的看出，误差随着网络的深度的增加而减低。 训练时，使用可变的尺度训练要比固定尺度的效果要好 5.2 多尺度评估 固定S训练的网络，考虑到训练与测试尺度之间的巨大差异会导致性能的下降，测试使用的Q={S-32， S， S+32}。 尺度S抖动训练的网络，则使用Q={Smin, 0.5(Smin, Smax), Smax}。 通过和单尺度的结果对比，可以发现在测试时使用多尺度(尺度抖动)，可以提升准确率 5.3 多重裁剪评估 可以看到，多重裁剪要比密集评估（多尺度）的结果要好。 论文结论，两种方法结合的效果会更好（softmax输出求均值），两种方法是互补的。 原因：卷积层提取特征时的padding方式不同。多重裁剪：补0；多尺度：补的是它们附近的像素 &nbsp; 5.4 多个网络结合 论文还将训练的多个网络模型的结果相结合得到最终的分类结果，显然，这会提升分类的准确率以及稳定性，结果如下： 5.5 多种方法的对比 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="目录 1 简介 2 网络结构 网络结构的特点 使用多个3*3叠加的原因： 使用1*1的卷积核 3 VGGNet网络的参数： 参数和计算量的对比 4 VGGNet的训练 超参数的设置 图片的处理 5 VGGNet的测试 将全连接换成全卷积 多重裁剪评估方式 5 VGGNet实验结果 5.1 单尺度评估 5.2 多尺度评估 5.3 多重裁剪评估 5.4 多个网络结合 5.5 多种方法的对比 1 简介 VGGNet由牛津大学的视觉几何组（Visual Geometry Group）提出，获得了2014年ILSVRC竞赛的分类任务第二名和定位任务第一名，主要贡献在于证明了使用3x3小卷积核，增加网络深度可以有效提升模型性能，并且对于其他数据集也有很好的泛化性能。 论文链接：Very deep convolutional networks for large-scale image recognition 2 网络结构 论文中一共提供了6种网络配置，层数从浅到深分别为11层、13层、16层和19层。其中11层时，主要比较了Local Response Normalisation（LRN）的作用，结果是LRN并没有提升网络性能。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 网络结构的特点 网络有5段卷积，每段卷积内分别有22333个卷积层，每段后面跟着个最大池化层 每段内的卷积核数量一样，越后边段内卷积核数量越多，依次为64-128-256-512-512 使用多个3*3叠加的原因： （两个3*3相当于5*5的感受野） 保持感受野相同的情况下，参数更少 增加了非线性，使网络对特征的学习能力更强 使用1*1的卷积核 在不影响感受野的情况下，增加非线性 用来调整特征图的channel的维度（这里没有使用） 3 VGGNet网络的参数： 现象：网络逐渐变深，参数量并没有增加很多。 原因：参数量主要消耗在最后的3个全连接层，前面的卷积层数虽多，但参数消耗量不大。（但卷积层训练比较耗时，因为计算量大） 参数和计算量的对比 在计算量这里，为了突出小卷积核的优势，我拿同样conv3x3、conv5x5、conv7x7、conv9x9和conv11x11，在224x224x3的RGB图上（output_channel=96）做卷积，卷积层的参数规模和得到的feature map的大小如下： kernel conv&nbsp;param:&nbsp;A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(与步长、padding无关) feature&nbsp;map:&nbsp;B &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 计算量 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; kernel_param * B *&nbsp;2 conv3x3 3x3x3x96&nbsp;&nbsp;（2592） 224x224x96&nbsp;(4816896) ‭260,112,384‬ conv5x5 5x5x3x96&nbsp;&nbsp;（7200） 224x224x96&nbsp;(4816896) ‭722,534,400‬ conv7x7 7x7x3x96&nbsp;&nbsp;（14112） 224x224x96&nbsp;(4816896) ‭1,416,167,424‬ conv9x9 9x9x3x96&nbsp;&nbsp;（23328） 224x224x96&nbsp;(4816896) ‭2,341,011,456‬ conv11x11 11x11x3x96&nbsp;&nbsp;（34848） 224x224x96&nbsp;(4816896) ‭3,497,066,496‬ &nbsp; &nbsp; &nbsp; &nbsp; 训练时网络的参数量:&nbsp;&nbsp;&nbsp;(A&nbsp;+&nbsp;B) 计算量:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_param * B *&nbsp;2 计算量：思考角度为特征图，特征图B上的每一个参数都需要经过kernel_param个计算。前项和后向共两次，所以再乘以2。 对于该例子：网络本身的参数占少数，网络每层出来的特征图参数400w；conv3*3的计算量达到2亿，conv11*11的计算量达到30亿。所以，使用两个conv3*3代替conv5*5，网络本身参数由7200--&gt;‭5184‬(2592+2592)，计算量由7亿--&gt;5亿(2.6亿+2.6亿） &nbsp; 4 VGGNet的训练 超参数的设置 训练使用加动量的小批基于反向传播的梯度下降法来优化多项逻辑回归目标 batch size为256，momentum设为0.9，权重衰减系数为0.0005 前两层的全连接后面跟着Dropout，设置为0.5 学习率初始为0.01，当验证集的准确率停止提高时，学习率除以10。最终学习率降低了三次以后，网络就收敛了 &nbsp;对于深度网络来说，网络的权重初始化十分重要。为此，论文中首先训练了一前程的网络结构A（第一个图中），训练折后网络时，随机初始化他的权重就足够得到比较好的结果。然后，当训练深层的网络时，前四层卷积核最后三个全连接层使用的是学习好的A网络的权重来进行初始化，其余层则随机初始化。（即某些层的预初始化） 图片的处理 （1）首先将图片保持高宽比进行缩放，使最短边缩放至S(S&gt;=224)。由于要求输入224*224大小，再对缩放后的图片进行crop。 S的取值方法： 固定的尺寸。对所有的图片，S都是固定的值。论文考察了S=256和S=384两种情况下分别训练得到的网络性能。为了加速，训练S=384的网络时，使用的训练好的S=256的网络来初始化权重，并且学习率更小，为0.001 可变的尺寸。设置一个范围[Smin, Smax]，每一张图片都从这个范围中随机选取一个数作为它的S。论文中使用的范围是[256, 512]。这个方法叫做尺度抖动scal jittering，有利于训练集增强，这使得训练在一个很大范围的图像尺度之上进行。为了加速，通过微调S=384的固定尺寸的网络来训练得到可变尺度的网络 （2）为了更进一步的增加训练集，对每张图片进行水平翻转以及进行随机RGB色差调整 &nbsp; 5 VGGNet的测试 将全连接换成全卷积 相同处： 两种方式，网络的参数是相同的。对于输入固定的清凉，效果也是相同的。 不同处： 全连接的多次输入时，必须保持输入维度不变。 需要保证网络的测试的图片保持相同的大小。 一张大图裁剪成多张224*224小图，重复的部分就会多次进入网络进行计算，产生很多冗余操作 卷积是可以接受不同尺寸的信息，当图片较大的情况下，网络输出的feature map尺寸就不是1*1*1000。这样： 图片输入时，不需要提前裁剪成224*224。 网络的输出还保留了得分的位置信息。在输出channel上求平均后，在输入softmax层。减少冗余操作（下图例子） 多重裁剪评估方式 在GoogleLeNet中描述的详细过程如下： 将图片缩放到不同的4种尺寸(纵横比不变，GoogleLeNet使用的4种尺寸为：缩放后的最短边长度分别为：256,288,320和352)。 对于得到的每个尺寸的图像，取左、中、右三个位置的正方形图像(边长就是最短边的长度。对于纵向图像来说，则取上、中、下三个位置)，因此每个尺寸的图像得到3个正方形图像； 然后再在每个正方形图像的4个crop顶点和中心位置处crop处224*224的图像，此外再加上将这个正方形图像缩放到224*224大小的图像，因此每个正方形图像得到6个224*224的图像； 最后，再将所有得到的224*224的图像水平翻转。因此，每个图像可以得到4*3*6*2=144个224*224大小的图像。 将这些图像分别输入神经网络进行分类，最后取平均，作为这个图像最终的分类结果。 而VGG中则使用的是3种尺寸，每个尺寸在5个位置处取正方形图像，每个正方形图像crop出5个224*224大小的图像，最后水平翻转，即3*5*5*2=150。 &nbsp; 5 VGGNet实验结果 &nbsp; &nbsp; 数据集 在本章，我们讲述了卷积神经网络在ILSVRC2012数据集上的分类结果。数据集包含1000个类别，被分为三部分：训练集（1.3M张图片），验证集（50K张图片），测试集（100K张图片，没有标签）。分类性能使用两个办法评估：top-1和top-5 error。前者是一个多类分类错误率，即错误分类图像的比例；后者是在ILSVRC上的主要评估标准，即真实类别不在top-5预测类别之中的图像的比例。 5.1 单尺度评估 对上面提出的模型进行测试。测试时，对于固定的S，去Q=S；对于可变的S，取Q=0.5*(Smin + Smax)。实验结果如下： A和A-LRN对比：后者性能并没有提升，LRN没有好的效果 B和C对比（C优）：C有多的1*1的卷积。说明额外的非线性可以提升网络的性能 C和D对比（D优）：C有些1*1的卷积，D相应的是3*3卷积。说明抓取空间局部信息同样重要 总的看出，误差随着网络的深度的增加而减低。 训练时，使用可变的尺度训练要比固定尺度的效果要好 5.2 多尺度评估 固定S训练的网络，考虑到训练与测试尺度之间的巨大差异会导致性能的下降，测试使用的Q={S-32， S， S+32}。 尺度S抖动训练的网络，则使用Q={Smin, 0.5(Smin, Smax), Smax}。 通过和单尺度的结果对比，可以发现在测试时使用多尺度(尺度抖动)，可以提升准确率 5.3 多重裁剪评估 可以看到，多重裁剪要比密集评估（多尺度）的结果要好。 论文结论，两种方法结合的效果会更好（softmax输出求均值），两种方法是互补的。 原因：卷积层提取特征时的padding方式不同。多重裁剪：补0；多尺度：补的是它们附近的像素 &nbsp; 5.4 多个网络结合 论文还将训练的多个网络模型的结果相结合得到最终的分类结果，显然，这会提升分类的准确率以及稳定性，结果如下： 5.5 多种方法的对比 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/08/02/792390.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/02/792390.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-02T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"目录 1 简介 2 网络结构 网络结构的特点 使用多个3*3叠加的原因： 使用1*1的卷积核 3 VGGNet网络的参数： 参数和计算量的对比 4 VGGNet的训练 超参数的设置 图片的处理 5 VGGNet的测试 将全连接换成全卷积 多重裁剪评估方式 5 VGGNet实验结果 5.1 单尺度评估 5.2 多尺度评估 5.3 多重裁剪评估 5.4 多个网络结合 5.5 多种方法的对比 1 简介 VGGNet由牛津大学的视觉几何组（Visual Geometry Group）提出，获得了2014年ILSVRC竞赛的分类任务第二名和定位任务第一名，主要贡献在于证明了使用3x3小卷积核，增加网络深度可以有效提升模型性能，并且对于其他数据集也有很好的泛化性能。 论文链接：Very deep convolutional networks for large-scale image recognition 2 网络结构 论文中一共提供了6种网络配置，层数从浅到深分别为11层、13层、16层和19层。其中11层时，主要比较了Local Response Normalisation（LRN）的作用，结果是LRN并没有提升网络性能。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 网络结构的特点 网络有5段卷积，每段卷积内分别有22333个卷积层，每段后面跟着个最大池化层 每段内的卷积核数量一样，越后边段内卷积核数量越多，依次为64-128-256-512-512 使用多个3*3叠加的原因： （两个3*3相当于5*5的感受野） 保持感受野相同的情况下，参数更少 增加了非线性，使网络对特征的学习能力更强 使用1*1的卷积核 在不影响感受野的情况下，增加非线性 用来调整特征图的channel的维度（这里没有使用） 3 VGGNet网络的参数： 现象：网络逐渐变深，参数量并没有增加很多。 原因：参数量主要消耗在最后的3个全连接层，前面的卷积层数虽多，但参数消耗量不大。（但卷积层训练比较耗时，因为计算量大） 参数和计算量的对比 在计算量这里，为了突出小卷积核的优势，我拿同样conv3x3、conv5x5、conv7x7、conv9x9和conv11x11，在224x224x3的RGB图上（output_channel=96）做卷积，卷积层的参数规模和得到的feature map的大小如下： kernel conv&nbsp;param:&nbsp;A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(与步长、padding无关) feature&nbsp;map:&nbsp;B &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 计算量 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; kernel_param * B *&nbsp;2 conv3x3 3x3x3x96&nbsp;&nbsp;（2592） 224x224x96&nbsp;(4816896) ‭260,112,384‬ conv5x5 5x5x3x96&nbsp;&nbsp;（7200） 224x224x96&nbsp;(4816896) ‭722,534,400‬ conv7x7 7x7x3x96&nbsp;&nbsp;（14112） 224x224x96&nbsp;(4816896) ‭1,416,167,424‬ conv9x9 9x9x3x96&nbsp;&nbsp;（23328） 224x224x96&nbsp;(4816896) ‭2,341,011,456‬ conv11x11 11x11x3x96&nbsp;&nbsp;（34848） 224x224x96&nbsp;(4816896) ‭3,497,066,496‬ &nbsp; &nbsp; &nbsp; &nbsp; 训练时网络的参数量:&nbsp;&nbsp;&nbsp;(A&nbsp;+&nbsp;B) 计算量:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_param * B *&nbsp;2 计算量：思考角度为特征图，特征图B上的每一个参数都需要经过kernel_param个计算。前项和后向共两次，所以再乘以2。 对于该例子：网络本身的参数占少数，网络每层出来的特征图参数400w；conv3*3的计算量达到2亿，conv11*11的计算量达到30亿。所以，使用两个conv3*3代替conv5*5，网络本身参数由7200--&gt;‭5184‬(2592+2592)，计算量由7亿--&gt;5亿(2.6亿+2.6亿） &nbsp; 4 VGGNet的训练 超参数的设置 训练使用加动量的小批基于反向传播的梯度下降法来优化多项逻辑回归目标 batch size为256，momentum设为0.9，权重衰减系数为0.0005 前两层的全连接后面跟着Dropout，设置为0.5 学习率初始为0.01，当验证集的准确率停止提高时，学习率除以10。最终学习率降低了三次以后，网络就收敛了 &nbsp;对于深度网络来说，网络的权重初始化十分重要。为此，论文中首先训练了一前程的网络结构A（第一个图中），训练折后网络时，随机初始化他的权重就足够得到比较好的结果。然后，当训练深层的网络时，前四层卷积核最后三个全连接层使用的是学习好的A网络的权重来进行初始化，其余层则随机初始化。（即某些层的预初始化） 图片的处理 （1）首先将图片保持高宽比进行缩放，使最短边缩放至S(S&gt;=224)。由于要求输入224*224大小，再对缩放后的图片进行crop。 S的取值方法： 固定的尺寸。对所有的图片，S都是固定的值。论文考察了S=256和S=384两种情况下分别训练得到的网络性能。为了加速，训练S=384的网络时，使用的训练好的S=256的网络来初始化权重，并且学习率更小，为0.001 可变的尺寸。设置一个范围[Smin, Smax]，每一张图片都从这个范围中随机选取一个数作为它的S。论文中使用的范围是[256, 512]。这个方法叫做尺度抖动scal jittering，有利于训练集增强，这使得训练在一个很大范围的图像尺度之上进行。为了加速，通过微调S=384的固定尺寸的网络来训练得到可变尺度的网络 （2）为了更进一步的增加训练集，对每张图片进行水平翻转以及进行随机RGB色差调整 &nbsp; 5 VGGNet的测试 将全连接换成全卷积 相同处： 两种方式，网络的参数是相同的。对于输入固定的清凉，效果也是相同的。 不同处： 全连接的多次输入时，必须保持输入维度不变。 需要保证网络的测试的图片保持相同的大小。 一张大图裁剪成多张224*224小图，重复的部分就会多次进入网络进行计算，产生很多冗余操作 卷积是可以接受不同尺寸的信息，当图片较大的情况下，网络输出的feature map尺寸就不是1*1*1000。这样： 图片输入时，不需要提前裁剪成224*224。 网络的输出还保留了得分的位置信息。在输出channel上求平均后，在输入softmax层。减少冗余操作（下图例子） 多重裁剪评估方式 在GoogleLeNet中描述的详细过程如下： 将图片缩放到不同的4种尺寸(纵横比不变，GoogleLeNet使用的4种尺寸为：缩放后的最短边长度分别为：256,288,320和352)。 对于得到的每个尺寸的图像，取左、中、右三个位置的正方形图像(边长就是最短边的长度。对于纵向图像来说，则取上、中、下三个位置)，因此每个尺寸的图像得到3个正方形图像； 然后再在每个正方形图像的4个crop顶点和中心位置处crop处224*224的图像，此外再加上将这个正方形图像缩放到224*224大小的图像，因此每个正方形图像得到6个224*224的图像； 最后，再将所有得到的224*224的图像水平翻转。因此，每个图像可以得到4*3*6*2=144个224*224大小的图像。 将这些图像分别输入神经网络进行分类，最后取平均，作为这个图像最终的分类结果。 而VGG中则使用的是3种尺寸，每个尺寸在5个位置处取正方形图像，每个正方形图像crop出5个224*224大小的图像，最后水平翻转，即3*5*5*2=150。 &nbsp; 5 VGGNet实验结果 &nbsp; &nbsp; 数据集 在本章，我们讲述了卷积神经网络在ILSVRC2012数据集上的分类结果。数据集包含1000个类别，被分为三部分：训练集（1.3M张图片），验证集（50K张图片），测试集（100K张图片，没有标签）。分类性能使用两个办法评估：top-1和top-5 error。前者是一个多类分类错误率，即错误分类图像的比例；后者是在ILSVRC上的主要评估标准，即真实类别不在top-5预测类别之中的图像的比例。 5.1 单尺度评估 对上面提出的模型进行测试。测试时，对于固定的S，去Q=S；对于可变的S，取Q=0.5*(Smin + Smax)。实验结果如下： A和A-LRN对比：后者性能并没有提升，LRN没有好的效果 B和C对比（C优）：C有多的1*1的卷积。说明额外的非线性可以提升网络的性能 C和D对比（D优）：C有些1*1的卷积，D相应的是3*3卷积。说明抓取空间局部信息同样重要 总的看出，误差随着网络的深度的增加而减低。 训练时，使用可变的尺度训练要比固定尺度的效果要好 5.2 多尺度评估 固定S训练的网络，考虑到训练与测试尺度之间的巨大差异会导致性能的下降，测试使用的Q={S-32， S， S+32}。 尺度S抖动训练的网络，则使用Q={Smin, 0.5(Smin, Smax), Smax}。 通过和单尺度的结果对比，可以发现在测试时使用多尺度(尺度抖动)，可以提升准确率 5.3 多重裁剪评估 可以看到，多重裁剪要比密集评估（多尺度）的结果要好。 论文结论，两种方法结合的效果会更好（softmax输出求均值），两种方法是互补的。 原因：卷积层提取特征时的padding方式不同。多重裁剪：补0；多尺度：补的是它们附近的像素 &nbsp; 5.4 多个网络结合 论文还将训练的多个网络模型的结果相结合得到最终的分类结果，显然，这会提升分类的准确率以及稳定性，结果如下： 5.5 多种方法的对比 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/08/02/792390.html","headline":"深度学习之图像分类–VGG","dateModified":"2019-08-02T00:00:00+08:00","datePublished":"2019-08-02T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/02/792390.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>深度学习之图像分类--VGG</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="1%20%E7%AE%80%E4%BB%8B-toc" style="margin-left:0px;"><a href="#1%20%E7%AE%80%E4%BB%8B" rel="nofollow" data-token="b8117bb60561f90424c7433e8cf8da4d">1 简介</a></p> 
  <p id="2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:0px;"><a href="#2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow" data-token="3cf9ba49cc5bf82547622b17b45f02f6">2 网络结构</a></p> 
  <p id="%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E7%89%B9%E7%82%B9-toc" style="margin-left:80px;"><a href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E7%89%B9%E7%82%B9" rel="nofollow" data-token="a84695fff9885c287ad881cb01a79e17">网络结构的特点</a></p> 
  <p id="%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA3*3%E5%8F%A0%E5%8A%A0%E7%9A%84%E5%8E%9F%E5%9B%A0%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA3*3%E5%8F%A0%E5%8A%A0%E7%9A%84%E5%8E%9F%E5%9B%A0%EF%BC%9A" rel="nofollow" data-token="7b202f6c72bbfc94984d0439d74d43e1">使用多个3*3叠加的原因：</a></p> 
  <p id="%E4%BD%BF%E7%94%A81*1%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8-toc" style="margin-left:80px;"><a href="#%E4%BD%BF%E7%94%A81*1%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8" rel="nofollow" data-token="68de7d84487a1ea957a2f075078b6b39">使用1*1的卷积核</a></p> 
  <p id="3%20VGGNet%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%9A-toc" style="margin-left:0px;"><a href="#3%20VGGNet%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%9A" rel="nofollow" data-token="3d0b9ff8aa4c65c8dcaf813a452fb620">3 VGGNet网络的参数：</a></p> 
  <p id="%E5%8F%82%E6%95%B0%E5%92%8C%E8%AE%A1%E7%AE%97%E9%87%8F%E7%9A%84%E5%AF%B9%E6%AF%94-toc" style="margin-left:80px;"><a href="#%E5%8F%82%E6%95%B0%E5%92%8C%E8%AE%A1%E7%AE%97%E9%87%8F%E7%9A%84%E5%AF%B9%E6%AF%94" rel="nofollow" data-token="03844e69863944defc578ef3d855e30c">参数和计算量的对比</a></p> 
  <p id="4%20GGNet%E7%9A%84%E8%AE%AD%E7%BB%83-toc" style="margin-left:0px;"><a href="#4%20GGNet%E7%9A%84%E8%AE%AD%E7%BB%83" rel="nofollow" data-token="716e4586a07cf23e16a33070eaf8e27e">4 VGGNet的训练</a></p> 
  <p id="%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E8%AE%BE%E7%BD%AE-toc" style="margin-left:80px;"><a href="#%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E8%AE%BE%E7%BD%AE" rel="nofollow" data-token="57b3e5c05c8bbb7dafaf052606bcf1cc">超参数的设置</a></p> 
  <p id="%E5%9B%BE%E7%89%87%E7%9A%84%E5%A4%84%E7%90%86-toc" style="margin-left:80px;"><a href="#%E5%9B%BE%E7%89%87%E7%9A%84%E5%A4%84%E7%90%86" rel="nofollow" data-token="95ae5d19e9239eeb6796536802123a58">图片的处理</a></p> 
  <p id="5%20VGGNet%E7%9A%84%E6%B5%8B%E8%AF%95-toc" style="margin-left:0px;"><a href="#5%20VGGNet%E7%9A%84%E6%B5%8B%E8%AF%95" rel="nofollow" data-token="30b5ce799c33a321ba39aa1af423729c">5 VGGNet的测试</a></p> 
  <p id="%E5%B0%86%E5%85%A8%E8%BF%9E%E6%8E%A5%E6%8D%A2%E6%88%90%E5%85%A8%E5%8D%B7%E7%A7%AF-toc" style="margin-left:80px;"><a href="#%E5%B0%86%E5%85%A8%E8%BF%9E%E6%8E%A5%E6%8D%A2%E6%88%90%E5%85%A8%E5%8D%B7%E7%A7%AF" rel="nofollow" data-token="88f1d830f1c5018c420073620c587da5">将全连接换成全卷积</a></p> 
  <p id="%E5%A4%9A%E9%87%8D%E8%A3%81%E5%89%AA%E8%AF%84%E4%BC%B0%E6%96%B9%E5%BC%8F-toc" style="margin-left:80px;"><a href="#%E5%A4%9A%E9%87%8D%E8%A3%81%E5%89%AA%E8%AF%84%E4%BC%B0%E6%96%B9%E5%BC%8F" rel="nofollow" data-token="005c9991a969a163b952dcbbead73c5f">多重裁剪评估方式</a></p> 
  <p id="5%20VGGNet%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-toc" style="margin-left:0px;"><a href="#5%20VGGNet%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C" rel="nofollow" data-token="f09ef0c9fe094468de398e7cd0e8681d">5 VGGNet实验结果</a></p> 
  <p id="5.1%20%E5%8D%95%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BC%B0-toc" style="margin-left:80px;"><a href="#5.1%20%E5%8D%95%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BC%B0" rel="nofollow" data-token="491d5502b02a72c889629f044e2db604">5.1 单尺度评估</a></p> 
  <p id="5.2%20%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BC%B0-toc" style="margin-left:80px;"><a href="#5.2%20%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BC%B0" rel="nofollow" data-token="a5c3382e8e449e1997a55d7375799cd4">5.2 多尺度评估</a></p> 
  <p id="5.3%20%E5%A4%9A%E9%87%8D%E8%A3%81%E5%89%AA%E8%AF%84%E4%BC%B0-toc" style="margin-left:80px;"><a href="#5.3%20%E5%A4%9A%E9%87%8D%E8%A3%81%E5%89%AA%E8%AF%84%E4%BC%B0" rel="nofollow" data-token="c7627f0b7f22c035b1acf98cede43c39">5.3 多重裁剪评估</a></p> 
  <p id="5.4%20%E5%A4%9A%E4%B8%AA%E7%BD%91%E7%BB%9C%E7%BB%93%E5%90%88-toc" style="margin-left:80px;"><a href="#5.4%20%E5%A4%9A%E4%B8%AA%E7%BD%91%E7%BB%9C%E7%BB%93%E5%90%88" rel="nofollow" data-token="61c5cbcff55655cfab335ecaf9a4a9e7">5.4 多个网络结合</a></p> 
  <p id="5.5%20%E5%A4%9A%E7%A7%8D%E6%96%B9%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94-toc" style="margin-left:80px;"><a href="#5.5%20%E5%A4%9A%E7%A7%8D%E6%96%B9%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94" rel="nofollow" data-token="c12aee1fa98c5b9e370daa467b3957e7">5.5 多种方法的对比</a></p> 
  <hr id="hr-toc">
  <h1 id="1%20%E7%AE%80%E4%BB%8B">1 简介</h1> 
  <p>VGGNet由牛津大学的视觉几何组（Visual Geometry Group）提出，获得了2014年ILSVRC竞赛的分类任务第二名和定位任务第一名，主要贡献在于证明了使用3x3小卷积核，增加网络深度可以有效提升模型性能，并且对于其他数据集也有很好的泛化性能。</p> 
  <p>论文链接：<a href="https://arxiv.org/pdf/1409.1556.pdf" rel="nofollow" data-token="397f92891635b67235166bbdf2f08266">Very deep convolutional networks for large-scale image recognition</a></p> 
  <h1 id="2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">2 网络结构</h1> 
  <p>论文中一共提供了6种网络配置，层数从浅到深分别为11层、13层、16层和19层。其中11层时，主要比较了Local Response Normalisation（LRN）的作用，结果是LRN并没有提升网络性能。</p> 
  <p><img alt="" class="has" height="476" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731095543600.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hZ2ljX2xs,size_16,color_FFFFFF,t_70" width="469">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="" class="has" height="444" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731100929488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hZ2ljX2xs,size_16,color_FFFFFF,t_70" width="194"></p> 
  <h3 id="%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E7%89%B9%E7%82%B9">网络结构的特点</h3> 
  <ol>
   <li>网络有5段卷积，每段卷积内分别有22333个卷积层，每段后面跟着个最大池化层</li> 
   <li>每段内的卷积核数量一样，越后边段内卷积核数量越多，依次为64-128-256-512-512</li> 
  </ol>
  <h3 id="%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA3*3%E5%8F%A0%E5%8A%A0%E7%9A%84%E5%8E%9F%E5%9B%A0%EF%BC%9A">使用多个3*3叠加的原因：</h3> 
  <p>（两个3*3相当于5*5的感受野）</p> 
  <ol>
   <li>保持感受野相同的情况下，参数更少</li> 
   <li>增加了非线性，使网络对特征的学习能力更强</li> 
  </ol>
  <h3 id="%E4%BD%BF%E7%94%A81*1%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8">使用1*1的卷积核</h3> 
  <ol>
   <li>在不影响感受野的情况下，增加非线性</li> 
   <li>用来调整特征图的channel的维度（这里没有使用）</li> 
  </ol>
  <h1 id="3%20VGGNet%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%9A">3 VGGNet网络的参数：</h1> 
  <p><img alt="" class="has" height="48" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731142124342.png" width="358"></p> 
  <ul>
   <li>现象：网络逐渐变深，参数量并没有增加很多。</li> 
   <li>原因：参数量主要消耗在最后的3个全连接层，前面的卷积层数虽多，但参数消耗量不大。（但卷积层训练比较耗时，因为计算量大）</li> 
  </ul>
  <h3 id="%E5%8F%82%E6%95%B0%E5%92%8C%E8%AE%A1%E7%AE%97%E9%87%8F%E7%9A%84%E5%AF%B9%E6%AF%94">参数和计算量的对比</h3> 
  <p>在计算量这里，为了突出小卷积核的优势，我拿同样conv3x3、conv5x5、conv7x7、conv9x9和conv11x11，在224x224x3的RGB图上（output_channel=96）做卷积，卷积层的参数规模和得到的feature map的大小如下：</p> 
  <table border="1" cellspacing="0" style="width:487.52pt;">
   <tbody>
    <tr>
     <td style="text-align:center;vertical-align:middle;width:79.53pt;"><span style="color:#000000;">kernel</span></td> 
     <td style="text-align:center;vertical-align:middle;width:129.03pt;"><span style="color:#000000;">conv&nbsp;param:&nbsp;A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(与步长、padding无关)</span></td> 
     <td style="text-align:center;vertical-align:middle;width:153.78pt;"><span style="color:#000000;">feature&nbsp;map:&nbsp;B</span></td> 
     <td style="text-align:center;vertical-align:middle;width:125.28pt;"><span style="color:#000000;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 计算量 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; kernel_param * B *&nbsp;2</span></td> 
    </tr>
    <tr>
     <td style="text-align:center;vertical-align:middle;width:79.53pt;"><span style="color:#000000;">conv3x3</span></td> 
     <td style="text-align:center;vertical-align:middle;width:129.03pt;"><span style="color:#000000;">3x3x3x96&nbsp;&nbsp;（2592）</span></td> 
     <td style="text-align:center;vertical-align:middle;width:153.78pt;"><span style="color:#000000;">224x224x96&nbsp;(4816896)</span></td> 
     <td style="text-align:center;vertical-align:middle;width:125.28pt;"><span style="color:#000000;">‭260,112,384‬</span></td> 
    </tr>
    <tr>
     <td style="text-align:center;vertical-align:middle;width:79.53pt;"><span style="color:#000000;">conv5x5</span></td> 
     <td style="text-align:center;vertical-align:middle;width:129.03pt;"><span style="color:#000000;">5x5x3x96&nbsp;&nbsp;（7200）</span></td> 
     <td style="text-align:center;vertical-align:middle;width:153.78pt;"><span style="color:#000000;">224x224x96&nbsp;(4816896)</span></td> 
     <td style="text-align:center;vertical-align:middle;width:125.28pt;"><span style="color:#000000;">‭722,534,400‬</span></td> 
    </tr>
    <tr>
     <td style="text-align:center;vertical-align:middle;width:79.53pt;"><span style="color:#000000;">conv7x7</span></td> 
     <td style="text-align:center;vertical-align:middle;width:129.03pt;"><span style="color:#000000;">7x7x3x96&nbsp;&nbsp;（14112）</span></td> 
     <td style="text-align:center;vertical-align:middle;width:153.78pt;"><span style="color:#000000;">224x224x96&nbsp;(4816896)</span></td> 
     <td style="text-align:center;vertical-align:middle;width:125.28pt;"><span style="color:#000000;">‭1,416,167,424‬</span></td> 
    </tr>
    <tr>
     <td style="text-align:center;vertical-align:middle;width:79.53pt;"><span style="color:#000000;">conv9x9</span></td> 
     <td style="text-align:center;vertical-align:middle;width:129.03pt;"><span style="color:#000000;">9x9x3x96&nbsp;&nbsp;（23328）</span></td> 
     <td style="text-align:center;vertical-align:middle;width:153.78pt;"><span style="color:#000000;">224x224x96&nbsp;(4816896)</span></td> 
     <td style="text-align:center;vertical-align:middle;width:125.28pt;"><span style="color:#000000;">‭2,341,011,456‬</span></td> 
    </tr>
    <tr>
     <td style="text-align:center;vertical-align:middle;width:79.53pt;"><span style="color:#000000;">conv11x11</span></td> 
     <td style="text-align:center;vertical-align:middle;width:129.03pt;"><span style="color:#000000;">11x11x3x96&nbsp;&nbsp;（34848）</span></td> 
     <td style="text-align:center;vertical-align:middle;width:153.78pt;"><span style="color:#000000;">224x224x96&nbsp;(4816896)</span></td> 
     <td style="text-align:center;vertical-align:middle;width:125.28pt;"><span style="color:#000000;">‭3,497,066,496‬</span></td> 
    </tr>
    <tr>
     <td style="vertical-align:middle;width:79.53pt;">&nbsp;</td> 
     <td style="vertical-align:middle;width:129.03pt;">&nbsp;</td> 
     <td style="vertical-align:middle;width:153.78pt;">&nbsp;</td> 
     <td style="vertical-align:middle;width:125.28pt;">&nbsp;</td> 
    </tr>
    <tr>
     <td colspan="4" style="text-align:left;vertical-align:middle;width:441.77pt;"><span style="color:#000000;">训练时网络的参数量:&nbsp;&nbsp;&nbsp;(A&nbsp;+&nbsp;B)</span></td> 
    </tr>
    <tr>
     <td colspan="4" style="text-align:left;vertical-align:middle;width:441.77pt;"><span style="color:#000000;">计算量:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kernel_param * B *&nbsp;2</span></td> 
    </tr>
   </tbody>
  </table>
  <ul>
   <li>计算量：思考角度为特征图，特征图B上的每一个参数都需要经过kernel_param个计算。前项和后向共两次，所以再乘以2。</li> 
   <li>对于该例子：网络本身的参数占少数，网络每层出来的特征图参数400w；conv3*3的计算量达到2亿，conv11*11的计算量达到30亿。所以，使用两个conv3*3代替conv5*5，网络本身参数由7200--&gt;‭5184‬(2592+2592)，计算量由7亿--&gt;5亿(2.6亿+2.6亿）</li> 
  </ul>
  <p>&nbsp;</p> 
  <h1 id="4%20GGNet%E7%9A%84%E8%AE%AD%E7%BB%83">4 VGGNet的训练</h1> 
  <h3 id="%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E8%AE%BE%E7%BD%AE">超参数的设置</h3> 
  <ol>
   <li>训练使用加动量的小批基于反向传播的<strong>梯度下降法</strong>来优化多项逻辑回归目标</li> 
   <li><strong>batch size</strong>为256，<strong>momentum</strong>设为0.9，<strong>权重衰减系数</strong>为0.0005</li> 
   <li>前两层的全连接后面跟着<strong>Dropout</strong>，设置为0.5</li> 
   <li><strong>学习率</strong>初始为0.01，当验证集的准确率停止提高时，学习率除以10。最终学习率降低了三次以后，网络就收敛了</li> 
   <li>&nbsp;对于深度网络来说，网络的<strong>权重初始化</strong>十分重要。为此，论文中首先训练了一前程的网络结构A（第一个图中），训练折后网络时，随机初始化他的权重就足够得到比较好的结果。然后，<u>当训练深层的网络时，前四层卷积核最后三个全连接层使用的是学习好的A网络的权重来进行初始化，其余层则随机初始化。</u>（即某些层的预初始化）</li> 
  </ol>
  <h3 id="%E5%9B%BE%E7%89%87%E7%9A%84%E5%A4%84%E7%90%86">图片的处理</h3> 
  <p>（1）首先将图片保持高宽比进行缩放，使最短边缩放至S(S&gt;=224)。由于要求输入224*224大小，再对缩放后的图片进行crop。<br> S的取值方法：</p> 
  <ol>
   <li>固定的尺寸。对所有的图片，S都是固定的值。论文考察了S=256和S=384两种情况下分别训练得到的网络性能。<u>为了加速，训练S=384的网络时，使用的训练好的S=256的网络来初始化权重，并且学习率更小，为0.001</u></li> 
   <li>可变的尺寸。设置一个范围[Smin, Smax]，每一张图片都从这个范围中随机选取一个数作为它的S。论文中使用的范围是[256, 512]。这个方法叫做尺度抖动scal jittering，有利于训练集增强，这使得训练在一个很大范围的图像尺度之上进行。<u>为了加速，通过微调S=384的固定尺寸的网络来训练得到可变尺度的网络</u></li> 
  </ol>
  <p>（2）为了更进一步的增加训练集，对每张图片进行水平翻转以及进行随机RGB色差调整</p> 
  <p>&nbsp;</p> 
  <h1 id="5%20VGGNet%E7%9A%84%E6%B5%8B%E8%AF%95">5 VGGNet的测试</h1> 
  <h3 id="%E5%B0%86%E5%85%A8%E8%BF%9E%E6%8E%A5%E6%8D%A2%E6%88%90%E5%85%A8%E5%8D%B7%E7%A7%AF">将全连接换成全卷积</h3> 
  <p><img alt="" class="has" height="236" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190802102727514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hZ2ljX2xs,size_16,color_FFFFFF,t_70" width="360"></p> 
  <p>相同处：</p> 
  <ul>
   <li>两种方式，网络的参数是相同的。对于输入固定的清凉，效果也是相同的。</li> 
  </ul>
  <p>不同处：</p> 
  <ul>
   <li>全连接的多次输入时，必须<strong>保持输入维度不变</strong>。 
    <ul>
     <li>需要保证网络的测试的图片保持相同的大小。</li> 
     <li>一张大图裁剪成多张224*224小图，重复的部分就会多次进入网络进行计算，产生很多冗余操作</li> 
    </ul></li> 
   <li><strong>卷积是可以接受不同尺寸的信息</strong>，当图片较大的情况下，网络输出的feature map尺寸就不是1*1*1000。这样： 
    <ul>
     <li>图片输入时，不需要提前裁剪成224*224。</li> 
     <li>网络的输出还保留了得分的位置信息。在输出channel上求平均后，在输入softmax层。减少冗余操作（下图例子）</li> 
    </ul></li> 
  </ul>
  <p><img alt="" class="has" height="217" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy85MzY3MTI3LTgwNzBkZTQ0NWI1YWVmZDAucG5nP2ltYWdlTW9ncjIvYXV0by1vcmllbnQv" width="380"></p> 
  <h3 id="%E5%A4%9A%E9%87%8D%E8%A3%81%E5%89%AA%E8%AF%84%E4%BC%B0%E6%96%B9%E5%BC%8F">多重裁剪评估方式</h3> 
  <p>在GoogleLeNet中描述的详细过程如下：</p> 
  <ol>
   <li>将图片缩放到不同的<strong>4种尺寸</strong>(纵横比不变，GoogleLeNet使用的4种尺寸为：缩放后的最短边长度分别为：256,288,320和352)。</li> 
   <li>对于得到的每个尺寸的图像，取<strong>左、中、右三个位置的正方形图像</strong>(边长就是最短边的长度。对于纵向图像来说，则取上、中、下三个位置)，因此每个尺寸的图像得到3个正方形图像；</li> 
   <li>然后再在每个正方形图像的4个crop顶点和中心位置处crop处224*224的图像，此外再加上将这个正方形图像缩放到224*224大小的图像，因此每个正方形图像得到<strong>6个224*224的图像</strong>；</li> 
   <li>最后，再将所有得到的224*224的图像<strong>水平翻转</strong>。因此，每个图像可以得到<u>4*3*6*2=144个224*224大小的图像</u>。</li> 
   <li>将这些图像分别输入神经网络进行分类，最后取平均，作为这个图像最终的分类结果。</li> 
  </ol>
  <p>而VGG中则使用的是3种尺寸，每个尺寸在5个位置处取正方形图像，每个正方形图像crop出5个224*224大小的图像，最后水平翻转，即3*5*5*2=150。</p> 
  <p>&nbsp;</p> 
  <h1 id="5%20VGGNet%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C">5 VGGNet实验结果</h1> 
  <p>&nbsp; &nbsp; 数据集 在本章，我们讲述了卷积神经网络在ILSVRC2012数据集上的分类结果。数据集包含1000个类别，被分为三部分：训练集（1.3M张图片），验证集（50K张图片），测试集（100K张图片，没有标签）。分类性能使用两个办法评估：top-1和top-5 error。前者是一个多类分类错误率，即错误分类图像的比例；后者是在ILSVRC上的主要评估标准，即真实类别不在top-5预测类别之中的图像的比例。</p> 
  <h3 id="5.1%20%E5%8D%95%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BC%B0">5.1 单尺度评估</h3> 
  <p>对上面提出的模型进行测试。测试时，对于固定的S，去Q=S；对于可变的S，取Q=0.5*(Smin + Smax)。实验结果如下：</p> 
  <p><img alt="" class="has" height="248" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731143222242.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hZ2ljX2xs,size_16,color_FFFFFF,t_70" width="569"></p> 
  <ul>
   <li>A和A-LRN对比：后者性能并没有提升，LRN没有好的效果</li> 
   <li>B和C对比（C优）：C有多的1*1的卷积。说明额外的非线性可以提升网络的性能</li> 
   <li>C和D对比（D优）：C有些1*1的卷积，D相应的是3*3卷积。说明抓取空间局部信息同样重要</li> 
   <li>总的看出，误差随着网络的深度的增加而减低。</li> 
   <li>训练时，使用可变的尺度训练要比固定尺度的效果要好</li> 
  </ul>
  <h3 id="5.2%20%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BC%B0">5.2 多尺度评估</h3> 
  <p>固定S训练的网络，考虑到训练与测试尺度之间的巨大差异会导致性能的下降，测试使用的Q={S-32， S， S+32}。<br> 尺度S抖动训练的网络，则使用Q={Smin, 0.5(Smin, Smax), Smax}。</p> 
  <p><img alt="" class="has" height="204" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190802134658890.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hZ2ljX2xs,size_16,color_FFFFFF,t_70" width="573"></p> 
  <ul>
   <li>通过和单尺度的结果对比，可以发现在测试时使用多尺度(尺度抖动)，可以提升准确率</li> 
  </ul>
  <h3 id="5.3%20%E5%A4%9A%E9%87%8D%E8%A3%81%E5%89%AA%E8%AF%84%E4%BC%B0">5.3 多重裁剪评估</h3> 
  <p><img alt="" class="has" height="120" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190802135847950.png" width="571"></p> 
  <ul>
   <li>可以看到，多重裁剪要比密集评估（多尺度）的结果要好。</li> 
   <li>论文结论，两种方法结合的效果会更好（softmax输出求均值），两种方法是互补的。 
    <ul>
     <li>原因：卷积层提取特征时的padding方式不同。多重裁剪：补0；多尺度：补的是它们附近的像素</li> 
    </ul></li> 
  </ul>
  <p>&nbsp;</p> 
  <h3 id="5.4%20%E5%A4%9A%E4%B8%AA%E7%BD%91%E7%BB%9C%E7%BB%93%E5%90%88">5.4 多个网络结合</h3> 
  <p>论文还将训练的多个网络模型的结果相结合得到最终的分类结果，显然，这会提升分类的准确率以及稳定性，结果如下：</p> 
  <p><img alt="" class="has" height="158" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190802141218240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hZ2ljX2xs,size_16,color_FFFFFF,t_70" width="567"></p> 
  <h3 id="5.5%20%E5%A4%9A%E7%A7%8D%E6%96%B9%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94">5.5 多种方法的对比</h3> 
  <p><img alt="" class="has" height="241" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190802141237288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hZ2ljX2xs,size_16,color_FFFFFF,t_70" width="569"></p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
