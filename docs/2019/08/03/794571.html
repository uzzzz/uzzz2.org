<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Hadoop 核心-MapReduce（基础概念）–分区和排序 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Hadoop 核心-MapReduce（基础概念）–分区和排序" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hadoop-Mapreduce 1. MapReduce 介绍 MapReduce思想在生活中处处可见。或多或少都曾接触过这种思想。MapReduce的思想核心是“分而治之”，适用于大量复杂的任务处理场景（大规模数据处理场景）。 Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。 Reduce负责“合”，即对map阶段的结果进行全局汇总。 MapReduce运行在yarn集群 ResourceManager NodeManager 这两个阶段合起来正是MapReduce思想的体现。 还有一个比较形象的语言解释MapReduce: 我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“Map”。我们人越多，数书就更快。 现在我们到一起，把所有人的统计数加在一起。这就是“Reduce”。 1.1. MapReduce 设计构思 MapReduce是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在Hadoop集群上。 MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理： Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现.Map和Reduce,MapReduce处理的数据类型是&lt;key,value&gt;键值对。 Map: (k1; v1) → [(k2; v2)] Reduce: (k2; [v2]) → [(k3; v3)] 一个完整的mapreduce程序在分布式运行时有三类实例进程： MRAppMaster 负责整个程序的过程调度及状态协调 MapTask 负责map阶段的整个数据处理流程 ReduceTask 负责reduce阶段的整个数据处理流程 2. MapReduce 编程规范 MapReduce 的开发一共有八个步骤, 其中 Map 阶段分为 2 个步骤，Shuffle 阶段 4 个步骤，Reduce 阶段分为 2 个步骤 Map 阶段 2 个步骤 设置 InputFormat 类（TextInputFormat）, 将数据切分为 Key-Value (K1和V1) 对, 输入到第二步 自定义 Map 逻辑, 将第一步的结果转换成另外的 Key-Value（K2和V2） 对, 输出结果 Shuffle 阶段 4 个步骤 对输出的 Key-Value 对进行分区 对不同分区的数据按照相同的 Key 排序 (可选) 对分组过的数据初步规约, 降低数据的网络拷贝 对数据进行分组, 相同 Key 的 Value 放入一个集合中 Reduce 阶段 2 个步骤 对多个 Map 任务的结果进行排序以及合并, 编写 Reduce 函数实现自己的逻辑, 对输入的 Key-Value 进行处理, 转为新的 Key-Value（K3和V3）输出 设置 OutputFormat 处理并保存 Reduce 输出的 Key-Value 数据 3. WordCount 需求: 在一堆给定的文本文件中统计输出每一个单词出现的总次数 Step 1. 数据格式准备 创建一个新的文件 cd /export/servers vim wordcount.txt 向其中放入以下内容并保存 hello,world,hadoop hive,sqoop,flume,hello kitty,tom,jerry,world hadoop 上传到 HDFS hdfs dfs -mkdir /wordcount/ hdfs dfs -put wordcount.txt /wordcount/ Step 2. Mapper //重写mapper方法 public class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt; { @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { //将v1使用‘,’进行分割，存入到context上。 /*转化的类型为 hello 1 world 1 hadoop 1 String line = value.toString(); String[] split = line.split(&quot;,&quot;); for (String word : split) { context.write(new Text(word),new LongWritable(1)); } } } Step 3. Reducer //重写reduce方法 public class WordCountReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt; { /** * 自定义我们的reduce逻辑 * 所有的key都是我们的单词，所有的values都是我们单词出现的次数 * 统计单词出现的个数 */ @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException { long count = 0; for (LongWritable value : values) { count += value.get(); } context.write(key,new LongWritable(count)); } } Step 4. 定义主类, 描述 Job 并提交 Job public class JobMain extends Configured implements Tool { @Override public int run(String[] args) throws Exception { Job job = Job.getInstance(super.getConf(), JobMain.class.getSimpleName()); //打包到集群上面运行时候，必须要添加以下配置，指定程序的main函数 job.setJarByClass(JobMain.class); //第一步：读取输入文件解析成key，value对 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/wordcount&quot;)); //第二步：设置我们的mapper类 job.setMapperClass(WordCountMapper.class); //设置我们map阶段完成之后的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); //第三步，第四步，第五步，第六步，采取默认的设置-省略 //第七步：设置我们的reduce类 job.setReducerClass(WordCountReducer.class); //设置我们reduce阶段完成之后的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //第八步：设置输出类以及输出路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/wordcount_out&quot;)); boolean b = job.waitForCompletion(true); return b?0:1; } /** * 程序main函数的入口类 * @param args * @throws Exception */ public static void main(String[] args) throws Exception { Configuration configuration = new Configuration(); Tool tool = new JobMain(); int run = ToolRunner.run(configuration, tool, args); System.exit(run); } } 常见错误 如果遇到如下错误 Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=admin, access=WRITE, inode=&quot;/&quot;:root:supergroup:drwxr-xr-x 直接将hdfs-site.xml当中的权限关闭即可 &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 最后重启一下 HDFS 集群 小细节 本地运行完成之后，就可以打成jar包放到服务器上面去运行了，实际工作当中，都是将代码打成jar包，开发main方法作为程序的入口，然后放到集群上面去运行 4. MapReduce 运行模式 本地运行模式 MapReduce 程序被提交在本地以单进程的形式运行 处理的数据及输出结果可以在本地文件系统, 也可以在hdfs上 怎样实现本地运行? 将输入和输出的路径改为本地路径 本地模式非常便于进行业务逻辑的 Debug, 只要在 Eclipse 中打断点即可 configuration.set(&quot;mapreduce.framework.name&quot;,&quot;local&quot;); configuration.set(&quot; yarn.resourcemanager.hostname&quot;,&quot;local&quot;); TextInputFormat.addInputPath(job,new Path(&quot;file:///F:\\wordcount\\input&quot;)); TextOutputFormat.setOutputPath(job,new Path(&quot;file:///F:\\wordcount\\output&quot;)); 集群运行模式 将 MapReduce 程序提交给 Yarn 集群, 分发到很多的节点上并发执行 处理的数据和输出结果应该位于 HDFS 文件系统 提交集群的实现步骤: 将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动。（需要指定主类运行的路径） hadoop jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar cn.itcast.hdfs.demo1.JobMain 5. MapReduce 分区 在 MapReduce 中, 通过我们指定分区, 会将同一个分区的数据发送到同一个 Reduce 当中进行处理 例如: 为了数据的统计, 可以把一批类似的数据（以单词统计案例为例）发送到同一个 Reduce 当中, 在同一个 Reduce 当中统计相同类型的数据, 就可以实现类似的数据分区和统计等 其实就是相同类型的数据, 有共性的数据, 送到一起去处理 Reduce 当中默认的分区只有一个 Step 1. 定义 Mapper 这个 Mapper 程序不做任何逻辑, 也不对 Key-Value 做任何改变, 只是接收数据, 然后往下发送 public class MyMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { context.write(value,NullWritable.get()); } } Step 2. 定义 Reducer 逻辑 这个 Reducer 也不做任何处理, 将数据原封不动的输出即可 public class MyReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt; { @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { context.write(key,NullWritable.get()); } } Step 3. 自定义 Partitioner 主要的逻辑就在这里, 这也是这个案例的意义, 通过 Partitioner 将数据分发给不同的 Reducer public class PartitonerOwn extends Partitioner&lt;Text,LongWritable&gt; { @Override public int getPartition(Text text, LongWritable longWritable, int i) { //如果字符长度大于等于5放在一组 if(text.toString().length() &gt;=5 ){ return 0; }else{ //小于5放在一组 return 1; } } } Step 4. Main 入口 public class PartitionMain extends Configured implements Tool { public static void main(String[] args) throws Exception{ int run = ToolRunner.run(new Configuration(), new PartitionMain(), args); System.exit(run); } @Override public int run(String[] args) throws Exception { Job job = Job.getInstance(super.getConf(), PartitionMain.class.getSimpleName()); job.setJarByClass(PartitionMain.class); job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/partitioner&quot;)); job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setPartitionerClass(MyPartitioner.class); job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); /** * 设置我们的分区类，以及我们的reducetask的个数，注意reduceTask的个数一定要与我们的分区数保持一致 */ //设置reducetask的个数 job.setNumReduceTasks(2); job.setOutputFormatClass(TextOutputFormat.class); Path(&quot;hdfs://192.168.52.250:8020/outpartition&quot;)); boolean b = job.waitForCompletion(true); return b?0:1; } } 6. MapReduce 排序和序列化 序列化 (Serialization) 是指把结构化对象转化为字节流 反序列化 (Deserialization) 是序列化的逆过程. 把字节流转为结构化对象. 当要在进程间传递对象或持久化对象的时候, 就需要序列化对象成字节流, 反之当要将接收到或从磁盘读取的字节流转换为对象, 就要进行反序列化 Java 的序列化 (Serializable) 是一个重量级序列化框架, 一个对象被序列化后, 会附带很多额外的信息 (各种校验信息, header, 继承体系等）, 不便于在网络中高效传输. 所以, Hadoop 自己开发了一套序列化机制(Writable), 精简高效. 不用像 Java 对象类一样传输多层的父子关系, 需要哪个属性就传输哪个属性值, 大大的减少网络传输的开销 Writable 是 Hadoop 的序列化格式, Hadoop 定义了这样一个 Writable 接口. 一个类要支持可序列化只需实现这个接口即可 另外 Writable 有一个子接口是 WritableComparable, WritableComparable 是既可实现序列化, 也可以对key进行比较, 我们这里可以通过自定义 Key 实现 WritableComparable 来实现我们的排序功能 数据格式如下 a 1 a 9 b 3 a 7 b 8 b 10 a 5 要求: 第一列按照字典顺序进行排列 第一列相同的时候, 第二列按照升序进行排列 解决思路: 将 Map 端输出的 &lt;key,value&gt; 中的 key 和 value 组合成一个新的 key (newKey), value值不变 这里就变成 &lt;(key,value),value&gt;, 在针对 newKey 排序的时候, 如果 key 相同, 就再对value进行排序 Step 1. 自定义类型和比较器 public class SortBean implements WritableComparable&lt;SortBean&gt;{ private String word; private int num; public String getWord() { return word; } public void setWord(String word) { this.word = word; } public int getNum() { return num; } public void setNum(int num) { this.num = num; } @Override public String toString() { return word + &quot;\t&quot; + num ; } //实现比较器，指定排序的规则 /** *第一列（word）按照字典顺序进行排序 *第二列（num）按照升序进行排序 */ @Override public int compareTo(SortBean sortBean) { //先对第一列进行排序：word int result = this.word.compareTo(sortBean.word); //对第二列进行排序 if(result == 0){ return this.num - sortBean.num; } return result; } //实现序列化 @Override public void write(DataOutput out) throws IOException { out.writeUTF(word); out.writeInt(num); } //实现反序列化 @Override public void readFields(DataInput in) throws IOException { this.word = in.readUTF(); this.num = in.readInt(); } } Step 2. Mapper public class SortMapper extends Mapper&lt;LongWritable,Text,SortBean,NullWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] split = value.toString().split(&quot;\t&quot;); SortBean sortBean = new SortBean(); sortBean.setWord(split[0]); sortBean.setNum(Integer.parseInt(split[1])); context.write(sortBean,NullWritable.get()); } } Step 3. Reducer public class SortReduce extends Reducer&lt;SortBean,NullWritable,SortBean,NullWritable&gt; { @Override protected void reduce(SortBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { context.write(key,NullWritable.get()); } } Step 4. Main 入口 public class SortMain extends Configured implements Tool{ @Override public int run(String[] args) throws Exception { //1 创建job对象 Job job = Job.getInstance(super.getConf(), &quot;mapreduce_sort&quot;); //2 配置job任务 //八个步骤--设置输入类和输入对象 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job, new Path(&quot;hdfs://node01:8020/input/input_sort&quot;)); //--设置Mapper类和数据类型 job.setMapperClass(SortMapper.class); job.setMapOutputKeyClass(SortBean.class); job.setMapOutputValueClass(NullWritable.class); //----分区 排序 规约 分组 //--设置Redeuce类和数据类型 job.setReducerClass(SortReduce.class); job.setOutputKeyClass(SortBean.class); job.setOutputValueClass(NullWritable.class); //设置输出类和输出路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job, new Path(&quot;hdfs://node01:8020/out/sort_out&quot;)); //等待任务结束 boolean bl = job.waitForCompletion(true); return bl?0:1; } public static void main(String[] args) throws Exception { Configuration configuration = new Configuration(); int run = ToolRunner.run(configuration, new SortMain(), args); System.exit(run); } }" />
<meta property="og:description" content="Hadoop-Mapreduce 1. MapReduce 介绍 MapReduce思想在生活中处处可见。或多或少都曾接触过这种思想。MapReduce的思想核心是“分而治之”，适用于大量复杂的任务处理场景（大规模数据处理场景）。 Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。 Reduce负责“合”，即对map阶段的结果进行全局汇总。 MapReduce运行在yarn集群 ResourceManager NodeManager 这两个阶段合起来正是MapReduce思想的体现。 还有一个比较形象的语言解释MapReduce: 我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“Map”。我们人越多，数书就更快。 现在我们到一起，把所有人的统计数加在一起。这就是“Reduce”。 1.1. MapReduce 设计构思 MapReduce是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在Hadoop集群上。 MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理： Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现.Map和Reduce,MapReduce处理的数据类型是&lt;key,value&gt;键值对。 Map: (k1; v1) → [(k2; v2)] Reduce: (k2; [v2]) → [(k3; v3)] 一个完整的mapreduce程序在分布式运行时有三类实例进程： MRAppMaster 负责整个程序的过程调度及状态协调 MapTask 负责map阶段的整个数据处理流程 ReduceTask 负责reduce阶段的整个数据处理流程 2. MapReduce 编程规范 MapReduce 的开发一共有八个步骤, 其中 Map 阶段分为 2 个步骤，Shuffle 阶段 4 个步骤，Reduce 阶段分为 2 个步骤 Map 阶段 2 个步骤 设置 InputFormat 类（TextInputFormat）, 将数据切分为 Key-Value (K1和V1) 对, 输入到第二步 自定义 Map 逻辑, 将第一步的结果转换成另外的 Key-Value（K2和V2） 对, 输出结果 Shuffle 阶段 4 个步骤 对输出的 Key-Value 对进行分区 对不同分区的数据按照相同的 Key 排序 (可选) 对分组过的数据初步规约, 降低数据的网络拷贝 对数据进行分组, 相同 Key 的 Value 放入一个集合中 Reduce 阶段 2 个步骤 对多个 Map 任务的结果进行排序以及合并, 编写 Reduce 函数实现自己的逻辑, 对输入的 Key-Value 进行处理, 转为新的 Key-Value（K3和V3）输出 设置 OutputFormat 处理并保存 Reduce 输出的 Key-Value 数据 3. WordCount 需求: 在一堆给定的文本文件中统计输出每一个单词出现的总次数 Step 1. 数据格式准备 创建一个新的文件 cd /export/servers vim wordcount.txt 向其中放入以下内容并保存 hello,world,hadoop hive,sqoop,flume,hello kitty,tom,jerry,world hadoop 上传到 HDFS hdfs dfs -mkdir /wordcount/ hdfs dfs -put wordcount.txt /wordcount/ Step 2. Mapper //重写mapper方法 public class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt; { @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { //将v1使用‘,’进行分割，存入到context上。 /*转化的类型为 hello 1 world 1 hadoop 1 String line = value.toString(); String[] split = line.split(&quot;,&quot;); for (String word : split) { context.write(new Text(word),new LongWritable(1)); } } } Step 3. Reducer //重写reduce方法 public class WordCountReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt; { /** * 自定义我们的reduce逻辑 * 所有的key都是我们的单词，所有的values都是我们单词出现的次数 * 统计单词出现的个数 */ @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException { long count = 0; for (LongWritable value : values) { count += value.get(); } context.write(key,new LongWritable(count)); } } Step 4. 定义主类, 描述 Job 并提交 Job public class JobMain extends Configured implements Tool { @Override public int run(String[] args) throws Exception { Job job = Job.getInstance(super.getConf(), JobMain.class.getSimpleName()); //打包到集群上面运行时候，必须要添加以下配置，指定程序的main函数 job.setJarByClass(JobMain.class); //第一步：读取输入文件解析成key，value对 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/wordcount&quot;)); //第二步：设置我们的mapper类 job.setMapperClass(WordCountMapper.class); //设置我们map阶段完成之后的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); //第三步，第四步，第五步，第六步，采取默认的设置-省略 //第七步：设置我们的reduce类 job.setReducerClass(WordCountReducer.class); //设置我们reduce阶段完成之后的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //第八步：设置输出类以及输出路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/wordcount_out&quot;)); boolean b = job.waitForCompletion(true); return b?0:1; } /** * 程序main函数的入口类 * @param args * @throws Exception */ public static void main(String[] args) throws Exception { Configuration configuration = new Configuration(); Tool tool = new JobMain(); int run = ToolRunner.run(configuration, tool, args); System.exit(run); } } 常见错误 如果遇到如下错误 Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=admin, access=WRITE, inode=&quot;/&quot;:root:supergroup:drwxr-xr-x 直接将hdfs-site.xml当中的权限关闭即可 &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 最后重启一下 HDFS 集群 小细节 本地运行完成之后，就可以打成jar包放到服务器上面去运行了，实际工作当中，都是将代码打成jar包，开发main方法作为程序的入口，然后放到集群上面去运行 4. MapReduce 运行模式 本地运行模式 MapReduce 程序被提交在本地以单进程的形式运行 处理的数据及输出结果可以在本地文件系统, 也可以在hdfs上 怎样实现本地运行? 将输入和输出的路径改为本地路径 本地模式非常便于进行业务逻辑的 Debug, 只要在 Eclipse 中打断点即可 configuration.set(&quot;mapreduce.framework.name&quot;,&quot;local&quot;); configuration.set(&quot; yarn.resourcemanager.hostname&quot;,&quot;local&quot;); TextInputFormat.addInputPath(job,new Path(&quot;file:///F:\\wordcount\\input&quot;)); TextOutputFormat.setOutputPath(job,new Path(&quot;file:///F:\\wordcount\\output&quot;)); 集群运行模式 将 MapReduce 程序提交给 Yarn 集群, 分发到很多的节点上并发执行 处理的数据和输出结果应该位于 HDFS 文件系统 提交集群的实现步骤: 将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动。（需要指定主类运行的路径） hadoop jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar cn.itcast.hdfs.demo1.JobMain 5. MapReduce 分区 在 MapReduce 中, 通过我们指定分区, 会将同一个分区的数据发送到同一个 Reduce 当中进行处理 例如: 为了数据的统计, 可以把一批类似的数据（以单词统计案例为例）发送到同一个 Reduce 当中, 在同一个 Reduce 当中统计相同类型的数据, 就可以实现类似的数据分区和统计等 其实就是相同类型的数据, 有共性的数据, 送到一起去处理 Reduce 当中默认的分区只有一个 Step 1. 定义 Mapper 这个 Mapper 程序不做任何逻辑, 也不对 Key-Value 做任何改变, 只是接收数据, 然后往下发送 public class MyMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { context.write(value,NullWritable.get()); } } Step 2. 定义 Reducer 逻辑 这个 Reducer 也不做任何处理, 将数据原封不动的输出即可 public class MyReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt; { @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { context.write(key,NullWritable.get()); } } Step 3. 自定义 Partitioner 主要的逻辑就在这里, 这也是这个案例的意义, 通过 Partitioner 将数据分发给不同的 Reducer public class PartitonerOwn extends Partitioner&lt;Text,LongWritable&gt; { @Override public int getPartition(Text text, LongWritable longWritable, int i) { //如果字符长度大于等于5放在一组 if(text.toString().length() &gt;=5 ){ return 0; }else{ //小于5放在一组 return 1; } } } Step 4. Main 入口 public class PartitionMain extends Configured implements Tool { public static void main(String[] args) throws Exception{ int run = ToolRunner.run(new Configuration(), new PartitionMain(), args); System.exit(run); } @Override public int run(String[] args) throws Exception { Job job = Job.getInstance(super.getConf(), PartitionMain.class.getSimpleName()); job.setJarByClass(PartitionMain.class); job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/partitioner&quot;)); job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setPartitionerClass(MyPartitioner.class); job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); /** * 设置我们的分区类，以及我们的reducetask的个数，注意reduceTask的个数一定要与我们的分区数保持一致 */ //设置reducetask的个数 job.setNumReduceTasks(2); job.setOutputFormatClass(TextOutputFormat.class); Path(&quot;hdfs://192.168.52.250:8020/outpartition&quot;)); boolean b = job.waitForCompletion(true); return b?0:1; } } 6. MapReduce 排序和序列化 序列化 (Serialization) 是指把结构化对象转化为字节流 反序列化 (Deserialization) 是序列化的逆过程. 把字节流转为结构化对象. 当要在进程间传递对象或持久化对象的时候, 就需要序列化对象成字节流, 反之当要将接收到或从磁盘读取的字节流转换为对象, 就要进行反序列化 Java 的序列化 (Serializable) 是一个重量级序列化框架, 一个对象被序列化后, 会附带很多额外的信息 (各种校验信息, header, 继承体系等）, 不便于在网络中高效传输. 所以, Hadoop 自己开发了一套序列化机制(Writable), 精简高效. 不用像 Java 对象类一样传输多层的父子关系, 需要哪个属性就传输哪个属性值, 大大的减少网络传输的开销 Writable 是 Hadoop 的序列化格式, Hadoop 定义了这样一个 Writable 接口. 一个类要支持可序列化只需实现这个接口即可 另外 Writable 有一个子接口是 WritableComparable, WritableComparable 是既可实现序列化, 也可以对key进行比较, 我们这里可以通过自定义 Key 实现 WritableComparable 来实现我们的排序功能 数据格式如下 a 1 a 9 b 3 a 7 b 8 b 10 a 5 要求: 第一列按照字典顺序进行排列 第一列相同的时候, 第二列按照升序进行排列 解决思路: 将 Map 端输出的 &lt;key,value&gt; 中的 key 和 value 组合成一个新的 key (newKey), value值不变 这里就变成 &lt;(key,value),value&gt;, 在针对 newKey 排序的时候, 如果 key 相同, 就再对value进行排序 Step 1. 自定义类型和比较器 public class SortBean implements WritableComparable&lt;SortBean&gt;{ private String word; private int num; public String getWord() { return word; } public void setWord(String word) { this.word = word; } public int getNum() { return num; } public void setNum(int num) { this.num = num; } @Override public String toString() { return word + &quot;\t&quot; + num ; } //实现比较器，指定排序的规则 /** *第一列（word）按照字典顺序进行排序 *第二列（num）按照升序进行排序 */ @Override public int compareTo(SortBean sortBean) { //先对第一列进行排序：word int result = this.word.compareTo(sortBean.word); //对第二列进行排序 if(result == 0){ return this.num - sortBean.num; } return result; } //实现序列化 @Override public void write(DataOutput out) throws IOException { out.writeUTF(word); out.writeInt(num); } //实现反序列化 @Override public void readFields(DataInput in) throws IOException { this.word = in.readUTF(); this.num = in.readInt(); } } Step 2. Mapper public class SortMapper extends Mapper&lt;LongWritable,Text,SortBean,NullWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] split = value.toString().split(&quot;\t&quot;); SortBean sortBean = new SortBean(); sortBean.setWord(split[0]); sortBean.setNum(Integer.parseInt(split[1])); context.write(sortBean,NullWritable.get()); } } Step 3. Reducer public class SortReduce extends Reducer&lt;SortBean,NullWritable,SortBean,NullWritable&gt; { @Override protected void reduce(SortBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { context.write(key,NullWritable.get()); } } Step 4. Main 入口 public class SortMain extends Configured implements Tool{ @Override public int run(String[] args) throws Exception { //1 创建job对象 Job job = Job.getInstance(super.getConf(), &quot;mapreduce_sort&quot;); //2 配置job任务 //八个步骤--设置输入类和输入对象 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job, new Path(&quot;hdfs://node01:8020/input/input_sort&quot;)); //--设置Mapper类和数据类型 job.setMapperClass(SortMapper.class); job.setMapOutputKeyClass(SortBean.class); job.setMapOutputValueClass(NullWritable.class); //----分区 排序 规约 分组 //--设置Redeuce类和数据类型 job.setReducerClass(SortReduce.class); job.setOutputKeyClass(SortBean.class); job.setOutputValueClass(NullWritable.class); //设置输出类和输出路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job, new Path(&quot;hdfs://node01:8020/out/sort_out&quot;)); //等待任务结束 boolean bl = job.waitForCompletion(true); return bl?0:1; } public static void main(String[] args) throws Exception { Configuration configuration = new Configuration(); int run = ToolRunner.run(configuration, new SortMain(), args); System.exit(run); } }" />
<link rel="canonical" href="https://uzzz.org/2019/08/03/794571.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/03/794571.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-03T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Hadoop-Mapreduce 1. MapReduce 介绍 MapReduce思想在生活中处处可见。或多或少都曾接触过这种思想。MapReduce的思想核心是“分而治之”，适用于大量复杂的任务处理场景（大规模数据处理场景）。 Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。 Reduce负责“合”，即对map阶段的结果进行全局汇总。 MapReduce运行在yarn集群 ResourceManager NodeManager 这两个阶段合起来正是MapReduce思想的体现。 还有一个比较形象的语言解释MapReduce: 我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“Map”。我们人越多，数书就更快。 现在我们到一起，把所有人的统计数加在一起。这就是“Reduce”。 1.1. MapReduce 设计构思 MapReduce是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在Hadoop集群上。 MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理： Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现.Map和Reduce,MapReduce处理的数据类型是&lt;key,value&gt;键值对。 Map: (k1; v1) → [(k2; v2)] Reduce: (k2; [v2]) → [(k3; v3)] 一个完整的mapreduce程序在分布式运行时有三类实例进程： MRAppMaster 负责整个程序的过程调度及状态协调 MapTask 负责map阶段的整个数据处理流程 ReduceTask 负责reduce阶段的整个数据处理流程 2. MapReduce 编程规范 MapReduce 的开发一共有八个步骤, 其中 Map 阶段分为 2 个步骤，Shuffle 阶段 4 个步骤，Reduce 阶段分为 2 个步骤 Map 阶段 2 个步骤 设置 InputFormat 类（TextInputFormat）, 将数据切分为 Key-Value (K1和V1) 对, 输入到第二步 自定义 Map 逻辑, 将第一步的结果转换成另外的 Key-Value（K2和V2） 对, 输出结果 Shuffle 阶段 4 个步骤 对输出的 Key-Value 对进行分区 对不同分区的数据按照相同的 Key 排序 (可选) 对分组过的数据初步规约, 降低数据的网络拷贝 对数据进行分组, 相同 Key 的 Value 放入一个集合中 Reduce 阶段 2 个步骤 对多个 Map 任务的结果进行排序以及合并, 编写 Reduce 函数实现自己的逻辑, 对输入的 Key-Value 进行处理, 转为新的 Key-Value（K3和V3）输出 设置 OutputFormat 处理并保存 Reduce 输出的 Key-Value 数据 3. WordCount 需求: 在一堆给定的文本文件中统计输出每一个单词出现的总次数 Step 1. 数据格式准备 创建一个新的文件 cd /export/servers vim wordcount.txt 向其中放入以下内容并保存 hello,world,hadoop hive,sqoop,flume,hello kitty,tom,jerry,world hadoop 上传到 HDFS hdfs dfs -mkdir /wordcount/ hdfs dfs -put wordcount.txt /wordcount/ Step 2. Mapper //重写mapper方法 public class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt; { @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { //将v1使用‘,’进行分割，存入到context上。 /*转化的类型为 hello 1 world 1 hadoop 1 String line = value.toString(); String[] split = line.split(&quot;,&quot;); for (String word : split) { context.write(new Text(word),new LongWritable(1)); } } } Step 3. Reducer //重写reduce方法 public class WordCountReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt; { /** * 自定义我们的reduce逻辑 * 所有的key都是我们的单词，所有的values都是我们单词出现的次数 * 统计单词出现的个数 */ @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException { long count = 0; for (LongWritable value : values) { count += value.get(); } context.write(key,new LongWritable(count)); } } Step 4. 定义主类, 描述 Job 并提交 Job public class JobMain extends Configured implements Tool { @Override public int run(String[] args) throws Exception { Job job = Job.getInstance(super.getConf(), JobMain.class.getSimpleName()); //打包到集群上面运行时候，必须要添加以下配置，指定程序的main函数 job.setJarByClass(JobMain.class); //第一步：读取输入文件解析成key，value对 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/wordcount&quot;)); //第二步：设置我们的mapper类 job.setMapperClass(WordCountMapper.class); //设置我们map阶段完成之后的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); //第三步，第四步，第五步，第六步，采取默认的设置-省略 //第七步：设置我们的reduce类 job.setReducerClass(WordCountReducer.class); //设置我们reduce阶段完成之后的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //第八步：设置输出类以及输出路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/wordcount_out&quot;)); boolean b = job.waitForCompletion(true); return b?0:1; } /** * 程序main函数的入口类 * @param args * @throws Exception */ public static void main(String[] args) throws Exception { Configuration configuration = new Configuration(); Tool tool = new JobMain(); int run = ToolRunner.run(configuration, tool, args); System.exit(run); } } 常见错误 如果遇到如下错误 Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=admin, access=WRITE, inode=&quot;/&quot;:root:supergroup:drwxr-xr-x 直接将hdfs-site.xml当中的权限关闭即可 &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 最后重启一下 HDFS 集群 小细节 本地运行完成之后，就可以打成jar包放到服务器上面去运行了，实际工作当中，都是将代码打成jar包，开发main方法作为程序的入口，然后放到集群上面去运行 4. MapReduce 运行模式 本地运行模式 MapReduce 程序被提交在本地以单进程的形式运行 处理的数据及输出结果可以在本地文件系统, 也可以在hdfs上 怎样实现本地运行? 将输入和输出的路径改为本地路径 本地模式非常便于进行业务逻辑的 Debug, 只要在 Eclipse 中打断点即可 configuration.set(&quot;mapreduce.framework.name&quot;,&quot;local&quot;); configuration.set(&quot; yarn.resourcemanager.hostname&quot;,&quot;local&quot;); TextInputFormat.addInputPath(job,new Path(&quot;file:///F:\\\\wordcount\\\\input&quot;)); TextOutputFormat.setOutputPath(job,new Path(&quot;file:///F:\\\\wordcount\\\\output&quot;)); 集群运行模式 将 MapReduce 程序提交给 Yarn 集群, 分发到很多的节点上并发执行 处理的数据和输出结果应该位于 HDFS 文件系统 提交集群的实现步骤: 将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动。（需要指定主类运行的路径） hadoop jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar cn.itcast.hdfs.demo1.JobMain 5. MapReduce 分区 在 MapReduce 中, 通过我们指定分区, 会将同一个分区的数据发送到同一个 Reduce 当中进行处理 例如: 为了数据的统计, 可以把一批类似的数据（以单词统计案例为例）发送到同一个 Reduce 当中, 在同一个 Reduce 当中统计相同类型的数据, 就可以实现类似的数据分区和统计等 其实就是相同类型的数据, 有共性的数据, 送到一起去处理 Reduce 当中默认的分区只有一个 Step 1. 定义 Mapper 这个 Mapper 程序不做任何逻辑, 也不对 Key-Value 做任何改变, 只是接收数据, 然后往下发送 public class MyMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { context.write(value,NullWritable.get()); } } Step 2. 定义 Reducer 逻辑 这个 Reducer 也不做任何处理, 将数据原封不动的输出即可 public class MyReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt; { @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { context.write(key,NullWritable.get()); } } Step 3. 自定义 Partitioner 主要的逻辑就在这里, 这也是这个案例的意义, 通过 Partitioner 将数据分发给不同的 Reducer public class PartitonerOwn extends Partitioner&lt;Text,LongWritable&gt; { @Override public int getPartition(Text text, LongWritable longWritable, int i) { //如果字符长度大于等于5放在一组 if(text.toString().length() &gt;=5 ){ return 0; }else{ //小于5放在一组 return 1; } } } Step 4. Main 入口 public class PartitionMain extends Configured implements Tool { public static void main(String[] args) throws Exception{ int run = ToolRunner.run(new Configuration(), new PartitionMain(), args); System.exit(run); } @Override public int run(String[] args) throws Exception { Job job = Job.getInstance(super.getConf(), PartitionMain.class.getSimpleName()); job.setJarByClass(PartitionMain.class); job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/partitioner&quot;)); job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setPartitionerClass(MyPartitioner.class); job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); /** * 设置我们的分区类，以及我们的reducetask的个数，注意reduceTask的个数一定要与我们的分区数保持一致 */ //设置reducetask的个数 job.setNumReduceTasks(2); job.setOutputFormatClass(TextOutputFormat.class); Path(&quot;hdfs://192.168.52.250:8020/outpartition&quot;)); boolean b = job.waitForCompletion(true); return b?0:1; } } 6. MapReduce 排序和序列化 序列化 (Serialization) 是指把结构化对象转化为字节流 反序列化 (Deserialization) 是序列化的逆过程. 把字节流转为结构化对象. 当要在进程间传递对象或持久化对象的时候, 就需要序列化对象成字节流, 反之当要将接收到或从磁盘读取的字节流转换为对象, 就要进行反序列化 Java 的序列化 (Serializable) 是一个重量级序列化框架, 一个对象被序列化后, 会附带很多额外的信息 (各种校验信息, header, 继承体系等）, 不便于在网络中高效传输. 所以, Hadoop 自己开发了一套序列化机制(Writable), 精简高效. 不用像 Java 对象类一样传输多层的父子关系, 需要哪个属性就传输哪个属性值, 大大的减少网络传输的开销 Writable 是 Hadoop 的序列化格式, Hadoop 定义了这样一个 Writable 接口. 一个类要支持可序列化只需实现这个接口即可 另外 Writable 有一个子接口是 WritableComparable, WritableComparable 是既可实现序列化, 也可以对key进行比较, 我们这里可以通过自定义 Key 实现 WritableComparable 来实现我们的排序功能 数据格式如下 a 1 a 9 b 3 a 7 b 8 b 10 a 5 要求: 第一列按照字典顺序进行排列 第一列相同的时候, 第二列按照升序进行排列 解决思路: 将 Map 端输出的 &lt;key,value&gt; 中的 key 和 value 组合成一个新的 key (newKey), value值不变 这里就变成 &lt;(key,value),value&gt;, 在针对 newKey 排序的时候, 如果 key 相同, 就再对value进行排序 Step 1. 自定义类型和比较器 public class SortBean implements WritableComparable&lt;SortBean&gt;{ private String word; private int num; public String getWord() { return word; } public void setWord(String word) { this.word = word; } public int getNum() { return num; } public void setNum(int num) { this.num = num; } @Override public String toString() { return word + &quot;\\t&quot; + num ; } //实现比较器，指定排序的规则 /** *第一列（word）按照字典顺序进行排序 *第二列（num）按照升序进行排序 */ @Override public int compareTo(SortBean sortBean) { //先对第一列进行排序：word int result = this.word.compareTo(sortBean.word); //对第二列进行排序 if(result == 0){ return this.num - sortBean.num; } return result; } //实现序列化 @Override public void write(DataOutput out) throws IOException { out.writeUTF(word); out.writeInt(num); } //实现反序列化 @Override public void readFields(DataInput in) throws IOException { this.word = in.readUTF(); this.num = in.readInt(); } } Step 2. Mapper public class SortMapper extends Mapper&lt;LongWritable,Text,SortBean,NullWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] split = value.toString().split(&quot;\\t&quot;); SortBean sortBean = new SortBean(); sortBean.setWord(split[0]); sortBean.setNum(Integer.parseInt(split[1])); context.write(sortBean,NullWritable.get()); } } Step 3. Reducer public class SortReduce extends Reducer&lt;SortBean,NullWritable,SortBean,NullWritable&gt; { @Override protected void reduce(SortBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { context.write(key,NullWritable.get()); } } Step 4. Main 入口 public class SortMain extends Configured implements Tool{ @Override public int run(String[] args) throws Exception { //1 创建job对象 Job job = Job.getInstance(super.getConf(), &quot;mapreduce_sort&quot;); //2 配置job任务 //八个步骤--设置输入类和输入对象 job.setInputFormatClass(TextInputFormat.class); TextInputFormat.addInputPath(job, new Path(&quot;hdfs://node01:8020/input/input_sort&quot;)); //--设置Mapper类和数据类型 job.setMapperClass(SortMapper.class); job.setMapOutputKeyClass(SortBean.class); job.setMapOutputValueClass(NullWritable.class); //----分区 排序 规约 分组 //--设置Redeuce类和数据类型 job.setReducerClass(SortReduce.class); job.setOutputKeyClass(SortBean.class); job.setOutputValueClass(NullWritable.class); //设置输出类和输出路径 job.setOutputFormatClass(TextOutputFormat.class); TextOutputFormat.setOutputPath(job, new Path(&quot;hdfs://node01:8020/out/sort_out&quot;)); //等待任务结束 boolean bl = job.waitForCompletion(true); return bl?0:1; } public static void main(String[] args) throws Exception { Configuration configuration = new Configuration(); int run = ToolRunner.run(configuration, new SortMain(), args); System.exit(run); } }","@type":"BlogPosting","url":"https://uzzz.org/2019/08/03/794571.html","headline":"Hadoop 核心-MapReduce（基础概念）–分区和排序","dateModified":"2019-08-03T00:00:00+08:00","datePublished":"2019-08-03T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/03/794571.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Hadoop 核心-MapReduce（基础概念）--分区和排序</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h1><a id="HadoopMapreduce_0"></a>Hadoop-Mapreduce</h1> 
  <h2><a id="1_MapReduce__2"></a>1. MapReduce 介绍</h2> 
  <p>MapReduce思想在生活中处处可见。或多或少都曾接触过这种思想。MapReduce的思想核心是“分而治之”，适用于大量复杂的任务处理场景（大规模数据处理场景）。</p> 
  <ul> 
   <li>Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。</li> 
   <li>Reduce负责“合”，即对map阶段的结果进行全局汇总。</li> 
   <li>MapReduce运行在yarn集群 
    <ol> 
     <li>ResourceManager</li> 
     <li>NodeManager</li> 
    </ol> </li> 
  </ul> 
  <p>这两个阶段合起来正是MapReduce思想的体现。</p> 
  <p>还有一个比较形象的语言解释MapReduce:</p> 
  <p>我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“Map”。我们人越多，数书就更快。</p> 
  <p>现在我们到一起，把所有人的统计数加在一起。这就是“Reduce”。</p> 
  <h3><a id="11_MapReduce__23"></a>1.1. MapReduce 设计构思</h3> 
  <p>MapReduce是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在Hadoop集群上。</p> 
  <p>MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理：</p> 
  <p>Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现.Map和Reduce,MapReduce处理的数据类型是&lt;key,value&gt;键值对。</p> 
  <ul> 
   <li> <p>Map: <code>(k1; v1) → [(k2; v2)]</code></p> </li> 
   <li> <p>Reduce: <code>(k2; [v2]) → [(k3; v3)]</code></p> </li> 
  </ul> 
  <p>一个完整的mapreduce程序在分布式运行时有三类实例进程：</p> 
  <ol> 
   <li><code>MRAppMaster</code> 负责整个程序的过程调度及状态协调</li> 
   <li><code>MapTask</code> 负责map阶段的整个数据处理流程</li> 
   <li><code>ReduceTask</code> 负责reduce阶段的整个数据处理流程</li> 
  </ol> 
  <h2><a id="2_MapReduce__47"></a>2. MapReduce 编程规范</h2> 
  <blockquote> 
   <p>MapReduce 的开发一共有八个步骤, 其中 Map 阶段分为 2 个步骤，Shuffle 阶段 4 个步骤，Reduce 阶段分为 2 个步骤</p> 
  </blockquote> 
  <h5><a id="Map__2__52"></a>Map 阶段 2 个步骤</h5> 
  <ol> 
   <li>设置 InputFormat 类（TextInputFormat）, 将数据切分为 Key-Value <strong>(K1和V1)</strong> 对, 输入到第二步</li> 
   <li>自定义 Map 逻辑, 将第一步的结果转换成另外的 Key-Value（<strong>K2和V2</strong>） 对, 输出结果</li> 
  </ol> 
  <h5><a id="Shuffle__4__57"></a>Shuffle 阶段 4 个步骤</h5> 
  <ol start="3"> 
   <li>对输出的 Key-Value 对进行<strong>分区</strong></li> 
   <li>对不同分区的数据按照相同的 Key <strong>排序</strong></li> 
   <li>(可选) 对分组过的数据初步<strong>规约</strong>, 降低数据的网络拷贝</li> 
   <li>对数据进行<strong>分组</strong>, 相同 Key 的 Value 放入一个集合中</li> 
  </ol> 
  <h5><a id="Reduce__2__64"></a>Reduce 阶段 2 个步骤</h5> 
  <ol start="7"> 
   <li>对多个 Map 任务的结果进行排序以及合并, 编写 Reduce 函数实现自己的逻辑, 对输入的 Key-Value 进行处理, 转为新的 Key-Value（<strong>K3和V3</strong>）输出</li> 
   <li>设置 OutputFormat 处理并保存 Reduce 输出的 Key-Value 数据</li> 
  </ol> 
  <h2><a id="3_WordCount_69"></a>3. WordCount</h2> 
  <blockquote> 
   <p>需求: 在一堆给定的文本文件中统计输出每一个单词出现的总次数</p> 
  </blockquote> 
  <h5><a id="Step_1__73"></a>Step 1. 数据格式准备</h5> 
  <ol> 
   <li>创建一个新的文件</li> 
  </ol> 
  <pre><code class="prism language-text">  cd /export/servers
  vim wordcount.txt
</code></pre> 
  <ol start="2"> 
   <li>向其中放入以下内容并保存</li> 
  </ol> 
  <pre><code class="prism language-text">  hello,world,hadoop
  hive,sqoop,flume,hello
  kitty,tom,jerry,world
  hadoop
</code></pre> 
  <ol start="3"> 
   <li>上传到 HDFS</li> 
  </ol> 
  <pre><code class="prism language-text">  hdfs dfs -mkdir /wordcount/
  hdfs dfs -put wordcount.txt /wordcount/
</code></pre> 
  <h5><a id="Step_2_Mapper_98"></a>Step 2. Mapper</h5> 
  <pre><code class="prism language-java"><span class="token comment">//重写mapper方法</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordCountMapper</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token generics function"><span class="token punctuation">&lt;</span>LongWritable<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>LongWritable<span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>LongWritable key<span class="token punctuation">,</span> Text value<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>
    <span class="token comment">//将v1使用‘,’进行分割，存入到context上。</span>
    <span class="token comment">/*转化的类型为 hello 1 world 1 hadoop 1 String line = value.toString(); String[] split = line.split(","); for (String word : split) { context.write(new Text(word),new LongWritable(1)); } } } </span></code></pre> 
  <h5><a id="Step_3_Reducer_119"></a>Step 3. Reducer</h5> 
  <pre><code class="prism language-java"><span class="token comment">//重写reduce方法</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordCountReducer</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token generics function"><span class="token punctuation">&lt;</span>Text<span class="token punctuation">,</span>LongWritable<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>LongWritable<span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>
    <span class="token comment">/** * 自定义我们的reduce逻辑 * 所有的key都是我们的单词，所有的values都是我们单词出现的次数 * 统计单词出现的个数 */</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span>Text key<span class="token punctuation">,</span> Iterable<span class="token generics function"><span class="token punctuation">&lt;</span>LongWritable<span class="token punctuation">&gt;</span></span> values<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>
        <span class="token keyword">long</span> count <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>LongWritable value <span class="token operator">:</span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>
            count <span class="token operator">+=</span> value<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">LongWritable</span><span class="token punctuation">(</span>count<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <h5><a id="Step_4___Job__Job_140"></a>Step 4. 定义主类, 描述 Job 并提交 Job</h5> 
  <pre><code class="prism language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">JobMain</span> <span class="token keyword">extends</span> <span class="token class-name">Configured</span> <span class="token keyword">implements</span> <span class="token class-name">Tool</span> <span class="token punctuation">{</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">run</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
        Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span><span class="token keyword">super</span><span class="token punctuation">.</span><span class="token function">getConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> JobMain<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">.</span><span class="token function">getSimpleName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//打包到集群上面运行时候，必须要添加以下配置，指定程序的main函数</span>
        job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>JobMain<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//第一步：读取输入文件解析成key，value对</span>
        job<span class="token punctuation">.</span><span class="token function">setInputFormatClass</span><span class="token punctuation">(</span>TextInputFormat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        TextInputFormat<span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"hdfs://192.168.52.250:8020/wordcount"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//第二步：设置我们的mapper类</span>
        job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>WordCountMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//设置我们map阶段完成之后的输出类型</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>LongWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//第三步，第四步，第五步，第六步，采取默认的设置-省略</span>
        <span class="token comment">//第七步：设置我们的reduce类</span>
        job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>WordCountReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//设置我们reduce阶段完成之后的输出类型</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>LongWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//第八步：设置输出类以及输出路径</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputFormatClass</span><span class="token punctuation">(</span>TextOutputFormat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        TextOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"hdfs://192.168.52.250:8020/wordcount_out"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">boolean</span> b <span class="token operator">=</span> job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> b<span class="token operator">?</span><span class="token number">0</span><span class="token operator">:</span><span class="token number">1</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">/** * 程序main函数的入口类 * @param args * @throws Exception */</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        Tool tool  <span class="token operator">=</span>  <span class="token keyword">new</span> <span class="token class-name">JobMain</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">int</span> run <span class="token operator">=</span> ToolRunner<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span>configuration<span class="token punctuation">,</span> tool<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">;</span>
        System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>run<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <h5><a id="_185"></a>常见错误</h5> 
  <p>如果遇到如下错误</p> 
  <pre><code class="prism language-text">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=admin, access=WRITE, inode="/":root:supergroup:drwxr-xr-x
</code></pre> 
  <p>直接将hdfs-site.xml当中的权限关闭即可</p> 
  <pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.permissions<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
  <p>最后重启一下 HDFS 集群</p> 
  <h5><a id="_204"></a>小细节</h5> 
  <p>本地运行完成之后，就可以打成jar包放到服务器上面去运行了，实际工作当中，都是将代码打成jar包，开发main方法作为程序的入口，然后放到集群上面去运行</p> 
  <h2><a id="4_MapReduce__208"></a>4. MapReduce 运行模式</h2> 
  <h5><a id="_210"></a>本地运行模式</h5> 
  <ol> 
   <li>MapReduce 程序被提交在本地以单进程的形式运行</li> 
   <li>处理的数据及输出结果可以在本地文件系统, 也可以在hdfs上</li> 
   <li>怎样实现本地运行? 将输入和输出的路径改为本地路径</li> 
   <li>本地模式非常便于进行业务逻辑的 <code>Debug</code>, 只要在 <code>Eclipse</code> 中打断点即可</li> 
  </ol> 
  <pre><code class="prism language-java">configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"mapreduce.framework.name"</span><span class="token punctuation">,</span><span class="token string">"local"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">" yarn.resourcemanager.hostname"</span><span class="token punctuation">,</span><span class="token string">"local"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
TextInputFormat<span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"file:///F:\\wordcount\\input"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
TextOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"file:///F:\\wordcount\\output"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
  <h5><a id="_224"></a>集群运行模式</h5> 
  <ol> 
   <li>将 MapReduce 程序提交给 Yarn 集群, 分发到很多的节点上并发执行</li> 
   <li>处理的数据和输出结果应该位于 HDFS 文件系统</li> 
   <li>提交集群的实现步骤: 将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动。（需要指定主类运行的路径）</li> 
  </ol> 
  <pre><code class="prism language-shell">hadoop jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar cn.itcast.hdfs.demo1.JobMain
</code></pre> 
  <h2><a id="5_MapReduce__234"></a>5. MapReduce 分区</h2> 
  <p>在 MapReduce 中, 通过我们指定分区, 会将同一个分区的数据发送到同一个 Reduce 当中进行处理</p> 
  <p>例如: 为了数据的统计, 可以把一批类似的数据（以单词统计案例为例）发送到同一个 Reduce 当中, 在同一个 Reduce 当中统计相同类型的数据, 就可以实现类似的数据分区和统计等</p> 
  <p>其实就是相同类型的数据, 有共性的数据, 送到一起去处理</p> 
  <p>Reduce 当中默认的分区只有一个</p> 
  <blockquote> 
   <p>Step 1. 定义 Mapper</p> 
  </blockquote> 
  <p>这个 Mapper 程序不做任何逻辑, 也不对 Key-Value 做任何改变, 只是接收数据, 然后往下发送</p> 
  <pre><code class="prism language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MyMapper</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token generics function"><span class="token punctuation">&lt;</span>LongWritable<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>NullWritable<span class="token punctuation">&gt;</span></span><span class="token punctuation">{</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>LongWritable key<span class="token punctuation">,</span> Text value<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>
        context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span>NullWritable<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <h5><a id="Step_2__Reducer__257"></a>Step 2. 定义 Reducer 逻辑</h5> 
  <p>这个 Reducer 也不做任何处理, 将数据原封不动的输出即可</p> 
  <pre><code class="prism language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MyReducer</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token generics function"><span class="token punctuation">&lt;</span>Text<span class="token punctuation">,</span>NullWritable<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>NullWritable<span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span>Text key<span class="token punctuation">,</span> Iterable<span class="token generics function"><span class="token punctuation">&lt;</span>NullWritable<span class="token punctuation">&gt;</span></span> values<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>
        context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span>NullWritable<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <h5><a id="Step_3__Partitioner_270"></a>Step 3. 自定义 Partitioner</h5> 
  <p>主要的逻辑就在这里, 这也是这个案例的意义, 通过 Partitioner 将数据分发给不同的 Reducer</p> 
  <pre><code class="prism language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">PartitonerOwn</span> <span class="token keyword">extends</span> <span class="token class-name">Partitioner</span><span class="token generics function"><span class="token punctuation">&lt;</span>Text<span class="token punctuation">,</span>LongWritable<span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">getPartition</span><span class="token punctuation">(</span>Text text<span class="token punctuation">,</span> LongWritable longWritable<span class="token punctuation">,</span> <span class="token keyword">int</span> i<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token comment">//如果字符长度大于等于5放在一组</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span>text<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">length</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;=</span><span class="token number">5</span> <span class="token punctuation">)</span><span class="token punctuation">{</span>
            <span class="token keyword">return</span>  <span class="token number">0</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>
        <span class="token comment">//小于5放在一组</span>
            <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <h5><a id="Step_4_Main__289"></a>Step 4. Main 入口</h5> 
  <pre><code class="prism language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">PartitionMain</span>  <span class="token keyword">extends</span> <span class="token class-name">Configured</span> <span class="token keyword">implements</span> <span class="token class-name">Tool</span> <span class="token punctuation">{</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span>  Exception<span class="token punctuation">{</span>
        <span class="token keyword">int</span> run <span class="token operator">=</span> ToolRunner<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">PartitionMain</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">;</span>
        System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>run<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">run</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
        Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span><span class="token keyword">super</span><span class="token punctuation">.</span><span class="token function">getConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PartitionMain<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">.</span><span class="token function">getSimpleName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span>PartitionMain<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setInputFormatClass</span><span class="token punctuation">(</span>TextInputFormat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        TextInputFormat<span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"hdfs://192.168.52.250:8020/partitioner"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>MyMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>NullWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        
        job<span class="token punctuation">.</span><span class="token function">setPartitionerClass</span><span class="token punctuation">(</span>MyPartitioner<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        
        job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>MyReducer<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>Text<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>NullWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
       
        <span class="token comment">/** * 设置我们的分区类，以及我们的reducetask的个数，注意reduceTask的个数一定要与我们的分区数保持一致 */</span>
        <span class="token comment">//设置reducetask的个数</span>
        job<span class="token punctuation">.</span><span class="token function">setNumReduceTasks</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputFormatClass</span><span class="token punctuation">(</span>TextOutputFormat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">Path</span><span class="token punctuation">(</span><span class="token string">"hdfs://192.168.52.250:8020/outpartition"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">boolean</span> b <span class="token operator">=</span> job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> b<span class="token operator">?</span><span class="token number">0</span><span class="token operator">:</span><span class="token number">1</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <h2><a id="6_MapReduce__326"></a>6. MapReduce 排序和序列化</h2> 
  <ul> 
   <li>序列化 (Serialization) 是指把结构化对象转化为字节流</li> 
   <li>反序列化 (Deserialization) 是序列化的逆过程. 把字节流转为结构化对象. 当要在进程间传递对象或持久化对象的时候, 就需要序列化对象成字节流, 反之当要将接收到或从磁盘读取的字节流转换为对象, 就要进行反序列化</li> 
   <li>Java 的序列化 (Serializable) 是一个重量级序列化框架, 一个对象被序列化后, 会附带很多额外的信息 (各种校验信息, header, 继承体系等）, 不便于在网络中高效传输. 所以, Hadoop 自己开发了一套序列化机制(Writable), 精简高效. 不用像 Java 对象类一样传输多层的父子关系, 需要哪个属性就传输哪个属性值, 大大的减少网络传输的开销</li> 
   <li>Writable 是 Hadoop 的序列化格式, Hadoop 定义了这样一个 Writable 接口. 一个类要支持可序列化只需实现这个接口即可</li> 
   <li>另外 Writable 有一个子接口是 WritableComparable, WritableComparable 是既可实现序列化, 也可以对key进行比较, 我们这里可以通过自定义 Key 实现 WritableComparable 来实现我们的排序功能</li> 
  </ul> 
  <p>数据格式如下</p> 
  <pre><code class="prism language-text">a	1
a	9
b	3
a	7
b	8
b	10
a	5
</code></pre> 
  <p>要求:</p> 
  <ul> 
   <li>第一列按照字典顺序进行排列</li> 
   <li>第一列相同的时候, 第二列按照升序进行排列</li> 
  </ul> 
  <p>解决思路:</p> 
  <ul> 
   <li>将 Map 端输出的 <code>&lt;key,value&gt;</code> 中的 key 和 value 组合成一个新的 key (newKey), value值不变</li> 
   <li>这里就变成 <code>&lt;(key,value),value&gt;</code>, 在针对 newKey 排序的时候, 如果 key 相同, 就再对value进行排序</li> 
  </ul> 
  <h5><a id="Step_1__356"></a>Step 1. 自定义类型和比较器</h5> 
  <pre><code class="prism language-java"><span class="token keyword">public</span> <span class="token keyword">class</span>  <span class="token class-name">SortBean</span> <span class="token keyword">implements</span> <span class="token class-name">WritableComparable</span><span class="token generics function"><span class="token punctuation">&lt;</span>SortBean<span class="token punctuation">&gt;</span></span><span class="token punctuation">{</span>
    <span class="token keyword">private</span> String word<span class="token punctuation">;</span>
    <span class="token keyword">private</span> <span class="token keyword">int</span> num<span class="token punctuation">;</span>

    <span class="token keyword">public</span> String <span class="token function">getWord</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">return</span> word<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">setWord</span><span class="token punctuation">(</span>String word<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">this</span><span class="token punctuation">.</span>word <span class="token operator">=</span> word<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">getNum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">return</span> num<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">setNum</span><span class="token punctuation">(</span><span class="token keyword">int</span> num<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">this</span><span class="token punctuation">.</span>num <span class="token operator">=</span> num<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> String <span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">return</span> word <span class="token operator">+</span> <span class="token string">"\t"</span> <span class="token operator">+</span> num <span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">//实现比较器，指定排序的规则</span>

    <span class="token comment">/** *第一列（word）按照字典顺序进行排序 *第二列（num）按照升序进行排序 */</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">compareTo</span><span class="token punctuation">(</span>SortBean sortBean<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token comment">//先对第一列进行排序：word</span>
        <span class="token keyword">int</span> result <span class="token operator">=</span> <span class="token keyword">this</span><span class="token punctuation">.</span>word<span class="token punctuation">.</span><span class="token function">compareTo</span><span class="token punctuation">(</span>sortBean<span class="token punctuation">.</span>word<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//对第二列进行排序</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span>result <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
            <span class="token keyword">return</span>  <span class="token keyword">this</span><span class="token punctuation">.</span>num <span class="token operator">-</span> sortBean<span class="token punctuation">.</span>num<span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        <span class="token keyword">return</span>  result<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">//实现序列化</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">write</span><span class="token punctuation">(</span>DataOutput out<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException <span class="token punctuation">{</span>
        out<span class="token punctuation">.</span><span class="token function">writeUTF</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">;</span>
        out<span class="token punctuation">.</span><span class="token function">writeInt</span><span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token punctuation">}</span>
    <span class="token comment">//实现反序列化</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">readFields</span><span class="token punctuation">(</span>DataInput in<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException <span class="token punctuation">{</span>
        <span class="token keyword">this</span><span class="token punctuation">.</span>word <span class="token operator">=</span> in<span class="token punctuation">.</span><span class="token function">readUTF</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">this</span><span class="token punctuation">.</span>num <span class="token operator">=</span> in<span class="token punctuation">.</span><span class="token function">readInt</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <h5><a id="Step_2_Mapper_415"></a>Step 2. Mapper</h5> 
  <pre><code class="prism language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">SortMapper</span> <span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token generics function"><span class="token punctuation">&lt;</span>LongWritable<span class="token punctuation">,</span>Text<span class="token punctuation">,</span>SortBean<span class="token punctuation">,</span>NullWritable<span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span>LongWritable key<span class="token punctuation">,</span> Text value<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>
        String<span class="token punctuation">[</span><span class="token punctuation">]</span> split <span class="token operator">=</span> value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        SortBean sortBean <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SortBean</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        sortBean<span class="token punctuation">.</span><span class="token function">setWord</span><span class="token punctuation">(</span>split<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        sortBean<span class="token punctuation">.</span><span class="token function">setNum</span><span class="token punctuation">(</span>Integer<span class="token punctuation">.</span><span class="token function">parseInt</span><span class="token punctuation">(</span>split<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>sortBean<span class="token punctuation">,</span>NullWritable<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <h5><a id="Step_3_Reducer_430"></a>Step 3. Reducer</h5> 
  <pre><code class="prism language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">SortReduce</span> <span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token generics function"><span class="token punctuation">&lt;</span>SortBean<span class="token punctuation">,</span>NullWritable<span class="token punctuation">,</span>SortBean<span class="token punctuation">,</span>NullWritable<span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span>SortBean key<span class="token punctuation">,</span> Iterable<span class="token generics function"><span class="token punctuation">&lt;</span>NullWritable<span class="token punctuation">&gt;</span></span> values<span class="token punctuation">,</span> Context context<span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException <span class="token punctuation">{</span>
        context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span>NullWritable<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

</code></pre> 
  <h5><a id="Step_4_Main__442"></a>Step 4. Main 入口</h5> 
  <pre><code class="prism language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">SortMain</span> <span class="token keyword">extends</span> <span class="token class-name">Configured</span> <span class="token keyword">implements</span> <span class="token class-name">Tool</span><span class="token punctuation">{</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">run</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
        <span class="token comment">//1 创建job对象</span>
        Job job <span class="token operator">=</span> Job<span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span><span class="token keyword">super</span><span class="token punctuation">.</span><span class="token function">getConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"mapreduce_sort"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//2 配置job任务</span>
        <span class="token comment">//八个步骤--设置输入类和输入对象</span>
        job<span class="token punctuation">.</span><span class="token function">setInputFormatClass</span><span class="token punctuation">(</span>TextInputFormat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        TextInputFormat<span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"hdfs://node01:8020/input/input_sort"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//--设置Mapper类和数据类型</span>
        job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span>SortMapper<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputKeyClass</span><span class="token punctuation">(</span>SortBean<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setMapOutputValueClass</span><span class="token punctuation">(</span>NullWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//----分区 排序 规约 分组</span>
        <span class="token comment">//--设置Redeuce类和数据类型</span>
        job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span>SortReduce<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span>SortBean<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span>NullWritable<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//设置输出类和输出路径</span>
        job<span class="token punctuation">.</span><span class="token function">setOutputFormatClass</span><span class="token punctuation">(</span>TextOutputFormat<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        TextOutputFormat<span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"hdfs://node01:8020/out/sort_out"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//等待任务结束</span>
        <span class="token keyword">boolean</span> bl <span class="token operator">=</span> job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> bl<span class="token operator">?</span><span class="token number">0</span><span class="token operator">:</span><span class="token number">1</span><span class="token punctuation">;</span>

    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">int</span> run <span class="token operator">=</span> ToolRunner<span class="token punctuation">.</span><span class="token function">run</span><span class="token punctuation">(</span>configuration<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">SortMain</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">;</span>
        System<span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>run<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
