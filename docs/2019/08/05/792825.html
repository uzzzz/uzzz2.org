<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>（一）HDFS的认识及使用Java对其的简单操作 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="（一）HDFS的认识及使用Java对其的简单操作" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="目录 一、HDFS概述 1.优点 2.缺点 3.HDFS组成架构 4.HDFS文件块大小 二、HDFS的Shell操作 三、HDFS客户端操作 1.环境准备 2.HDFS的API操作 3.HDFS的I/O流操作 四、HDFS的数据流 1.HDFS写数据流程 2.1剖析文件写入 2.2网络拓扑 - 节点距离计算 2.3Hadoop2.7.2 副本节点选择 2.HDFS读数据流程 一、HDFS概述 HDFS(Hadoop distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 使用场景：适合一次写入，多次读写的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。 1.优点 高容错性 （1）数据自动保存多个副本。它通过增加副本的形式，提高容错性。 （2）某一个副本丢失以后，它可以自动恢复。 适合处理大数据 （1）数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据； （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 可构建在廉价机器上，通过多副本机制，提高可靠性。 2.缺点 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。 无法高效的对大量小文件进行存储。 （1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的； （2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。 不支持并发写入、文件随机修改。 （1）一个文件只能有一个写，不允许多个线程同时写； （2）仅支持数据append（追加），不支持文件的随时修改。 3.HDFS组成架构 先通过一张图了解一下： NameNode（nn）：就是Master，它是一个主管、管理者。 （1）管理HDFS的名称空间； （2）配置副本策略； （3）管理数据库（Block）映射信息； （4）处理客户端读写请求。 DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 （1）存储实际的数据块； （2）执行数据块的读/写操作。 Secondary NameNode：并非NameNode的热备份。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 （1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode； （2）在紧急情况下，可辅助恢复NameNode。 4.HDFS文件块大小 HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M。 【问题：】为什么块的大小不能设置太小，也不能设置太大？ （1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置； （2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始所需的时间。导致程序在处理这块数据时，会非常慢。 总结：HDFS块的大小设置主要取决于磁盘传输速率。 二、HDFS的Shell操作 基本语法：（有两个） bin/hadoop fs 具体命令 或者 bin/hdfs dfs 具体命令 dfs 是 fs 的实现类。 常用命令实操： 启动Hadoop集群： [fseast@hadoop102 hadoop-2.7.2]$ sbin/ start-dfs.sh [fseast@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （1）-help：输出这个命令参数 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -help rm （2）-ls：显示目录信息 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -ls / （3）-mkdir：在HDFS上创建目录 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -mkdir -p /user/fseast/ （4）-moveFromLocal：从本地剪切粘贴到HDFS [fseast@hadoop102 hadoop-2.7.2]$ vim testmoveFrom.txt [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -moveFromLocal ./testmoveFrom.txt /user/fseast/ （5）-appendToFile：追加一个文件到已经存在的文件末尾 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -appendToFile NOTICE.txt /user/fseast/testmoveFrom.txt （6）-cat：显示文件内容 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -cat /user/fseast/testmoveFrom.txt （7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限 （8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyFromLocal test001.txt / （9）-copyToLocal：从HDFS拷贝到本地 [fseast@hadoop102 hadoop-2.7.2]$ rm -rf test001.txt [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyToLocal /test001.txt （10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径 （11）-mv：在HDFS目录中移动文件 （12）-get：等同于copyToLocal，就是从HDFS下载文件到本地 （13）-getmerge：合并下载多个文件，比如HDFS的目录/user/fseast/test 下有多个文件：log.1 , log.2 , log.3 … （14）-put：等同于copyFromLocal （15）-tail：显示一个文件的末尾 （16）-rm：删除文件或文件夹 （17）-rmdir：删除空目录 （18）-du统计文件夹的大小信息 （19）-setrep：设置HDFS中文件的副本数量 三、HDFS客户端操作 1.环境准备 创建Maven工程并导入相应得的依赖坐标 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 需要在项目的src/main/resources目录下，新建一个文件，命名为log4j.properties，并在文件中填入（为了查看日志）： log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 编写一个类，测试是否连接成功 /** * 连通客户端与HDFS的连接 * * @throws Exception */ @Test public void testClientConnectHDFS() throws Exception { Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), conf, &quot;fseast&quot;); //在HDFS上创建一个目录 fs.mkdirs(new Path(&quot;/HdfsDemo02/test01&quot;)); //关闭资源 fs.close(); } 2.HDFS的API操作 HDFS文件上传（参数优先级） /** * 测试1：上传文件到HDFS */ @Test public void testCopyFromLocal() throws Exception { //1.获取FileSystem对象 Configuration configuration = new Configuration(); //可以设置多少个副本，优先级最高 configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); //2.操作 fileSystem.copyFromLocalFile(new Path(&quot;e:/file/test/fsdong.txt&quot;), new Path(&quot;/HdfsDemo02/test01&quot;)); //3. 关闭资源 fileSystem.close(); } 可以将hdfs-site.xml拷贝到项目的根目录下。 参数优先级： 参数优先级排序： （1）客户端代码中设置的值 &gt; （2）ClassPath 下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置 HDFS文件下载 @Test public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 执行下载操作 // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件校验 fs.copyToLocalFile(false, new Path(&quot;/HdfsDemo02/test01/wc.input&quot;), new Path(&quot;e:/file/test/&quot;), true); // 3 关闭资源 fs.close(); } HDFS文件夹删除 @Test public void testDelete() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 执行删除 fs.delete(new Path(&quot;/HdfsDemo01/&quot;), true); // 3 关闭资源 fs.close(); } HDFS文件名更改 @Test public void testRename() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 修改文件名称 fs.rename(new Path(&quot;/HdfsDemo02/test01/fsdong.txt&quot;), new Path(&quot;/fseast.txt&quot;)); // 3 关闭资源 fs.close(); } HDFS 文件详情查看 查看文件名称、权限、长度、块信息 @Test public void testListFiles() throws IOException, InterruptedException, URISyntaxException{ // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while(listFiles.hasNext()){ LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 System.out.println(status.getPath().getName()); // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 分组 System.out.println(status.getGroup()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) { // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) { System.out.println(host); } } } // 3 关闭资源 fs.close(); } HDFS文件和文件夹判断 @Test public void testListStatus() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件配置信息 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 判断是文件还是文件夹 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); for (FileStatus fileStatus : listStatus) { // 如果是文件 if (fileStatus.isFile()) { System.out.println(&quot;f:&quot;+fileStatus.getPath().getName()); }else { System.out.println(&quot;d:&quot;+fileStatus.getPath().getName()); } } // 3 关闭资源 fs.close(); } 传入一个路径，递归将该路径下的所有的文件还有目录打印到控制台 @Test public void testListStatus() throws Exception { conf = new Configuration(); fs = FileSystem.get(new URI(uri), conf,user); printFileOrDir(&quot;/&quot;, fs); } /** * 传入一个路径 ，递归将该路径下的所有的文件还有目录打印到控制台 */ public void printFileOrDir(String path , FileSystem fs) throws Exception { FileStatus[] listStatus = fs.listStatus(new Path(path)); for (FileStatus fileStatus : listStatus) { //判断是文件还是目录 if(fileStatus.isFile()) { System.out.println(&quot;File:&quot; + path + &quot;/&quot; + fileStatus.getPath().getName()); }else { // path: hdfs://hadoop102:9000/0508 String currentPath = fileStatus.getPath().toString().substring(&quot;hdfs://hadoop102:9000&quot;.length()); //打印当前的目录 System.out.println(&quot;Dir:&quot; + currentPath); //继续迭代当前目录下的子目录及文件 printFileOrDir(currentPath, fs); fs.close(); } } } 3.HDFS的I/O流操作 1. HDFS文件上传 需求：把本地E盘上 test02.txt 文件上传到HDFS根目录 @Test public void putFileToHDFS() throws IOException, InterruptedException, URISyntaxException { // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 创建输入流 FileInputStream fis = new FileInputStream(new File(&quot;e:/test02.txt&quot;)); // 3 获取输出流 FSDataOutputStream fos = fs.create(new Path(&quot;/test02.txt&quot;)); // 4 流对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); } 2. HDFS文件下载 需求：从HDFS上下载test02.txt文件到本地e盘上 // 文件下载 @Test public void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/text02.txt&quot;)); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/test02..txt&quot;)); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); } 3. 定位文件读取 需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz（188M，大于128M所以上传到集群默认分成两块进行存储） （1）下载第一块： @Test public void readFileSeek1() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;)); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++){ fis.read(buf); fos.write(buf); } // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); fs.close(); } （2）下载第二块 @Test public void readFileSeek2() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;)); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } （3）将下载到的两块文件合并查看是否完整 在Windows命令窗口中进入到目录E:\，然后执行如下命令，对数据进行合并： type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 合并完成后，将hadoop-2.7.2.tar.gz.part1 重新命名为hadoop-2.7.2.tar.gz。解压发现该包是完整的。 四、HDFS的数据流 1.HDFS写数据流程 2.1剖析文件写入 步骤分析： （1）客户端通过 Distributed FileSystem 模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 （2）NameNode返回是否可以上传。 （3）客户端请求第一个 Block 上传到那几个 DataNode 服务器上。 （4）NameNode 返回3个 DataNode节点（默认备份3份的情况下），分别为dn1，dn2，dn3,。 （5）客户端通过 FSDataOutputStream 模块请求dn1上传数据，dn1 收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 （6）通道建立完成后，dn3应答dn2，dn2会答dn1，最后dn1应答客户端。 （7）客户端开始往dn1 上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。（DataNode收到Packet会先存到本地磁盘，再把缓存中的Packet传到下一个DataNode）。全部存完从dn3开始往回应答。 （8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block到服务器。重复执行3-7的步骤。 （9）传输完成以后，客户端会告诉NameNode数据传输完成。 2.2网络拓扑 - 节点距离计算 在HDFS写数据的过程中，NameNode会选择距离带上传数据最近距离的DataNode接受数据。 最近距离如何计算 节点距离：两个节点到达最近的共同祖先的距离总和。看图更清晰： 假设有数据中心d1机架r1中的节点n1.该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。 2.3Hadoop2.7.2 副本节点选择 默认三个副本情况下的副本节点选择： 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本和第一个副本位于相同机架，随机节点。 第三个副本位于不同机架，随机节点。 2.HDFS读数据流程 步骤分析: （1）客户端通过 Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查找元数据，找到文件块所在的DataNode地址。 （2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。 （3）DataNode开始传输数据给客户端（从磁盘读取数据输入流，以Packet）为单位来做校验。 （4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。 （5）然后接着读取 第二个Block。 下一篇：（二）HDFS——节点分析及新特性" />
<meta property="og:description" content="目录 一、HDFS概述 1.优点 2.缺点 3.HDFS组成架构 4.HDFS文件块大小 二、HDFS的Shell操作 三、HDFS客户端操作 1.环境准备 2.HDFS的API操作 3.HDFS的I/O流操作 四、HDFS的数据流 1.HDFS写数据流程 2.1剖析文件写入 2.2网络拓扑 - 节点距离计算 2.3Hadoop2.7.2 副本节点选择 2.HDFS读数据流程 一、HDFS概述 HDFS(Hadoop distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 使用场景：适合一次写入，多次读写的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。 1.优点 高容错性 （1）数据自动保存多个副本。它通过增加副本的形式，提高容错性。 （2）某一个副本丢失以后，它可以自动恢复。 适合处理大数据 （1）数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据； （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 可构建在廉价机器上，通过多副本机制，提高可靠性。 2.缺点 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。 无法高效的对大量小文件进行存储。 （1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的； （2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。 不支持并发写入、文件随机修改。 （1）一个文件只能有一个写，不允许多个线程同时写； （2）仅支持数据append（追加），不支持文件的随时修改。 3.HDFS组成架构 先通过一张图了解一下： NameNode（nn）：就是Master，它是一个主管、管理者。 （1）管理HDFS的名称空间； （2）配置副本策略； （3）管理数据库（Block）映射信息； （4）处理客户端读写请求。 DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 （1）存储实际的数据块； （2）执行数据块的读/写操作。 Secondary NameNode：并非NameNode的热备份。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 （1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode； （2）在紧急情况下，可辅助恢复NameNode。 4.HDFS文件块大小 HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M。 【问题：】为什么块的大小不能设置太小，也不能设置太大？ （1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置； （2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始所需的时间。导致程序在处理这块数据时，会非常慢。 总结：HDFS块的大小设置主要取决于磁盘传输速率。 二、HDFS的Shell操作 基本语法：（有两个） bin/hadoop fs 具体命令 或者 bin/hdfs dfs 具体命令 dfs 是 fs 的实现类。 常用命令实操： 启动Hadoop集群： [fseast@hadoop102 hadoop-2.7.2]$ sbin/ start-dfs.sh [fseast@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （1）-help：输出这个命令参数 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -help rm （2）-ls：显示目录信息 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -ls / （3）-mkdir：在HDFS上创建目录 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -mkdir -p /user/fseast/ （4）-moveFromLocal：从本地剪切粘贴到HDFS [fseast@hadoop102 hadoop-2.7.2]$ vim testmoveFrom.txt [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -moveFromLocal ./testmoveFrom.txt /user/fseast/ （5）-appendToFile：追加一个文件到已经存在的文件末尾 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -appendToFile NOTICE.txt /user/fseast/testmoveFrom.txt （6）-cat：显示文件内容 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -cat /user/fseast/testmoveFrom.txt （7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限 （8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyFromLocal test001.txt / （9）-copyToLocal：从HDFS拷贝到本地 [fseast@hadoop102 hadoop-2.7.2]$ rm -rf test001.txt [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyToLocal /test001.txt （10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径 （11）-mv：在HDFS目录中移动文件 （12）-get：等同于copyToLocal，就是从HDFS下载文件到本地 （13）-getmerge：合并下载多个文件，比如HDFS的目录/user/fseast/test 下有多个文件：log.1 , log.2 , log.3 … （14）-put：等同于copyFromLocal （15）-tail：显示一个文件的末尾 （16）-rm：删除文件或文件夹 （17）-rmdir：删除空目录 （18）-du统计文件夹的大小信息 （19）-setrep：设置HDFS中文件的副本数量 三、HDFS客户端操作 1.环境准备 创建Maven工程并导入相应得的依赖坐标 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 需要在项目的src/main/resources目录下，新建一个文件，命名为log4j.properties，并在文件中填入（为了查看日志）： log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 编写一个类，测试是否连接成功 /** * 连通客户端与HDFS的连接 * * @throws Exception */ @Test public void testClientConnectHDFS() throws Exception { Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), conf, &quot;fseast&quot;); //在HDFS上创建一个目录 fs.mkdirs(new Path(&quot;/HdfsDemo02/test01&quot;)); //关闭资源 fs.close(); } 2.HDFS的API操作 HDFS文件上传（参数优先级） /** * 测试1：上传文件到HDFS */ @Test public void testCopyFromLocal() throws Exception { //1.获取FileSystem对象 Configuration configuration = new Configuration(); //可以设置多少个副本，优先级最高 configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); //2.操作 fileSystem.copyFromLocalFile(new Path(&quot;e:/file/test/fsdong.txt&quot;), new Path(&quot;/HdfsDemo02/test01&quot;)); //3. 关闭资源 fileSystem.close(); } 可以将hdfs-site.xml拷贝到项目的根目录下。 参数优先级： 参数优先级排序： （1）客户端代码中设置的值 &gt; （2）ClassPath 下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置 HDFS文件下载 @Test public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 执行下载操作 // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件校验 fs.copyToLocalFile(false, new Path(&quot;/HdfsDemo02/test01/wc.input&quot;), new Path(&quot;e:/file/test/&quot;), true); // 3 关闭资源 fs.close(); } HDFS文件夹删除 @Test public void testDelete() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 执行删除 fs.delete(new Path(&quot;/HdfsDemo01/&quot;), true); // 3 关闭资源 fs.close(); } HDFS文件名更改 @Test public void testRename() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 修改文件名称 fs.rename(new Path(&quot;/HdfsDemo02/test01/fsdong.txt&quot;), new Path(&quot;/fseast.txt&quot;)); // 3 关闭资源 fs.close(); } HDFS 文件详情查看 查看文件名称、权限、长度、块信息 @Test public void testListFiles() throws IOException, InterruptedException, URISyntaxException{ // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while(listFiles.hasNext()){ LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 System.out.println(status.getPath().getName()); // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 分组 System.out.println(status.getGroup()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) { // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) { System.out.println(host); } } } // 3 关闭资源 fs.close(); } HDFS文件和文件夹判断 @Test public void testListStatus() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件配置信息 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 判断是文件还是文件夹 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); for (FileStatus fileStatus : listStatus) { // 如果是文件 if (fileStatus.isFile()) { System.out.println(&quot;f:&quot;+fileStatus.getPath().getName()); }else { System.out.println(&quot;d:&quot;+fileStatus.getPath().getName()); } } // 3 关闭资源 fs.close(); } 传入一个路径，递归将该路径下的所有的文件还有目录打印到控制台 @Test public void testListStatus() throws Exception { conf = new Configuration(); fs = FileSystem.get(new URI(uri), conf,user); printFileOrDir(&quot;/&quot;, fs); } /** * 传入一个路径 ，递归将该路径下的所有的文件还有目录打印到控制台 */ public void printFileOrDir(String path , FileSystem fs) throws Exception { FileStatus[] listStatus = fs.listStatus(new Path(path)); for (FileStatus fileStatus : listStatus) { //判断是文件还是目录 if(fileStatus.isFile()) { System.out.println(&quot;File:&quot; + path + &quot;/&quot; + fileStatus.getPath().getName()); }else { // path: hdfs://hadoop102:9000/0508 String currentPath = fileStatus.getPath().toString().substring(&quot;hdfs://hadoop102:9000&quot;.length()); //打印当前的目录 System.out.println(&quot;Dir:&quot; + currentPath); //继续迭代当前目录下的子目录及文件 printFileOrDir(currentPath, fs); fs.close(); } } } 3.HDFS的I/O流操作 1. HDFS文件上传 需求：把本地E盘上 test02.txt 文件上传到HDFS根目录 @Test public void putFileToHDFS() throws IOException, InterruptedException, URISyntaxException { // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 创建输入流 FileInputStream fis = new FileInputStream(new File(&quot;e:/test02.txt&quot;)); // 3 获取输出流 FSDataOutputStream fos = fs.create(new Path(&quot;/test02.txt&quot;)); // 4 流对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); } 2. HDFS文件下载 需求：从HDFS上下载test02.txt文件到本地e盘上 // 文件下载 @Test public void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/text02.txt&quot;)); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/test02..txt&quot;)); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); } 3. 定位文件读取 需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz（188M，大于128M所以上传到集群默认分成两块进行存储） （1）下载第一块： @Test public void readFileSeek1() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;)); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++){ fis.read(buf); fos.write(buf); } // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); fs.close(); } （2）下载第二块 @Test public void readFileSeek2() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;)); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } （3）将下载到的两块文件合并查看是否完整 在Windows命令窗口中进入到目录E:\，然后执行如下命令，对数据进行合并： type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 合并完成后，将hadoop-2.7.2.tar.gz.part1 重新命名为hadoop-2.7.2.tar.gz。解压发现该包是完整的。 四、HDFS的数据流 1.HDFS写数据流程 2.1剖析文件写入 步骤分析： （1）客户端通过 Distributed FileSystem 模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 （2）NameNode返回是否可以上传。 （3）客户端请求第一个 Block 上传到那几个 DataNode 服务器上。 （4）NameNode 返回3个 DataNode节点（默认备份3份的情况下），分别为dn1，dn2，dn3,。 （5）客户端通过 FSDataOutputStream 模块请求dn1上传数据，dn1 收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 （6）通道建立完成后，dn3应答dn2，dn2会答dn1，最后dn1应答客户端。 （7）客户端开始往dn1 上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。（DataNode收到Packet会先存到本地磁盘，再把缓存中的Packet传到下一个DataNode）。全部存完从dn3开始往回应答。 （8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block到服务器。重复执行3-7的步骤。 （9）传输完成以后，客户端会告诉NameNode数据传输完成。 2.2网络拓扑 - 节点距离计算 在HDFS写数据的过程中，NameNode会选择距离带上传数据最近距离的DataNode接受数据。 最近距离如何计算 节点距离：两个节点到达最近的共同祖先的距离总和。看图更清晰： 假设有数据中心d1机架r1中的节点n1.该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。 2.3Hadoop2.7.2 副本节点选择 默认三个副本情况下的副本节点选择： 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本和第一个副本位于相同机架，随机节点。 第三个副本位于不同机架，随机节点。 2.HDFS读数据流程 步骤分析: （1）客户端通过 Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查找元数据，找到文件块所在的DataNode地址。 （2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。 （3）DataNode开始传输数据给客户端（从磁盘读取数据输入流，以Packet）为单位来做校验。 （4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。 （5）然后接着读取 第二个Block。 下一篇：（二）HDFS——节点分析及新特性" />
<link rel="canonical" href="https://uzzz.org/2019/08/05/792825.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/05/792825.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-05T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"目录 一、HDFS概述 1.优点 2.缺点 3.HDFS组成架构 4.HDFS文件块大小 二、HDFS的Shell操作 三、HDFS客户端操作 1.环境准备 2.HDFS的API操作 3.HDFS的I/O流操作 四、HDFS的数据流 1.HDFS写数据流程 2.1剖析文件写入 2.2网络拓扑 - 节点距离计算 2.3Hadoop2.7.2 副本节点选择 2.HDFS读数据流程 一、HDFS概述 HDFS(Hadoop distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 使用场景：适合一次写入，多次读写的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。 1.优点 高容错性 （1）数据自动保存多个副本。它通过增加副本的形式，提高容错性。 （2）某一个副本丢失以后，它可以自动恢复。 适合处理大数据 （1）数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据； （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。 可构建在廉价机器上，通过多副本机制，提高可靠性。 2.缺点 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。 无法高效的对大量小文件进行存储。 （1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的； （2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。 不支持并发写入、文件随机修改。 （1）一个文件只能有一个写，不允许多个线程同时写； （2）仅支持数据append（追加），不支持文件的随时修改。 3.HDFS组成架构 先通过一张图了解一下： NameNode（nn）：就是Master，它是一个主管、管理者。 （1）管理HDFS的名称空间； （2）配置副本策略； （3）管理数据库（Block）映射信息； （4）处理客户端读写请求。 DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 （1）存储实际的数据块； （2）执行数据块的读/写操作。 Secondary NameNode：并非NameNode的热备份。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 （1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode； （2）在紧急情况下，可辅助恢复NameNode。 4.HDFS文件块大小 HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M。 【问题：】为什么块的大小不能设置太小，也不能设置太大？ （1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置； （2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始所需的时间。导致程序在处理这块数据时，会非常慢。 总结：HDFS块的大小设置主要取决于磁盘传输速率。 二、HDFS的Shell操作 基本语法：（有两个） bin/hadoop fs 具体命令 或者 bin/hdfs dfs 具体命令 dfs 是 fs 的实现类。 常用命令实操： 启动Hadoop集群： [fseast@hadoop102 hadoop-2.7.2]$ sbin/ start-dfs.sh [fseast@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （1）-help：输出这个命令参数 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -help rm （2）-ls：显示目录信息 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -ls / （3）-mkdir：在HDFS上创建目录 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -mkdir -p /user/fseast/ （4）-moveFromLocal：从本地剪切粘贴到HDFS [fseast@hadoop102 hadoop-2.7.2]$ vim testmoveFrom.txt [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -moveFromLocal ./testmoveFrom.txt /user/fseast/ （5）-appendToFile：追加一个文件到已经存在的文件末尾 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -appendToFile NOTICE.txt /user/fseast/testmoveFrom.txt （6）-cat：显示文件内容 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -cat /user/fseast/testmoveFrom.txt （7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限 （8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去 [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyFromLocal test001.txt / （9）-copyToLocal：从HDFS拷贝到本地 [fseast@hadoop102 hadoop-2.7.2]$ rm -rf test001.txt [fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyToLocal /test001.txt （10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径 （11）-mv：在HDFS目录中移动文件 （12）-get：等同于copyToLocal，就是从HDFS下载文件到本地 （13）-getmerge：合并下载多个文件，比如HDFS的目录/user/fseast/test 下有多个文件：log.1 , log.2 , log.3 … （14）-put：等同于copyFromLocal （15）-tail：显示一个文件的末尾 （16）-rm：删除文件或文件夹 （17）-rmdir：删除空目录 （18）-du统计文件夹的大小信息 （19）-setrep：设置HDFS中文件的副本数量 三、HDFS客户端操作 1.环境准备 创建Maven工程并导入相应得的依赖坐标 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 需要在项目的src/main/resources目录下，新建一个文件，命名为log4j.properties，并在文件中填入（为了查看日志）： log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 编写一个类，测试是否连接成功 /** * 连通客户端与HDFS的连接 * * @throws Exception */ @Test public void testClientConnectHDFS() throws Exception { Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), conf, &quot;fseast&quot;); //在HDFS上创建一个目录 fs.mkdirs(new Path(&quot;/HdfsDemo02/test01&quot;)); //关闭资源 fs.close(); } 2.HDFS的API操作 HDFS文件上传（参数优先级） /** * 测试1：上传文件到HDFS */ @Test public void testCopyFromLocal() throws Exception { //1.获取FileSystem对象 Configuration configuration = new Configuration(); //可以设置多少个副本，优先级最高 configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); //2.操作 fileSystem.copyFromLocalFile(new Path(&quot;e:/file/test/fsdong.txt&quot;), new Path(&quot;/HdfsDemo02/test01&quot;)); //3. 关闭资源 fileSystem.close(); } 可以将hdfs-site.xml拷贝到项目的根目录下。 参数优先级： 参数优先级排序： （1）客户端代码中设置的值 &gt; （2）ClassPath 下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置 HDFS文件下载 @Test public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 执行下载操作 // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件校验 fs.copyToLocalFile(false, new Path(&quot;/HdfsDemo02/test01/wc.input&quot;), new Path(&quot;e:/file/test/&quot;), true); // 3 关闭资源 fs.close(); } HDFS文件夹删除 @Test public void testDelete() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 执行删除 fs.delete(new Path(&quot;/HdfsDemo01/&quot;), true); // 3 关闭资源 fs.close(); } HDFS文件名更改 @Test public void testRename() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 修改文件名称 fs.rename(new Path(&quot;/HdfsDemo02/test01/fsdong.txt&quot;), new Path(&quot;/fseast.txt&quot;)); // 3 关闭资源 fs.close(); } HDFS 文件详情查看 查看文件名称、权限、长度、块信息 @Test public void testListFiles() throws IOException, InterruptedException, URISyntaxException{ // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while(listFiles.hasNext()){ LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 System.out.println(status.getPath().getName()); // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 分组 System.out.println(status.getGroup()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) { // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) { System.out.println(host); } } } // 3 关闭资源 fs.close(); } HDFS文件和文件夹判断 @Test public void testListStatus() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件配置信息 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 判断是文件还是文件夹 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); for (FileStatus fileStatus : listStatus) { // 如果是文件 if (fileStatus.isFile()) { System.out.println(&quot;f:&quot;+fileStatus.getPath().getName()); }else { System.out.println(&quot;d:&quot;+fileStatus.getPath().getName()); } } // 3 关闭资源 fs.close(); } 传入一个路径，递归将该路径下的所有的文件还有目录打印到控制台 @Test public void testListStatus() throws Exception { conf = new Configuration(); fs = FileSystem.get(new URI(uri), conf,user); printFileOrDir(&quot;/&quot;, fs); } /** * 传入一个路径 ，递归将该路径下的所有的文件还有目录打印到控制台 */ public void printFileOrDir(String path , FileSystem fs) throws Exception { FileStatus[] listStatus = fs.listStatus(new Path(path)); for (FileStatus fileStatus : listStatus) { //判断是文件还是目录 if(fileStatus.isFile()) { System.out.println(&quot;File:&quot; + path + &quot;/&quot; + fileStatus.getPath().getName()); }else { // path: hdfs://hadoop102:9000/0508 String currentPath = fileStatus.getPath().toString().substring(&quot;hdfs://hadoop102:9000&quot;.length()); //打印当前的目录 System.out.println(&quot;Dir:&quot; + currentPath); //继续迭代当前目录下的子目录及文件 printFileOrDir(currentPath, fs); fs.close(); } } } 3.HDFS的I/O流操作 1. HDFS文件上传 需求：把本地E盘上 test02.txt 文件上传到HDFS根目录 @Test public void putFileToHDFS() throws IOException, InterruptedException, URISyntaxException { // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 创建输入流 FileInputStream fis = new FileInputStream(new File(&quot;e:/test02.txt&quot;)); // 3 获取输出流 FSDataOutputStream fos = fs.create(new Path(&quot;/test02.txt&quot;)); // 4 流对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); } 2. HDFS文件下载 需求：从HDFS上下载test02.txt文件到本地e盘上 // 文件下载 @Test public void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/text02.txt&quot;)); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/test02..txt&quot;)); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); } 3. 定位文件读取 需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz（188M，大于128M所以上传到集群默认分成两块进行存储） （1）下载第一块： @Test public void readFileSeek1() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;)); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++){ fis.read(buf); fos.write(buf); } // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); fs.close(); } （2）下载第二块 @Test public void readFileSeek2() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;fseast&quot;); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;)); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } （3）将下载到的两块文件合并查看是否完整 在Windows命令窗口中进入到目录E:\\，然后执行如下命令，对数据进行合并： type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 合并完成后，将hadoop-2.7.2.tar.gz.part1 重新命名为hadoop-2.7.2.tar.gz。解压发现该包是完整的。 四、HDFS的数据流 1.HDFS写数据流程 2.1剖析文件写入 步骤分析： （1）客户端通过 Distributed FileSystem 模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 （2）NameNode返回是否可以上传。 （3）客户端请求第一个 Block 上传到那几个 DataNode 服务器上。 （4）NameNode 返回3个 DataNode节点（默认备份3份的情况下），分别为dn1，dn2，dn3,。 （5）客户端通过 FSDataOutputStream 模块请求dn1上传数据，dn1 收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 （6）通道建立完成后，dn3应答dn2，dn2会答dn1，最后dn1应答客户端。 （7）客户端开始往dn1 上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。（DataNode收到Packet会先存到本地磁盘，再把缓存中的Packet传到下一个DataNode）。全部存完从dn3开始往回应答。 （8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block到服务器。重复执行3-7的步骤。 （9）传输完成以后，客户端会告诉NameNode数据传输完成。 2.2网络拓扑 - 节点距离计算 在HDFS写数据的过程中，NameNode会选择距离带上传数据最近距离的DataNode接受数据。 最近距离如何计算 节点距离：两个节点到达最近的共同祖先的距离总和。看图更清晰： 假设有数据中心d1机架r1中的节点n1.该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。 2.3Hadoop2.7.2 副本节点选择 默认三个副本情况下的副本节点选择： 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本和第一个副本位于相同机架，随机节点。 第三个副本位于不同机架，随机节点。 2.HDFS读数据流程 步骤分析: （1）客户端通过 Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查找元数据，找到文件块所在的DataNode地址。 （2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。 （3）DataNode开始传输数据给客户端（从磁盘读取数据输入流，以Packet）为单位来做校验。 （4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。 （5）然后接着读取 第二个Block。 下一篇：（二）HDFS——节点分析及新特性","@type":"BlogPosting","url":"https://uzzz.org/2019/08/05/792825.html","headline":"（一）HDFS的认识及使用Java对其的简单操作","dateModified":"2019-08-05T00:00:00+08:00","datePublished":"2019-08-05T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/05/792825.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>（一）HDFS的认识及使用Java对其的简单操作</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>目录</h3>
   <ul>
    <ul>
     <li><a href="#HDFS_1" rel="nofollow" data-token="8046048b3612e8c82107a40984d8051c">一、HDFS概述</a></li>
     <ul>
      <li><a href="#1_4" rel="nofollow" data-token="f1034f96f4797348a79c7f9e374b0502">1.优点</a></li>
      <li><a href="#2_13" rel="nofollow" data-token="c5a1f3190bd84a328ddbf3f29851ef85">2.缺点</a></li>
      <li><a href="#3HDFS_22" rel="nofollow" data-token="379cbdfde1d37421127591af5a7214d8">3.HDFS组成架构</a></li>
      <li><a href="#4HDFS_37" rel="nofollow" data-token="ae4637f5a209ce1edfa6366ef5f0a95d">4.HDFS文件块大小</a></li>
     </ul>
     <li><a href="#HDFSShell_48" rel="nofollow" data-token="861a7b7627fd05e170cda5f8d78c9645">二、HDFS的Shell操作</a></li>
     <li><a href="#HDFS_125" rel="nofollow" data-token="4cfd21a11d08a2759cb4ccaea80bc170">三、HDFS客户端操作</a></li>
     <ul>
      <li><a href="#1_126" rel="nofollow" data-token="9df4df5cd67ba6fe5ca2b3000f748db4">1.环境准备</a></li>
      <li><a href="#2HDFSAPI_203" rel="nofollow" data-token="f4f708ad72cae12a54eca7bde1eab5d8">2.HDFS的API操作</a></li>
      <li><a href="#3HDFSIO_411" rel="nofollow" data-token="01d197708cc6e39023e79eb39c45504a">3.HDFS的I/O流操作</a></li>
     </ul>
     <li><a href="#HDFS_539" rel="nofollow" data-token="028c9710dd87b6d0e03ed75689e08c54">四、HDFS的数据流</a></li>
     <ul>
      <li><a href="#1HDFS_540" rel="nofollow" data-token="8332ad459dbf709125bdfd1455eb4bb3">1.HDFS写数据流程</a></li>
      <ul>
       <li><a href="#21_541" rel="nofollow" data-token="b7de8fb8444bbd9555c02755fd134abe">2.1剖析文件写入</a></li>
       <li><a href="#22___556" rel="nofollow" data-token="0df45fb0c198e518a582f0597f0685f3">2.2网络拓扑 - 节点距离计算</a></li>
       <li><a href="#23Hadoop272__563" rel="nofollow" data-token="4acb3310297746d52b23ea9407646fdc">2.3Hadoop2.7.2 副本节点选择</a></li>
      </ul>
      <li><a href="#2HDFS_569" rel="nofollow" data-token="7915f8c6940aafca9aa98b2d2608f7bb">2.HDFS读数据流程</a></li>
     </ul>
    </ul>
   </ul>
  </div>
  <p></p> 
  <h2><a id="HDFS_1"></a>一、HDFS概述</h2> 
  <p>HDFS(Hadoop distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。<br> 使用场景：适合一次写入，多次读写的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。</p> 
  <h3><a id="1_4"></a>1.优点</h3> 
  <ol> 
   <li>高容错性<br> （1）数据自动保存多个副本。它通过增加副本的形式，提高容错性。<br> （2）某一个副本丢失以后，它可以自动恢复。</li> 
   <li>适合处理大数据<br> （1）数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据；<br> （2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。</li> 
   <li>可构建在廉价机器上，通过多副本机制，提高可靠性。</li> 
  </ol> 
  <h3><a id="2_13"></a>2.缺点</h3> 
  <ol> 
   <li>不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。</li> 
   <li>无法高效的对大量小文件进行存储。<br> （1）存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；<br> （2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。</li> 
   <li>不支持并发写入、文件随机修改。<br> （1）一个文件只能有一个写，不允许多个线程同时写；<br> （2）仅支持数据append（追加），不支持文件的随时修改。</li> 
  </ol> 
  <h3><a id="3HDFS_22"></a>3.HDFS组成架构</h3> 
  <p>先通过一张图了解一下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190720215137559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <ol> 
   <li>NameNode（nn）：就是Master，它是一个主管、管理者。<br> （1）管理HDFS的名称空间；<br> （2）配置副本策略；<br> （3）管理数据库（Block）映射信息；<br> （4）处理客户端读写请求。</li> 
   <li>DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。<br> （1）存储实际的数据块；<br> （2）执行数据块的读/写操作。</li> 
   <li>Secondary NameNode：并非NameNode的热备份。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。<br> （1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode；<br> （2）在紧急情况下，可辅助恢复NameNode。</li> 
  </ol> 
  <h3><a id="4HDFS_37"></a>4.HDFS文件块大小</h3> 
  <p>HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019072022325670.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="HDFS文件块大小"></p> 
  <p>【问题：】为什么块的大小不能设置太小，也不能设置太大？<br> （1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；<br> （2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始所需的时间。导致程序在处理这块数据时，会非常慢。</p> 
  <p>总结：HDFS块的大小设置主要取决于磁盘传输速率。</p> 
  <h2><a id="HDFSShell_48"></a>二、HDFS的Shell操作</h2> 
  <ol> 
   <li>基本语法：（有两个）</li> 
  </ol> 
  <p><code>bin/hadoop fs 具体命令</code> 或者 <code>bin/hdfs dfs 具体命令</code><br> dfs 是 fs 的实现类。</p> 
  <ol start="2"> 
   <li>常用命令实操：<br> 启动Hadoop集群：</li> 
  </ol> 
  <pre><code>[fseast@hadoop102 hadoop-2.7.2]$ sbin/ start-dfs.sh
[fseast@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh
</code></pre> 
  <p>（1）-help：输出这个命令参数</p> 
  <pre><code>[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -help rm
</code></pre> 
  <p>（2）-ls：显示目录信息</p> 
  <pre><code>[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -ls /
</code></pre> 
  <p>（3）-mkdir：在HDFS上创建目录</p> 
  <pre><code>[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -mkdir -p /user/fseast/
</code></pre> 
  <p>（4）-moveFromLocal：从本地剪切粘贴到HDFS</p> 
  <pre><code>[fseast@hadoop102 hadoop-2.7.2]$ vim testmoveFrom.txt 
[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -moveFromLocal ./testmoveFrom.txt /user/fseast/
</code></pre> 
  <p>（5）-appendToFile：追加一个文件到已经存在的文件末尾</p> 
  <pre><code>[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -appendToFile NOTICE.txt /user/fseast/testmoveFrom.txt
</code></pre> 
  <p>（6）-cat：显示文件内容</p> 
  <pre><code>[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -cat /user/fseast/testmoveFrom.txt
</code></pre> 
  <p>（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</p> 
  <p>（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</p> 
  <pre><code>[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyFromLocal test001.txt /
</code></pre> 
  <p>（9）-copyToLocal：从HDFS拷贝到本地</p> 
  <pre><code>[fseast@hadoop102 hadoop-2.7.2]$ rm -rf test001.txt 
[fseast@hadoop102 hadoop-2.7.2]$ hdfs dfs -copyToLocal /test001.txt
</code></pre> 
  <p>（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</p> 
  <p>（11）-mv：在HDFS目录中移动文件</p> 
  <p>（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地</p> 
  <p>（13）-getmerge：合并下载多个文件，比如HDFS的目录/user/fseast/test 下有多个文件：log.1 , log.2 , log.3 …</p> 
  <p>（14）-put：等同于copyFromLocal</p> 
  <p>（15）-tail：显示一个文件的末尾</p> 
  <p>（16）-rm：删除文件或文件夹</p> 
  <p>（17）-rmdir：删除空目录</p> 
  <p>（18）-du统计文件夹的大小信息</p> 
  <p>（19）-setrep：设置HDFS中文件的副本数量</p> 
  <h2><a id="HDFS_125"></a>三、HDFS客户端操作</h2> 
  <h3><a id="1_126"></a>1.环境准备</h3> 
  <ol> 
   <li>创建Maven工程并导入相应得的依赖坐标</li> 
  </ol> 
  <pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>RELEASE<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.logging.log4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>log4j-core<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.8.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>hadoop-common<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>hadoop-client<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>hadoop-hdfs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>jdk.tools<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>jdk.tools<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>1.8<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>scope</span><span class="token punctuation">&gt;</span></span>system<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>scope</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>systemPath</span><span class="token punctuation">&gt;</span></span>${JAVA_HOME}/lib/tools.jar<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>systemPath</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">&gt;</span></span>

</code></pre> 
  <ol start="2"> 
   <li>需要在项目的src/main/resources目录下，新建一个文件，命名为log4j.properties，并在文件中填入（为了查看日志）：</li> 
  </ol> 
  <pre><code>log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n

</code></pre> 
  <ol start="3"> 
   <li>编写一个类，测试是否连接成功</li> 
  </ol> 
  <pre><code class="prism language-java">   <span class="token comment">/** * 连通客户端与HDFS的连接 * * @throws Exception */</span>
    <span class="token annotation punctuation">@Test</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testClientConnectHDFS</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
        Configuration conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conf<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>


        <span class="token comment">//在HDFS上创建一个目录</span>
        fs<span class="token punctuation">.</span><span class="token function">mkdirs</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/HdfsDemo02/test01"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//关闭资源</span>
        fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
</code></pre> 
  <h3><a id="2HDFSAPI_203"></a>2.HDFS的API操作</h3> 
  <ol> 
   <li>HDFS文件上传（参数优先级）</li> 
  </ol> 
  <pre><code class="prism language-java">  <span class="token comment">/** * 测试1：上传文件到HDFS */</span>
    <span class="token annotation punctuation">@Test</span>
    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testCopyFromLocal</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
        <span class="token comment">//1.获取FileSystem对象</span>
        Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">//可以设置多少个副本，优先级最高</span>
        configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"dfs.replication"</span><span class="token punctuation">,</span> <span class="token string">"2"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        FileSystem fileSystem <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//2.操作</span>
        fileSystem<span class="token punctuation">.</span><span class="token function">copyFromLocalFile</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"e:/file/test/fsdong.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/HdfsDemo02/test01"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//3. 关闭资源</span>
        fileSystem<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token punctuation">}</span>
</code></pre> 
  <p>可以将hdfs-site.xml拷贝到项目的根目录下。<br> <strong>参数优先级：</strong><br> 参数优先级排序：<br> （1）客户端代码中设置的值 &gt; （2）ClassPath 下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置</p> 
  <ol start="2"> 
   <li>HDFS文件下载</li> 
  </ol> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testCopyToLocalFile</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>

		<span class="token comment">// 1 获取文件系统</span>
		Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token comment">// 2 执行下载操作</span>
		<span class="token comment">// boolean delSrc 指是否将原文件删除</span>
		<span class="token comment">// Path src 指要下载的文件路径</span>
		<span class="token comment">// Path dst 指将文件下载到的路径</span>
		<span class="token comment">// boolean useRawLocalFileSystem 是否开启文件校验</span>
		fs<span class="token punctuation">.</span><span class="token function">copyToLocalFile</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/HdfsDemo02/test01/wc.input"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"e:/file/test/"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token comment">// 3 关闭资源</span>
		fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
  <ol start="3"> 
   <li>HDFS文件夹删除</li> 
  </ol> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testDelete</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>

	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 2 执行删除</span>
	fs<span class="token punctuation">.</span><span class="token function">delete</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/HdfsDemo01/"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 3 关闭资源</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
  <ol start="4"> 
   <li>HDFS文件名更改</li> 
  </ol> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testRename</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>

	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
		
	<span class="token comment">// 2 修改文件名称</span>
	fs<span class="token punctuation">.</span><span class="token function">rename</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/HdfsDemo02/test01/fsdong.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/fseast.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 3 关闭资源</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
  <ol start="5"> 
   <li>HDFS 文件详情查看<br> 查看文件名称、权限、长度、块信息</li> 
  </ol> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testListFiles</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>

	<span class="token comment">// 1获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
		
	<span class="token comment">// 2 获取文件详情</span>
	RemoteIterator<span class="token generics function"><span class="token punctuation">&lt;</span>LocatedFileStatus<span class="token punctuation">&gt;</span></span> listFiles <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">listFiles</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token keyword">while</span><span class="token punctuation">(</span>listFiles<span class="token punctuation">.</span><span class="token function">hasNext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
		LocatedFileStatus status <span class="token operator">=</span> listFiles<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			
		<span class="token comment">// 输出详情</span>
		<span class="token comment">// 文件名称</span>
		System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 长度</span>
		System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getLen</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 权限</span>
		System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getPermission</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 分组</span>
		System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getGroup</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			
		<span class="token comment">// 获取存储的块信息</span>
		BlockLocation<span class="token punctuation">[</span><span class="token punctuation">]</span> blockLocations <span class="token operator">=</span> status<span class="token punctuation">.</span><span class="token function">getBlockLocations</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			
		<span class="token keyword">for</span> <span class="token punctuation">(</span>BlockLocation blockLocation <span class="token operator">:</span> blockLocations<span class="token punctuation">)</span> <span class="token punctuation">{</span>
				
			<span class="token comment">// 获取块存储的主机节点</span>
			String<span class="token punctuation">[</span><span class="token punctuation">]</span> hosts <span class="token operator">=</span> blockLocation<span class="token punctuation">.</span><span class="token function">getHosts</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
				
			<span class="token keyword">for</span> <span class="token punctuation">(</span>String host <span class="token operator">:</span> hosts<span class="token punctuation">)</span> <span class="token punctuation">{</span>
				System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>host<span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
			
	<span class="token punctuation">}</span>

<span class="token comment">// 3 关闭资源</span>
fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
  <ol start="6"> 
   <li>HDFS文件和文件夹判断</li> 
  </ol> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testListStatus</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
		
	<span class="token comment">// 1 获取文件配置信息</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 2 判断是文件还是文件夹</span>
	FileStatus<span class="token punctuation">[</span><span class="token punctuation">]</span> listStatus <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">listStatus</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token keyword">for</span> <span class="token punctuation">(</span>FileStatus fileStatus <span class="token operator">:</span> listStatus<span class="token punctuation">)</span> <span class="token punctuation">{</span>
		
		<span class="token comment">// 如果是文件</span>
		<span class="token keyword">if</span> <span class="token punctuation">(</span>fileStatus<span class="token punctuation">.</span><span class="token function">isFile</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
				System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"f:"</span><span class="token operator">+</span>fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span>
				System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"d:"</span><span class="token operator">+</span>fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
		
	<span class="token comment">// 3 关闭资源</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
  <p>传入一个路径，递归将该路径下的所有的文件还有目录打印到控制台</p> 
  <pre><code class="prism language-java">
	<span class="token annotation punctuation">@Test</span>
	<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testListStatus</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
		conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		fs  <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span>uri<span class="token punctuation">)</span><span class="token punctuation">,</span> conf<span class="token punctuation">,</span>user<span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token function">printFileOrDir</span><span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">,</span> fs<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token punctuation">}</span>
	
	<span class="token comment">/** * 传入一个路径 ，递归将该路径下的所有的文件还有目录打印到控制台 */</span>
	<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">printFileOrDir</span><span class="token punctuation">(</span>String path <span class="token punctuation">,</span> FileSystem fs<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token punctuation">{</span>
		
		FileStatus<span class="token punctuation">[</span><span class="token punctuation">]</span> listStatus <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">listStatus</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
		<span class="token keyword">for</span> <span class="token punctuation">(</span>FileStatus fileStatus <span class="token operator">:</span> listStatus<span class="token punctuation">)</span> <span class="token punctuation">{</span>
			<span class="token comment">//判断是文件还是目录</span>
			<span class="token keyword">if</span><span class="token punctuation">(</span>fileStatus<span class="token punctuation">.</span><span class="token function">isFile</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
				System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"File:"</span> <span class="token operator">+</span> path <span class="token operator">+</span> <span class="token string">"/"</span> <span class="token operator">+</span> fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span>
				<span class="token comment">// path: hdfs://hadoop102:9000/0508</span>
				String currentPath <span class="token operator">=</span> fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">.</span><span class="token function">length</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
				
				<span class="token comment">//打印当前的目录</span>
				System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Dir:"</span> <span class="token operator">+</span> currentPath<span class="token punctuation">)</span><span class="token punctuation">;</span>
				
				<span class="token comment">//继续迭代当前目录下的子目录及文件</span>
				<span class="token function">printFileOrDir</span><span class="token punctuation">(</span>currentPath<span class="token punctuation">,</span> fs<span class="token punctuation">)</span><span class="token punctuation">;</span>
				fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span>
			
		<span class="token punctuation">}</span>
		
	<span class="token punctuation">}</span>
</code></pre> 
  <h3><a id="3HDFSIO_411"></a>3.HDFS的I/O流操作</h3> 
  <p><font size="4" color="#4169E1"> 1. HDFS文件上传 </font><br> 需求：把本地E盘上 test02.txt 文件上传到HDFS根目录</p> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">putFileToHDFS</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException <span class="token punctuation">{</span>

	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">// 2 创建输入流</span>
	FileInputStream fis <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FileInputStream</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">File</span><span class="token punctuation">(</span><span class="token string">"e:/test02.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">// 3 获取输出流</span>
	FSDataOutputStream fos <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/test02.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">// 4 流对拷</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">copyBytes</span><span class="token punctuation">(</span>fis<span class="token punctuation">,</span> fos<span class="token punctuation">,</span> configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">// 5 关闭资源</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fos<span class="token punctuation">)</span><span class="token punctuation">;</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fis<span class="token punctuation">)</span><span class="token punctuation">;</span>
    fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
  <p><font size="4" color="#4169E1"> 2. HDFS文件下载 </font><br> 需求：从HDFS上下载test02.txt文件到本地e盘上</p> 
  <pre><code class="prism language-java"><span class="token comment">// 文件下载</span>
<span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">getFileFromHDFS</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>

	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 2 获取输入流</span>
	FSDataInputStream fis <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">open</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/text02.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 3 获取输出流</span>
	FileOutputStream fos <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FileOutputStream</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">File</span><span class="token punctuation">(</span><span class="token string">"e:/test02..txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 4 流的对拷</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">copyBytes</span><span class="token punctuation">(</span>fis<span class="token punctuation">,</span> fos<span class="token punctuation">,</span> configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 5 关闭资源</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fos<span class="token punctuation">)</span><span class="token punctuation">;</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fis<span class="token punctuation">)</span><span class="token punctuation">;</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
  <p><font size="4" color="#4169E1"> 3. 定位文件读取 </font><br> 需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz（188M，大于128M所以上传到集群默认分成两块进行存储）<br> <font size="3" color="#4169E1">（1）下载第一块： </font></p> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">readFileSeek1</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>

	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 2 获取输入流</span>
	FSDataInputStream fis <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">open</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/hadoop-2.7.2.tar.gz"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 3 创建输出流</span>
	FileOutputStream fos <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FileOutputStream</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">File</span><span class="token punctuation">(</span><span class="token string">"e:/hadoop-2.7.2.tar.gz.part1"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 4 流的拷贝</span>
	<span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> buf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">byte</span><span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
		
	<span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span><span class="token number">0</span> <span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">128</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
		fis<span class="token punctuation">.</span><span class="token function">read</span><span class="token punctuation">(</span>buf<span class="token punctuation">)</span><span class="token punctuation">;</span>
		fos<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>buf<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>
		
	<span class="token comment">// 5关闭资源</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fis<span class="token punctuation">)</span><span class="token punctuation">;</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fos<span class="token punctuation">)</span><span class="token punctuation">;</span>
fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
  <p><font size="3" color="#4169E1">（2）下载第二块</font></p> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">readFileSeek2</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>

	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"fseast"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 2 打开输入流</span>
	FSDataInputStream fis <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">open</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/hadoop-2.7.2.tar.gz"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 3 定位输入数据位置</span>
	fis<span class="token punctuation">.</span><span class="token function">seek</span><span class="token punctuation">(</span><span class="token number">1024</span><span class="token operator">*</span><span class="token number">1024</span><span class="token operator">*</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 4 创建输出流</span>
	FileOutputStream fos <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FileOutputStream</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">File</span><span class="token punctuation">(</span><span class="token string">"e:/hadoop-2.7.2.tar.gz.part2"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 5 流的对拷</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">copyBytes</span><span class="token punctuation">(</span>fis<span class="token punctuation">,</span> fos<span class="token punctuation">,</span> configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>
		
	<span class="token comment">// 6 关闭资源</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fis<span class="token punctuation">)</span><span class="token punctuation">;</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fos<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
  <font size="3" color="#4169E1"> （3）将下载到的两块文件合并查看是否完整 在Windows命令窗口中进入到目录E:\，然后执行如下命令，对数据进行合并： </font>
  <pre><code>type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1
</code></pre> 
  <p>合并完成后，将hadoop-2.7.2.tar.gz.part1 重新命名为hadoop-2.7.2.tar.gz。解压发现该包是完整的。</p> 
  <h2><a id="HDFS_539"></a>四、HDFS的数据流</h2> 
  <h3><a id="1HDFS_540"></a>1.HDFS写数据流程</h3> 
  <h4><a id="21_541"></a>2.1剖析文件写入</h4> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728145944142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <font size="4" color="#4169E1">步骤分析： </font></p> 
  <p>（1）客户端通过 Distributed FileSystem 模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。<br> （2）NameNode返回是否可以上传。<br> （3）客户端请求第一个 Block 上传到那几个 DataNode 服务器上。<br> （4）NameNode 返回3个 DataNode节点（默认备份3份的情况下），分别为dn1，dn2，dn3,。<br> （5）客户端通过 FSDataOutputStream 模块请求dn1上传数据，dn1 收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。<br> （6）通道建立完成后，dn3应答dn2，dn2会答dn1，最后dn1应答客户端。<br> （7）客户端开始往dn1 上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。（DataNode收到Packet会先存到本地磁盘，再把缓存中的Packet传到下一个DataNode）。全部存完从dn3开始往回应答。<br> （8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block到服务器。重复执行3-7的步骤。<br> （9）传输完成以后，客户端会告诉NameNode数据传输完成。</p> 
  <h4><a id="22___556"></a>2.2网络拓扑 - 节点距离计算</h4> 
  <p>在HDFS写数据的过程中，NameNode会选择距离带上传数据最近距离的DataNode接受数据。<br> <font size="4" color="#4169E1">最近距离如何计算</font><br> 节点距离：两个节点到达最近的共同祖先的距离总和。看图更清晰：<br> 假设有数据中心d1机架r1中的节点n1.该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728204453690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h4><a id="23Hadoop272__563"></a>2.3Hadoop2.7.2 副本节点选择</h4> 
  <p>默认三个副本情况下的副本节点选择：<br> 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。<br> 第二个副本和第一个副本位于相同机架，随机节点。<br> 第三个副本位于不同机架，随机节点。</p> 
  <h3><a id="2HDFS_569"></a>2.HDFS读数据流程</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728225310129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZzZWFzdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <font size="4" color="#4169E1">步骤分析:</font><br> （1）客户端通过 Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查找元数据，找到文件块所在的DataNode地址。<br> （2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。<br> （3）DataNode开始传输数据给客户端（从磁盘读取数据输入流，以Packet）为单位来做校验。<br> （4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。<br> （5）然后接着读取 第二个Block。</p> 
  <p><a href="https://blog.csdn.net/fseast/article/details/97809140" rel="nofollow" data-token="32491a2c8d61c905c90b696ef73d8d81">下一篇：（二）HDFS——节点分析及新特性</a></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
