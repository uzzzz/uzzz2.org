<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>机器翻译 Transformer代码笔记 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="机器翻译 Transformer代码笔记" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="(未完) 代码来源Github：kyubyong/transformer/tf1.2_legacy 作者已更新较新版本tensorflow对应的transformer代码，本笔记基于老代码 做笔记使用 代码 介绍 hyperhparams.py 超参数设定 prepro.py 生成字典 data_load.py 格式化数据，生成batch modules.py 网络模型 train.py 训练 eval.py 评估 代码1：hyperparams.py 定义超参数文件 # -*- coding: utf-8 -*- #/usr/bin/python2 &#39;&#39;&#39; June 2017 by kyubyong park. kbpark.linguist@gmail.com. https://www.github.com/kyubyong/transformer &#39;&#39;&#39; class Hyperparams: #超参数 &#39;&#39;&#39;Hyperparameters&#39;&#39;&#39; # data 训练集与测试集 source_train = &#39;corpora/train.tags.de-en.de&#39; target_train = &#39;corpora/train.tags.de-en.en&#39; source_test = &#39;corpora/IWSLT16.TED.tst2014.de-en.de.xml&#39; target_test = &#39;corpora/IWSLT16.TED.tst2014.de-en.en.xml&#39; # training #batch_size调参重点 #mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。 batch_size = 32 # alias = N 在实际机翻训练过程中batch_size一般设置从4000—8000不等，要具体情况具体分析 lr = 0.0001 # learning rate. In paper, learning rate is adjusted to the global step. # 在实际训练中，一般动态设置学习率，从大到小以达到细分精度找到“最优解” logdir = &#39;logdir&#39; # log directory # model maxlen = 10 # alias = T. 单词最大长度，实习训练中忽略此项的限制 # Feel free to increase this if you are ambitious. #min_cnt调参 min_cnt = 20 # words whose occurred less than min_cnt are encoded as &lt;UNK&gt;. #调参重点 hidden_units = 512 # alias = C 隐藏节点 num_blocks = 6 # number of encoder/decoder blocks num_epochs = 20 #迭代20次所有样本 num_heads = 8 #多头注意力机制中的层数H dropout_rate = 0.1 #残差丢弃正则化，根据实际情况可继续增大 sinusoid = False # If True, use sinusoid. If false, positional embedding. 不使用正弦曲线 代码2：prepro.py 生成词汇表 # -*- coding: utf-8 -*- #/usr/bin/python2 &#39;&#39;&#39; June 2017 by kyubyong park. kbpark.linguist@gmail.com. https://www.github.com/kyubyong/transformer &#39;&#39;&#39; from __future__ import print_function #本机python2，使用python3的print()函数 from hyperparams import Hyperparams as hp #超参数 import tensorflow as tf import numpy as np import codecs #使用codecs.open()读写文件，避免编码不统一报错 import os import regex #正则表达式 from collections import Counter #计数器 def make_vocab(fpath, fname): #生成词汇表 &#39;&#39;&#39;Constructs vocabulary. Args: fpath: A string. Input file path. 输入路径，训练集 fname: A string. Output file name. 输出路径，词汇表 Writes vocabulary line by line to `preprocessed/fname` &#39;&#39;&#39; text = codecs.open(fpath, &#39;r&#39;, &#39;utf-8&#39;).read() #用unicode编码方式读取 text = regex.sub(&quot;[^\s\p{Latin}&#39;]&quot;, &quot;&quot;, text) #正则表达式，只保留英文单词 words = text.split() word2cnt = Counter(words) #计数器，输出词典：key=单词，value=个数 if not os.path.exists(&#39;preprocessed&#39;): os.mkdir(&#39;preprocessed&#39;) #输出路径 #使用with语句：不用close()，同时避免异常 #str.format()格式化函数，类似于print(&#39;&#39;，% )中的% with codecs.open(&#39;preprocessed/{}&#39;.format(fname), &#39;w&#39;, &#39;utf-8&#39;) as fout: #先写入四个特殊词 #&lt;PAD&gt;主要用来进行字符补全，编号0 #&lt;UNK&gt;未登录词/低频词，编号1 #&lt;S&gt;句子开始的标识，编号2 # &lt;/S&gt;句子结尾的标识，编号3 fout.write(&quot;{}\t1000000000\n{}\t1000000000\n{}\t1000000000\n{}\t1000000000\n&quot;.format(&quot;&lt;PAD&gt;&quot;, &quot;&lt;UNK&gt;&quot;, &quot;&lt;S&gt;&quot;, &quot;&lt;/S&gt;&quot;)) #collections.Counter.most_common(N)按照频次从大到小排列词典，只显示前N个单词。 for word, cnt in word2cnt.most_common(len(word2cnt)): fout.write(u&quot;{}\t{}\n&quot;.format(word, cnt)) #按照 单词\t频次\n来写入 if __name__ == &#39;__main__&#39;: make_vocab(hp.source_train, &quot;de.vocab.tsv&quot;) make_vocab(hp.target_train, &quot;en.vocab.tsv&quot;) #两个词汇表 print(&quot;Done&quot;) 代码3：data_load.py 格式化数据，生成batch # -*- coding: utf-8 -*- #/usr/bin/python2 &#39;&#39;&#39; June 2017 by kyubyong park. kbpark.linguist@gmail.com. https://www.github.com/kyubyong/transformer &#39;&#39;&#39; from __future__ import print_function from hyperparams import Hyperparams as hp import tensorflow as tf import numpy as np import codecs import regex #词汇表转化为字典格式，并删除频次较低的 def load_de_vocab(): #德语 # splitlines()按行切片(\n,\r,\r\n)；line.split()[0]将遍历好的每一行（一个单词一个频次）列表化，并取第0个元素/单词；如果单词的频次大于我们在hyperhparams.py中设定的参数就保存该单词到一个列表中 vocab = [line.split()[0] for line in codecs.open(&#39;preprocessed/de.vocab.tsv&#39;, &#39;r&#39;, &#39;utf-8&#39;).read().splitlines() if int(line.split()[1])&gt;=hp.min_cnt] word2idx = {word: idx for idx, word in enumerate(vocab)} #转换为字典的形式进行保存单词，并给每个单词进行编号 idx2word = {idx: word for idx, word in enumerate(vocab)} return word2idx, idx2word def load_en_vocab(): #英语 vocab = [line.split()[0] for line in codecs.open(&#39;preprocessed/en.vocab.tsv&#39;, &#39;r&#39;, &#39;utf-8&#39;).read().splitlines() if int(line.split()[1])&gt;=hp.min_cnt] word2idx = {word: idx for idx, word in enumerate(vocab)} idx2word = {idx: word for idx, word in enumerate(vocab)} return word2idx, idx2word &#39;&#39;&#39; 举例： word2idx ={&#39;&lt;PAD&gt;&#39;: 0, &#39;&lt;UNK&gt;&#39;: 1, &#39;&lt;STR&gt;&#39;: 2, &#39;&lt;EOS&gt;&#39;: 3, &#39;有&#39;: 4, &#39;的&#39;: 5, &#39;`&#39;: 6, &#39;-&#39;: 7, &#39;卦&#39;: 8, &#39;八&#39;: 9, ..., &#39;爬&#39;: 1642, &#39;U&#39;: 1643} idx2word={{0: &#39;&lt;PAD&gt;&#39;, 1: &#39;&lt;UNK&gt;&#39;, 2: &#39;&lt;STR&gt;&#39;, 3: &#39;&lt;EOS&gt;&#39;, 4: &#39;有&#39;, 5: &#39;的&#39;, 6: &#39;`&#39;, 7: &#39;-&#39;, 8: &#39;卦&#39;, 9: &#39;八&#39;, ..., 1642: &#39;爬&#39;, 1643: &#39;U&#39;}} &#39;&#39;&#39; #数据处理 def create_data(source_sents, target_sents): #source_sents存放源语言句子的列表, target_sents目标语言句子 de2idx, idx2de = load_de_vocab() en2idx, idx2en = load_en_vocab() # Index 索引 x_list, y_list, Sources, Targets = [], [], [], [] for source_sent, target_sent in zip(source_sents, target_sents): #使用zip()函数同时遍历两个句子列表 #x,y 一个新句子 x = [de2idx.get(word, 1) for word in (source_sent + u&quot; &lt;/S&gt;&quot;).split()] # 1: OOV, &lt;/S&gt;: End of Text y = [en2idx.get(word, 1) for word in (target_sent + u&quot; &lt;/S&gt;&quot;).split()] #给每一个句子的末尾加上&lt;/s&gt;终止符，并遍历句子中的每一个单词，将已经存在于word2idx中的那个单词对应的ID添加到新的列表中，如果这个单词不存在于word2idx中，那么就返回 ID‘1’到新列表中组成一个新的‘ID句子’(其中1代表&lt;UNK&gt;) if max(len(x), len(y)) &lt;= hp.maxlen: #我们在hyperhparams.py中设置的最大句子长度 x_list.append(np.array(x)) #源语言ID句子 y_list.append(np.array(y)) #目标语言ID句子 Sources.append(source_sent) #源语言句子 Targets.append(target_sent) #目标语言句子 #超过长度阈值的丢弃 # Pad 填充 对应site特殊词中的编号0：&lt;PAD&gt; X = np.zeros([len(x_list), hp.maxlen], np.int32) #二维0矩阵：句子个数*最大句长 Y = np.zeros([len(y_list), hp.maxlen], np.int32) for i, (x, y) in enumerate(zip(x_list, y_list)): #保证每个ID句子的长度/元素个数都是相同的 X[i] = np.lib.pad(x, [0, hp.maxlen-len(x)], &#39;constant&#39;, constant_values=(0, 0)) #对每一个ID句子做填充，左侧填充0个0，右侧填充hp.maxlen-len(x)个0，并且0也是四个特殊词中的一个：&lt;PAD&gt;编号0 Y[i] = np.lib.pad(y, [0, hp.maxlen-len(y)], &#39;constant&#39;, constant_values=(0, 0)) return X, Y, Sources, Targets #X和Y的shape为[len(x_list), hp.maxlen]，Sources, Targets的shape为[1, len(x_list)] #加载训练集，对训练集做数据处理，返回定长ID句子 def load_train_data(): de_sents = [regex.sub(&quot;[^\s\p{Latin}&#39;]&quot;, &quot;&quot;, line) for line in codecs.open(hp.source_train, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\n&quot;) if line and line[0] != &quot;&lt;&quot;] #不加载特殊词 en_sents = [regex.sub(&quot;[^\s\p{Latin}&#39;]&quot;, &quot;&quot;, line) for line in codecs.open(hp.target_train, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\n&quot;) if line and line[0] != &quot;&lt;&quot;] X, Y, Sources, Targets = create_data(de_sents, en_sents) return X, Y #加载测试集，对测试集做数据处理，返回定长ID句子 def load_test_data(): def _refine(line): line = regex.sub(&quot;&lt;[^&gt;]+&gt;&quot;, &quot;&quot;, line) #删除所有非空的&lt; &gt;项 line = regex.sub(&quot;[^\s\p{Latin}&#39;]&quot;, &quot;&quot;, line) return line.strip() #删除字符串首尾的指定字符（默认为空格） #读取源语言与目标语言文本，切片并处理；将句子开头前4个字符为&lt;seg的保存到新列表中 de_sents = [_refine(line) for line in codecs.open(hp.source_test, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\n&quot;) if line and line[:4] == &quot;&lt;seg&quot;] en_sents = [_refine(line) for line in codecs.open(hp.target_test, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\n&quot;) if line and line[:4] == &quot;&lt;seg&quot;] X, Y, Sources, Targets = create_data(de_sents, en_sents) #数据处理为定长ID句子 return X, Sources, Targets # (1064, 150) 测试集不需要处理目标语言为ID句子 #生成batch数据，每次得到一个batch def get_batch_data(): # Load data X, Y = load_train_data() #训练集定长ID句子 # calc total batch count num_batch = len(X) // hp.batch_size #需要几个batch来表示总数据，len(X)表示行数/句子个数 # Convert to tensor #将python的数据类型转换成TensorFlow可用的tensor数据类型 X = tf.convert_to_tensor(X, tf.int32) Y = tf.convert_to_tensor(Y, tf.int32) # Create Queues #创建文件名队列 #从输入中每次取一个切片返回到一个输入队列里，该队列作为之后tf.train.shuffle_batch的一个参数，用以生成!一个!batch的数据。 input_queues = tf.train.slice_input_producer([X, Y]) # create batch queues #队头出队，队尾补充数据入队，打乱顺序；num_threads多线程入队，batch_size每次从队列中出队的数据数量, #capacity队列中最大元素数量，min_after_dequeue队列中最少存在的元素数量，allow_smaller_final_batch队列中最后小于batch_size的样本不进行出队 x, y = tf.train.shuffle_batch(input_queues, num_threads=8, batch_size=hp.batch_size, capacity=hp.batch_size*64, min_after_dequeue=hp.batch_size*32, allow_smaller_final_batch=False) #不过现在好像都改用tf.data下的函数进行数据处理了 return x, y, num_batch # shape分别为(N, T), (N, T)；N为batch_size的大小，T为最大句子长度maxlen；一个batch" />
<meta property="og:description" content="(未完) 代码来源Github：kyubyong/transformer/tf1.2_legacy 作者已更新较新版本tensorflow对应的transformer代码，本笔记基于老代码 做笔记使用 代码 介绍 hyperhparams.py 超参数设定 prepro.py 生成字典 data_load.py 格式化数据，生成batch modules.py 网络模型 train.py 训练 eval.py 评估 代码1：hyperparams.py 定义超参数文件 # -*- coding: utf-8 -*- #/usr/bin/python2 &#39;&#39;&#39; June 2017 by kyubyong park. kbpark.linguist@gmail.com. https://www.github.com/kyubyong/transformer &#39;&#39;&#39; class Hyperparams: #超参数 &#39;&#39;&#39;Hyperparameters&#39;&#39;&#39; # data 训练集与测试集 source_train = &#39;corpora/train.tags.de-en.de&#39; target_train = &#39;corpora/train.tags.de-en.en&#39; source_test = &#39;corpora/IWSLT16.TED.tst2014.de-en.de.xml&#39; target_test = &#39;corpora/IWSLT16.TED.tst2014.de-en.en.xml&#39; # training #batch_size调参重点 #mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。 batch_size = 32 # alias = N 在实际机翻训练过程中batch_size一般设置从4000—8000不等，要具体情况具体分析 lr = 0.0001 # learning rate. In paper, learning rate is adjusted to the global step. # 在实际训练中，一般动态设置学习率，从大到小以达到细分精度找到“最优解” logdir = &#39;logdir&#39; # log directory # model maxlen = 10 # alias = T. 单词最大长度，实习训练中忽略此项的限制 # Feel free to increase this if you are ambitious. #min_cnt调参 min_cnt = 20 # words whose occurred less than min_cnt are encoded as &lt;UNK&gt;. #调参重点 hidden_units = 512 # alias = C 隐藏节点 num_blocks = 6 # number of encoder/decoder blocks num_epochs = 20 #迭代20次所有样本 num_heads = 8 #多头注意力机制中的层数H dropout_rate = 0.1 #残差丢弃正则化，根据实际情况可继续增大 sinusoid = False # If True, use sinusoid. If false, positional embedding. 不使用正弦曲线 代码2：prepro.py 生成词汇表 # -*- coding: utf-8 -*- #/usr/bin/python2 &#39;&#39;&#39; June 2017 by kyubyong park. kbpark.linguist@gmail.com. https://www.github.com/kyubyong/transformer &#39;&#39;&#39; from __future__ import print_function #本机python2，使用python3的print()函数 from hyperparams import Hyperparams as hp #超参数 import tensorflow as tf import numpy as np import codecs #使用codecs.open()读写文件，避免编码不统一报错 import os import regex #正则表达式 from collections import Counter #计数器 def make_vocab(fpath, fname): #生成词汇表 &#39;&#39;&#39;Constructs vocabulary. Args: fpath: A string. Input file path. 输入路径，训练集 fname: A string. Output file name. 输出路径，词汇表 Writes vocabulary line by line to `preprocessed/fname` &#39;&#39;&#39; text = codecs.open(fpath, &#39;r&#39;, &#39;utf-8&#39;).read() #用unicode编码方式读取 text = regex.sub(&quot;[^\s\p{Latin}&#39;]&quot;, &quot;&quot;, text) #正则表达式，只保留英文单词 words = text.split() word2cnt = Counter(words) #计数器，输出词典：key=单词，value=个数 if not os.path.exists(&#39;preprocessed&#39;): os.mkdir(&#39;preprocessed&#39;) #输出路径 #使用with语句：不用close()，同时避免异常 #str.format()格式化函数，类似于print(&#39;&#39;，% )中的% with codecs.open(&#39;preprocessed/{}&#39;.format(fname), &#39;w&#39;, &#39;utf-8&#39;) as fout: #先写入四个特殊词 #&lt;PAD&gt;主要用来进行字符补全，编号0 #&lt;UNK&gt;未登录词/低频词，编号1 #&lt;S&gt;句子开始的标识，编号2 # &lt;/S&gt;句子结尾的标识，编号3 fout.write(&quot;{}\t1000000000\n{}\t1000000000\n{}\t1000000000\n{}\t1000000000\n&quot;.format(&quot;&lt;PAD&gt;&quot;, &quot;&lt;UNK&gt;&quot;, &quot;&lt;S&gt;&quot;, &quot;&lt;/S&gt;&quot;)) #collections.Counter.most_common(N)按照频次从大到小排列词典，只显示前N个单词。 for word, cnt in word2cnt.most_common(len(word2cnt)): fout.write(u&quot;{}\t{}\n&quot;.format(word, cnt)) #按照 单词\t频次\n来写入 if __name__ == &#39;__main__&#39;: make_vocab(hp.source_train, &quot;de.vocab.tsv&quot;) make_vocab(hp.target_train, &quot;en.vocab.tsv&quot;) #两个词汇表 print(&quot;Done&quot;) 代码3：data_load.py 格式化数据，生成batch # -*- coding: utf-8 -*- #/usr/bin/python2 &#39;&#39;&#39; June 2017 by kyubyong park. kbpark.linguist@gmail.com. https://www.github.com/kyubyong/transformer &#39;&#39;&#39; from __future__ import print_function from hyperparams import Hyperparams as hp import tensorflow as tf import numpy as np import codecs import regex #词汇表转化为字典格式，并删除频次较低的 def load_de_vocab(): #德语 # splitlines()按行切片(\n,\r,\r\n)；line.split()[0]将遍历好的每一行（一个单词一个频次）列表化，并取第0个元素/单词；如果单词的频次大于我们在hyperhparams.py中设定的参数就保存该单词到一个列表中 vocab = [line.split()[0] for line in codecs.open(&#39;preprocessed/de.vocab.tsv&#39;, &#39;r&#39;, &#39;utf-8&#39;).read().splitlines() if int(line.split()[1])&gt;=hp.min_cnt] word2idx = {word: idx for idx, word in enumerate(vocab)} #转换为字典的形式进行保存单词，并给每个单词进行编号 idx2word = {idx: word for idx, word in enumerate(vocab)} return word2idx, idx2word def load_en_vocab(): #英语 vocab = [line.split()[0] for line in codecs.open(&#39;preprocessed/en.vocab.tsv&#39;, &#39;r&#39;, &#39;utf-8&#39;).read().splitlines() if int(line.split()[1])&gt;=hp.min_cnt] word2idx = {word: idx for idx, word in enumerate(vocab)} idx2word = {idx: word for idx, word in enumerate(vocab)} return word2idx, idx2word &#39;&#39;&#39; 举例： word2idx ={&#39;&lt;PAD&gt;&#39;: 0, &#39;&lt;UNK&gt;&#39;: 1, &#39;&lt;STR&gt;&#39;: 2, &#39;&lt;EOS&gt;&#39;: 3, &#39;有&#39;: 4, &#39;的&#39;: 5, &#39;`&#39;: 6, &#39;-&#39;: 7, &#39;卦&#39;: 8, &#39;八&#39;: 9, ..., &#39;爬&#39;: 1642, &#39;U&#39;: 1643} idx2word={{0: &#39;&lt;PAD&gt;&#39;, 1: &#39;&lt;UNK&gt;&#39;, 2: &#39;&lt;STR&gt;&#39;, 3: &#39;&lt;EOS&gt;&#39;, 4: &#39;有&#39;, 5: &#39;的&#39;, 6: &#39;`&#39;, 7: &#39;-&#39;, 8: &#39;卦&#39;, 9: &#39;八&#39;, ..., 1642: &#39;爬&#39;, 1643: &#39;U&#39;}} &#39;&#39;&#39; #数据处理 def create_data(source_sents, target_sents): #source_sents存放源语言句子的列表, target_sents目标语言句子 de2idx, idx2de = load_de_vocab() en2idx, idx2en = load_en_vocab() # Index 索引 x_list, y_list, Sources, Targets = [], [], [], [] for source_sent, target_sent in zip(source_sents, target_sents): #使用zip()函数同时遍历两个句子列表 #x,y 一个新句子 x = [de2idx.get(word, 1) for word in (source_sent + u&quot; &lt;/S&gt;&quot;).split()] # 1: OOV, &lt;/S&gt;: End of Text y = [en2idx.get(word, 1) for word in (target_sent + u&quot; &lt;/S&gt;&quot;).split()] #给每一个句子的末尾加上&lt;/s&gt;终止符，并遍历句子中的每一个单词，将已经存在于word2idx中的那个单词对应的ID添加到新的列表中，如果这个单词不存在于word2idx中，那么就返回 ID‘1’到新列表中组成一个新的‘ID句子’(其中1代表&lt;UNK&gt;) if max(len(x), len(y)) &lt;= hp.maxlen: #我们在hyperhparams.py中设置的最大句子长度 x_list.append(np.array(x)) #源语言ID句子 y_list.append(np.array(y)) #目标语言ID句子 Sources.append(source_sent) #源语言句子 Targets.append(target_sent) #目标语言句子 #超过长度阈值的丢弃 # Pad 填充 对应site特殊词中的编号0：&lt;PAD&gt; X = np.zeros([len(x_list), hp.maxlen], np.int32) #二维0矩阵：句子个数*最大句长 Y = np.zeros([len(y_list), hp.maxlen], np.int32) for i, (x, y) in enumerate(zip(x_list, y_list)): #保证每个ID句子的长度/元素个数都是相同的 X[i] = np.lib.pad(x, [0, hp.maxlen-len(x)], &#39;constant&#39;, constant_values=(0, 0)) #对每一个ID句子做填充，左侧填充0个0，右侧填充hp.maxlen-len(x)个0，并且0也是四个特殊词中的一个：&lt;PAD&gt;编号0 Y[i] = np.lib.pad(y, [0, hp.maxlen-len(y)], &#39;constant&#39;, constant_values=(0, 0)) return X, Y, Sources, Targets #X和Y的shape为[len(x_list), hp.maxlen]，Sources, Targets的shape为[1, len(x_list)] #加载训练集，对训练集做数据处理，返回定长ID句子 def load_train_data(): de_sents = [regex.sub(&quot;[^\s\p{Latin}&#39;]&quot;, &quot;&quot;, line) for line in codecs.open(hp.source_train, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\n&quot;) if line and line[0] != &quot;&lt;&quot;] #不加载特殊词 en_sents = [regex.sub(&quot;[^\s\p{Latin}&#39;]&quot;, &quot;&quot;, line) for line in codecs.open(hp.target_train, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\n&quot;) if line and line[0] != &quot;&lt;&quot;] X, Y, Sources, Targets = create_data(de_sents, en_sents) return X, Y #加载测试集，对测试集做数据处理，返回定长ID句子 def load_test_data(): def _refine(line): line = regex.sub(&quot;&lt;[^&gt;]+&gt;&quot;, &quot;&quot;, line) #删除所有非空的&lt; &gt;项 line = regex.sub(&quot;[^\s\p{Latin}&#39;]&quot;, &quot;&quot;, line) return line.strip() #删除字符串首尾的指定字符（默认为空格） #读取源语言与目标语言文本，切片并处理；将句子开头前4个字符为&lt;seg的保存到新列表中 de_sents = [_refine(line) for line in codecs.open(hp.source_test, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\n&quot;) if line and line[:4] == &quot;&lt;seg&quot;] en_sents = [_refine(line) for line in codecs.open(hp.target_test, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\n&quot;) if line and line[:4] == &quot;&lt;seg&quot;] X, Y, Sources, Targets = create_data(de_sents, en_sents) #数据处理为定长ID句子 return X, Sources, Targets # (1064, 150) 测试集不需要处理目标语言为ID句子 #生成batch数据，每次得到一个batch def get_batch_data(): # Load data X, Y = load_train_data() #训练集定长ID句子 # calc total batch count num_batch = len(X) // hp.batch_size #需要几个batch来表示总数据，len(X)表示行数/句子个数 # Convert to tensor #将python的数据类型转换成TensorFlow可用的tensor数据类型 X = tf.convert_to_tensor(X, tf.int32) Y = tf.convert_to_tensor(Y, tf.int32) # Create Queues #创建文件名队列 #从输入中每次取一个切片返回到一个输入队列里，该队列作为之后tf.train.shuffle_batch的一个参数，用以生成!一个!batch的数据。 input_queues = tf.train.slice_input_producer([X, Y]) # create batch queues #队头出队，队尾补充数据入队，打乱顺序；num_threads多线程入队，batch_size每次从队列中出队的数据数量, #capacity队列中最大元素数量，min_after_dequeue队列中最少存在的元素数量，allow_smaller_final_batch队列中最后小于batch_size的样本不进行出队 x, y = tf.train.shuffle_batch(input_queues, num_threads=8, batch_size=hp.batch_size, capacity=hp.batch_size*64, min_after_dequeue=hp.batch_size*32, allow_smaller_final_batch=False) #不过现在好像都改用tf.data下的函数进行数据处理了 return x, y, num_batch # shape分别为(N, T), (N, T)；N为batch_size的大小，T为最大句子长度maxlen；一个batch" />
<link rel="canonical" href="https://uzzz.org/2019/08/05/792906.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/05/792906.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-05T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"(未完) 代码来源Github：kyubyong/transformer/tf1.2_legacy 作者已更新较新版本tensorflow对应的transformer代码，本笔记基于老代码 做笔记使用 代码 介绍 hyperhparams.py 超参数设定 prepro.py 生成字典 data_load.py 格式化数据，生成batch modules.py 网络模型 train.py 训练 eval.py 评估 代码1：hyperparams.py 定义超参数文件 # -*- coding: utf-8 -*- #/usr/bin/python2 &#39;&#39;&#39; June 2017 by kyubyong park. kbpark.linguist@gmail.com. https://www.github.com/kyubyong/transformer &#39;&#39;&#39; class Hyperparams: #超参数 &#39;&#39;&#39;Hyperparameters&#39;&#39;&#39; # data 训练集与测试集 source_train = &#39;corpora/train.tags.de-en.de&#39; target_train = &#39;corpora/train.tags.de-en.en&#39; source_test = &#39;corpora/IWSLT16.TED.tst2014.de-en.de.xml&#39; target_test = &#39;corpora/IWSLT16.TED.tst2014.de-en.en.xml&#39; # training #batch_size调参重点 #mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。 batch_size = 32 # alias = N 在实际机翻训练过程中batch_size一般设置从4000—8000不等，要具体情况具体分析 lr = 0.0001 # learning rate. In paper, learning rate is adjusted to the global step. # 在实际训练中，一般动态设置学习率，从大到小以达到细分精度找到“最优解” logdir = &#39;logdir&#39; # log directory # model maxlen = 10 # alias = T. 单词最大长度，实习训练中忽略此项的限制 # Feel free to increase this if you are ambitious. #min_cnt调参 min_cnt = 20 # words whose occurred less than min_cnt are encoded as &lt;UNK&gt;. #调参重点 hidden_units = 512 # alias = C 隐藏节点 num_blocks = 6 # number of encoder/decoder blocks num_epochs = 20 #迭代20次所有样本 num_heads = 8 #多头注意力机制中的层数H dropout_rate = 0.1 #残差丢弃正则化，根据实际情况可继续增大 sinusoid = False # If True, use sinusoid. If false, positional embedding. 不使用正弦曲线 代码2：prepro.py 生成词汇表 # -*- coding: utf-8 -*- #/usr/bin/python2 &#39;&#39;&#39; June 2017 by kyubyong park. kbpark.linguist@gmail.com. https://www.github.com/kyubyong/transformer &#39;&#39;&#39; from __future__ import print_function #本机python2，使用python3的print()函数 from hyperparams import Hyperparams as hp #超参数 import tensorflow as tf import numpy as np import codecs #使用codecs.open()读写文件，避免编码不统一报错 import os import regex #正则表达式 from collections import Counter #计数器 def make_vocab(fpath, fname): #生成词汇表 &#39;&#39;&#39;Constructs vocabulary. Args: fpath: A string. Input file path. 输入路径，训练集 fname: A string. Output file name. 输出路径，词汇表 Writes vocabulary line by line to `preprocessed/fname` &#39;&#39;&#39; text = codecs.open(fpath, &#39;r&#39;, &#39;utf-8&#39;).read() #用unicode编码方式读取 text = regex.sub(&quot;[^\\s\\p{Latin}&#39;]&quot;, &quot;&quot;, text) #正则表达式，只保留英文单词 words = text.split() word2cnt = Counter(words) #计数器，输出词典：key=单词，value=个数 if not os.path.exists(&#39;preprocessed&#39;): os.mkdir(&#39;preprocessed&#39;) #输出路径 #使用with语句：不用close()，同时避免异常 #str.format()格式化函数，类似于print(&#39;&#39;，% )中的% with codecs.open(&#39;preprocessed/{}&#39;.format(fname), &#39;w&#39;, &#39;utf-8&#39;) as fout: #先写入四个特殊词 #&lt;PAD&gt;主要用来进行字符补全，编号0 #&lt;UNK&gt;未登录词/低频词，编号1 #&lt;S&gt;句子开始的标识，编号2 # &lt;/S&gt;句子结尾的标识，编号3 fout.write(&quot;{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n&quot;.format(&quot;&lt;PAD&gt;&quot;, &quot;&lt;UNK&gt;&quot;, &quot;&lt;S&gt;&quot;, &quot;&lt;/S&gt;&quot;)) #collections.Counter.most_common(N)按照频次从大到小排列词典，只显示前N个单词。 for word, cnt in word2cnt.most_common(len(word2cnt)): fout.write(u&quot;{}\\t{}\\n&quot;.format(word, cnt)) #按照 单词\\t频次\\n来写入 if __name__ == &#39;__main__&#39;: make_vocab(hp.source_train, &quot;de.vocab.tsv&quot;) make_vocab(hp.target_train, &quot;en.vocab.tsv&quot;) #两个词汇表 print(&quot;Done&quot;) 代码3：data_load.py 格式化数据，生成batch # -*- coding: utf-8 -*- #/usr/bin/python2 &#39;&#39;&#39; June 2017 by kyubyong park. kbpark.linguist@gmail.com. https://www.github.com/kyubyong/transformer &#39;&#39;&#39; from __future__ import print_function from hyperparams import Hyperparams as hp import tensorflow as tf import numpy as np import codecs import regex #词汇表转化为字典格式，并删除频次较低的 def load_de_vocab(): #德语 # splitlines()按行切片(\\n,\\r,\\r\\n)；line.split()[0]将遍历好的每一行（一个单词一个频次）列表化，并取第0个元素/单词；如果单词的频次大于我们在hyperhparams.py中设定的参数就保存该单词到一个列表中 vocab = [line.split()[0] for line in codecs.open(&#39;preprocessed/de.vocab.tsv&#39;, &#39;r&#39;, &#39;utf-8&#39;).read().splitlines() if int(line.split()[1])&gt;=hp.min_cnt] word2idx = {word: idx for idx, word in enumerate(vocab)} #转换为字典的形式进行保存单词，并给每个单词进行编号 idx2word = {idx: word for idx, word in enumerate(vocab)} return word2idx, idx2word def load_en_vocab(): #英语 vocab = [line.split()[0] for line in codecs.open(&#39;preprocessed/en.vocab.tsv&#39;, &#39;r&#39;, &#39;utf-8&#39;).read().splitlines() if int(line.split()[1])&gt;=hp.min_cnt] word2idx = {word: idx for idx, word in enumerate(vocab)} idx2word = {idx: word for idx, word in enumerate(vocab)} return word2idx, idx2word &#39;&#39;&#39; 举例： word2idx ={&#39;&lt;PAD&gt;&#39;: 0, &#39;&lt;UNK&gt;&#39;: 1, &#39;&lt;STR&gt;&#39;: 2, &#39;&lt;EOS&gt;&#39;: 3, &#39;有&#39;: 4, &#39;的&#39;: 5, &#39;`&#39;: 6, &#39;-&#39;: 7, &#39;卦&#39;: 8, &#39;八&#39;: 9, ..., &#39;爬&#39;: 1642, &#39;U&#39;: 1643} idx2word={{0: &#39;&lt;PAD&gt;&#39;, 1: &#39;&lt;UNK&gt;&#39;, 2: &#39;&lt;STR&gt;&#39;, 3: &#39;&lt;EOS&gt;&#39;, 4: &#39;有&#39;, 5: &#39;的&#39;, 6: &#39;`&#39;, 7: &#39;-&#39;, 8: &#39;卦&#39;, 9: &#39;八&#39;, ..., 1642: &#39;爬&#39;, 1643: &#39;U&#39;}} &#39;&#39;&#39; #数据处理 def create_data(source_sents, target_sents): #source_sents存放源语言句子的列表, target_sents目标语言句子 de2idx, idx2de = load_de_vocab() en2idx, idx2en = load_en_vocab() # Index 索引 x_list, y_list, Sources, Targets = [], [], [], [] for source_sent, target_sent in zip(source_sents, target_sents): #使用zip()函数同时遍历两个句子列表 #x,y 一个新句子 x = [de2idx.get(word, 1) for word in (source_sent + u&quot; &lt;/S&gt;&quot;).split()] # 1: OOV, &lt;/S&gt;: End of Text y = [en2idx.get(word, 1) for word in (target_sent + u&quot; &lt;/S&gt;&quot;).split()] #给每一个句子的末尾加上&lt;/s&gt;终止符，并遍历句子中的每一个单词，将已经存在于word2idx中的那个单词对应的ID添加到新的列表中，如果这个单词不存在于word2idx中，那么就返回 ID‘1’到新列表中组成一个新的‘ID句子’(其中1代表&lt;UNK&gt;) if max(len(x), len(y)) &lt;= hp.maxlen: #我们在hyperhparams.py中设置的最大句子长度 x_list.append(np.array(x)) #源语言ID句子 y_list.append(np.array(y)) #目标语言ID句子 Sources.append(source_sent) #源语言句子 Targets.append(target_sent) #目标语言句子 #超过长度阈值的丢弃 # Pad 填充 对应site特殊词中的编号0：&lt;PAD&gt; X = np.zeros([len(x_list), hp.maxlen], np.int32) #二维0矩阵：句子个数*最大句长 Y = np.zeros([len(y_list), hp.maxlen], np.int32) for i, (x, y) in enumerate(zip(x_list, y_list)): #保证每个ID句子的长度/元素个数都是相同的 X[i] = np.lib.pad(x, [0, hp.maxlen-len(x)], &#39;constant&#39;, constant_values=(0, 0)) #对每一个ID句子做填充，左侧填充0个0，右侧填充hp.maxlen-len(x)个0，并且0也是四个特殊词中的一个：&lt;PAD&gt;编号0 Y[i] = np.lib.pad(y, [0, hp.maxlen-len(y)], &#39;constant&#39;, constant_values=(0, 0)) return X, Y, Sources, Targets #X和Y的shape为[len(x_list), hp.maxlen]，Sources, Targets的shape为[1, len(x_list)] #加载训练集，对训练集做数据处理，返回定长ID句子 def load_train_data(): de_sents = [regex.sub(&quot;[^\\s\\p{Latin}&#39;]&quot;, &quot;&quot;, line) for line in codecs.open(hp.source_train, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\\n&quot;) if line and line[0] != &quot;&lt;&quot;] #不加载特殊词 en_sents = [regex.sub(&quot;[^\\s\\p{Latin}&#39;]&quot;, &quot;&quot;, line) for line in codecs.open(hp.target_train, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\\n&quot;) if line and line[0] != &quot;&lt;&quot;] X, Y, Sources, Targets = create_data(de_sents, en_sents) return X, Y #加载测试集，对测试集做数据处理，返回定长ID句子 def load_test_data(): def _refine(line): line = regex.sub(&quot;&lt;[^&gt;]+&gt;&quot;, &quot;&quot;, line) #删除所有非空的&lt; &gt;项 line = regex.sub(&quot;[^\\s\\p{Latin}&#39;]&quot;, &quot;&quot;, line) return line.strip() #删除字符串首尾的指定字符（默认为空格） #读取源语言与目标语言文本，切片并处理；将句子开头前4个字符为&lt;seg的保存到新列表中 de_sents = [_refine(line) for line in codecs.open(hp.source_test, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\\n&quot;) if line and line[:4] == &quot;&lt;seg&quot;] en_sents = [_refine(line) for line in codecs.open(hp.target_test, &#39;r&#39;, &#39;utf-8&#39;).read().split(&quot;\\n&quot;) if line and line[:4] == &quot;&lt;seg&quot;] X, Y, Sources, Targets = create_data(de_sents, en_sents) #数据处理为定长ID句子 return X, Sources, Targets # (1064, 150) 测试集不需要处理目标语言为ID句子 #生成batch数据，每次得到一个batch def get_batch_data(): # Load data X, Y = load_train_data() #训练集定长ID句子 # calc total batch count num_batch = len(X) // hp.batch_size #需要几个batch来表示总数据，len(X)表示行数/句子个数 # Convert to tensor #将python的数据类型转换成TensorFlow可用的tensor数据类型 X = tf.convert_to_tensor(X, tf.int32) Y = tf.convert_to_tensor(Y, tf.int32) # Create Queues #创建文件名队列 #从输入中每次取一个切片返回到一个输入队列里，该队列作为之后tf.train.shuffle_batch的一个参数，用以生成!一个!batch的数据。 input_queues = tf.train.slice_input_producer([X, Y]) # create batch queues #队头出队，队尾补充数据入队，打乱顺序；num_threads多线程入队，batch_size每次从队列中出队的数据数量, #capacity队列中最大元素数量，min_after_dequeue队列中最少存在的元素数量，allow_smaller_final_batch队列中最后小于batch_size的样本不进行出队 x, y = tf.train.shuffle_batch(input_queues, num_threads=8, batch_size=hp.batch_size, capacity=hp.batch_size*64, min_after_dequeue=hp.batch_size*32, allow_smaller_final_batch=False) #不过现在好像都改用tf.data下的函数进行数据处理了 return x, y, num_batch # shape分别为(N, T), (N, T)；N为batch_size的大小，T为最大句子长度maxlen；一个batch","@type":"BlogPosting","url":"https://uzzz.org/2019/08/05/792906.html","headline":"机器翻译 Transformer代码笔记","dateModified":"2019-08-05T00:00:00+08:00","datePublished":"2019-08-05T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/05/792906.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>机器翻译 Transformer代码笔记</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-tomorrow-night-eighties"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>(未完)<br> 代码来源<a href="https://github.com/Kyubyong/transformer/tree/master/tf1.2_legacy" rel="nofollow" data-token="7bb8c0d88c06d9a6cbf8f2f101bb1817">Github：kyubyong/transformer/tf1.2_legacy</a><br> 作者已更新较新版本tensorflow对应的transformer代码，本笔记基于老代码<br> 做笔记使用</p> 
  <table> 
   <thead> 
    <tr> 
     <th>代码</th> 
     <th>介绍</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td><a href="http://hyperhparams.py" rel="nofollow" data-token="60c4890a676b77af27e383d68439791e">hyperhparams.py</a></td> 
     <td>超参数设定</td> 
    </tr> 
    <tr> 
     <td><a href="http://prepro.py" rel="nofollow" data-token="1d44f67b244eb5faab13eec730ec5412">prepro.py</a></td> 
     <td>生成字典</td> 
    </tr> 
    <tr> 
     <td>data_load.py</td> 
     <td>格式化数据，生成batch</td> 
    </tr> 
    <tr> 
     <td><a href="http://modules.py" rel="nofollow" data-token="2dcc520ee976a21d19f0c992528b8791">modules.py</a></td> 
     <td>网络模型</td> 
    </tr> 
    <tr> 
     <td><a href="http://train.py" rel="nofollow" data-token="a7a1ebea00a8a9e30d33bd385964a60f">train.py</a></td> 
     <td>训练</td> 
    </tr> 
    <tr> 
     <td><a href="http://eval.py" rel="nofollow" data-token="cfe3415ae282d1b74f6f871abcd90089">eval.py</a></td> 
     <td>评估</td> 
    </tr> 
   </tbody> 
  </table>
  <p>代码1：<a href="http://hyperparams.py" rel="nofollow" data-token="13fdd0e5544678235e2f0b6904793628">hyperparams.py</a> 定义超参数文件</p> 
  <pre><code># -*- coding: utf-8 -*-
#/usr/bin/python2
'''
June 2017 by kyubyong park. 
kbpark.linguist@gmail.com.
https://www.github.com/kyubyong/transformer
'''
class Hyperparams: #超参数
    '''Hyperparameters'''
    # data 训练集与测试集
    source_train = 'corpora/train.tags.de-en.de'
    target_train = 'corpora/train.tags.de-en.en'
    source_test = 'corpora/IWSLT16.TED.tst2014.de-en.de.xml'
    target_test = 'corpora/IWSLT16.TED.tst2014.de-en.en.xml'
    
    # training
    #batch_size调参重点
    #mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。
    batch_size = 32 # alias = N 在实际机翻训练过程中batch_size一般设置从4000—8000不等，要具体情况具体分析
    lr = 0.0001 # learning rate. In paper, learning rate is adjusted to the global step.
    # 在实际训练中，一般动态设置学习率，从大到小以达到细分精度找到“最优解”
    logdir = 'logdir' # log directory
    
    # model
    maxlen = 10 # alias = T. 单词最大长度，实习训练中忽略此项的限制
                # Feel free to increase this if you are ambitious.
    #min_cnt调参
    min_cnt = 20 # words whose occurred less than min_cnt are encoded as &lt;UNK&gt;.
    #调参重点
    hidden_units = 512 # alias = C 隐藏节点
    num_blocks = 6 # number of encoder/decoder blocks
    num_epochs = 20 #迭代20次所有样本
    num_heads = 8 #多头注意力机制中的层数H
    dropout_rate = 0.1 #残差丢弃正则化，根据实际情况可继续增大
    sinusoid = False # If True, use sinusoid. If false, positional embedding. 不使用正弦曲线
</code></pre> 
  <p>代码2：<a href="http://prepro.py" rel="nofollow" data-token="1d44f67b244eb5faab13eec730ec5412">prepro.py</a> 生成词汇表</p> 
  <pre><code># -*- coding: utf-8 -*-
#/usr/bin/python2
'''
June 2017 by kyubyong park. 
kbpark.linguist@gmail.com.
https://www.github.com/kyubyong/transformer
'''
from __future__ import print_function #本机python2，使用python3的print()函数
from hyperparams import Hyperparams as hp #超参数
import tensorflow as tf
import numpy as np
import codecs #使用codecs.open()读写文件，避免编码不统一报错
import os
import regex #正则表达式
from collections import Counter #计数器

def make_vocab(fpath, fname): #生成词汇表
    '''Constructs vocabulary.
    
    Args:
      fpath: A string. Input file path. 输入路径，训练集
      fname: A string. Output file name. 输出路径，词汇表
    
    Writes vocabulary line by line to `preprocessed/fname`
    '''  
    text = codecs.open(fpath, 'r', 'utf-8').read() #用unicode编码方式读取
    text = regex.sub("[^\s\p{Latin}']", "", text) #正则表达式，只保留英文单词
    words = text.split()
    word2cnt = Counter(words) #计数器，输出词典：key=单词，value=个数
    if not os.path.exists('preprocessed'): os.mkdir('preprocessed') #输出路径
    #使用with语句：不用close()，同时避免异常
    #str.format()格式化函数，类似于print(''，% )中的%
    with codecs.open('preprocessed/{}'.format(fname), 'w', 'utf-8') as fout:
        #先写入四个特殊词
        #&lt;PAD&gt;主要用来进行字符补全，编号0
        #&lt;UNK&gt;未登录词/低频词，编号1
        #&lt;S&gt;句子开始的标识，编号2
        # &lt;/S&gt;句子结尾的标识，编号3
        fout.write("{}\t1000000000\n{}\t1000000000\n{}\t1000000000\n{}\t1000000000\n".format("&lt;PAD&gt;", "&lt;UNK&gt;", "&lt;S&gt;", "&lt;/S&gt;"))
        #collections.Counter.most_common(N)按照频次从大到小排列词典，只显示前N个单词。
        for word, cnt in word2cnt.most_common(len(word2cnt)):
            fout.write(u"{}\t{}\n".format(word, cnt)) #按照 单词\t频次\n来写入

if __name__ == '__main__':
    make_vocab(hp.source_train, "de.vocab.tsv")
    make_vocab(hp.target_train, "en.vocab.tsv") #两个词汇表
    print("Done")
</code></pre> 
  <p>代码3：data_load.py 格式化数据，生成batch</p> 
  <pre><code># -*- coding: utf-8 -*-
#/usr/bin/python2
'''
June 2017 by kyubyong park. 
kbpark.linguist@gmail.com.
https://www.github.com/kyubyong/transformer
'''
from __future__ import print_function
from hyperparams import Hyperparams as hp
import tensorflow as tf
import numpy as np
import codecs
import regex

#词汇表转化为字典格式，并删除频次较低的
def load_de_vocab(): #德语
    # splitlines()按行切片(\n,\r,\r\n)；line.split()[0]将遍历好的每一行（一个单词一个频次）列表化，并取第0个元素/单词；如果单词的频次大于我们在hyperhparams.py中设定的参数就保存该单词到一个列表中
    vocab = [line.split()[0] for line in codecs.open('preprocessed/de.vocab.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])&gt;=hp.min_cnt]
    word2idx = {word: idx for idx, word in enumerate(vocab)} #转换为字典的形式进行保存单词，并给每个单词进行编号
    idx2word = {idx: word for idx, word in enumerate(vocab)}
    return word2idx, idx2word

def load_en_vocab(): #英语
    vocab = [line.split()[0] for line in codecs.open('preprocessed/en.vocab.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])&gt;=hp.min_cnt]
    word2idx = {word: idx for idx, word in enumerate(vocab)}
    idx2word = {idx: word for idx, word in enumerate(vocab)}
    return word2idx, idx2word
'''
举例：
word2idx ={'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1, '&lt;STR&gt;': 2, '&lt;EOS&gt;': 3, '有': 4, 
'的': 5, '`': 6, '-': 7, '卦': 8, '八': 9, ..., '爬': 1642, 'U': 1643}
idx2word={{0: '&lt;PAD&gt;', 1: '&lt;UNK&gt;', 2: '&lt;STR&gt;', 3: '&lt;EOS&gt;', 4: '有', 
5: '的', 6: '`', 7: '-', 8: '卦', 9: '八', ..., 1642: '爬', 1643: 'U'}}
'''

#数据处理
def create_data(source_sents, target_sents): #source_sents存放源语言句子的列表, target_sents目标语言句子
    de2idx, idx2de = load_de_vocab()
    en2idx, idx2en = load_en_vocab()
    
    # Index 索引
    x_list, y_list, Sources, Targets = [], [], [], []
    for source_sent, target_sent in zip(source_sents, target_sents): #使用zip()函数同时遍历两个句子列表
        #x,y 一个新句子
        x = [de2idx.get(word, 1) for word in (source_sent + u" &lt;/S&gt;").split()] # 1: OOV, &lt;/S&gt;: End of Text
        y = [en2idx.get(word, 1) for word in (target_sent + u" &lt;/S&gt;").split()] #给每一个句子的末尾加上&lt;/s&gt;终止符，并遍历句子中的每一个单词，将已经存在于word2idx中的那个单词对应的ID添加到新的列表中，如果这个单词不存在于word2idx中，那么就返回 ID‘1’到新列表中组成一个新的‘ID句子’(其中1代表&lt;UNK&gt;)
        if max(len(x), len(y)) &lt;= hp.maxlen: #我们在hyperhparams.py中设置的最大句子长度
            x_list.append(np.array(x)) #源语言ID句子
            y_list.append(np.array(y)) #目标语言ID句子
            Sources.append(source_sent) #源语言句子
            Targets.append(target_sent) #目标语言句子
            #超过长度阈值的丢弃
    
    # Pad 填充 对应site特殊词中的编号0：&lt;PAD&gt;
    X = np.zeros([len(x_list), hp.maxlen], np.int32) #二维0矩阵：句子个数*最大句长
    Y = np.zeros([len(y_list), hp.maxlen], np.int32)
    for i, (x, y) in enumerate(zip(x_list, y_list)):
        #保证每个ID句子的长度/元素个数都是相同的
        X[i] = np.lib.pad(x, [0, hp.maxlen-len(x)], 'constant', constant_values=(0, 0)) #对每一个ID句子做填充，左侧填充0个0，右侧填充hp.maxlen-len(x)个0，并且0也是四个特殊词中的一个：&lt;PAD&gt;编号0
        Y[i] = np.lib.pad(y, [0, hp.maxlen-len(y)], 'constant', constant_values=(0, 0))
    
    return X, Y, Sources, Targets #X和Y的shape为[len(x_list), hp.maxlen]，Sources, Targets的shape为[1, len(x_list)]

#加载训练集，对训练集做数据处理，返回定长ID句子
def load_train_data():
    de_sents = [regex.sub("[^\s\p{Latin}']", "", line) for line in codecs.open(hp.source_train, 'r', 'utf-8').read().split("\n") if line and line[0] != "&lt;"] #不加载特殊词
    en_sents = [regex.sub("[^\s\p{Latin}']", "", line) for line in codecs.open(hp.target_train, 'r', 'utf-8').read().split("\n") if line and line[0] != "&lt;"]
    
    X, Y, Sources, Targets = create_data(de_sents, en_sents)
    return X, Y

#加载测试集，对测试集做数据处理，返回定长ID句子
def load_test_data():
    def _refine(line):
        line = regex.sub("&lt;[^&gt;]+&gt;", "", line) #删除所有非空的&lt; &gt;项
        line = regex.sub("[^\s\p{Latin}']", "", line) 
        return line.strip() #删除字符串首尾的指定字符（默认为空格）
    #读取源语言与目标语言文本，切片并处理；将句子开头前4个字符为&lt;seg的保存到新列表中
    de_sents = [_refine(line) for line in codecs.open(hp.source_test, 'r', 'utf-8').read().split("\n") if line and line[:4] == "&lt;seg"]
    en_sents = [_refine(line) for line in codecs.open(hp.target_test, 'r', 'utf-8').read().split("\n") if line and line[:4] == "&lt;seg"]
        
    X, Y, Sources, Targets = create_data(de_sents, en_sents) #数据处理为定长ID句子
    return X, Sources, Targets # (1064, 150) 测试集不需要处理目标语言为ID句子

#生成batch数据，每次得到一个batch
def get_batch_data():
    # Load data
    X, Y = load_train_data() #训练集定长ID句子

    # calc total batch count
    num_batch = len(X) // hp.batch_size #需要几个batch来表示总数据，len(X)表示行数/句子个数
    
    # Convert to tensor
    #将python的数据类型转换成TensorFlow可用的tensor数据类型
    X = tf.convert_to_tensor(X, tf.int32)
    Y = tf.convert_to_tensor(Y, tf.int32)
    
    # Create Queues
    #创建文件名队列
    #从输入中每次取一个切片返回到一个输入队列里，该队列作为之后tf.train.shuffle_batch的一个参数，用以生成!一个!batch的数据。
    input_queues = tf.train.slice_input_producer([X, Y])
            
    # create batch queues
    #队头出队，队尾补充数据入队，打乱顺序；num_threads多线程入队，batch_size每次从队列中出队的数据数量,
    #capacity队列中最大元素数量，min_after_dequeue队列中最少存在的元素数量，allow_smaller_final_batch队列中最后小于batch_size的样本不进行出队
    x, y = tf.train.shuffle_batch(input_queues,
                                num_threads=8,
                                batch_size=hp.batch_size, 
                                capacity=hp.batch_size*64,   
                                min_after_dequeue=hp.batch_size*32, 
                                allow_smaller_final_batch=False)
    #不过现在好像都改用tf.data下的函数进行数据处理了
    return x, y, num_batch # shape分别为(N, T), (N, T)；N为batch_size的大小，T为最大句子长度maxlen；一个batch
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
