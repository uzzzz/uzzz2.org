<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>机器学习算法基础问题(一)PCA SVM 贝叶斯 过拟合 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="机器学习算法基础问题(一)PCA SVM 贝叶斯 过拟合" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="目录 一、贝叶斯与生成式判别式 1.1 生成式模型与判别式模型 1.2 贝叶斯分类器 1.3 相关贝叶斯题 二、决策树与随机森林 2.1 决策树 2.2 决策树的构造过程(training) 基于信息熵的构造 2.3 随机森林 三、协同滤波 3.1 协同滤波 四、经典算法 PCA算法 SVM 五、线性回归 7.1 最小二乘法 六、防止过拟合的方法 一、贝叶斯与生成式判别式 1.1 生成式模型与判别式模型 https://www.cnblogs.com/fanyabo/p/4067295.html 这个较容易判别其他几项正确，但是关于生成式模型和判别式模型： 判别式模型（Discriminative Model）是直接对条件概率p(y|x;θ)建模。常见的判别式模型有 线性回归模型、线性判别分析、支持向量机SVM、神经网络等。 生成式模型（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi，即： 常见的生成式模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等。 判别式模型根据求y即根据x来预测y 生成式模型求y则根据x，y的联合分布，反推最可能的p(y|x) 1.2 贝叶斯分类器 贝叶斯分类器 https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/1739590?fr=aladdin 此类基础的问题经常出现，贝叶斯概型，先验概率，后验概率等等。是机器学习很基础的问题。 贝叶斯分类器是各种分类器中分类错误概率最小或者在预先给定代价的情况下平均风险最小的分类器。它的设计方法是一种最基本的统计分类方法。其分类原理是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。 先验概率： https://baike.baidu.com/item/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87/6106649 先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为&quot;由因求果&quot;问题中的&quot;因&quot;出现的概率。在贝叶斯统计推断中，不确定数量的先验概率分布是在考虑一些因素之前表达对这一数量的置信程度的概率分布。例如，先验概率分布可能代表在将来的选举中投票给特定政治家的选民相对比例的概率分布。未知的数量可以是模型的参数或者是潜在变量。 后验概率： https://baike.baidu.com/item/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87 后验概率是信息理论的基本概念之一。在一个通信系统中，在收到某个消息之后，接收端所了解到的该消息发送的概率称为后验概率。 后验概率的计算要以先验概率为基础。后验概率可以根据通过贝叶斯公式，用先验概率和似然函数计算出来 后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的&quot;果&quot;。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础 。 事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。 1.3 相关贝叶斯题 下面关于贝叶斯分类器描述错误的是（ ） 正确答案: B&nbsp; &nbsp; 以贝叶斯定理为基础 是基于后验概率，推导出先验概率 可以解决有监督学习的问题 可以用极大似然估计法解贝叶斯分类器 解析：第二个说反了，贝叶斯概型都是根据先验推导后验，都是由结果找原因。 二、决策树与随机森林 2.1 决策树 https://www.cnblogs.com/xiemaycherry/p/10475067.html 决策树属于也只能非参数学习算法、可以用于解决(多)分类问题，回归问题。 回归问题的结果，叶子结点的平均值是回归问题的解。 根节点：决策树具有数据结构里面的二叉树、树的全部属性 非叶子节点 ：（决策点） 代表测试的条件，数据的属性的测试 叶子节点 ：分类后获得分类标记 分支： 测试的结果 决策树对抗过拟合的方法就是剪枝。 2.2 决策树的构造过程(training) https://www.jianshu.com/p/655d8e555494 基于信息熵的构造 当选择某个特征作为节点时，我们就希望这个特征的信息熵越小越好，那么不确定性越小 https://www.e-learn.cn/content/qita/1223365 2.3 随机森林 随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 a. 在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合 b. 在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力 c. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化 d. 可生成一个Proximities=（pij）矩阵，用于度量样本之间的相似性： pij=aij/N, aij表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数 e. 在创建随机森林的时候，对generlization error使用的是无偏估计 f. 训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量 g. 在训练过程中，能够检测到feature间的互相影响 h. 容易做成并行化方法 i. 实现比较简单 三、协同滤波 https://blog.csdn.net/qq_34555202/article/details/81909144 3.1 协同滤波 协同过滤是利用集体智慧的一个典型方法。推荐系统的首要问题是了解你的用户，然后才能给出更好的推荐。 概念：协同过滤一般是在海量的用户中发掘出一小部分和你品位（偏好）比较类似的，在协同过滤中，这些用户成为邻居，然后根据他们喜欢的其他东西组织成一个排序的目录作为推荐给你。 四、经典算法 PCA算法 下面关于主分量分析（PCA）的描述错误的是（ ）？ 正确答案: A&nbsp; &nbsp; 是一种非线性的方法 是一种对数据集降维的方法 它将一组可能相关的变量变换为同样数量的不相关的变量 它的第一个主分量尽可能大的反映数据中的发散性 解析：PCA是线性变换。 SVM &nbsp;下面关于支持向量机（SVM）的描述错误的是（ ）？ 正确答案: C&nbsp; &nbsp; 是一种监督式学习的方法（正确，需要标签样本） 可用于多分类的问题（正确，多个超平面） 是一种生成式模型（是判别式模型，因为由因索果。不是生成模型的由果索因） 支持非线性的核函数（正确） logistic与SVM错误的是 SVM的目标是结构风险最小化（正确，支持向量机方法是建立在统计学习理论的VC维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性(即对特定训练样本的学习精度，Accuracy)和学习能力(即无错误地识别任意样本的能力)之间寻求最佳折衷） SVM有效避免过拟合（错误，SVM也存在过拟合的问题，特别运用核函数的时候） logistic回归可以预测事件发生概率的大小（正确） logistic回归是为了目标函数最小化后验概率（错误，最小化先验概率） 准确率与召回率 Precison，与recall 准确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量，下面关于召回率描述正确的是（ ） 正确答案: C&nbsp; &nbsp; 衡量的是提取出的正确信息多少是准确的 召回率 = 提取出的正确信息条数 / 提取出的信息条数 召回率 = 提取出的正确信息条数 / 样本中相关的信息条数 召回率 = 提取出的正确信息条数 / 样本中总的信息条数 解析：这个挺简单的，因为博主前段时间还在做多标签分类，经常与这几个指标打交道。 输出多标签分类模型每class指标OP,OR,OF1,CP,CR,CF1 KNN近邻方法 一般情况下，KNN最近邻方法在（ ）情况下效果最好 正确答案: C&nbsp;&nbsp; （有争议吧，应该是样本出现团状分布的时候效果较好） 样本呈现团状分布 样本呈现链状分布 样本数量较大 样本数量较小 KNN算法 https://baike.baidu.com/item/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/1151153?fr=aladdin 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别， &nbsp; &nbsp; 五、线性回归 7.1 最小二乘法 给定数据集D = {(x1,y1), (x2,y2), …, (xn,yn)}，其中xi是p维的输入数据，yi对应的标签。要求通过“线性回归”方法来学得一个线性模型。 1） 请写出线性模型的的预测函数； 2） 请描述如何使用“最小二乘法”来进行参数估计，请写出主要公式推导； 解析： https://blog.csdn.net/u011026329/article/details/79183114 &nbsp; 六、防止过拟合的方法 https://www.cnblogs.com/june0507/p/7600924.html early stopping 加入验证集，训练一定epoch之后，训练集误差大于验证集且验证集的误差开始上升，说明此时网络出现过拟合，停止训练。 数据集增扩 从数据源头采集更多数据 复制原有数据并加上随机噪声 重采样 根据当前数据集估计数据分布参数，使用该分布产生更多数据等 正则化 加入权重的L1范数或者L2范数，惩罚大权重，减少网络复杂度。 剪枝与dropout 剪枝：删掉一些小权重连接，测试时不加回来。 dropout：训练时候随机删掉一些权重和连接，测试时候加回来。" />
<meta property="og:description" content="目录 一、贝叶斯与生成式判别式 1.1 生成式模型与判别式模型 1.2 贝叶斯分类器 1.3 相关贝叶斯题 二、决策树与随机森林 2.1 决策树 2.2 决策树的构造过程(training) 基于信息熵的构造 2.3 随机森林 三、协同滤波 3.1 协同滤波 四、经典算法 PCA算法 SVM 五、线性回归 7.1 最小二乘法 六、防止过拟合的方法 一、贝叶斯与生成式判别式 1.1 生成式模型与判别式模型 https://www.cnblogs.com/fanyabo/p/4067295.html 这个较容易判别其他几项正确，但是关于生成式模型和判别式模型： 判别式模型（Discriminative Model）是直接对条件概率p(y|x;θ)建模。常见的判别式模型有 线性回归模型、线性判别分析、支持向量机SVM、神经网络等。 生成式模型（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi，即： 常见的生成式模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等。 判别式模型根据求y即根据x来预测y 生成式模型求y则根据x，y的联合分布，反推最可能的p(y|x) 1.2 贝叶斯分类器 贝叶斯分类器 https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/1739590?fr=aladdin 此类基础的问题经常出现，贝叶斯概型，先验概率，后验概率等等。是机器学习很基础的问题。 贝叶斯分类器是各种分类器中分类错误概率最小或者在预先给定代价的情况下平均风险最小的分类器。它的设计方法是一种最基本的统计分类方法。其分类原理是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。 先验概率： https://baike.baidu.com/item/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87/6106649 先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为&quot;由因求果&quot;问题中的&quot;因&quot;出现的概率。在贝叶斯统计推断中，不确定数量的先验概率分布是在考虑一些因素之前表达对这一数量的置信程度的概率分布。例如，先验概率分布可能代表在将来的选举中投票给特定政治家的选民相对比例的概率分布。未知的数量可以是模型的参数或者是潜在变量。 后验概率： https://baike.baidu.com/item/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87 后验概率是信息理论的基本概念之一。在一个通信系统中，在收到某个消息之后，接收端所了解到的该消息发送的概率称为后验概率。 后验概率的计算要以先验概率为基础。后验概率可以根据通过贝叶斯公式，用先验概率和似然函数计算出来 后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的&quot;果&quot;。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础 。 事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。 1.3 相关贝叶斯题 下面关于贝叶斯分类器描述错误的是（ ） 正确答案: B&nbsp; &nbsp; 以贝叶斯定理为基础 是基于后验概率，推导出先验概率 可以解决有监督学习的问题 可以用极大似然估计法解贝叶斯分类器 解析：第二个说反了，贝叶斯概型都是根据先验推导后验，都是由结果找原因。 二、决策树与随机森林 2.1 决策树 https://www.cnblogs.com/xiemaycherry/p/10475067.html 决策树属于也只能非参数学习算法、可以用于解决(多)分类问题，回归问题。 回归问题的结果，叶子结点的平均值是回归问题的解。 根节点：决策树具有数据结构里面的二叉树、树的全部属性 非叶子节点 ：（决策点） 代表测试的条件，数据的属性的测试 叶子节点 ：分类后获得分类标记 分支： 测试的结果 决策树对抗过拟合的方法就是剪枝。 2.2 决策树的构造过程(training) https://www.jianshu.com/p/655d8e555494 基于信息熵的构造 当选择某个特征作为节点时，我们就希望这个特征的信息熵越小越好，那么不确定性越小 https://www.e-learn.cn/content/qita/1223365 2.3 随机森林 随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 a. 在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合 b. 在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力 c. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化 d. 可生成一个Proximities=（pij）矩阵，用于度量样本之间的相似性： pij=aij/N, aij表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数 e. 在创建随机森林的时候，对generlization error使用的是无偏估计 f. 训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量 g. 在训练过程中，能够检测到feature间的互相影响 h. 容易做成并行化方法 i. 实现比较简单 三、协同滤波 https://blog.csdn.net/qq_34555202/article/details/81909144 3.1 协同滤波 协同过滤是利用集体智慧的一个典型方法。推荐系统的首要问题是了解你的用户，然后才能给出更好的推荐。 概念：协同过滤一般是在海量的用户中发掘出一小部分和你品位（偏好）比较类似的，在协同过滤中，这些用户成为邻居，然后根据他们喜欢的其他东西组织成一个排序的目录作为推荐给你。 四、经典算法 PCA算法 下面关于主分量分析（PCA）的描述错误的是（ ）？ 正确答案: A&nbsp; &nbsp; 是一种非线性的方法 是一种对数据集降维的方法 它将一组可能相关的变量变换为同样数量的不相关的变量 它的第一个主分量尽可能大的反映数据中的发散性 解析：PCA是线性变换。 SVM &nbsp;下面关于支持向量机（SVM）的描述错误的是（ ）？ 正确答案: C&nbsp; &nbsp; 是一种监督式学习的方法（正确，需要标签样本） 可用于多分类的问题（正确，多个超平面） 是一种生成式模型（是判别式模型，因为由因索果。不是生成模型的由果索因） 支持非线性的核函数（正确） logistic与SVM错误的是 SVM的目标是结构风险最小化（正确，支持向量机方法是建立在统计学习理论的VC维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性(即对特定训练样本的学习精度，Accuracy)和学习能力(即无错误地识别任意样本的能力)之间寻求最佳折衷） SVM有效避免过拟合（错误，SVM也存在过拟合的问题，特别运用核函数的时候） logistic回归可以预测事件发生概率的大小（正确） logistic回归是为了目标函数最小化后验概率（错误，最小化先验概率） 准确率与召回率 Precison，与recall 准确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量，下面关于召回率描述正确的是（ ） 正确答案: C&nbsp; &nbsp; 衡量的是提取出的正确信息多少是准确的 召回率 = 提取出的正确信息条数 / 提取出的信息条数 召回率 = 提取出的正确信息条数 / 样本中相关的信息条数 召回率 = 提取出的正确信息条数 / 样本中总的信息条数 解析：这个挺简单的，因为博主前段时间还在做多标签分类，经常与这几个指标打交道。 输出多标签分类模型每class指标OP,OR,OF1,CP,CR,CF1 KNN近邻方法 一般情况下，KNN最近邻方法在（ ）情况下效果最好 正确答案: C&nbsp;&nbsp; （有争议吧，应该是样本出现团状分布的时候效果较好） 样本呈现团状分布 样本呈现链状分布 样本数量较大 样本数量较小 KNN算法 https://baike.baidu.com/item/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/1151153?fr=aladdin 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别， &nbsp; &nbsp; 五、线性回归 7.1 最小二乘法 给定数据集D = {(x1,y1), (x2,y2), …, (xn,yn)}，其中xi是p维的输入数据，yi对应的标签。要求通过“线性回归”方法来学得一个线性模型。 1） 请写出线性模型的的预测函数； 2） 请描述如何使用“最小二乘法”来进行参数估计，请写出主要公式推导； 解析： https://blog.csdn.net/u011026329/article/details/79183114 &nbsp; 六、防止过拟合的方法 https://www.cnblogs.com/june0507/p/7600924.html early stopping 加入验证集，训练一定epoch之后，训练集误差大于验证集且验证集的误差开始上升，说明此时网络出现过拟合，停止训练。 数据集增扩 从数据源头采集更多数据 复制原有数据并加上随机噪声 重采样 根据当前数据集估计数据分布参数，使用该分布产生更多数据等 正则化 加入权重的L1范数或者L2范数，惩罚大权重，减少网络复杂度。 剪枝与dropout 剪枝：删掉一些小权重连接，测试时不加回来。 dropout：训练时候随机删掉一些权重和连接，测试时候加回来。" />
<link rel="canonical" href="https://uzzz.org/2019/08/06/793039.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/06/793039.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-06T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"目录 一、贝叶斯与生成式判别式 1.1 生成式模型与判别式模型 1.2 贝叶斯分类器 1.3 相关贝叶斯题 二、决策树与随机森林 2.1 决策树 2.2 决策树的构造过程(training) 基于信息熵的构造 2.3 随机森林 三、协同滤波 3.1 协同滤波 四、经典算法 PCA算法 SVM 五、线性回归 7.1 最小二乘法 六、防止过拟合的方法 一、贝叶斯与生成式判别式 1.1 生成式模型与判别式模型 https://www.cnblogs.com/fanyabo/p/4067295.html 这个较容易判别其他几项正确，但是关于生成式模型和判别式模型： 判别式模型（Discriminative Model）是直接对条件概率p(y|x;θ)建模。常见的判别式模型有 线性回归模型、线性判别分析、支持向量机SVM、神经网络等。 生成式模型（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi，即： 常见的生成式模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等。 判别式模型根据求y即根据x来预测y 生成式模型求y则根据x，y的联合分布，反推最可能的p(y|x) 1.2 贝叶斯分类器 贝叶斯分类器 https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/1739590?fr=aladdin 此类基础的问题经常出现，贝叶斯概型，先验概率，后验概率等等。是机器学习很基础的问题。 贝叶斯分类器是各种分类器中分类错误概率最小或者在预先给定代价的情况下平均风险最小的分类器。它的设计方法是一种最基本的统计分类方法。其分类原理是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。 先验概率： https://baike.baidu.com/item/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87/6106649 先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为&quot;由因求果&quot;问题中的&quot;因&quot;出现的概率。在贝叶斯统计推断中，不确定数量的先验概率分布是在考虑一些因素之前表达对这一数量的置信程度的概率分布。例如，先验概率分布可能代表在将来的选举中投票给特定政治家的选民相对比例的概率分布。未知的数量可以是模型的参数或者是潜在变量。 后验概率： https://baike.baidu.com/item/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87 后验概率是信息理论的基本概念之一。在一个通信系统中，在收到某个消息之后，接收端所了解到的该消息发送的概率称为后验概率。 后验概率的计算要以先验概率为基础。后验概率可以根据通过贝叶斯公式，用先验概率和似然函数计算出来 后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的&quot;果&quot;。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础 。 事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。 1.3 相关贝叶斯题 下面关于贝叶斯分类器描述错误的是（ ） 正确答案: B&nbsp; &nbsp; 以贝叶斯定理为基础 是基于后验概率，推导出先验概率 可以解决有监督学习的问题 可以用极大似然估计法解贝叶斯分类器 解析：第二个说反了，贝叶斯概型都是根据先验推导后验，都是由结果找原因。 二、决策树与随机森林 2.1 决策树 https://www.cnblogs.com/xiemaycherry/p/10475067.html 决策树属于也只能非参数学习算法、可以用于解决(多)分类问题，回归问题。 回归问题的结果，叶子结点的平均值是回归问题的解。 根节点：决策树具有数据结构里面的二叉树、树的全部属性 非叶子节点 ：（决策点） 代表测试的条件，数据的属性的测试 叶子节点 ：分类后获得分类标记 分支： 测试的结果 决策树对抗过拟合的方法就是剪枝。 2.2 决策树的构造过程(training) https://www.jianshu.com/p/655d8e555494 基于信息熵的构造 当选择某个特征作为节点时，我们就希望这个特征的信息熵越小越好，那么不确定性越小 https://www.e-learn.cn/content/qita/1223365 2.3 随机森林 随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 a. 在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合 b. 在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力 c. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化 d. 可生成一个Proximities=（pij）矩阵，用于度量样本之间的相似性： pij=aij/N, aij表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数 e. 在创建随机森林的时候，对generlization error使用的是无偏估计 f. 训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量 g. 在训练过程中，能够检测到feature间的互相影响 h. 容易做成并行化方法 i. 实现比较简单 三、协同滤波 https://blog.csdn.net/qq_34555202/article/details/81909144 3.1 协同滤波 协同过滤是利用集体智慧的一个典型方法。推荐系统的首要问题是了解你的用户，然后才能给出更好的推荐。 概念：协同过滤一般是在海量的用户中发掘出一小部分和你品位（偏好）比较类似的，在协同过滤中，这些用户成为邻居，然后根据他们喜欢的其他东西组织成一个排序的目录作为推荐给你。 四、经典算法 PCA算法 下面关于主分量分析（PCA）的描述错误的是（ ）？ 正确答案: A&nbsp; &nbsp; 是一种非线性的方法 是一种对数据集降维的方法 它将一组可能相关的变量变换为同样数量的不相关的变量 它的第一个主分量尽可能大的反映数据中的发散性 解析：PCA是线性变换。 SVM &nbsp;下面关于支持向量机（SVM）的描述错误的是（ ）？ 正确答案: C&nbsp; &nbsp; 是一种监督式学习的方法（正确，需要标签样本） 可用于多分类的问题（正确，多个超平面） 是一种生成式模型（是判别式模型，因为由因索果。不是生成模型的由果索因） 支持非线性的核函数（正确） logistic与SVM错误的是 SVM的目标是结构风险最小化（正确，支持向量机方法是建立在统计学习理论的VC维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性(即对特定训练样本的学习精度，Accuracy)和学习能力(即无错误地识别任意样本的能力)之间寻求最佳折衷） SVM有效避免过拟合（错误，SVM也存在过拟合的问题，特别运用核函数的时候） logistic回归可以预测事件发生概率的大小（正确） logistic回归是为了目标函数最小化后验概率（错误，最小化先验概率） 准确率与召回率 Precison，与recall 准确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量，下面关于召回率描述正确的是（ ） 正确答案: C&nbsp; &nbsp; 衡量的是提取出的正确信息多少是准确的 召回率 = 提取出的正确信息条数 / 提取出的信息条数 召回率 = 提取出的正确信息条数 / 样本中相关的信息条数 召回率 = 提取出的正确信息条数 / 样本中总的信息条数 解析：这个挺简单的，因为博主前段时间还在做多标签分类，经常与这几个指标打交道。 输出多标签分类模型每class指标OP,OR,OF1,CP,CR,CF1 KNN近邻方法 一般情况下，KNN最近邻方法在（ ）情况下效果最好 正确答案: C&nbsp;&nbsp; （有争议吧，应该是样本出现团状分布的时候效果较好） 样本呈现团状分布 样本呈现链状分布 样本数量较大 样本数量较小 KNN算法 https://baike.baidu.com/item/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/1151153?fr=aladdin 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别， &nbsp; &nbsp; 五、线性回归 7.1 最小二乘法 给定数据集D = {(x1,y1), (x2,y2), …, (xn,yn)}，其中xi是p维的输入数据，yi对应的标签。要求通过“线性回归”方法来学得一个线性模型。 1） 请写出线性模型的的预测函数； 2） 请描述如何使用“最小二乘法”来进行参数估计，请写出主要公式推导； 解析： https://blog.csdn.net/u011026329/article/details/79183114 &nbsp; 六、防止过拟合的方法 https://www.cnblogs.com/june0507/p/7600924.html early stopping 加入验证集，训练一定epoch之后，训练集误差大于验证集且验证集的误差开始上升，说明此时网络出现过拟合，停止训练。 数据集增扩 从数据源头采集更多数据 复制原有数据并加上随机噪声 重采样 根据当前数据集估计数据分布参数，使用该分布产生更多数据等 正则化 加入权重的L1范数或者L2范数，惩罚大权重，减少网络复杂度。 剪枝与dropout 剪枝：删掉一些小权重连接，测试时不加回来。 dropout：训练时候随机删掉一些权重和连接，测试时候加回来。","@type":"BlogPosting","url":"https://uzzz.org/2019/08/06/793039.html","headline":"机器学习算法基础问题(一)PCA SVM 贝叶斯 过拟合","dateModified":"2019-08-06T00:00:00+08:00","datePublished":"2019-08-06T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/06/793039.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>机器学习算法基础问题(一)PCA|SVM|贝叶斯|过拟合</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="%E4%B8%80%E3%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E7%94%9F%E6%88%90%E5%BC%8F%E5%88%A4%E5%88%AB%E5%BC%8F-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E7%94%9F%E6%88%90%E5%BC%8F%E5%88%A4%E5%88%AB%E5%BC%8F" rel="nofollow" data-token="421cda8e0601039242a5a73e14869048">一、贝叶斯与生成式判别式</a></p> 
  <p id="1.1%20%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;"><a href="#1.1%20%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B" rel="nofollow" data-token="431f0afaba55e28ee86abaa16ed74fbb">1.1 生成式模型与判别式模型</a></p> 
  <p id="1.2%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8-toc" style="margin-left:40px;"><a href="#1.2%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8" rel="nofollow" data-token="b80403132ee41cb1fc3c209711a4bce4">1.2 贝叶斯分类器</a></p> 
  <p id="1.3%20%E7%9B%B8%E5%85%B3%E8%B4%9D%E5%8F%B6%E6%96%AF%E9%A2%98-toc" style="margin-left:80px;"><a href="#1.3%20%E7%9B%B8%E5%85%B3%E8%B4%9D%E5%8F%B6%E6%96%AF%E9%A2%98" rel="nofollow" data-token="e9a4c2c238c1f9faf2e35c649f8c83a1">1.3 相关贝叶斯题</a></p> 
  <p id="%E4%BA%8C%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97" rel="nofollow" data-token="3f1daed0770197e35fdbfbbe30979c76">二、决策树与随机森林</a></p> 
  <p id="2.1%20%E5%86%B3%E7%AD%96%E6%A0%91-toc" style="margin-left:40px;"><a href="#2.1%20%E5%86%B3%E7%AD%96%E6%A0%91" rel="nofollow" data-token="55051b5b488afc99b5edc3a2d757cdb6">2.1 决策树</a></p> 
  <p id="2.2%20%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%9E%84%E9%80%A0%E8%BF%87%E7%A8%8B(training)-toc" style="margin-left:40px;"><a href="#2.2%20%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%9E%84%E9%80%A0%E8%BF%87%E7%A8%8B(training)" rel="nofollow" data-token="ca8203d5b288a21ef9f7bd68080ceb19">2.2 决策树的构造过程(training)</a></p> 
  <p id="%E5%9F%BA%E4%BA%8E%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E6%9E%84%E9%80%A0-toc" style="margin-left:80px;"><a href="#%E5%9F%BA%E4%BA%8E%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E6%9E%84%E9%80%A0" rel="nofollow" data-token="00676af0821ce9a67d76e19095c778ed">基于信息熵的构造</a></p> 
  <p id="2.3%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-toc" style="margin-left:40px;"><a href="#2.3%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97" rel="nofollow" data-token="62d917d495540e53b52bb7e3cf3666ad">2.3 随机森林</a></p> 
  <p id="%E4%B8%89%E3%80%81%E5%8D%8F%E5%90%8C%E6%BB%A4%E6%B3%A2-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E5%8D%8F%E5%90%8C%E6%BB%A4%E6%B3%A2" rel="nofollow" data-token="decdaa3a20d49e1dbe53080b378cc7bf">三、协同滤波</a></p> 
  <p id="3.1%20%E5%8D%8F%E5%90%8C%E6%BB%A4%E6%B3%A2-toc" style="margin-left:40px;"><a href="#3.1%20%E5%8D%8F%E5%90%8C%E6%BB%A4%E6%B3%A2" rel="nofollow" data-token="521f1444278ea5a9c91dd480d721d699">3.1 协同滤波</a></p> 
  <p id="%E5%9B%9B%E3%80%81%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95" rel="nofollow" data-token="bc426e17dfcfc495db0631ecc172e732">四、经典算法</a></p> 
  <p id="PCA%E7%AE%97%E6%B3%95-toc" style="margin-left:80px;"><a href="#PCA%E7%AE%97%E6%B3%95" rel="nofollow" data-token="5d1b331b3e44d6911739646e62d7a9c3">PCA算法</a></p> 
  <p id="SVM-toc" style="margin-left:80px;"><a href="#SVM" rel="nofollow" data-token="dd1dabb1e4f5ef1256be0f1fc3598e11">SVM</a></p> 
  <p id="%E4%BA%94%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92" rel="nofollow" data-token="144ffd013eb47165d3c3c97f22044614">五、线性回归</a></p> 
  <p id="7.1%20%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95-toc" style="margin-left:40px;"><a href="#7.1%20%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" rel="nofollow" data-token="84c105c2d47c8867ab4fbb673dd47485">7.1 最小二乘法</a></p> 
  <p id="%E5%85%AD%E3%80%81%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95-toc" style="margin-left:0px;"><a href="#%E5%85%AD%E3%80%81%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95" rel="nofollow" data-token="8d66f331166847d204627c2cf2fd8f3b">六、防止过拟合的方法</a></p> 
  <hr id="hr-toc">
  <h1 id="%E4%B8%80%E3%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E7%94%9F%E6%88%90%E5%BC%8F%E5%88%A4%E5%88%AB%E5%BC%8F">一、贝叶斯与生成式判别式</h1> 
  <h2 id="1.1%20%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B">1.1 生成式模型与判别式模型</h2> 
  <p><a href="https://www.cnblogs.com/fanyabo/p/4067295.html" rel="nofollow" data-token="1e92e9886075f29a90b97a1082505829">https://www.cnblogs.com/fanyabo/p/4067295.html</a></p> 
  <p>这个较容易判别其他几项正确，但是关于生成式模型和判别式模型：</p> 
  <ul>
   <li>判别式模型（Discriminative Model）是直接对条件概率p(y|x;θ)建模。常见的判别式模型有 线性回归模型、线性判别分析、支持向量机SVM、神经网络等。</li> 
   <li>生成式模型（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi，即：</li> 
  </ul>
  <p><img alt="" class="has" height="93" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMwLmNuYmxvZ3MuY29tL2Jsb2cvMzkyMjI4LzIwMTQxMS8wMTEzNDIxMTUwMzY5MzMuanBn" width="362"></p> 
  <p>常见的生成式模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等。</p> 
  <p>判别式模型根据求y即根据x来预测y</p> 
  <p>生成式模型求y则根据x，y的联合分布，反推最可能的p(y|x)</p> 
  <h2 id="1.2%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8">1.2 贝叶斯分类器</h2> 
  <p><strong>贝叶斯分类器</strong></p> 
  <p><a href="https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/1739590?fr=aladdin" rel="nofollow" data-token="54552a4e3b2e0d8f21210660bcb049c0">https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/1739590?fr=aladdin</a></p> 
  <p>此类基础的问题经常出现，贝叶斯概型，先验概率，后验概率等等。是机器学习很基础的问题。</p> 
  <p>贝叶斯分类器是各种分类器中分类错误概率最小或者在预先给定代价的情况下平均风险最小的分类器。它的设计方法是一种最基本的统计<a href="https://baike.baidu.com/item/%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/9508629" rel="nofollow" data-token="630977aa2139641ed492e2dd8c0ef76b">分类方法</a>。其分类原理是通过某对象的<a href="https://baike.baidu.com/item/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87/6106649" rel="nofollow" data-token="386c62089b95082c6053d255a7e02422">先验概率</a>，利用<a href="https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F/9683982" rel="nofollow" data-token="e3f81859d20280fba8a64ffc6431a622">贝叶斯公式</a>计算出其<a href="https://baike.baidu.com/item/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87" rel="nofollow" data-token="92ef5cd544b8ab2fb4fede0e95f1d0a9">后验概率</a>，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。</p> 
  <p><strong>先验概率：</strong></p> 
  <p><a href="https://baike.baidu.com/item/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87/6106649" rel="nofollow" data-token="386c62089b95082c6053d255a7e02422">https://baike.baidu.com/item/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87/6106649</a></p> 
  <p>先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为"由因求果"问题中的"因"出现的概率。在贝叶斯统计推断中，不确定数量的先验概率分布是在考虑一些因素之前表达对这一数量的置信程度的概率分布。例如，先验概率分布可能代表在将来的选举中投票给特定政治家的选民相对比例的概率分布。未知的数量可以是模型的参数或者是潜在变量。</p> 
  <p><strong>后验概率：</strong></p> 
  <p><a href="https://baike.baidu.com/item/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87" rel="nofollow" data-token="92ef5cd544b8ab2fb4fede0e95f1d0a9">https://baike.baidu.com/item/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87</a></p> 
  <p>后验概率是<a href="https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E7%90%86%E8%AE%BA/2424000" rel="nofollow" data-token="12f6198f345a5907eee4614260de39da">信息理论</a>的基本概念之一。在一个通信系统中，在收到某个消息之后，接收端所了解到的该消息发送的概率称为后验概率。</p> 
  <p>后验概率的计算要以<a href="https://baike.baidu.com/item/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87/6106649" rel="nofollow" data-token="386c62089b95082c6053d255a7e02422">先验概率</a>为基础。后验概率可以根据通过<a href="https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F/9683982" rel="nofollow" data-token="e3f81859d20280fba8a64ffc6431a622">贝叶斯公式</a>，用先验概率和<a href="https://baike.baidu.com/item/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/6011241" rel="nofollow" data-token="19777542cfa8eb9057a8b85b1e49541b">似然函数</a>计算出来</p> 
  <p>后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的"果"。<a href="https://baike.baidu.com/item/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87" rel="nofollow" data-token="96f3d9f9bd4330bc74747753d80e3dee">先验概率</a>与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础 <a name="ref_[2]_336754"></a>。</p> 
  <p>事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。</p> 
  <h3 id="1.3%20%E7%9B%B8%E5%85%B3%E8%B4%9D%E5%8F%B6%E6%96%AF%E9%A2%98">1.3 相关贝叶斯题</h3> 
  <p>下面关于贝叶斯分类器描述错误的是（ ）<br> 正确答案: B&nbsp; &nbsp;</p> 
  <ul>
   <li>以贝叶斯定理为基础</li> 
   <li>是基于后验概率，推导出先验概率</li> 
   <li>可以解决有监督学习的问题</li> 
   <li>可以用极大似然估计法解贝叶斯分类器</li> 
  </ul>
  <p>解析：第二个说反了，贝叶斯概型都是根据先验推导后验，都是由结果找原因。</p> 
  <h1 id="%E4%BA%8C%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97">二、决策树与随机森林</h1> 
  <h2 id="2.1%20%E5%86%B3%E7%AD%96%E6%A0%91">2.1 决策树</h2> 
  <p><a href="https://www.cnblogs.com/xiemaycherry/p/10475067.html" rel="nofollow" data-token="dd542877c5024796459dd39998910ab1">https://www.cnblogs.com/xiemaycherry/p/10475067.html</a></p> 
  <p>决策树属于也只能非参数学习算法、可以用于解决(多)分类问题，回归问题。 回归问题的结果，叶子结点的平均值是回归问题的解。</p> 
  <p><img alt="" class="has" height="273" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730110852257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjQ3NDgwOQ==,size_16,color_FFFFFF,t_70" width="430"></p> 
  <ul>
   <li>根节点：决策树具有数据结构里面的二叉树、树的全部属性</li> 
   <li>非叶子节点 ：（决策点） 代表测试的条件，数据的属性的测试</li> 
   <li>叶子节点 ：分类后获得分类标记</li> 
   <li>分支： 测试的结果</li> 
  </ul>
  <p>决策树对抗过拟合的方法就是剪枝。</p> 
  <h2 id="2.2%20%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%9E%84%E9%80%A0%E8%BF%87%E7%A8%8B(training)">2.2 决策树的构造过程(training)</h2> 
  <p><a href="https://www.jianshu.com/p/655d8e555494" rel="nofollow" data-token="a49e53ed92616673d4611a0e003515dd">https://www.jianshu.com/p/655d8e555494</a></p> 
  <h3 id="%E5%9F%BA%E4%BA%8E%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E6%9E%84%E9%80%A0">基于信息熵的构造</h3> 
  <p>当选择某个特征作为节点时，我们就希望这个特征的信息熵越小越好，那么不确定性越小</p> 
  <p><a href="https://www.e-learn.cn/content/qita/1223365" rel="nofollow" data-token="d8bbc6825c61261d7a4bb7c87f22b754">https://www.e-learn.cn/content/qita/1223365</a></p> 
  <h2 id="2.3%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97">2.3 随机森林</h2> 
  <p>随机森林顾名思义，是用随机的方式建立一个森林，<strong>森林里面有很多的决策树组成，</strong>随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。</p> 
  <ul>
   <li>a. 在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合</li> 
   <li>b. 在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力</li> 
   <li>c. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li> 
   <li>d. 可生成一个Proximities=（pij）矩阵，用于度量样本之间的相似性： pij=aij/N, aij表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数</li> 
   <li>e. 在创建随机森林的时候，对generlization error使用的是无偏估计</li> 
   <li>f. 训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量</li> 
   <li>g. 在训练过程中，能够检测到feature间的互相影响</li> 
   <li>h. 容易做成并行化方法</li> 
   <li>i. 实现比较简单</li> 
  </ul>
  <h1 id="%E4%B8%89%E3%80%81%E5%8D%8F%E5%90%8C%E6%BB%A4%E6%B3%A2">三、协同滤波</h1> 
  <p><a href="https://blog.csdn.net/qq_34555202/article/details/81909144" rel="nofollow" data-token="8a5e94888f03f39b9fe6e097705b4473">https://blog.csdn.net/qq_34555202/article/details/81909144</a></p> 
  <h2 id="3.1%20%E5%8D%8F%E5%90%8C%E6%BB%A4%E6%B3%A2">3.1 协同滤波</h2> 
  <p>协同过滤是利用集体智慧的一个典型方法。推荐系统的首要问题是了解你的用户，然后才能给出更好的推荐。</p> 
  <p><strong>概念：</strong>协同过滤一般是在海量的用户中发掘出一小部分和你品位（偏好）比较类似的，在协同过滤中，这些用户成为邻居，然后根据他们喜欢的其他东西组织成一个排序的目录作为推荐给你。</p> 
  <h1 id="%E5%9B%9B%E3%80%81%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95">四、经典算法</h1> 
  <h3 id="PCA%E7%AE%97%E6%B3%95">PCA算法</h3> 
  <p>下面关于主分量分析（PCA）的描述错误的是（ ）？<br> 正确答案: A&nbsp; &nbsp;<br> 是一种非线性的方法<br> 是一种对数据集降维的方法<br> 它将一组可能相关的变量变换为同样数量的不相关的变量<br> 它的第一个主分量尽可能大的反映数据中的发散性</p> 
  <p>解析：PCA是线性变换。</p> 
  <h3 id="SVM">SVM</h3> 
  <p>&nbsp;下面关于支持向量机（SVM）的描述错误的是（ ）？<br> 正确答案: C&nbsp; &nbsp;</p> 
  <ul>
   <li>是一种监督式学习的方法（正确，需要标签样本）</li> 
   <li>可用于多分类的问题（正确，多个超平面）</li> 
   <li>是一种生成式模型（是判别式模型，因为由因索果。不是生成模型的由果索因）</li> 
   <li>支持非线性的核函数（正确）</li> 
  </ul>
  <p>logistic与SVM错误的是</p> 
  <ul>
   <li>SVM的目标是结构风险最小化（正确，支持向量机方法是建立在统计学习理论的VC维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性(即对特定训练样本的学习精度，Accuracy)和学习能力(即无错误地识别任意样本的能力)之间寻求最佳折衷）</li> 
   <li>SVM有效避免过拟合（错误，SVM也存在过拟合的问题，特别运用核函数的时候）</li> 
   <li>logistic回归可以预测事件发生概率的大小（正确）</li> 
   <li>logistic回归是为了目标函数最小化后验概率（错误，最小化先验概率）</li> 
  </ul>
  <p><strong>准确率与召回率</strong></p> 
  <p>Precison，与recall</p> 
  <p>准确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量，下面关于召回率描述正确的是（ ）<br> 正确答案: C&nbsp; &nbsp;<br> 衡量的是提取出的正确信息多少是准确的<br> 召回率 = 提取出的正确信息条数 / 提取出的信息条数<br> 召回率 = 提取出的正确信息条数 / 样本中相关的信息条数<br> 召回率 = 提取出的正确信息条数 / 样本中总的信息条数</p> 
  <p>解析：这个挺简单的，因为博主前段时间还在做多标签分类，经常与这几个指标打交道。</p> 
  <p><a href="https://blog.csdn.net/weixin_36474809/article/details/90231745" rel="nofollow" data-token="63ef292560916f16f1260cf8e2c03d7b">输出多标签分类模型每class指标OP,OR,OF1,CP,CR,CF1 </a></p> 
  <p><strong>KNN近邻方法</strong></p> 
  <p>一般情况下，KNN最近邻方法在（ ）情况下效果最好<br> 正确答案: C&nbsp;&nbsp; （有争议吧，应该是样本出现团状分布的时候效果较好）<br> 样本呈现团状分布<br> 样本呈现链状分布<br> 样本数量较大<br> 样本数量较小</p> 
  <p>KNN算法</p> 
  <p><a href="https://baike.baidu.com/item/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/1151153?fr=aladdin" rel="nofollow" data-token="69bce4b5550e79513d6278571554cb02">https://baike.baidu.com/item/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/1151153?fr=aladdin</a></p> 
  <p>如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h1 id="%E4%BA%94%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92">五、线性回归</h1> 
  <h2 id="7.1%20%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95">7.1 最小二乘法</h2> 
  <p>给定数据集D = {(x1,y1), (x2,y2), …, (xn,yn)}，其中xi是p维的输入数据，yi对应的标签。要求通过“线性回归”方法来学得一个线性模型。</p> 
  <p>1） 请写出线性模型的的预测函数；</p> 
  <p>2） 请描述如何使用“最小二乘法”来进行参数估计，请写出主要公式推导；</p> 
  <p>解析：</p> 
  <p><a href="https://blog.csdn.net/u011026329/article/details/79183114" rel="nofollow" data-token="74849da24feb06dee671cb7f5ffccf26">https://blog.csdn.net/u011026329/article/details/79183114</a></p> 
  <p>&nbsp;</p> 
  <h1 id="%E5%85%AD%E3%80%81%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95">六、防止过拟合的方法</h1> 
  <p><a href="https://www.cnblogs.com/june0507/p/7600924.html" rel="nofollow" data-token="babe76deaad702e8984f773a761a60fd">https://www.cnblogs.com/june0507/p/7600924.html</a></p> 
  <p><strong>early stopping</strong></p> 
  <p>加入验证集，训练一定epoch之后，训练集误差大于验证集且验证集的误差开始上升，说明此时网络出现过拟合，停止训练。</p> 
  <p><strong>数据集增扩</strong></p> 
  <ul>
   <li>从数据源头采集更多数据</li> 
   <li>复制原有数据并加上随机噪声</li> 
   <li>重采样</li> 
   <li>根据当前数据集估计数据分布参数，使用该分布产生更多数据等</li> 
  </ul>
  <p><strong>正则化</strong></p> 
  <p>加入权重的L1范数或者L2范数，惩罚大权重，减少网络复杂度。</p> 
  <p><strong>剪枝与dropout</strong></p> 
  <p>剪枝：删掉一些小权重连接，测试时不加回来。</p> 
  <p>dropout：训练时候随机删掉一些权重和连接，测试时候加回来。</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
