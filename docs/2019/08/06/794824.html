<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Hadoop之HDFS | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Hadoop之HDFS" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="hdfs常用命令 hdfs namenode -format 启动前先格式化文件系统 hadoop-daemon.sh start namenode|datanode 启动namenode和datanode hdfs dfs -rm -r /user/it/output 删除文件系统的文件 hdfs dfs -mkdir -p /usr/atguigu/input 在hdfs上创建文件夹 hdfs dfs -put ./wcinput/wc.input /usr/it/input 向hdfs上传文件 hdfs dfs -ls /user/atguigu/output 查看文件 hdfs dfs -cat /user/atguigu/output/part-r-00000 jps java下查看进程和命令，这是一个java命令 HDFS的优缺点 优点 1.高容错性 2.适合大数据处理，规模达到GB、TB甚至PB，文件规模可以达到百万数量级 3.数据流失访问，能保证数据的一致性 4.可构建在廉价机器上，通过多副本机制，提高可靠性 缺点 1.不适合低延时数据访问 2.无法高效对小文件进行存储（占用namenode内存、小文件的寻址时间大于读取时间） 3.不支持并发写入、文件随机修改，支持追加 HDFS的组成 Client：就是客户端。 （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储； （2）与NameNode交互，获取文件的位置信息； （3）与DataNode交互，读取或者写入数据； （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS； （5）Client可以通过一些命令来访问HDFS； NameNode：就是Master，它是一个主管、管理者。 （1）管理HDFS的名称空间； （2）管理数据块（Block）映射信息； （3）配置副本策略； （4）处理客户端读写请求。 DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 （1）存储实际的数据块； （2）执行数据块的读/写操作。 Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 （1）辅助NameNode，分担其工作量； （2）定期合并Fsimage和Edits，并推送给NameNode； （3）在紧急情况下，可辅助恢复NameNode。 HDFS文件块大小 HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。 如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB(10ms100100M/s = 100M)。默认的块大小128MB。 HFDS的Shell操作 基本语法： hadoop fs 具体命令 常用命令： （0）启动Hadoop集群（方便后续的测试） [testuser@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh [testuser@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （1）-help：输出这个命令参数 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -help rm （2）-ls: 显示目录信息 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -ls / （3）-mkdir：在hdfs上创建目录 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /sanguo/shuguo （4）-moveFromLocal从本地剪切粘贴到hdfs [testuser@hadoop102 hadoop-2.7.2]$ touch kongming.txt [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo （5）-appendToFile ：追加一个文件到已经存在的文件末尾 [testuser@hadoop102 hadoop-2.7.2]$ touch liubei.txt [testuser@hadoop102 hadoop-2.7.2]$ vi liubei.txt 输入 san gu mao lu [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt （6）-cat：显示文件内容 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /sanguo/shuguo/kongming.txt （7）-tail：显示一个文件的末尾 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /sanguo/shuguo/kongming.txt （8）-chgrp 、-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -chown testuser:testuser /sanguo/shuguo/kongming.txt （9）-copyFromLocal：从本地文件系统中拷贝文件到hdfs路径去 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt / （10）-copyToLocal：从hdfs拷贝到本地 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./ （11）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt （12）-mv：在hdfs目录中移动文件 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/ （13）-get：等同于copyToLocal，就是从hdfs下载文件到本地 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./ （14）-getmerge ：合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,... [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/testuser/test/* ./zaiyiqi.txt （15）-put：等同于copyFromLocal [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/testuser/test/ （16）-rm：删除文件或文件夹 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/testuser/test/jinlian2.txt （17）-rmdir：删除空目录 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test （18）-du统计文件夹的大小信息 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/testuser/test 2.7 K /user/testuser/test [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/testuser/test 1.3 K /user/testuser/test/README.txt 15 /user/testuser/test/jinlian.txt 1.4 K /user/testuser/test/zaiyiqi.txt （19）-setrep：设置hdfs中文件的副本数量 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt HDFS客户端操作 maven pom文件 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j.properties log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n HDFS的API操作 连接到hdfs，并新建文件夹 @Test public void testMkdirs() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 // configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); // FileSystem fs = FileSystem.get(configuration); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 创建目录 fs.mkdirs(new Path(&quot;/0103/test/user&quot;)); // 3 关闭资源 fs.close(); } 文件上传 @Test public void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException { // 1 获取文件系统 Configuration configuration = new Configuration(); configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 上传文件 fs.copyFromLocalFile(new Path(&quot;e:/hello.txt&quot;), new Path(&quot;/hello.txt&quot;)); // 3 关闭资源 fs.close(); System.out.println(&quot;over&quot;); } 文件下载 @Test public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 执行下载操作 // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件校验 fs.copyToLocalFile(false, new Path(&quot;/hello1.txt&quot;), new Path(&quot;e:/hello1.txt&quot;), true); // 3 关闭资源 fs.close(); } 文件夹删除 @Test public void testDelete() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 执行删除 fs.delete(new Path(&quot;/0103/&quot;), true); // 3 关闭资源 fs.close(); } 文件名更改 @Test public void testRename() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 修改文件名称 fs.rename(new Path(&quot;/hello.txt&quot;), new Path(&quot;/hello6.txt&quot;)); // 3 关闭资源 fs.close(); } 文件详情查看，查看文件名称、权限、长度、块信息 @Test public void testListFiles() throws IOException, InterruptedException, URISyntaxException{ // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while(listFiles.hasNext()){ LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 组 System.out.println(status.getGroup()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) { // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) { System.out.println(host); } System.out.println(&quot;----------------分割线-----------------&quot;); } } 判断文件和文件夹 @Test public void testListStatus() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件配置信息 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 判断是文件还是文件夹 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); for (FileStatus fileStatus : listStatus) { // 如果是文件 if (fileStatus.isFile()) { System.out.println(&quot;f:&quot;+fileStatus.getPath().getName()); }else { System.out.println(&quot;d:&quot;+fileStatus.getPath().getName()); } } // 3 关闭资源 fs.close(); } 使用hdfs的IO流实现文件上传 @Test public void testPut() throws Exception{ //获取hdfs的客户端 FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), new Configuration(), &quot;testuser&quot;); //创建输入流 FileInputStream fis = new FileInputStream(&quot;h://hello.txt&quot;); //创建输出流 FSDataOutputStream fos = fileSystem.create(new Path(&quot;/999/dianda/hello.txt&quot;)); //流的拷贝 IOUtils.copyBytes(fis,fos,new Configuration()); //关闭资源 fileSystem.close(); System.out.println(&quot;over&quot;); } 程序运行报错 Exception in thread &quot;main&quot; java.lang.UnsatisfiedLinkError:org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(II[BI[BIILjava/lang/String;JZ)V at org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(Native Method) at org.apache.hadoop.util.NativeCrc32.calculateChunkedSumsByteArray(NativeCrc32.java:86) at org.apache.hadoop.util.DataChecksum.calculateChunkedSums(DataChecksum.java:430) at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:202) ............. 需要选择正确的版本并且在 Hadoop\bin和 C：\windows\system32 上添加/替换 ‘hadoop.dll’和‘winutils.exe 注意如果这个hadoop.dll的版本要和hadoop的一致，可以稍微高一点，低了可能就会报这个异常 下载的路径：https://github.com/steveloughran/winutils 使用HDFS的IO流实现文件下载 // 文件下载 @Test public void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hello1.txt&quot;)); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hello1.txt&quot;)); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); fs.close(); } 定位文件读取 1.下载第一块 @Test public void readFileSeek1() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;)); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++){ fis.read(buf); fos.write(buf); } // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } 下载第二块 @Test public void readFileSeek2() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;)); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } 合并文件 在window命令窗口中执行 type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1. HDFS写数据流程 1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 2）NameNode返回是否可以上传。 3）客户端请求第一个 block上传到哪几个datanode服务器上。 4）NameNode返回3个datanode节点，分别为dn1、dn2、dn3。 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 8）当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步）。 网络拓扑 节点距离：两个节点到达最近的共同祖先的距离总和。 Hadoop2.7.2副本节点选择 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本和第一个副本位于相同机架，随机节点。 第三个副本位于不同机架，随机节点。 HDFS读数据流程 1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。 2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。 3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以packet为单位来做校验）。 4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。 NameNode和SecondaryNameNode 工作机制 第一阶段：NameNode启动 （1）第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。 （2）客户端对元数据进行增删改的请求。 （3）NameNode记录操作日志，更新滚动日志。 （4）NameNode在内存中对数据进行增删改查。 第二阶段：Secondary NameNode工作 （1）Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。 （2）Secondary NameNode请求执行checkpoint。 （3）NameNode滚动正在写的edits日志。 （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。 （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 （6）生成新的镜像文件fsimage.chkpoint。 （7）拷贝fsimage.chkpoint到NameNode。 （8）NameNode将fsimage.chkpoint重新命名成fsimage。 NN和2NN工作机制详解： Fsimage：namenode内存中元数据序列化后形成的文件。 Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。 namenode启动时，先滚动edits并生成一个空的edits.inprogress，然后加载edits和fsimage到内存中，此时namenode内存就持有最新的元数据信息。client开始对namenode发送元数据的增删改查的请求，这些请求的操作首先会被记录的edits.inprogress中（查询元数据的操作不会被记录在edits中，因为查询操作不会更改元数据信息），如果此时namenode挂掉，重启后会从edits中读取元数据的信息。然后，namenode会在内存中执行元数据的增删改查的操作。 由于edits中记录的操作会越来越多，edits文件会越来越大，导致namenode在启动加载edits时会很慢，所以需要对edits和fsimage进行合并（所谓合并，就是将edits和fsimage加载到内存中，照着edits中的操作一步步执行，最终形成新的fsimage）。secondarynamenode的作用就是帮助namenode进行edits和fsimage的合并工作。 secondarynamenode首先会询问namenode是否需要checkpoint（触发checkpoint需要满足两个条件中的任意一个，定时时间到和edits中数据写满了）。直接带回namenode是否检查结果。secondarynamenode执行checkpoint操作，首先会让namenode滚动edits并生成一个空的edits.inprogress，滚动edits的目的是给edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的edits和fsimage会拷贝到secondarynamenode的本地，然后将拷贝的edits和fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给namenode，重命名为fsimage后替换掉原来的fsimage。namenode在启动时就只需要加载之前未合并的edits和fsimage即可，因为合并过的edits中的元数据信息已经被记录在fsimage中。 Fsimage和Edits namenode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件 fsimage_0000000000000000000 fsimage_0000000000000000000.md5 seen_txid VERSION 1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。 2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。 3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字 4）每次NameNode启动的时候都会将fsimage文件读入内存，并edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将fsimage和edits文件进行了合并。 oiv查看fsimage文件 基本语法 hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径 [testuser@hadoop102 current]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/name/current [testuser@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml [testuser@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml fsimage.xml &lt;inode&gt; &lt;id&gt;16386&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;user&lt;/name&gt; &lt;mtime&gt;1512722284477&lt;/mtime&gt; &lt;permission&gt;testuser:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt; &lt;/inode&gt; &lt;inode&gt; &lt;id&gt;16387&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;testuser&lt;/name&gt; &lt;mtime&gt;1512790549080&lt;/mtime&gt; &lt;permission&gt;testuser:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt; &lt;/inode&gt; &lt;inode&gt; &lt;id&gt;16389&lt;/id&gt; &lt;type&gt;FILE&lt;/type&gt; &lt;name&gt;wc.input&lt;/name&gt; &lt;replication&gt;3&lt;/replication&gt; &lt;mtime&gt;1512722322219&lt;/mtime&gt; &lt;atime&gt;1512722321610&lt;/atime&gt; &lt;perferredBlockSize&gt;134217728&lt;/perferredBlockSize&gt; &lt;permission&gt;testuser:supergroup:rw-r--r--&lt;/permission&gt; &lt;blocks&gt; &lt;block&gt; &lt;id&gt;1073741825&lt;/id&gt; &lt;genstamp&gt;1001&lt;/genstamp&gt; &lt;numBytes&gt;59&lt;/numBytes&gt; &lt;/block&gt; &lt;/blocks&gt; &lt;/inode &gt; 可以看出，fsimage中没有记录块所对应datanode， oev查看edits文件 基本语法 hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径 [testuser@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml [testuser@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml edits.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;EDITS&gt; &lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;129&lt;/TXID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;130&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;16407&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943607866&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_-1544295051_1&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;192.168.1.5&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;testuser&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;RPC_CLIENTID&gt;908eafd4-9aec-4288-96f1-e8011d181561&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ALLOCATE_BLOCK_ID&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;131&lt;/TXID&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_SET_GENSTAMP_V2&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;132&lt;/TXID&gt; &lt;GENSTAMPV2&gt;1016&lt;/GENSTAMPV2&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD_BLOCK&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;133&lt;/TXID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;0&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;RPC_CLIENTID&gt;&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;-2&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_CLOSE&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;134&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;0&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943608761&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;false&lt;/OVERWRITE&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;25&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;testuser&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;/EDITS &gt; checkpoint时间设置 1）通常情况下，SecondaryNameNode每隔一小时执行一次，默认配置在[hdfs-default.xml]文件中，用户如果修改在hdfs-site中修改 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt; &lt;/property &gt; 2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作动作次数&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt; &lt;/property &gt; NameNode故障处理 NameNode故障后，可以采用如下两种方法恢复数据。 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录； 1. kill -9 namenode进程 2. 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [testuser@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* 3. 拷贝SecondaryNameNode中数据到原NameNode存储数据目录 [testuser@hadoop102 dfs]$ scp -r testuser@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/ 4. 重新启动namenode [testuser@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。 1.修改hdfs-site.xml中的 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; 2. kill -9 namenode进程 3. 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [testuser@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* 4. 如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 [testuser@hadoop102 dfs]$ scp -r testuser@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./ [testuser@hadoop102 namesecondary]$ rm -rf in_use.lock [testuser@hadoop102 dfs]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs [testuser@hadoop102 dfs]$ ls data name namesecondary 5. 导入检查点数据（等待一会ctrl+c结束掉） [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint 6. 启动namenode [testuser@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 集群安全模式 NameNode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作（合并fsimage和edits）。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。但是此刻，NameNode运行在安全模式，即NameNode的文件系统对于客户端来说是只读的。 系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。 基本语法 bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态，监控安全模式） 案例： （1）先进入安全模式 [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter （2）执行下面的脚本，编辑一个脚本 #!/bin/bash bin/hdfs dfsadmin -safemode wait bin/hdfs dfs -put ~/hello.txt /root/hello.txt （3）再打开一个窗口，执行 [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave NameNode多目录配置 NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性 配置如下: （1）在hdfs-site.xml文件中增加如下内容 &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///${hadoop.tmp.dir}/dfs/name1,file:///${hadoop.tmp.dir}/dfs/name2&lt;/value&gt; &lt;/property&gt; （2）停止集群，删除data和logs中所有数据。 [testuser@hadoop102 hadoop-2.7.2]$ stop-dfs.sh [testuser@hadoop102 hadoop-2.7.2]$ rm -rf data/ logs/ [testuser@hadoop103 hadoop-2.7.2]$ rm -rf data/ logs/ [testuser@hadoop104 hadoop-2.7.2]$ rm -rf data/ logs/ （3）格式化集群并启动。 [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format [testuser@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh （4）查看结果 [testuser@hadoop102 dfs]$ ll 总用量 12 drwx------. 3 testuser testuser 4096 12月 11 08:03 data drwxrwxr-x. 3 testuser testuser 4096 12月 11 08:03 name1 drwxrwxr-x. 3 testuser testuser 4096 12月 11 08:03 name2 DataNode 工作机制 1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。 3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。 4）集群运行中可以安全加入和退出一些节点。 数据完整性 1）当DataNode读取block的时候，它会计算checksum。 2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。 3）client读取其他DataNode上的block。 4）datanode在其文件创建后周期验证checksum， 掉线时限参数设置 DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： timeout = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。 而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。 &lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt; dfs.heartbeat.interval &lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; 服役新数据节点 随着业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点 环境准备 （1）克隆一台虚拟机 （2）修改ip地址和主机名称 （3）修改xsync文件，增加新增节点的ssh无密登录配置 （4）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data） 服役新节点具体步骤 （1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件白名单名字可以随便写，只要绝对路径指向正确既可以[testuser@hadoop105 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [testuser@hadoop105 hadoop]$ touch dfs.hosts [testuser@hadoop105 hadoop]$ vi dfs.hosts 添加如下主机名称（包含新服役的节点） hadoop102 hadoop103 hadoop104 hadoop105 （2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts属性&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt; &lt;/property&gt; （3）刷新namenode [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful （4）更新resourcemanager节点[testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 18/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 （5）在NameNode的slaves文件中增加新主机名称 hadoop105 （6）单独命令启动新的数据节点和节点管理器[testuser@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-testuser-datanode-hadoop105.out [testuser@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-testuser-nodemanager-hadoop105.out （7）在web浏览器上检查是否启动 退役旧数据节点 在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件[testuser@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [testuser@hadoop102 hadoop]$ touch dfs.hosts.exclude [testuser@hadoop102 hadoop]$ vi dfs.hosts.exclude 2．在namenode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt; &lt;/property&gt; 3．刷新namenode、刷新resourcemanager[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 18/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 4．检查web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点 5．等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役 [testuser@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode stopping datanode [testuser@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager stopping nodemanager 6．从include文件中删除退役节点，再运行刷新节点的命令 （1）从namenode的dfs.hosts文件中删除退役节点hadoop105 （2）刷新namenode，刷新resourcemanager[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 18/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 7．从namenode的slave文件中删除退役节点hadoop105 8．如果数据不均衡，可以用命令实现集群的再平衡[testuser@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-testuser-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved Datanode多目录配置 1．datanode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本 2．具体配置如下 hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2&lt;/value&gt; &lt;/property&gt; hadoop集群之间的递归数据复制 [testuser@hadoop102 hadoop-2.7.2]$ bin/hadoop discp hdfs://haoop102:9000/user/testuser/hello.txt hdfs://hadoop103:9000/user/testuser/hello.txt Hadoop存档 hdfs存储小文件弊端每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。 解决存储小文件办法之一Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件对内（对于文件本身）还是一个一个独立文件，对NameNode（对外）而言却是一个整体，减少了NameNode的内存。 例： （1）需要启动yarn进程（存档相当于走的是mapreduce程序） [testuser@hadoop102 hadoop-2.7.2]$ start-yarn.sh （2）归档文件 把/user/atguigu目录里面的所有文件归档成一个叫myhar.har的归档文件，并把归档后文件存储到/user/my路径下。 [testuser@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName myhar.har -p /user/testuser /user/myhar.har （3）查看归档 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/my/myhar.har [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/my/myhar.har （4）解归档文件 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/my/myhar.har/* /user/testuser 快照管理 快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 基本语法 （1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能） （2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用） （3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照） （4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照） （5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照） （6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有可快照目录） （7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处） （8）hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; （功能描述：删除快照） 例： （1）开启/禁用指定目录的快照功能 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/testuser/data [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/testuser/data （2）对目录创建快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/testuser/data 通过web访问hdfs://hadoop102:50070/user/testuser/data/.snapshot/s…..// 快照和源文件使用相同数据 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/testuser/data/.snapshot/ （3）指定名称创建快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/testuser/data mysnap （4）重命名快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/testuser/data/ mysnap mysnapshot （5）列出当前用户所有可快照目录 [testuser@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir （6）比较两个快照目录的不同之处 相对于后面的路径来说的 + 表示增 -表示少 [testuser@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/testuser/data/ . .snapshot/mysnapshot （7）恢复快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/testuser/input/.snapshot/mysnapshot /user 回收站 1．默认回收站，如图，默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。 2．启用回收站 修改core-site.xml，配置垃圾回收时间为1分钟。 &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 3．查看回收站 回收站在集群中的路径是删除目录下的 .Trash文件中：删除/user/testuser下的所有文件，则在该目录下新建.Trash目录，即：/user/testuser/.Trash/…. 4．修改访问垃圾回收站用户名称，进入垃圾回收站用户名称，默认是dr.who，修改为testuser用户 [core-site.xml] &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;testuser&lt;/value&gt; &lt;/property&gt; 5．通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 Trash trash = New Trash(conf); trash.moveToTrash(path); 6．恢复回收站数据 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/testuser/.Trash/Current/user/testuser/input /user/testuser/input 7．清空回收站 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -expunge yarn命令 使用yarn进行任务调度 hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /usr/atguigu/input /user/atguigu/output mr-jobhistory-daemon.sh start historyserver 启动历史服务器，在mapred-site.xml中配置 使用yarn进行任务调到" />
<meta property="og:description" content="hdfs常用命令 hdfs namenode -format 启动前先格式化文件系统 hadoop-daemon.sh start namenode|datanode 启动namenode和datanode hdfs dfs -rm -r /user/it/output 删除文件系统的文件 hdfs dfs -mkdir -p /usr/atguigu/input 在hdfs上创建文件夹 hdfs dfs -put ./wcinput/wc.input /usr/it/input 向hdfs上传文件 hdfs dfs -ls /user/atguigu/output 查看文件 hdfs dfs -cat /user/atguigu/output/part-r-00000 jps java下查看进程和命令，这是一个java命令 HDFS的优缺点 优点 1.高容错性 2.适合大数据处理，规模达到GB、TB甚至PB，文件规模可以达到百万数量级 3.数据流失访问，能保证数据的一致性 4.可构建在廉价机器上，通过多副本机制，提高可靠性 缺点 1.不适合低延时数据访问 2.无法高效对小文件进行存储（占用namenode内存、小文件的寻址时间大于读取时间） 3.不支持并发写入、文件随机修改，支持追加 HDFS的组成 Client：就是客户端。 （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储； （2）与NameNode交互，获取文件的位置信息； （3）与DataNode交互，读取或者写入数据； （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS； （5）Client可以通过一些命令来访问HDFS； NameNode：就是Master，它是一个主管、管理者。 （1）管理HDFS的名称空间； （2）管理数据块（Block）映射信息； （3）配置副本策略； （4）处理客户端读写请求。 DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 （1）存储实际的数据块； （2）执行数据块的读/写操作。 Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 （1）辅助NameNode，分担其工作量； （2）定期合并Fsimage和Edits，并推送给NameNode； （3）在紧急情况下，可辅助恢复NameNode。 HDFS文件块大小 HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。 如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB(10ms100100M/s = 100M)。默认的块大小128MB。 HFDS的Shell操作 基本语法： hadoop fs 具体命令 常用命令： （0）启动Hadoop集群（方便后续的测试） [testuser@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh [testuser@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （1）-help：输出这个命令参数 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -help rm （2）-ls: 显示目录信息 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -ls / （3）-mkdir：在hdfs上创建目录 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /sanguo/shuguo （4）-moveFromLocal从本地剪切粘贴到hdfs [testuser@hadoop102 hadoop-2.7.2]$ touch kongming.txt [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo （5）-appendToFile ：追加一个文件到已经存在的文件末尾 [testuser@hadoop102 hadoop-2.7.2]$ touch liubei.txt [testuser@hadoop102 hadoop-2.7.2]$ vi liubei.txt 输入 san gu mao lu [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt （6）-cat：显示文件内容 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /sanguo/shuguo/kongming.txt （7）-tail：显示一个文件的末尾 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /sanguo/shuguo/kongming.txt （8）-chgrp 、-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -chown testuser:testuser /sanguo/shuguo/kongming.txt （9）-copyFromLocal：从本地文件系统中拷贝文件到hdfs路径去 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt / （10）-copyToLocal：从hdfs拷贝到本地 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./ （11）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt （12）-mv：在hdfs目录中移动文件 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/ （13）-get：等同于copyToLocal，就是从hdfs下载文件到本地 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./ （14）-getmerge ：合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,... [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/testuser/test/* ./zaiyiqi.txt （15）-put：等同于copyFromLocal [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/testuser/test/ （16）-rm：删除文件或文件夹 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/testuser/test/jinlian2.txt （17）-rmdir：删除空目录 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test （18）-du统计文件夹的大小信息 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/testuser/test 2.7 K /user/testuser/test [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/testuser/test 1.3 K /user/testuser/test/README.txt 15 /user/testuser/test/jinlian.txt 1.4 K /user/testuser/test/zaiyiqi.txt （19）-setrep：设置hdfs中文件的副本数量 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt HDFS客户端操作 maven pom文件 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j.properties log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n HDFS的API操作 连接到hdfs，并新建文件夹 @Test public void testMkdirs() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 // configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); // FileSystem fs = FileSystem.get(configuration); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 创建目录 fs.mkdirs(new Path(&quot;/0103/test/user&quot;)); // 3 关闭资源 fs.close(); } 文件上传 @Test public void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException { // 1 获取文件系统 Configuration configuration = new Configuration(); configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 上传文件 fs.copyFromLocalFile(new Path(&quot;e:/hello.txt&quot;), new Path(&quot;/hello.txt&quot;)); // 3 关闭资源 fs.close(); System.out.println(&quot;over&quot;); } 文件下载 @Test public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 执行下载操作 // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件校验 fs.copyToLocalFile(false, new Path(&quot;/hello1.txt&quot;), new Path(&quot;e:/hello1.txt&quot;), true); // 3 关闭资源 fs.close(); } 文件夹删除 @Test public void testDelete() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 执行删除 fs.delete(new Path(&quot;/0103/&quot;), true); // 3 关闭资源 fs.close(); } 文件名更改 @Test public void testRename() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 修改文件名称 fs.rename(new Path(&quot;/hello.txt&quot;), new Path(&quot;/hello6.txt&quot;)); // 3 关闭资源 fs.close(); } 文件详情查看，查看文件名称、权限、长度、块信息 @Test public void testListFiles() throws IOException, InterruptedException, URISyntaxException{ // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while(listFiles.hasNext()){ LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 组 System.out.println(status.getGroup()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) { // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) { System.out.println(host); } System.out.println(&quot;----------------分割线-----------------&quot;); } } 判断文件和文件夹 @Test public void testListStatus() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件配置信息 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 判断是文件还是文件夹 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); for (FileStatus fileStatus : listStatus) { // 如果是文件 if (fileStatus.isFile()) { System.out.println(&quot;f:&quot;+fileStatus.getPath().getName()); }else { System.out.println(&quot;d:&quot;+fileStatus.getPath().getName()); } } // 3 关闭资源 fs.close(); } 使用hdfs的IO流实现文件上传 @Test public void testPut() throws Exception{ //获取hdfs的客户端 FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), new Configuration(), &quot;testuser&quot;); //创建输入流 FileInputStream fis = new FileInputStream(&quot;h://hello.txt&quot;); //创建输出流 FSDataOutputStream fos = fileSystem.create(new Path(&quot;/999/dianda/hello.txt&quot;)); //流的拷贝 IOUtils.copyBytes(fis,fos,new Configuration()); //关闭资源 fileSystem.close(); System.out.println(&quot;over&quot;); } 程序运行报错 Exception in thread &quot;main&quot; java.lang.UnsatisfiedLinkError:org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(II[BI[BIILjava/lang/String;JZ)V at org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(Native Method) at org.apache.hadoop.util.NativeCrc32.calculateChunkedSumsByteArray(NativeCrc32.java:86) at org.apache.hadoop.util.DataChecksum.calculateChunkedSums(DataChecksum.java:430) at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:202) ............. 需要选择正确的版本并且在 Hadoop\bin和 C：\windows\system32 上添加/替换 ‘hadoop.dll’和‘winutils.exe 注意如果这个hadoop.dll的版本要和hadoop的一致，可以稍微高一点，低了可能就会报这个异常 下载的路径：https://github.com/steveloughran/winutils 使用HDFS的IO流实现文件下载 // 文件下载 @Test public void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hello1.txt&quot;)); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hello1.txt&quot;)); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); fs.close(); } 定位文件读取 1.下载第一块 @Test public void readFileSeek1() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;)); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++){ fis.read(buf); fos.write(buf); } // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } 下载第二块 @Test public void readFileSeek2() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;)); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } 合并文件 在window命令窗口中执行 type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1. HDFS写数据流程 1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 2）NameNode返回是否可以上传。 3）客户端请求第一个 block上传到哪几个datanode服务器上。 4）NameNode返回3个datanode节点，分别为dn1、dn2、dn3。 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 8）当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步）。 网络拓扑 节点距离：两个节点到达最近的共同祖先的距离总和。 Hadoop2.7.2副本节点选择 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本和第一个副本位于相同机架，随机节点。 第三个副本位于不同机架，随机节点。 HDFS读数据流程 1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。 2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。 3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以packet为单位来做校验）。 4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。 NameNode和SecondaryNameNode 工作机制 第一阶段：NameNode启动 （1）第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。 （2）客户端对元数据进行增删改的请求。 （3）NameNode记录操作日志，更新滚动日志。 （4）NameNode在内存中对数据进行增删改查。 第二阶段：Secondary NameNode工作 （1）Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。 （2）Secondary NameNode请求执行checkpoint。 （3）NameNode滚动正在写的edits日志。 （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。 （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 （6）生成新的镜像文件fsimage.chkpoint。 （7）拷贝fsimage.chkpoint到NameNode。 （8）NameNode将fsimage.chkpoint重新命名成fsimage。 NN和2NN工作机制详解： Fsimage：namenode内存中元数据序列化后形成的文件。 Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。 namenode启动时，先滚动edits并生成一个空的edits.inprogress，然后加载edits和fsimage到内存中，此时namenode内存就持有最新的元数据信息。client开始对namenode发送元数据的增删改查的请求，这些请求的操作首先会被记录的edits.inprogress中（查询元数据的操作不会被记录在edits中，因为查询操作不会更改元数据信息），如果此时namenode挂掉，重启后会从edits中读取元数据的信息。然后，namenode会在内存中执行元数据的增删改查的操作。 由于edits中记录的操作会越来越多，edits文件会越来越大，导致namenode在启动加载edits时会很慢，所以需要对edits和fsimage进行合并（所谓合并，就是将edits和fsimage加载到内存中，照着edits中的操作一步步执行，最终形成新的fsimage）。secondarynamenode的作用就是帮助namenode进行edits和fsimage的合并工作。 secondarynamenode首先会询问namenode是否需要checkpoint（触发checkpoint需要满足两个条件中的任意一个，定时时间到和edits中数据写满了）。直接带回namenode是否检查结果。secondarynamenode执行checkpoint操作，首先会让namenode滚动edits并生成一个空的edits.inprogress，滚动edits的目的是给edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的edits和fsimage会拷贝到secondarynamenode的本地，然后将拷贝的edits和fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给namenode，重命名为fsimage后替换掉原来的fsimage。namenode在启动时就只需要加载之前未合并的edits和fsimage即可，因为合并过的edits中的元数据信息已经被记录在fsimage中。 Fsimage和Edits namenode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件 fsimage_0000000000000000000 fsimage_0000000000000000000.md5 seen_txid VERSION 1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。 2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。 3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字 4）每次NameNode启动的时候都会将fsimage文件读入内存，并edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将fsimage和edits文件进行了合并。 oiv查看fsimage文件 基本语法 hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径 [testuser@hadoop102 current]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/name/current [testuser@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml [testuser@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml fsimage.xml &lt;inode&gt; &lt;id&gt;16386&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;user&lt;/name&gt; &lt;mtime&gt;1512722284477&lt;/mtime&gt; &lt;permission&gt;testuser:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt; &lt;/inode&gt; &lt;inode&gt; &lt;id&gt;16387&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;testuser&lt;/name&gt; &lt;mtime&gt;1512790549080&lt;/mtime&gt; &lt;permission&gt;testuser:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt; &lt;/inode&gt; &lt;inode&gt; &lt;id&gt;16389&lt;/id&gt; &lt;type&gt;FILE&lt;/type&gt; &lt;name&gt;wc.input&lt;/name&gt; &lt;replication&gt;3&lt;/replication&gt; &lt;mtime&gt;1512722322219&lt;/mtime&gt; &lt;atime&gt;1512722321610&lt;/atime&gt; &lt;perferredBlockSize&gt;134217728&lt;/perferredBlockSize&gt; &lt;permission&gt;testuser:supergroup:rw-r--r--&lt;/permission&gt; &lt;blocks&gt; &lt;block&gt; &lt;id&gt;1073741825&lt;/id&gt; &lt;genstamp&gt;1001&lt;/genstamp&gt; &lt;numBytes&gt;59&lt;/numBytes&gt; &lt;/block&gt; &lt;/blocks&gt; &lt;/inode &gt; 可以看出，fsimage中没有记录块所对应datanode， oev查看edits文件 基本语法 hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径 [testuser@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml [testuser@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml edits.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;EDITS&gt; &lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;129&lt;/TXID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;130&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;16407&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943607866&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_-1544295051_1&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;192.168.1.5&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;testuser&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;RPC_CLIENTID&gt;908eafd4-9aec-4288-96f1-e8011d181561&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ALLOCATE_BLOCK_ID&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;131&lt;/TXID&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_SET_GENSTAMP_V2&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;132&lt;/TXID&gt; &lt;GENSTAMPV2&gt;1016&lt;/GENSTAMPV2&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD_BLOCK&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;133&lt;/TXID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;0&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;RPC_CLIENTID&gt;&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;-2&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_CLOSE&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;134&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;0&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943608761&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;false&lt;/OVERWRITE&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;25&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;testuser&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;/EDITS &gt; checkpoint时间设置 1）通常情况下，SecondaryNameNode每隔一小时执行一次，默认配置在[hdfs-default.xml]文件中，用户如果修改在hdfs-site中修改 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt; &lt;/property &gt; 2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作动作次数&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt; &lt;/property &gt; NameNode故障处理 NameNode故障后，可以采用如下两种方法恢复数据。 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录； 1. kill -9 namenode进程 2. 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [testuser@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* 3. 拷贝SecondaryNameNode中数据到原NameNode存储数据目录 [testuser@hadoop102 dfs]$ scp -r testuser@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/ 4. 重新启动namenode [testuser@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。 1.修改hdfs-site.xml中的 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; 2. kill -9 namenode进程 3. 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [testuser@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* 4. 如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 [testuser@hadoop102 dfs]$ scp -r testuser@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./ [testuser@hadoop102 namesecondary]$ rm -rf in_use.lock [testuser@hadoop102 dfs]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs [testuser@hadoop102 dfs]$ ls data name namesecondary 5. 导入检查点数据（等待一会ctrl+c结束掉） [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint 6. 启动namenode [testuser@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 集群安全模式 NameNode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作（合并fsimage和edits）。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。但是此刻，NameNode运行在安全模式，即NameNode的文件系统对于客户端来说是只读的。 系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。 基本语法 bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态，监控安全模式） 案例： （1）先进入安全模式 [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter （2）执行下面的脚本，编辑一个脚本 #!/bin/bash bin/hdfs dfsadmin -safemode wait bin/hdfs dfs -put ~/hello.txt /root/hello.txt （3）再打开一个窗口，执行 [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave NameNode多目录配置 NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性 配置如下: （1）在hdfs-site.xml文件中增加如下内容 &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///${hadoop.tmp.dir}/dfs/name1,file:///${hadoop.tmp.dir}/dfs/name2&lt;/value&gt; &lt;/property&gt; （2）停止集群，删除data和logs中所有数据。 [testuser@hadoop102 hadoop-2.7.2]$ stop-dfs.sh [testuser@hadoop102 hadoop-2.7.2]$ rm -rf data/ logs/ [testuser@hadoop103 hadoop-2.7.2]$ rm -rf data/ logs/ [testuser@hadoop104 hadoop-2.7.2]$ rm -rf data/ logs/ （3）格式化集群并启动。 [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format [testuser@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh （4）查看结果 [testuser@hadoop102 dfs]$ ll 总用量 12 drwx------. 3 testuser testuser 4096 12月 11 08:03 data drwxrwxr-x. 3 testuser testuser 4096 12月 11 08:03 name1 drwxrwxr-x. 3 testuser testuser 4096 12月 11 08:03 name2 DataNode 工作机制 1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。 3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。 4）集群运行中可以安全加入和退出一些节点。 数据完整性 1）当DataNode读取block的时候，它会计算checksum。 2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。 3）client读取其他DataNode上的block。 4）datanode在其文件创建后周期验证checksum， 掉线时限参数设置 DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： timeout = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。 而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。 &lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt; dfs.heartbeat.interval &lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; 服役新数据节点 随着业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点 环境准备 （1）克隆一台虚拟机 （2）修改ip地址和主机名称 （3）修改xsync文件，增加新增节点的ssh无密登录配置 （4）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data） 服役新节点具体步骤 （1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件白名单名字可以随便写，只要绝对路径指向正确既可以[testuser@hadoop105 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [testuser@hadoop105 hadoop]$ touch dfs.hosts [testuser@hadoop105 hadoop]$ vi dfs.hosts 添加如下主机名称（包含新服役的节点） hadoop102 hadoop103 hadoop104 hadoop105 （2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts属性&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt; &lt;/property&gt; （3）刷新namenode [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful （4）更新resourcemanager节点[testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 18/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 （5）在NameNode的slaves文件中增加新主机名称 hadoop105 （6）单独命令启动新的数据节点和节点管理器[testuser@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-testuser-datanode-hadoop105.out [testuser@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-testuser-nodemanager-hadoop105.out （7）在web浏览器上检查是否启动 退役旧数据节点 在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件[testuser@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [testuser@hadoop102 hadoop]$ touch dfs.hosts.exclude [testuser@hadoop102 hadoop]$ vi dfs.hosts.exclude 2．在namenode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt; &lt;/property&gt; 3．刷新namenode、刷新resourcemanager[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 18/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 4．检查web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点 5．等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役 [testuser@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode stopping datanode [testuser@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager stopping nodemanager 6．从include文件中删除退役节点，再运行刷新节点的命令 （1）从namenode的dfs.hosts文件中删除退役节点hadoop105 （2）刷新namenode，刷新resourcemanager[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 18/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 7．从namenode的slave文件中删除退役节点hadoop105 8．如果数据不均衡，可以用命令实现集群的再平衡[testuser@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-testuser-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved Datanode多目录配置 1．datanode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本 2．具体配置如下 hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2&lt;/value&gt; &lt;/property&gt; hadoop集群之间的递归数据复制 [testuser@hadoop102 hadoop-2.7.2]$ bin/hadoop discp hdfs://haoop102:9000/user/testuser/hello.txt hdfs://hadoop103:9000/user/testuser/hello.txt Hadoop存档 hdfs存储小文件弊端每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。 解决存储小文件办法之一Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件对内（对于文件本身）还是一个一个独立文件，对NameNode（对外）而言却是一个整体，减少了NameNode的内存。 例： （1）需要启动yarn进程（存档相当于走的是mapreduce程序） [testuser@hadoop102 hadoop-2.7.2]$ start-yarn.sh （2）归档文件 把/user/atguigu目录里面的所有文件归档成一个叫myhar.har的归档文件，并把归档后文件存储到/user/my路径下。 [testuser@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName myhar.har -p /user/testuser /user/myhar.har （3）查看归档 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/my/myhar.har [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/my/myhar.har （4）解归档文件 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/my/myhar.har/* /user/testuser 快照管理 快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 基本语法 （1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能） （2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用） （3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照） （4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照） （5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照） （6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有可快照目录） （7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处） （8）hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; （功能描述：删除快照） 例： （1）开启/禁用指定目录的快照功能 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/testuser/data [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/testuser/data （2）对目录创建快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/testuser/data 通过web访问hdfs://hadoop102:50070/user/testuser/data/.snapshot/s…..// 快照和源文件使用相同数据 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/testuser/data/.snapshot/ （3）指定名称创建快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/testuser/data mysnap （4）重命名快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/testuser/data/ mysnap mysnapshot （5）列出当前用户所有可快照目录 [testuser@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir （6）比较两个快照目录的不同之处 相对于后面的路径来说的 + 表示增 -表示少 [testuser@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/testuser/data/ . .snapshot/mysnapshot （7）恢复快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/testuser/input/.snapshot/mysnapshot /user 回收站 1．默认回收站，如图，默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。 2．启用回收站 修改core-site.xml，配置垃圾回收时间为1分钟。 &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 3．查看回收站 回收站在集群中的路径是删除目录下的 .Trash文件中：删除/user/testuser下的所有文件，则在该目录下新建.Trash目录，即：/user/testuser/.Trash/…. 4．修改访问垃圾回收站用户名称，进入垃圾回收站用户名称，默认是dr.who，修改为testuser用户 [core-site.xml] &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;testuser&lt;/value&gt; &lt;/property&gt; 5．通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 Trash trash = New Trash(conf); trash.moveToTrash(path); 6．恢复回收站数据 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/testuser/.Trash/Current/user/testuser/input /user/testuser/input 7．清空回收站 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -expunge yarn命令 使用yarn进行任务调度 hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /usr/atguigu/input /user/atguigu/output mr-jobhistory-daemon.sh start historyserver 启动历史服务器，在mapred-site.xml中配置 使用yarn进行任务调到" />
<link rel="canonical" href="https://uzzz.org/2019/08/06/794824.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/06/794824.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-06T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"hdfs常用命令 hdfs namenode -format 启动前先格式化文件系统 hadoop-daemon.sh start namenode|datanode 启动namenode和datanode hdfs dfs -rm -r /user/it/output 删除文件系统的文件 hdfs dfs -mkdir -p /usr/atguigu/input 在hdfs上创建文件夹 hdfs dfs -put ./wcinput/wc.input /usr/it/input 向hdfs上传文件 hdfs dfs -ls /user/atguigu/output 查看文件 hdfs dfs -cat /user/atguigu/output/part-r-00000 jps java下查看进程和命令，这是一个java命令 HDFS的优缺点 优点 1.高容错性 2.适合大数据处理，规模达到GB、TB甚至PB，文件规模可以达到百万数量级 3.数据流失访问，能保证数据的一致性 4.可构建在廉价机器上，通过多副本机制，提高可靠性 缺点 1.不适合低延时数据访问 2.无法高效对小文件进行存储（占用namenode内存、小文件的寻址时间大于读取时间） 3.不支持并发写入、文件随机修改，支持追加 HDFS的组成 Client：就是客户端。 （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储； （2）与NameNode交互，获取文件的位置信息； （3）与DataNode交互，读取或者写入数据； （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS； （5）Client可以通过一些命令来访问HDFS； NameNode：就是Master，它是一个主管、管理者。 （1）管理HDFS的名称空间； （2）管理数据块（Block）映射信息； （3）配置副本策略； （4）处理客户端读写请求。 DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。 （1）存储实际的数据块； （2）执行数据块的读/写操作。 Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。 （1）辅助NameNode，分担其工作量； （2）定期合并Fsimage和Edits，并推送给NameNode； （3）在紧急情况下，可辅助恢复NameNode。 HDFS文件块大小 HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。 如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB(10ms100100M/s = 100M)。默认的块大小128MB。 HFDS的Shell操作 基本语法： hadoop fs 具体命令 常用命令： （0）启动Hadoop集群（方便后续的测试） [testuser@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh [testuser@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh （1）-help：输出这个命令参数 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -help rm （2）-ls: 显示目录信息 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -ls / （3）-mkdir：在hdfs上创建目录 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /sanguo/shuguo （4）-moveFromLocal从本地剪切粘贴到hdfs [testuser@hadoop102 hadoop-2.7.2]$ touch kongming.txt [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo （5）-appendToFile ：追加一个文件到已经存在的文件末尾 [testuser@hadoop102 hadoop-2.7.2]$ touch liubei.txt [testuser@hadoop102 hadoop-2.7.2]$ vi liubei.txt 输入 san gu mao lu [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt （6）-cat：显示文件内容 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /sanguo/shuguo/kongming.txt （7）-tail：显示一个文件的末尾 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /sanguo/shuguo/kongming.txt （8）-chgrp 、-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -chown testuser:testuser /sanguo/shuguo/kongming.txt （9）-copyFromLocal：从本地文件系统中拷贝文件到hdfs路径去 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt / （10）-copyToLocal：从hdfs拷贝到本地 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./ （11）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt （12）-mv：在hdfs目录中移动文件 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/ （13）-get：等同于copyToLocal，就是从hdfs下载文件到本地 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./ （14）-getmerge ：合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,... [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/testuser/test/* ./zaiyiqi.txt （15）-put：等同于copyFromLocal [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/testuser/test/ （16）-rm：删除文件或文件夹 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/testuser/test/jinlian2.txt （17）-rmdir：删除空目录 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test （18）-du统计文件夹的大小信息 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/testuser/test 2.7 K /user/testuser/test [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/testuser/test 1.3 K /user/testuser/test/README.txt 15 /user/testuser/test/jinlian.txt 1.4 K /user/testuser/test/zaiyiqi.txt （19）-setrep：设置hdfs中文件的副本数量 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt HDFS客户端操作 maven pom文件 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; log4j.properties log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n HDFS的API操作 连接到hdfs，并新建文件夹 @Test public void testMkdirs() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 // configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); // FileSystem fs = FileSystem.get(configuration); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 创建目录 fs.mkdirs(new Path(&quot;/0103/test/user&quot;)); // 3 关闭资源 fs.close(); } 文件上传 @Test public void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException { // 1 获取文件系统 Configuration configuration = new Configuration(); configuration.set(&quot;dfs.replication&quot;, &quot;2&quot;); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 上传文件 fs.copyFromLocalFile(new Path(&quot;e:/hello.txt&quot;), new Path(&quot;/hello.txt&quot;)); // 3 关闭资源 fs.close(); System.out.println(&quot;over&quot;); } 文件下载 @Test public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 执行下载操作 // boolean delSrc 指是否将原文件删除 // Path src 指要下载的文件路径 // Path dst 指将文件下载到的路径 // boolean useRawLocalFileSystem 是否开启文件校验 fs.copyToLocalFile(false, new Path(&quot;/hello1.txt&quot;), new Path(&quot;e:/hello1.txt&quot;), true); // 3 关闭资源 fs.close(); } 文件夹删除 @Test public void testDelete() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 执行删除 fs.delete(new Path(&quot;/0103/&quot;), true); // 3 关闭资源 fs.close(); } 文件名更改 @Test public void testRename() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 修改文件名称 fs.rename(new Path(&quot;/hello.txt&quot;), new Path(&quot;/hello6.txt&quot;)); // 3 关闭资源 fs.close(); } 文件详情查看，查看文件名称、权限、长度、块信息 @Test public void testListFiles() throws IOException, InterruptedException, URISyntaxException{ // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while(listFiles.hasNext()){ LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 组 System.out.println(status.getGroup()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) { // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) { System.out.println(host); } System.out.println(&quot;----------------分割线-----------------&quot;); } } 判断文件和文件夹 @Test public void testListStatus() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件配置信息 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 判断是文件还是文件夹 FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); for (FileStatus fileStatus : listStatus) { // 如果是文件 if (fileStatus.isFile()) { System.out.println(&quot;f:&quot;+fileStatus.getPath().getName()); }else { System.out.println(&quot;d:&quot;+fileStatus.getPath().getName()); } } // 3 关闭资源 fs.close(); } 使用hdfs的IO流实现文件上传 @Test public void testPut() throws Exception{ //获取hdfs的客户端 FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), new Configuration(), &quot;testuser&quot;); //创建输入流 FileInputStream fis = new FileInputStream(&quot;h://hello.txt&quot;); //创建输出流 FSDataOutputStream fos = fileSystem.create(new Path(&quot;/999/dianda/hello.txt&quot;)); //流的拷贝 IOUtils.copyBytes(fis,fos,new Configuration()); //关闭资源 fileSystem.close(); System.out.println(&quot;over&quot;); } 程序运行报错 Exception in thread &quot;main&quot; java.lang.UnsatisfiedLinkError:org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(II[BI[BIILjava/lang/String;JZ)V at org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(Native Method) at org.apache.hadoop.util.NativeCrc32.calculateChunkedSumsByteArray(NativeCrc32.java:86) at org.apache.hadoop.util.DataChecksum.calculateChunkedSums(DataChecksum.java:430) at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:202) ............. 需要选择正确的版本并且在 Hadoop\\bin和 C：\\windows\\system32 上添加/替换 ‘hadoop.dll’和‘winutils.exe 注意如果这个hadoop.dll的版本要和hadoop的一致，可以稍微高一点，低了可能就会报这个异常 下载的路径：https://github.com/steveloughran/winutils 使用HDFS的IO流实现文件下载 // 文件下载 @Test public void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hello1.txt&quot;)); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hello1.txt&quot;)); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); fs.close(); } 定位文件读取 1.下载第一块 @Test public void readFileSeek1() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;)); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++){ fis.read(buf); fos.write(buf); } // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } 下载第二块 @Test public void readFileSeek2() throws IOException, InterruptedException, URISyntaxException{ // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;testuser&quot;); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;)); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); } 合并文件 在window命令窗口中执行 type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1. HDFS写数据流程 1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 2）NameNode返回是否可以上传。 3）客户端请求第一个 block上传到哪几个datanode服务器上。 4）NameNode返回3个datanode节点，分别为dn1、dn2、dn3。 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 8）当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步）。 网络拓扑 节点距离：两个节点到达最近的共同祖先的距离总和。 Hadoop2.7.2副本节点选择 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本和第一个副本位于相同机架，随机节点。 第三个副本位于不同机架，随机节点。 HDFS读数据流程 1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。 2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。 3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以packet为单位来做校验）。 4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。 NameNode和SecondaryNameNode 工作机制 第一阶段：NameNode启动 （1）第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。 （2）客户端对元数据进行增删改的请求。 （3）NameNode记录操作日志，更新滚动日志。 （4）NameNode在内存中对数据进行增删改查。 第二阶段：Secondary NameNode工作 （1）Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。 （2）Secondary NameNode请求执行checkpoint。 （3）NameNode滚动正在写的edits日志。 （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。 （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 （6）生成新的镜像文件fsimage.chkpoint。 （7）拷贝fsimage.chkpoint到NameNode。 （8）NameNode将fsimage.chkpoint重新命名成fsimage。 NN和2NN工作机制详解： Fsimage：namenode内存中元数据序列化后形成的文件。 Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。 namenode启动时，先滚动edits并生成一个空的edits.inprogress，然后加载edits和fsimage到内存中，此时namenode内存就持有最新的元数据信息。client开始对namenode发送元数据的增删改查的请求，这些请求的操作首先会被记录的edits.inprogress中（查询元数据的操作不会被记录在edits中，因为查询操作不会更改元数据信息），如果此时namenode挂掉，重启后会从edits中读取元数据的信息。然后，namenode会在内存中执行元数据的增删改查的操作。 由于edits中记录的操作会越来越多，edits文件会越来越大，导致namenode在启动加载edits时会很慢，所以需要对edits和fsimage进行合并（所谓合并，就是将edits和fsimage加载到内存中，照着edits中的操作一步步执行，最终形成新的fsimage）。secondarynamenode的作用就是帮助namenode进行edits和fsimage的合并工作。 secondarynamenode首先会询问namenode是否需要checkpoint（触发checkpoint需要满足两个条件中的任意一个，定时时间到和edits中数据写满了）。直接带回namenode是否检查结果。secondarynamenode执行checkpoint操作，首先会让namenode滚动edits并生成一个空的edits.inprogress，滚动edits的目的是给edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的edits和fsimage会拷贝到secondarynamenode的本地，然后将拷贝的edits和fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给namenode，重命名为fsimage后替换掉原来的fsimage。namenode在启动时就只需要加载之前未合并的edits和fsimage即可，因为合并过的edits中的元数据信息已经被记录在fsimage中。 Fsimage和Edits namenode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件 fsimage_0000000000000000000 fsimage_0000000000000000000.md5 seen_txid VERSION 1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。 2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。 3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字 4）每次NameNode启动的时候都会将fsimage文件读入内存，并edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将fsimage和edits文件进行了合并。 oiv查看fsimage文件 基本语法 hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径 [testuser@hadoop102 current]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs/name/current [testuser@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml [testuser@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml fsimage.xml &lt;inode&gt; &lt;id&gt;16386&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;user&lt;/name&gt; &lt;mtime&gt;1512722284477&lt;/mtime&gt; &lt;permission&gt;testuser:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt; &lt;/inode&gt; &lt;inode&gt; &lt;id&gt;16387&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;testuser&lt;/name&gt; &lt;mtime&gt;1512790549080&lt;/mtime&gt; &lt;permission&gt;testuser:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt; &lt;/inode&gt; &lt;inode&gt; &lt;id&gt;16389&lt;/id&gt; &lt;type&gt;FILE&lt;/type&gt; &lt;name&gt;wc.input&lt;/name&gt; &lt;replication&gt;3&lt;/replication&gt; &lt;mtime&gt;1512722322219&lt;/mtime&gt; &lt;atime&gt;1512722321610&lt;/atime&gt; &lt;perferredBlockSize&gt;134217728&lt;/perferredBlockSize&gt; &lt;permission&gt;testuser:supergroup:rw-r--r--&lt;/permission&gt; &lt;blocks&gt; &lt;block&gt; &lt;id&gt;1073741825&lt;/id&gt; &lt;genstamp&gt;1001&lt;/genstamp&gt; &lt;numBytes&gt;59&lt;/numBytes&gt; &lt;/block&gt; &lt;/blocks&gt; &lt;/inode &gt; 可以看出，fsimage中没有记录块所对应datanode， oev查看edits文件 基本语法 hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径 [testuser@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml [testuser@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml edits.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;EDITS&gt; &lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;129&lt;/TXID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;130&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;16407&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943607866&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;DFSClient_NONMAPREDUCE_-1544295051_1&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;192.168.1.5&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;true&lt;/OVERWRITE&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;testuser&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;RPC_CLIENTID&gt;908eafd4-9aec-4288-96f1-e8011d181561&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;0&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ALLOCATE_BLOCK_ID&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;131&lt;/TXID&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_SET_GENSTAMP_V2&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;132&lt;/TXID&gt; &lt;GENSTAMPV2&gt;1016&lt;/GENSTAMPV2&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_ADD_BLOCK&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;133&lt;/TXID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;0&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;RPC_CLIENTID&gt;&lt;/RPC_CLIENTID&gt; &lt;RPC_CALLID&gt;-2&lt;/RPC_CALLID&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_CLOSE&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;134&lt;/TXID&gt; &lt;LENGTH&gt;0&lt;/LENGTH&gt; &lt;INODEID&gt;0&lt;/INODEID&gt; &lt;PATH&gt;/hello7.txt&lt;/PATH&gt; &lt;REPLICATION&gt;2&lt;/REPLICATION&gt; &lt;MTIME&gt;1512943608761&lt;/MTIME&gt; &lt;ATIME&gt;1512943607866&lt;/ATIME&gt; &lt;BLOCKSIZE&gt;134217728&lt;/BLOCKSIZE&gt; &lt;CLIENT_NAME&gt;&lt;/CLIENT_NAME&gt; &lt;CLIENT_MACHINE&gt;&lt;/CLIENT_MACHINE&gt; &lt;OVERWRITE&gt;false&lt;/OVERWRITE&gt; &lt;BLOCK&gt; &lt;BLOCK_ID&gt;1073741839&lt;/BLOCK_ID&gt; &lt;NUM_BYTES&gt;25&lt;/NUM_BYTES&gt; &lt;GENSTAMP&gt;1016&lt;/GENSTAMP&gt; &lt;/BLOCK&gt; &lt;PERMISSION_STATUS&gt; &lt;USERNAME&gt;testuser&lt;/USERNAME&gt; &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt; &lt;MODE&gt;420&lt;/MODE&gt; &lt;/PERMISSION_STATUS&gt; &lt;/DATA&gt; &lt;/RECORD&gt; &lt;/EDITS &gt; checkpoint时间设置 1）通常情况下，SecondaryNameNode每隔一小时执行一次，默认配置在[hdfs-default.xml]文件中，用户如果修改在hdfs-site中修改 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt; &lt;/property &gt; 2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作动作次数&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt; &lt;/property &gt; NameNode故障处理 NameNode故障后，可以采用如下两种方法恢复数据。 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录； 1. kill -9 namenode进程 2. 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [testuser@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* 3. 拷贝SecondaryNameNode中数据到原NameNode存储数据目录 [testuser@hadoop102 dfs]$ scp -r testuser@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/ 4. 重新启动namenode [testuser@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。 1.修改hdfs-site.xml中的 &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; 2. kill -9 namenode进程 3. 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） [testuser@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/* 4. 如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 [testuser@hadoop102 dfs]$ scp -r testuser@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./ [testuser@hadoop102 namesecondary]$ rm -rf in_use.lock [testuser@hadoop102 dfs]$ pwd /opt/module/hadoop-2.7.2/data/tmp/dfs [testuser@hadoop102 dfs]$ ls data name namesecondary 5. 导入检查点数据（等待一会ctrl+c结束掉） [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint 6. 启动namenode [testuser@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode 集群安全模式 NameNode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作（合并fsimage和edits）。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。但是此刻，NameNode运行在安全模式，即NameNode的文件系统对于客户端来说是只读的。 系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。 基本语法 bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态，监控安全模式） 案例： （1）先进入安全模式 [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter （2）执行下面的脚本，编辑一个脚本 #!/bin/bash bin/hdfs dfsadmin -safemode wait bin/hdfs dfs -put ~/hello.txt /root/hello.txt （3）再打开一个窗口，执行 [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave NameNode多目录配置 NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性 配置如下: （1）在hdfs-site.xml文件中增加如下内容 &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///${hadoop.tmp.dir}/dfs/name1,file:///${hadoop.tmp.dir}/dfs/name2&lt;/value&gt; &lt;/property&gt; （2）停止集群，删除data和logs中所有数据。 [testuser@hadoop102 hadoop-2.7.2]$ stop-dfs.sh [testuser@hadoop102 hadoop-2.7.2]$ rm -rf data/ logs/ [testuser@hadoop103 hadoop-2.7.2]$ rm -rf data/ logs/ [testuser@hadoop104 hadoop-2.7.2]$ rm -rf data/ logs/ （3）格式化集群并启动。 [testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format [testuser@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh （4）查看结果 [testuser@hadoop102 dfs]$ ll 总用量 12 drwx------. 3 testuser testuser 4096 12月 11 08:03 data drwxrwxr-x. 3 testuser testuser 4096 12月 11 08:03 name1 drwxrwxr-x. 3 testuser testuser 4096 12月 11 08:03 name2 DataNode 工作机制 1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。 3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。 4）集群运行中可以安全加入和退出一些节点。 数据完整性 1）当DataNode读取block的时候，它会计算checksum。 2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。 3）client读取其他DataNode上的block。 4）datanode在其文件创建后周期验证checksum， 掉线时限参数设置 DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： timeout = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。 而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。 &lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt; dfs.heartbeat.interval &lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; 服役新数据节点 随着业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点 环境准备 （1）克隆一台虚拟机 （2）修改ip地址和主机名称 （3）修改xsync文件，增加新增节点的ssh无密登录配置 （4）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data） 服役新节点具体步骤 （1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件白名单名字可以随便写，只要绝对路径指向正确既可以[testuser@hadoop105 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [testuser@hadoop105 hadoop]$ touch dfs.hosts [testuser@hadoop105 hadoop]$ vi dfs.hosts 添加如下主机名称（包含新服役的节点） hadoop102 hadoop103 hadoop104 hadoop105 （2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts属性&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt; &lt;/property&gt; （3）刷新namenode [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful （4）更新resourcemanager节点[testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 18/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 （5）在NameNode的slaves文件中增加新主机名称 hadoop105 （6）单独命令启动新的数据节点和节点管理器[testuser@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-testuser-datanode-hadoop105.out [testuser@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-testuser-nodemanager-hadoop105.out （7）在web浏览器上检查是否启动 退役旧数据节点 在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件[testuser@hadoop102 hadoop]$ pwd /opt/module/hadoop-2.7.2/etc/hadoop [testuser@hadoop102 hadoop]$ touch dfs.hosts.exclude [testuser@hadoop102 hadoop]$ vi dfs.hosts.exclude 2．在namenode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt; &lt;/property&gt; 3．刷新namenode、刷新resourcemanager[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 18/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 4．检查web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点 5．等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役 [testuser@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode stopping datanode [testuser@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager stopping nodemanager 6．从include文件中删除退役节点，再运行刷新节点的命令 （1）从namenode的dfs.hosts文件中删除退役节点hadoop105 （2）刷新namenode，刷新resourcemanager[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes Refresh nodes successful [testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes 18/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033 7．从namenode的slave文件中删除退役节点hadoop105 8．如果数据不均衡，可以用命令实现集群的再平衡[testuser@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-testuser-balancer-hadoop102.out Time Stamp Iteration# Bytes Already Moved Bytes Left To Move Bytes Being Moved Datanode多目录配置 1．datanode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本 2．具体配置如下 hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2&lt;/value&gt; &lt;/property&gt; hadoop集群之间的递归数据复制 [testuser@hadoop102 hadoop-2.7.2]$ bin/hadoop discp hdfs://haoop102:9000/user/testuser/hello.txt hdfs://hadoop103:9000/user/testuser/hello.txt Hadoop存档 hdfs存储小文件弊端每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。 解决存储小文件办法之一Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件对内（对于文件本身）还是一个一个独立文件，对NameNode（对外）而言却是一个整体，减少了NameNode的内存。 例： （1）需要启动yarn进程（存档相当于走的是mapreduce程序） [testuser@hadoop102 hadoop-2.7.2]$ start-yarn.sh （2）归档文件 把/user/atguigu目录里面的所有文件归档成一个叫myhar.har的归档文件，并把归档后文件存储到/user/my路径下。 [testuser@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName myhar.har -p /user/testuser /user/myhar.har （3）查看归档 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/my/myhar.har [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/my/myhar.har （4）解归档文件 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/my/myhar.har/* /user/testuser 快照管理 快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 基本语法 （1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能） （2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用） （3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照） （4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照） （5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照） （6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有可快照目录） （7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处） （8）hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; （功能描述：删除快照） 例： （1）开启/禁用指定目录的快照功能 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/testuser/data [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/testuser/data （2）对目录创建快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/testuser/data 通过web访问hdfs://hadoop102:50070/user/testuser/data/.snapshot/s…..// 快照和源文件使用相同数据 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/testuser/data/.snapshot/ （3）指定名称创建快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/testuser/data mysnap （4）重命名快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/testuser/data/ mysnap mysnapshot （5）列出当前用户所有可快照目录 [testuser@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir （6）比较两个快照目录的不同之处 相对于后面的路径来说的 + 表示增 -表示少 [testuser@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff /user/testuser/data/ . .snapshot/mysnapshot （7）恢复快照 [testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/testuser/input/.snapshot/mysnapshot /user 回收站 1．默认回收站，如图，默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。 2．启用回收站 修改core-site.xml，配置垃圾回收时间为1分钟。 &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 3．查看回收站 回收站在集群中的路径是删除目录下的 .Trash文件中：删除/user/testuser下的所有文件，则在该目录下新建.Trash目录，即：/user/testuser/.Trash/…. 4．修改访问垃圾回收站用户名称，进入垃圾回收站用户名称，默认是dr.who，修改为testuser用户 [core-site.xml] &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;testuser&lt;/value&gt; &lt;/property&gt; 5．通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 Trash trash = New Trash(conf); trash.moveToTrash(path); 6．恢复回收站数据 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/testuser/.Trash/Current/user/testuser/input /user/testuser/input 7．清空回收站 [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -expunge yarn命令 使用yarn进行任务调度 hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /usr/atguigu/input /user/atguigu/output mr-jobhistory-daemon.sh start historyserver 启动历史服务器，在mapred-site.xml中配置 使用yarn进行任务调到","@type":"BlogPosting","url":"https://uzzz.org/2019/08/06/794824.html","headline":"Hadoop之HDFS","dateModified":"2019-08-06T00:00:00+08:00","datePublished":"2019-08-06T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/06/794824.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Hadoop之HDFS</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h2><a id="hdfs_0"></a>hdfs常用命令</h2> 
  <pre><code>hdfs namenode -format  启动前先格式化文件系统
hadoop-daemon.sh start namenode|datanode 		启动namenode和datanode
hdfs dfs -rm -r /user/it/output		删除文件系统的文件
hdfs dfs -mkdir -p  /usr/atguigu/input		在hdfs上创建文件夹
hdfs dfs -put ./wcinput/wc.input /usr/it/input 		向hdfs上传文件
hdfs dfs -ls /user/atguigu/output		查看文件
hdfs dfs -cat /user/atguigu/output/part-r-00000
jps 		java下查看进程和命令，这是一个java命令
</code></pre> 
  <p><strong>HDFS的优缺点</strong></p> 
  <ul> 
   <li>优点<br> 1.高容错性<br> 2.适合大数据处理，规模达到GB、TB甚至PB，文件规模可以达到百万数量级<br> 3.数据流失访问，能保证数据的一致性<br> 4.可构建在廉价机器上，通过多副本机制，提高可靠性</li> 
   <li>缺点<br> 1.不适合低延时数据访问<br> 2.无法高效对小文件进行存储（占用namenode内存、小文件的寻址时间大于读取时间）<br> 3.不支持并发写入、文件随机修改，支持追加</li> 
  </ul> 
  <p><strong>HDFS的组成</strong><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190724104739664.png" alt="在这里插入图片描述"></p> 
  <ul> 
   <li>Client：就是客户端。<br> （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储；<br> （2）与NameNode交互，获取文件的位置信息；<br> （3）与DataNode交互，读取或者写入数据；<br> （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS；<br> （5）Client可以通过一些命令来访问HDFS；</li> 
   <li>NameNode：就是Master，它是一个主管、管理者。<br> （1）管理HDFS的名称空间；<br> （2）管理数据块（Block）映射信息；<br> （3）配置副本策略；<br> （4）处理客户端读写请求。</li> 
   <li>DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。<br> （1）存储实际的数据块；<br> （2）执行数据块的读/写操作。</li> 
   <li>Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。<br> （1）辅助NameNode，分担其工作量；<br> （2）定期合并Fsimage和Edits，并推送给NameNode；<br> （3）在紧急情况下，可辅助恢复NameNode。</li> 
  </ul> 
  <p><strong>HDFS文件块大小</strong></p> 
  <ul> 
   <li>HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，<mark>传输一个由多个块组成的文件的时间取决于磁盘传输速率</mark>。</li> 
   <li>如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB(10ms<em>100</em>100M/s = 100M)。默认的块大小128MB。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190724110452561.png" alt="在这里插入图片描述"></li> 
  </ul> 
  <h4><a id="HFDSShell_47"></a>HFDS的Shell操作</h4> 
  <p>基本语法：<br> <code>hadoop fs 具体命令</code><br> 常用命令：</p> 
  <pre><code>（0）启动Hadoop集群（方便后续的测试）
	[testuser@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh
	[testuser@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh
（1）-help：输出这个命令参数
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -help rm
（2）-ls: 显示目录信息
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /
（3）-mkdir：在hdfs上创建目录
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /sanguo/shuguo
（4）-moveFromLocal从本地剪切粘贴到hdfs
	[testuser@hadoop102 hadoop-2.7.2]$ touch kongming.txt
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs  -moveFromLocal  ./kongming.txt  /sanguo/shuguo
（5）-appendToFile  ：追加一个文件到已经存在的文件末尾
	[testuser@hadoop102 hadoop-2.7.2]$ touch liubei.txt
	[testuser@hadoop102 hadoop-2.7.2]$ vi liubei.txt
		输入
		san gu mao lu
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt
（6）-cat：显示文件内容
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /sanguo/shuguo/kongming.txt
（7）-tail：显示一个文件的末尾
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /sanguo/shuguo/kongming.txt
（8）-chgrp 、-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs  -chmod  666  /sanguo/shuguo/kongming.txt
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs  -chown  testuser:testuser   /sanguo/shuguo/kongming.txt
（9）-copyFromLocal：从本地文件系统中拷贝文件到hdfs路径去
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /
（10）-copyToLocal：从hdfs拷贝到本地
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./
（11）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt
（12）-mv：在hdfs目录中移动文件
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/
（13）-get：等同于copyToLocal，就是从hdfs下载文件到本地
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./
（14）-getmerge  ：合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,...
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/testuser/test/* ./zaiyiqi.txt
（15）-put：等同于copyFromLocal
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/testuser/test/
（16）-rm：删除文件或文件夹
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/testuser/test/jinlian2.txt
（17）-rmdir：删除空目录
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test
（18）-du统计文件夹的大小信息
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/testuser/test
		2.7 K  /user/testuser/test
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -du  -h /user/testuser/test
		1.3 K  /user/testuser/test/README.txt
		15     /user/testuser/test/jinlian.txt
		1.4 K  /user/testuser/test/zaiyiqi.txt
（19）-setrep：设置hdfs中文件的副本数量
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt
</code></pre> 
  <h2><a id="HDFS_106"></a>HDFS客户端操作</h2> 
  <p>maven pom文件</p> 
  <pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependencies</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>junit<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>RELEASE<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.logging.log4j<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>log4j-core<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.8.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>hadoop-common<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>hadoop-client<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>hadoop-hdfs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.7.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependencies</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
  <p>log4j.properties</p> 
  <pre><code>log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
</code></pre> 
  <p><strong>HDFS的API操作</strong></p> 
  <ul> 
   <li>连接到hdfs，并新建文件夹</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testMkdirs</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 配置在集群上运行</span>
	<span class="token comment">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");</span>
	<span class="token comment">// FileSystem fs = FileSystem.get(configuration);</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 2 创建目录</span>
	fs<span class="token punctuation">.</span><span class="token function">mkdirs</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/0103/test/user"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 3 关闭资源</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ul> 
   <li>文件上传</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testCopyFromLocalFile</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException <span class="token punctuation">{</span>
	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	configuration<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"dfs.replication"</span><span class="token punctuation">,</span> <span class="token string">"2"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 2 上传文件</span>
	fs<span class="token punctuation">.</span><span class="token function">copyFromLocalFile</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"e:/hello.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/hello.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 3 关闭资源</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"over"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ul> 
   <li>文件下载</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testCopyToLocalFile</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 2 执行下载操作</span>
	<span class="token comment">// boolean delSrc 指是否将原文件删除</span>
	<span class="token comment">// Path src 指要下载的文件路径</span>
	<span class="token comment">// Path dst 指将文件下载到的路径</span>
	<span class="token comment">// boolean useRawLocalFileSystem 是否开启文件校验</span>
	fs<span class="token punctuation">.</span><span class="token function">copyToLocalFile</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/hello1.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"e:/hello1.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 3 关闭资源</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ul> 
   <li>文件夹删除</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testDelete</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 2 执行删除</span>
	fs<span class="token punctuation">.</span><span class="token function">delete</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/0103/"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 3 关闭资源</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ul> 
   <li>文件名更改</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testRename</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
	<span class="token comment">// 2 修改文件名称</span>
	fs<span class="token punctuation">.</span><span class="token function">rename</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/hello.txt"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/hello6.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 3 关闭资源</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ul> 
   <li>文件详情查看，查看文件名称、权限、长度、块信息</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testListFiles</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
	<span class="token comment">// 1获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
	<span class="token comment">// 2 获取文件详情</span>
	RemoteIterator<span class="token generics function"><span class="token punctuation">&lt;</span>LocatedFileStatus<span class="token punctuation">&gt;</span></span> listFiles <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">listFiles</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">while</span><span class="token punctuation">(</span>listFiles<span class="token punctuation">.</span><span class="token function">hasNext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
		LocatedFileStatus status <span class="token operator">=</span> listFiles<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 输出详情</span>
		<span class="token comment">// 文件名称</span>
		<span class="token comment">// 长度</span>
		System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getLen</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 权限</span>
		System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getPermission</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 组</span>
		System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>status<span class="token punctuation">.</span><span class="token function">getGroup</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// 获取存储的块信息</span>
		BlockLocation<span class="token punctuation">[</span><span class="token punctuation">]</span> blockLocations <span class="token operator">=</span> status<span class="token punctuation">.</span><span class="token function">getBlockLocations</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">for</span> <span class="token punctuation">(</span>BlockLocation blockLocation <span class="token operator">:</span> blockLocations<span class="token punctuation">)</span> <span class="token punctuation">{</span>
			<span class="token comment">// 获取块存储的主机节点</span>
			String<span class="token punctuation">[</span><span class="token punctuation">]</span> hosts <span class="token operator">=</span> blockLocation<span class="token punctuation">.</span><span class="token function">getHosts</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token keyword">for</span> <span class="token punctuation">(</span>String host <span class="token operator">:</span> hosts<span class="token punctuation">)</span> <span class="token punctuation">{</span>
				System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>host<span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span>
		System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"----------------分割线-----------------"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ul> 
   <li>判断文件和文件夹</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testListStatus</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
	<span class="token comment">// 1 获取文件配置信息</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 2 判断是文件还是文件夹</span>
	FileStatus<span class="token punctuation">[</span><span class="token punctuation">]</span> listStatus <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">listStatus</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span>FileStatus fileStatus <span class="token operator">:</span> listStatus<span class="token punctuation">)</span> <span class="token punctuation">{</span>
		<span class="token comment">// 如果是文件</span>
		<span class="token keyword">if</span> <span class="token punctuation">(</span>fileStatus<span class="token punctuation">.</span><span class="token function">isFile</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
			System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"f:"</span><span class="token operator">+</span>fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{</span>
			System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"d:"</span><span class="token operator">+</span>fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
	<span class="token comment">// 3 关闭资源</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ul> 
   <li>使用hdfs的IO流实现文件上传</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testPut</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception<span class="token punctuation">{</span>
	<span class="token comment">//获取hdfs的客户端</span>
	FileSystem fileSystem <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">//创建输入流</span>
	FileInputStream fis <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FileInputStream</span><span class="token punctuation">(</span><span class="token string">"h://hello.txt"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">//创建输出流</span>
	FSDataOutputStream fos <span class="token operator">=</span> fileSystem<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/999/dianda/hello.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">//流的拷贝</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">copyBytes</span><span class="token punctuation">(</span>fis<span class="token punctuation">,</span>fos<span class="token punctuation">,</span><span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">//关闭资源</span>
	fileSystem<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"over"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <p>程序运行报错</p> 
  <pre><code>Exception in thread "main" java.lang.UnsatisfiedLinkError:org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(II[BI[BIILjava/lang/String;JZ)V
at org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(Native Method)
at org.apache.hadoop.util.NativeCrc32.calculateChunkedSumsByteArray(NativeCrc32.java:86)
at org.apache.hadoop.util.DataChecksum.calculateChunkedSums(DataChecksum.java:430)
at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:202)
.............
</code></pre> 
  <blockquote> 
   <p>需要选择正确的版本并且在 Hadoop\bin和 C：\windows\system32 上添加/替换 ‘hadoop.dll’和‘winutils.exe<br> 注意如果这个hadoop.dll的版本要和hadoop的一致，可以稍微高一点，低了可能就会报这个异常<br> 下载的路径：<a href="https://github.com/steveloughran/winutils" rel="nofollow" data-token="483b6a8c8db7e16a10a660b87b74a234">https://github.com/steveloughran/winutils</a></p> 
  </blockquote> 
  <ul> 
   <li>使用HDFS的IO流实现文件下载</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token comment">// 文件下载</span>
<span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">getFileFromHDFS</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 2 获取输入流</span>
	FSDataInputStream fis <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">open</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/hello1.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 3 获取输出流</span>
	FileOutputStream fos <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FileOutputStream</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">File</span><span class="token punctuation">(</span><span class="token string">"e:/hello1.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 4 流的对拷</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">copyBytes</span><span class="token punctuation">(</span>fis<span class="token punctuation">,</span> fos<span class="token punctuation">,</span> configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 5 关闭资源</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fis<span class="token punctuation">)</span><span class="token punctuation">;</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fos<span class="token punctuation">)</span><span class="token punctuation">;</span>
	fs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ul> 
   <li>定位文件读取<br> 1.下载第一块</li> 
  </ul> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">readFileSeek1</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 2 获取输入流</span>
	FSDataInputStream fis <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">open</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/hadoop-2.7.2.tar.gz"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 3 创建输出流</span>
	FileOutputStream fos <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FileOutputStream</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">File</span><span class="token punctuation">(</span><span class="token string">"e:/hadoop-2.7.2.tar.gz.part1"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 4 流的拷贝</span>
	<span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> buf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">byte</span><span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
	<span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span><span class="token number">0</span> <span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">128</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
		fis<span class="token punctuation">.</span><span class="token function">read</span><span class="token punctuation">(</span>buf<span class="token punctuation">)</span><span class="token punctuation">;</span>
		fos<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>buf<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>
	<span class="token comment">// 5关闭资源</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fis<span class="token punctuation">)</span><span class="token punctuation">;</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fos<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ol start="2"> 
   <li>下载第二块</li> 
  </ol> 
  <pre><code class="prism language-java"><span class="token annotation punctuation">@Test</span>
<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">readFileSeek2</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> IOException<span class="token punctuation">,</span> InterruptedException<span class="token punctuation">,</span> URISyntaxException<span class="token punctuation">{</span>
	<span class="token comment">// 1 获取文件系统</span>
	Configuration configuration <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	FileSystem fs <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">URI</span><span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> configuration<span class="token punctuation">,</span> <span class="token string">"testuser"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 2 打开输入流</span>
	FSDataInputStream fis <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">open</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/hadoop-2.7.2.tar.gz"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 3 定位输入数据位置</span>
	fis<span class="token punctuation">.</span><span class="token function">seek</span><span class="token punctuation">(</span><span class="token number">1024</span><span class="token operator">*</span><span class="token number">1024</span><span class="token operator">*</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 4 创建输出流</span>
	FileOutputStream fos <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">FileOutputStream</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">File</span><span class="token punctuation">(</span><span class="token string">"e:/hadoop-2.7.2.tar.gz.part2"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 5 流的对拷</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">copyBytes</span><span class="token punctuation">(</span>fis<span class="token punctuation">,</span> fos<span class="token punctuation">,</span> configuration<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">// 6 关闭资源</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fis<span class="token punctuation">)</span><span class="token punctuation">;</span>
	IOUtils<span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>fos<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <ol start="3"> 
   <li>合并文件<br> <code>在window命令窗口中执行 type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1</code>.</li> 
  </ol> 
  <h3><a id="HDFS_372"></a>HDFS写数据流程</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725091522950.png" alt="在这里插入图片描述"></p> 
  <blockquote> 
   <p>1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。<br> 2）NameNode返回是否可以上传。<br> 3）客户端请求第一个 block上传到哪几个datanode服务器上。<br> 4）NameNode返回3个datanode节点，分别为dn1、dn2、dn3。<br> 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。<br> 6）dn1、dn2、dn3逐级应答客户端。<br> 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。<br> 8）当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步）。</p> 
  </blockquote> 
  <p><strong>网络拓扑</strong><br> 节点距离：两个节点到达最近的共同祖先的距离总和。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725093018903.png" alt="在这里插入图片描述"><br> <strong>Hadoop2.7.2副本节点选择</strong></p> 
  <blockquote> 
   <p>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。<br> 第二个副本和第一个副本位于相同机架，随机节点。<br> 第三个副本位于不同机架，随机节点。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725093136956.png" alt="在这里插入图片描述"></p> 
  </blockquote> 
  <h3><a id="HDFS_391"></a>HDFS读数据流程</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725093908747.png" alt="在这里插入图片描述"></p> 
  <blockquote> 
   <p>1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。<br> 2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。<br> 3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以packet为单位来做校验）。<br> 4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。</p> 
  </blockquote> 
  <h2><a id="NameNodeSecondaryNameNode_398"></a>NameNode和SecondaryNameNode</h2> 
  <h4><a id="_399"></a>工作机制</h4> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725094554669.png" alt="在这里插入图片描述"></p> 
  <blockquote> 
   <p>第一阶段：NameNode启动<br> （1）第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br> （2）客户端对元数据进行增删改的请求。<br> （3）NameNode记录操作日志，更新滚动日志。<br> （4）NameNode在内存中对数据进行增删改查。<br> 第二阶段：Secondary NameNode工作<br> （1）Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。<br> （2）Secondary NameNode请求执行checkpoint。<br> （3）NameNode滚动正在写的edits日志。<br> （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。<br> （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。<br> （6）生成新的镜像文件fsimage.chkpoint。<br> （7）拷贝fsimage.chkpoint到NameNode。<br> （8）NameNode将fsimage.chkpoint重新命名成fsimage。</p> 
  </blockquote> 
  <blockquote> 
   <p>NN和2NN工作机制详解：<br> Fsimage：namenode内存中元数据序列化后形成的文件。<br> Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。<br> namenode启动时，先滚动edits并生成一个空的edits.inprogress，然后加载edits和fsimage到内存中，此时namenode内存就持有最新的元数据信息。client开始对namenode发送元数据的增删改查的请求，这些请求的操作首先会被记录的edits.inprogress中（查询元数据的操作不会被记录在edits中，因为查询操作不会更改元数据信息），如果此时namenode挂掉，重启后会从edits中读取元数据的信息。然后，namenode会在内存中执行元数据的增删改查的操作。<br> 由于edits中记录的操作会越来越多，edits文件会越来越大，导致namenode在启动加载edits时会很慢，所以需要对edits和fsimage进行合并（所谓合并，就是将edits和fsimage加载到内存中，照着edits中的操作一步步执行，最终形成新的fsimage）。secondarynamenode的作用就是帮助namenode进行edits和fsimage的合并工作。<br> secondarynamenode首先会询问namenode是否需要checkpoint（触发checkpoint需要满足两个条件中的任意一个，定时时间到和edits中数据写满了）。直接带回namenode是否检查结果。secondarynamenode执行checkpoint操作，首先会让namenode滚动edits并生成一个空的edits.inprogress，滚动edits的目的是给edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的edits和fsimage会拷贝到secondarynamenode的本地，然后将拷贝的edits和fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给namenode，重命名为fsimage后替换掉原来的fsimage。namenode在启动时就只需要加载之前未合并的edits和fsimage即可，因为合并过的edits中的元数据信息已经被记录在fsimage中。</p> 
  </blockquote> 
  <h4><a id="FsimageEdits_423"></a>Fsimage和Edits</h4> 
  <p>namenode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件</p> 
  <blockquote> 
   <p>fsimage_0000000000000000000<br> fsimage_0000000000000000000.md5<br> seen_txid<br> VERSION</p> 
  </blockquote> 
  <p>1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。<br> 2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。<br> 3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字<br> 4）每次NameNode启动的时候都会将fsimage文件读入内存，并edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将fsimage和edits文件进行了合并。</p> 
  <p><strong>oiv查看fsimage文件</strong></p> 
  <ul> 
   <li>基本语法<br> <code>hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径</code></li> 
  </ul> 
  <pre><code>[testuser@hadoop102 current]$ pwd
/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current
[testuser@hadoop102 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml
[testuser@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/fsimage.xml
</code></pre> 
  <p>fsimage.xml</p> 
  <pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>inode</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">&gt;</span></span>16386<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>type</span><span class="token punctuation">&gt;</span></span>DIRECTORY<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>type</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>user<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mtime</span><span class="token punctuation">&gt;</span></span>1512722284477<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mtime</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>permission</span><span class="token punctuation">&gt;</span></span>testuser:supergroup:rwxr-xr-x<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>permission</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>nsquota</span><span class="token punctuation">&gt;</span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>nsquota</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dsquota</span><span class="token punctuation">&gt;</span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dsquota</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>inode</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>inode</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">&gt;</span></span>16387<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>type</span><span class="token punctuation">&gt;</span></span>DIRECTORY<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>type</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>testuser<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mtime</span><span class="token punctuation">&gt;</span></span>1512790549080<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mtime</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>permission</span><span class="token punctuation">&gt;</span></span>testuser:supergroup:rwxr-xr-x<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>permission</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>nsquota</span><span class="token punctuation">&gt;</span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>nsquota</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dsquota</span><span class="token punctuation">&gt;</span></span>-1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dsquota</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>inode</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>inode</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">&gt;</span></span>16389<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>type</span><span class="token punctuation">&gt;</span></span>FILE<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>type</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>wc.input<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>replication</span><span class="token punctuation">&gt;</span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>replication</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>mtime</span><span class="token punctuation">&gt;</span></span>1512722322219<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>mtime</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>atime</span><span class="token punctuation">&gt;</span></span>1512722321610<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>atime</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>perferredBlockSize</span><span class="token punctuation">&gt;</span></span>134217728<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>perferredBlockSize</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>permission</span><span class="token punctuation">&gt;</span></span>testuser:supergroup:rw-r--r--<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>permission</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>blocks</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>block</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>id</span><span class="token punctuation">&gt;</span></span>1073741825<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>id</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>genstamp</span><span class="token punctuation">&gt;</span></span>1001<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>genstamp</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>numBytes</span><span class="token punctuation">&gt;</span></span>59<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>numBytes</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>block</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>blocks</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>inode</span> <span class="token punctuation">&gt;</span></span>
</code></pre> 
  <p>可以看出，fsimage中没有记录块所对应datanode，</p> 
  <p><strong>oev查看edits文件</strong></p> 
  <ul> 
   <li>基本语法<br> <code>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</code></li> 
  </ul> 
  <pre><code>[testuser@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml
[testuser@hadoop102 current]$ cat /opt/module/hadoop-2.7.2/edits.xml
</code></pre> 
  <p>edits.xml</p> 
  <pre><code class="prism language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>EDITS</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>EDITS_VERSION</span><span class="token punctuation">&gt;</span></span>-63<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>EDITS_VERSION</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">&gt;</span></span>OP_START_LOG_SEGMENT<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">&gt;</span></span>129<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">&gt;</span></span>OP_ADD<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">&gt;</span></span>130<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>LENGTH</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>LENGTH</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>INODEID</span><span class="token punctuation">&gt;</span></span>16407<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>INODEID</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PATH</span><span class="token punctuation">&gt;</span></span>/hello7.txt<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PATH</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>REPLICATION</span><span class="token punctuation">&gt;</span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>REPLICATION</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>MTIME</span><span class="token punctuation">&gt;</span></span>1512943607866<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>MTIME</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ATIME</span><span class="token punctuation">&gt;</span></span>1512943607866<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ATIME</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCKSIZE</span><span class="token punctuation">&gt;</span></span>134217728<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCKSIZE</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>CLIENT_NAME</span><span class="token punctuation">&gt;</span></span>DFSClient_NONMAPREDUCE_-1544295051_1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>CLIENT_NAME</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>CLIENT_MACHINE</span><span class="token punctuation">&gt;</span></span>192.168.1.5<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>CLIENT_MACHINE</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OVERWRITE</span><span class="token punctuation">&gt;</span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OVERWRITE</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PERMISSION_STATUS</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>USERNAME</span><span class="token punctuation">&gt;</span></span>testuser<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>USERNAME</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GROUPNAME</span><span class="token punctuation">&gt;</span></span>supergroup<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GROUPNAME</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>MODE</span><span class="token punctuation">&gt;</span></span>420<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>MODE</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PERMISSION_STATUS</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RPC_CLIENTID</span><span class="token punctuation">&gt;</span></span>908eafd4-9aec-4288-96f1-e8011d181561<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RPC_CLIENTID</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RPC_CALLID</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RPC_CALLID</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">&gt;</span></span>OP_ALLOCATE_BLOCK_ID<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">&gt;</span></span>131<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK_ID</span><span class="token punctuation">&gt;</span></span>1073741839<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK_ID</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">&gt;</span></span>OP_SET_GENSTAMP_V2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">&gt;</span></span>132<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GENSTAMPV2</span><span class="token punctuation">&gt;</span></span>1016<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GENSTAMPV2</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">&gt;</span></span>OP_ADD_BLOCK<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">&gt;</span></span>133<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PATH</span><span class="token punctuation">&gt;</span></span>/hello7.txt<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PATH</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK_ID</span><span class="token punctuation">&gt;</span></span>1073741839<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK_ID</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>NUM_BYTES</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>NUM_BYTES</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GENSTAMP</span><span class="token punctuation">&gt;</span></span>1016<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GENSTAMP</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RPC_CLIENTID</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RPC_CLIENTID</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RPC_CALLID</span><span class="token punctuation">&gt;</span></span>-2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RPC_CALLID</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RECORD</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OPCODE</span><span class="token punctuation">&gt;</span></span>OP_CLOSE<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OPCODE</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>DATA</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TXID</span><span class="token punctuation">&gt;</span></span>134<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>TXID</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>LENGTH</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>LENGTH</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>INODEID</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>INODEID</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PATH</span><span class="token punctuation">&gt;</span></span>/hello7.txt<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PATH</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>REPLICATION</span><span class="token punctuation">&gt;</span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>REPLICATION</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>MTIME</span><span class="token punctuation">&gt;</span></span>1512943608761<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>MTIME</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ATIME</span><span class="token punctuation">&gt;</span></span>1512943607866<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ATIME</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCKSIZE</span><span class="token punctuation">&gt;</span></span>134217728<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCKSIZE</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>CLIENT_NAME</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>CLIENT_NAME</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>CLIENT_MACHINE</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>CLIENT_MACHINE</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>OVERWRITE</span><span class="token punctuation">&gt;</span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>OVERWRITE</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>BLOCK_ID</span><span class="token punctuation">&gt;</span></span>1073741839<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK_ID</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>NUM_BYTES</span><span class="token punctuation">&gt;</span></span>25<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>NUM_BYTES</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GENSTAMP</span><span class="token punctuation">&gt;</span></span>1016<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GENSTAMP</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>BLOCK</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PERMISSION_STATUS</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>USERNAME</span><span class="token punctuation">&gt;</span></span>testuser<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>USERNAME</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>GROUPNAME</span><span class="token punctuation">&gt;</span></span>supergroup<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>GROUPNAME</span><span class="token punctuation">&gt;</span></span>
				<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>MODE</span><span class="token punctuation">&gt;</span></span>420<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>MODE</span><span class="token punctuation">&gt;</span></span>
			<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PERMISSION_STATUS</span><span class="token punctuation">&gt;</span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>DATA</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RECORD</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>EDITS</span> <span class="token punctuation">&gt;</span></span>
</code></pre> 
  <h5><a id="checkpoint_581"></a>checkpoint时间设置</h5> 
  <p>1）通常情况下，SecondaryNameNode每隔一小时执行一次，默认配置在[hdfs-default.xml]文件中，用户如果修改在hdfs-site中修改</p> 
  <pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.namenode.checkpoint.period<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>3600<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span> <span class="token punctuation">&gt;</span></span>
</code></pre> 
  <p>2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p> 
  <pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.namenode.checkpoint.txns<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>1000000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>操作动作次数<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.namenode.checkpoint.check.period<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>60<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span> 1分钟检查一次操作次数<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span> <span class="token punctuation">&gt;</span></span>
</code></pre> 
  <h2><a id="NameNode_602"></a>NameNode故障处理</h2> 
  <p>NameNode故障后，可以采用如下两种方法恢复数据。</p> 
  <p><strong>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</strong></p> 
  <pre><code>1.  kill -9 namenode进程
2.  删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）
	[testuser@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*
3.  拷贝SecondaryNameNode中数据到原NameNode存储数据目录
	[testuser@hadoop102 dfs]$ scp -r testuser@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/
4.  重新启动namenode 
	[testuser@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode
</code></pre> 
  <p><strong>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</strong></p> 
  <pre><code>1.修改hdfs-site.xml中的
	&lt;property&gt;
	  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;
	  &lt;value&gt;120&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
	  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
	  &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;
	&lt;/property&gt;
2.  kill -9 namenode进程
3.	删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）
	[testuser@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*
4.	如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件
	[testuser@hadoop102 dfs]$ scp -r testuser@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./
	[testuser@hadoop102 namesecondary]$ rm -rf in_use.lock
	[testuser@hadoop102 dfs]$ pwd
	/opt/module/hadoop-2.7.2/data/tmp/dfs
	[testuser@hadoop102 dfs]$ ls
data  name  namesecondary
5.	导入检查点数据（等待一会ctrl+c结束掉）
	[testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint
6.	启动namenode
	[testuser@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode
</code></pre> 
  <h4><a id="_642"></a>集群安全模式</h4> 
  <p>NameNode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作（合并fsimage和edits）。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。但是此刻，NameNode运行在安全模式，即NameNode的文件系统对于<mark>客户端</mark>来说是只读的。<br> 系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。<mark>在安全模式下，各个DataNode会向NameNode发送最新的块列表信息</mark>，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。<br> 如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。</p> 
  <ul> 
   <li>基本语法<br> <code>bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态）</code><br> <code>bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）</code><br> <code>bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态）</code><br> <code>bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态，监控安全模式）</code></li> 
  </ul> 
  <p>案例：</p> 
  <pre><code>（1）先进入安全模式
	[testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter
（2）执行下面的脚本，编辑一个脚本
	#!/bin/bash
	bin/hdfs dfsadmin -safemode wait
	bin/hdfs dfs -put ~/hello.txt /root/hello.txt
（3）再打开一个窗口，执行
	[testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave
</code></pre> 
  <h4><a id="NameNode_664"></a>NameNode多目录配置</h4> 
  <p><strong>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性</strong></p> 
  <p>配置如下:</p> 
  <pre><code>（1）在hdfs-site.xml文件中增加如下内容
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
		&lt;value&gt;file:///${hadoop.tmp.dir}/dfs/name1,file:///${hadoop.tmp.dir}/dfs/name2&lt;/value&gt;
	&lt;/property&gt;
（2）停止集群，删除data和logs中所有数据。
	[testuser@hadoop102 hadoop-2.7.2]$ stop-dfs.sh
	[testuser@hadoop102 hadoop-2.7.2]$ rm -rf data/ logs/
	[testuser@hadoop103 hadoop-2.7.2]$ rm -rf data/ logs/
	[testuser@hadoop104 hadoop-2.7.2]$ rm -rf data/ logs/
（3）格式化集群并启动。
	[testuser@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode –format
	[testuser@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh
（4）查看结果
	[testuser@hadoop102 dfs]$ ll
	总用量 12
	drwx------. 3 testuser testuser 4096 12月 11 08:03 data
	drwxrwxr-x. 3 testuser testuser 4096 12月 11 08:03 name1
	drwxrwxr-x. 3 testuser testuser 4096 12月 11 08:03 name2
</code></pre> 
  <h2><a id="DataNode_690"></a>DataNode</h2> 
  <h4><a id="_691"></a>工作机制</h4> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725121546376.png" alt="在这里插入图片描述"></p> 
  <blockquote> 
   <p>1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。<br> 2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。<br> 3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。<br> 4）集群运行中可以安全加入和退出一些节点。</p> 
  </blockquote> 
  <h4><a id="_698"></a>数据完整性</h4> 
  <pre><code>1）当DataNode读取block的时候，它会计算checksum。
2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。
3）client读取其他DataNode上的block。
4）datanode在其文件创建后周期验证checksum，
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725144126981.png" alt="在这里插入图片描述"></p> 
  <h5><a id="_706"></a>掉线时限参数设置</h5> 
  <p>DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：<br> <strong>timeout = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。</strong><br> 而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。<br> 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p> 
  <pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.namenode.heartbeat.recheck-interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>300000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span> dfs.heartbeat.interval <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
  <h4><a id="_721"></a>服役新数据节点</h4> 
  <p>随着业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点</p> 
  <ol> 
   <li>环境准备<br> （1）克隆一台虚拟机<br> （2）修改ip地址和主机名称<br> （3）修改xsync文件，增加新增节点的ssh无密登录配置<br> （4）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data）</li> 
   <li>服役新节点具体步骤<br> （1）在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件白名单名字可以随便写，只要绝对路径指向正确既可以<pre><code>[testuser@hadoop105 hadoop]$ pwd
	/opt/module/hadoop-2.7.2/etc/hadoop
[testuser@hadoop105 hadoop]$ touch dfs.hosts
[testuser@hadoop105 hadoop]$ vi dfs.hosts
添加如下主机名称（包含新服役的节点）
	hadoop102
	hadoop103
	hadoop104
	hadoop105
</code></pre> （2）在namenode的hdfs-site.xml配置文件中增加dfs.hosts属性<pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.hosts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> （3）刷新namenode<br> <code>[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</code><br> Refresh nodes successful<br> （4）更新resourcemanager节点<pre><code>[testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes
18/06/24 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at 	hadoop103/192.168.1.103:8033
</code></pre> （5）在NameNode的slaves文件中增加新主机名称 hadoop105<br> （6）单独命令启动新的数据节点和节点管理器<pre><code>[testuser@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-testuser-datanode-hadoop105.out

[testuser@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager
starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-testuser-nodemanager-hadoop105.out
</code></pre> （7）在web浏览器上检查是否启动</li> 
  </ol> 
  <h4><a id="_766"></a>退役旧数据节点</h4> 
  <ol> 
   <li>在namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件<pre><code>[testuser@hadoop102 hadoop]$ pwd
	/opt/module/hadoop-2.7.2/etc/hadoop
[testuser@hadoop102 hadoop]$ touch dfs.hosts.exclude
[testuser@hadoop102 hadoop]$ vi dfs.hosts.exclude
</code></pre> 2．在namenode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性<pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.hosts.exclude<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 3．刷新namenode、刷新resourcemanager<pre><code>[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes
	Refresh nodes successful
[testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes
	18/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033
</code></pre> 4．检查web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725150008753.png" alt="在这里插入图片描述"><br> 5．等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725150109238.png" alt="在这里插入图片描述"><pre><code>[testuser@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode
	stopping datanode
[testuser@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager
	stopping nodemanager
</code></pre> 6．从include文件中删除退役节点，再运行刷新节点的命令<br> （1）从namenode的dfs.hosts文件中删除退役节点hadoop105<br> （2）刷新namenode，刷新resourcemanager<pre><code>[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes
Refresh nodes successful
[testuser@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes
18/06/24 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.1.103:8033
</code></pre> 7．从namenode的slave文件中删除退役节点hadoop105<br> 8．如果数据不均衡，可以用命令实现集群的再平衡<pre><code>[testuser@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh 
starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-testuser-balancer-hadoop102.out
Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
</code></pre> </li> 
  </ol> 
  <h4><a id="Datanode_814"></a>Datanode多目录配置</h4> 
  <p>1．datanode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本<br> 2．具体配置如下<br> hdfs-site.xml</p> 
  <pre><code>&lt;property&gt;
	&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
	&lt;value&gt;file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <h5><a id="hadoop_824"></a>hadoop集群之间的递归数据复制</h5> 
  <pre><code>[testuser@hadoop102 hadoop-2.7.2]$  bin/hadoop discp hdfs://haoop102:9000/user/testuser/hello.txt hdfs://hadoop103:9000/user/testuser/hello.txt
</code></pre> 
  <h3><a id="Hadoop_828"></a>Hadoop存档</h3> 
  <ul> 
   <li>hdfs存储小文件弊端每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。</li> 
   <li>解决存储小文件办法之一Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件对内（对于文件本身）还是一个一个独立文件，对NameNode（对外）而言却是一个整体，减少了NameNode的内存。</li> 
  </ul> 
  <p>例：</p> 
  <pre><code>（1）需要启动yarn进程（存档相当于走的是mapreduce程序）
	[testuser@hadoop102 hadoop-2.7.2]$ start-yarn.sh
（2）归档文件
	把/user/atguigu目录里面的所有文件归档成一个叫myhar.har的归档文件，并把归档后文件存储到/user/my路径下。
	[testuser@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName myhar.har -p /user/testuser /user/myhar.har
（3）查看归档
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/my/myhar.har
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/my/myhar.har
（4）解归档文件
	[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/my/myhar.har/*  /user/testuser
</code></pre> 
  <h4><a id="_845"></a>快照管理</h4> 
  <p>快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。</p> 
  <ul> 
   <li>基本语法<pre><code>  （1）hdfs dfsadmin -allowSnapshot 路径   （功能描述：开启指定目录的快照功能）
  （2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用）
  （3）hdfs dfs -createSnapshot 路径        （功能描述：对目录创建快照）
  （4）hdfs dfs -createSnapshot 路径 名称   （功能描述：指定名称创建快照）
  （5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照）
  （6）hdfs lsSnapshottableDir         	（功能描述：列出当前用户所有可快照目录）
  （7）hdfs snapshotDiff 路径1 路径2 	（功能描述：比较两个快照目录的不同之处）
  （8）hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt;  （功能描述：删除快照）
</code></pre> </li> 
  </ul> 
  <p>例：</p> 
  <pre><code>（1）开启/禁用指定目录的快照功能
	[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -allowSnapshot /user/testuser/data
	[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -disallowSnapshot /user/testuser/data
（2）对目录创建快照
	[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/testuser/data
		通过web访问hdfs://hadoop102:50070/user/testuser/data/.snapshot/s…..// 快照和源文件使用相同数据
	[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -lsr /user/testuser/data/.snapshot/
（3）指定名称创建快照
	[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -createSnapshot /user/testuser/data mysnap
（4）重命名快照
	[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -renameSnapshot /user/testuser/data/ mysnap mysnapshot
（5）列出当前用户所有可快照目录
	[testuser@hadoop102 hadoop-2.7.2]$ hdfs lsSnapshottableDir
（6）比较两个快照目录的不同之处  相对于后面的路径来说的  + 表示增 -表示少
	[testuser@hadoop102 hadoop-2.7.2]$ hdfs snapshotDiff  /user/testuser/data/  .  .snapshot/mysnapshot
（7）恢复快照
	[testuser@hadoop102 hadoop-2.7.2]$ hdfs dfs -cp /user/testuser/input/.snapshot/mysnapshot /user
</code></pre> 
  <h4><a id="_879"></a>回收站</h4> 
  <p>1．默认回收站，如图，默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725193350777.png" alt="在这里插入图片描述"><br> 2．启用回收站<br> 修改core-site.xml，配置垃圾回收时间为1分钟。</p> 
  <pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>fs.trash.interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
  <p>3．查看回收站<br> 回收站在集群中的路径是删除目录下的 .Trash文件中：删除/user/testuser下的所有文件，则在该目录下新建.Trash目录，即：/user/testuser/.Trash/….<br> 4．修改访问垃圾回收站用户名称，进入垃圾回收站用户名称，默认是dr.who，修改为testuser用户<br> [core-site.xml]</p> 
  <pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>hadoop.http.staticuser.user<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>testuser<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
  <p>5．通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站</p> 
  <blockquote> 
   <p>Trash trash = New Trash(conf);<br> trash.moveToTrash(path);</p> 
  </blockquote> 
  <p>6．恢复回收站数据<br> <code>[testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/testuser/.Trash/Current/user/testuser/input /user/testuser/input</code><br> 7．清空回收站<br> [testuser@hadoop102 hadoop-2.7.2]$ hadoop fs -expunge</p> 
  <h2><a id="yarn_910"></a>yarn命令</h2> 
  <pre><code>使用yarn进行任务调度
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /usr/atguigu/input /user/atguigu/output

mr-jobhistory-daemon.sh start historyserver		启动历史服务器，在mapred-site.xml中配置
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190723102436404.png" alt="在这里插入图片描述"><br> 使用yarn进行任务调到<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190723101725398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDg3MTQ1NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
