<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Spark Shell笔记 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Spark Shell笔记" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="学习感悟 (1)学习一定要敲，感觉很简单，但是也要敲一敲，不要眼高手低 (2)一定要懂函数式编程，一定，一定 (3)shell中的方法在scala写的项目中也会有对应的方法 (4)sc和spark是程序的入口，直接用 &nbsp; SparkShell 启动SparkShell ./bin/spark-shell &nbsp; WordCount案例 sc.textFile(&quot;hdfs://iZm5ea99qngm2v98asii1aZ:9000/README.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://iZm5ea99qngm2v98asii1aZ:9000/out&quot;) &nbsp; RDD创建(Shell) 从集合中创建RDD parallelize和makeRDD val rdd1246 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)) rdd1246.collect &nbsp; val rdd1617=sc.makeRDD(List(1,List((&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)),(2,List(&quot;d&quot;,&quot;e&quot;,&quot;f&quot;)))) rdd1617.collect 从外部存储创建RDD 由外部存储系统的数据集创建，包括本地文件系统，还有Hadoop支持的数据集，如HDFS，HBase sc.textFile(&quot;hdfs://iZm5ea99qngm2v98asii1aZ:9000/README.txt&quot;) 从其他RDD转换 &nbsp; 常用的Transformation和Action(Shell) map(func):返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成 scala&gt; var rdd1638 = sc.parallelize(1 to 10) scala&gt; rdd1638.collect scala&gt; rdd1638.map(_*2).collect &nbsp; filter(func):返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 scala&gt; var rdd1643 =sc.parallelize(1 to 10) scala&gt; rdd1643.filter(_&gt;5).collect flatMap(func):类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素） 注意：func 必须是将一个数据映射为0或多个输出元素 通俗点说：一个数据通过func函数产生的集合压平 val rdd3=sc.makeRDD(List(&quot;hello1_hello2_hello3&quot;,&quot;hello4_hello5&quot;)) scala&gt; rdd3.flatMap(_.split(&quot;_&quot;)).collect &nbsp; sample(withReplacement, fraction, seed)：以指定的随机种子随机抽样出数量为 fraction 的数据，withReplacement 表示是抽 出的数据是否放回，true 为有放回的抽样， false 为无放回的抽样，seed 用于指定随机 数生成器种子。例子从 RDD 中随机且有放 回的抽出 50%的数据，随机种子值为 3（即 可能以 1 2 3 的其中一个起始值） scala&gt; val rdd5 = sc.makeRDD(List(1,2,3,4,5,6,7)) scala&gt; rdd5.sample(false,0.2,3).collect takeSample：和 Sample 的区别是：takeSample 返回的是最终的结果集合。 union(otherDataset)：对源 RDD 和参数 RDD 求并集后返回一个 新的 RDD intersection(otherDataset)：对源 RDD 和参数 RDD 求交集后返回一个 新的 RDD intersection(otherDataset)：对源 RDD 和参数 RDD 求交集后返回一个 新的 RDD distinct([numTasks]))：对源 RDD 进行去重后返回一个新的 RDD. 默认情况下，只有 8 个并行任务来操作， 但是可以传入一个可选的 numTasks 参数 改变它。 rdd3 = sc.makeRDD(List(1,1,2,3,4,4,5)) rdd3.distinct(2).collect reduceByKey(func, [numTasks])：在一个(K,V)的 RDD 上调用，返回一个 (K,V)的 RDD，使用指定的 reduce 函数， 将相同 key 的值聚合到一起，reduce 任务 的个数可以通过第二个可选的参数来设置 groupByKey：groupByKey 也是对每个 key 进行操作，但只生成 一个 sequence。 sortByKey([ascending], [numTasks])：在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口，返回一个按照 key 进行排序 的(K,V)的 RDD sortBy(func,[ascending], [numTasks])：与 sortByKey 类似，但是更灵活,可以用 func 先对数据进行处理，按照处理后的数 据比较结果排序。 join(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的 RDD 上调用，返 回一个相同 key 对应的所有元素对在一起 的(K,(V,W))的 RDD cogroup(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的 RDD 上调用，返 回一个(K,(Iterable,Iterable))类型 的 RDD cartesian(otherDataset)：笛卡尔积 coalesce(numPartitions)：缩减分区数，用于大数据集过滤后，提高 小数据集的执行效率。 repartition(numPartitions):根据分区数，从新通过网络随机洗牌所有 数据。 glom:将每一个分区形成一个数组，形成新的 RDD 类型时 RDD[Array[T]] subtract:计算差的一种函数去除两个 RDD 中相同的 元素，不同的 RDD 将保留下来 mapValues:针对于(K,V)形式的类型只对 V 进行操作 &nbsp; reduce(func):通过 func 函数聚集 RDD 中的所有元素， 这个功能必须是可交换且可并联的 collect():在驱动程序中，以数组的形式返回数据 集的所有元素 count():返回 RDD 的元素个数 first():返回 RDD 的第一个元素（类似于 take(1)） take(n)；返回一个由数据集的前 n 个元素组成的 数组 takeOrdered(n)：返回前几个的排序 saveAsTextFile(path)：将数据集的元素以 textfile 的形式保存 到 HDFS 文件系统或者其他支持的文件 系统，对于每个元素，Spark 将会调用 toString 方法，将它装换为文件中的文 本 saveAsSequenceFile(path)：将数据集中的元素以 Hadoop sequencefile 的格式保存到指定的目录 下，可以使 HDFS 或者其他 Hadoop 支 持的文件系统。 saveAsObjectFile(path)：用于将 RDD 中的元素序列化成对象， 存储到文件中。 countByKey();针对(K,V)类型的 RDD，返回一个 (K,Int)的 map，表示每一个 key 对应的 元素个数。 数据读取与保存主要方式(Shell) 文本文件输入输出 val rdd1 =sc.textFile(&quot;hdfs://Master:9000/cbeann/README.txt&quot;) rdd.saveAsTextFile(&quot;hdfs://Master:9000/cbeann/README2.txt&quot;) JSON 、CSV文件输入输出(Shell) 先通过文本文件读入，然后通过fastjson等第三方库解析字符串为自定义的类型 先将自定义的类型通过第三方库转换为字符串，在同文本文件的形式保存到RDD中 SequenceFile 文件输入输出(Shell) SequenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对而设计的 一种平面文件(Flat File)。 val data=sc.parallelize(List((2,&quot;aa&quot;),(3,&quot;bb&quot;),(4,&quot;cc&quot;),(5,&quot;dd&quot;),(6,&quot;ee&quot;))) data.saveAsSequenceFile(&quot;hdfs://Master:9000/cbeann/seq&quot;) val sdata = sc.sequenceFile[Int,String](&quot;hdfs://Master:9000/cbeann/seq/p*&quot;) 对象文件输入输出(Shell) val data=sc.parallelize(List((2,&quot;aa&quot;),(3,&quot;bb&quot;),(4,&quot;cc&quot;),(5,&quot;dd&quot;),(6,&quot;ee&quot;))) data.saveAsObjectFile(&quot;hdfs://master01:9000/objfile&quot;) val objrdd:RDD[(Int,String)] = sc.objectFile[(Int,String)](&quot;hdfs://master01:9000/objfile/p*&quot;) Spark SQL(Shell) 启动SparkShell ./bin/spark-shell 读取数据，创建DataFrame 我的hdfs上/cbeann/person.json { &quot;name&quot;: &quot;王小二&quot;, &quot;age&quot;: 15} { &quot;name&quot;: &quot;王小三&quot;, &quot;age&quot;: 25} { &quot;name&quot;: &quot;王小四&quot;, &quot;age&quot;: 35} val df = spark.read.json(&quot;hdfs://Master:9000/cbeann/person.json&quot;) df.show &nbsp; 将数据注册一张表，表名为 people df.createOrReplaceTempView(&quot;people&quot;) 发送SQL spark.sql(&quot;select * from people where age &gt; 16&quot;).show 或者 &nbsp; RDD、DataFrame、DataSet之间的转化(Shell) RDD-》DataFrame val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lisi&quot;,13))) rdd.toDF(&quot;name&quot;,&quot;age&quot;).show 或者&nbsp; val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) case class Person(name:String, age:Int) val df = rdd.map(x=&gt;Person(x._1,x._2.toInt)).toDF &nbsp; DataFrame-》RDD val rdd1 = df.rdd &nbsp; RDD-》DataSet val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) val ds = rdd.toDS 或者 val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) case class Person(name:String, age:Int) rdd.map(x=&gt;Person(x._1,x._2.toInt)).toDS &nbsp; &nbsp; DataSet-》RDD ds.rdd &nbsp; DataFrame》DataSet scala&gt; val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) scala&gt; val df = rdd.toDF(&quot;name&quot;,&quot;age&quot;) scala&gt; case class Person(name:String, age:Int) scala&gt; val ds = df.as[Person] scala&gt; ds.collect &nbsp; DataSet-》DataFrame ds.toDF &nbsp; SparkSQl输入输出(Shell) &nbsp; val personDF= spark.read.format(&quot;json&quot;).load(&quot;hdfs://Master:9000/cbeann/person.json&quot;) 等价于&nbsp; val personDF1= spark.read.json(&quot;hdfs://Master:9000/cbeann/person.json&quot;) 相同的用法还有parquet,csv,text,jdbc &nbsp; personDF1.write.format(&quot;json&quot;).save(&quot;hdfs://Master:9000/cbeann/person&quot;) 等价于与 personDF1.write.json(&quot;hdfs://Master:9000/cbeann/person1&quot;) &nbsp;相同的用法还有parquet,csv,text,jdbc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="学习感悟 (1)学习一定要敲，感觉很简单，但是也要敲一敲，不要眼高手低 (2)一定要懂函数式编程，一定，一定 (3)shell中的方法在scala写的项目中也会有对应的方法 (4)sc和spark是程序的入口，直接用 &nbsp; SparkShell 启动SparkShell ./bin/spark-shell &nbsp; WordCount案例 sc.textFile(&quot;hdfs://iZm5ea99qngm2v98asii1aZ:9000/README.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://iZm5ea99qngm2v98asii1aZ:9000/out&quot;) &nbsp; RDD创建(Shell) 从集合中创建RDD parallelize和makeRDD val rdd1246 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)) rdd1246.collect &nbsp; val rdd1617=sc.makeRDD(List(1,List((&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)),(2,List(&quot;d&quot;,&quot;e&quot;,&quot;f&quot;)))) rdd1617.collect 从外部存储创建RDD 由外部存储系统的数据集创建，包括本地文件系统，还有Hadoop支持的数据集，如HDFS，HBase sc.textFile(&quot;hdfs://iZm5ea99qngm2v98asii1aZ:9000/README.txt&quot;) 从其他RDD转换 &nbsp; 常用的Transformation和Action(Shell) map(func):返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成 scala&gt; var rdd1638 = sc.parallelize(1 to 10) scala&gt; rdd1638.collect scala&gt; rdd1638.map(_*2).collect &nbsp; filter(func):返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 scala&gt; var rdd1643 =sc.parallelize(1 to 10) scala&gt; rdd1643.filter(_&gt;5).collect flatMap(func):类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素） 注意：func 必须是将一个数据映射为0或多个输出元素 通俗点说：一个数据通过func函数产生的集合压平 val rdd3=sc.makeRDD(List(&quot;hello1_hello2_hello3&quot;,&quot;hello4_hello5&quot;)) scala&gt; rdd3.flatMap(_.split(&quot;_&quot;)).collect &nbsp; sample(withReplacement, fraction, seed)：以指定的随机种子随机抽样出数量为 fraction 的数据，withReplacement 表示是抽 出的数据是否放回，true 为有放回的抽样， false 为无放回的抽样，seed 用于指定随机 数生成器种子。例子从 RDD 中随机且有放 回的抽出 50%的数据，随机种子值为 3（即 可能以 1 2 3 的其中一个起始值） scala&gt; val rdd5 = sc.makeRDD(List(1,2,3,4,5,6,7)) scala&gt; rdd5.sample(false,0.2,3).collect takeSample：和 Sample 的区别是：takeSample 返回的是最终的结果集合。 union(otherDataset)：对源 RDD 和参数 RDD 求并集后返回一个 新的 RDD intersection(otherDataset)：对源 RDD 和参数 RDD 求交集后返回一个 新的 RDD intersection(otherDataset)：对源 RDD 和参数 RDD 求交集后返回一个 新的 RDD distinct([numTasks]))：对源 RDD 进行去重后返回一个新的 RDD. 默认情况下，只有 8 个并行任务来操作， 但是可以传入一个可选的 numTasks 参数 改变它。 rdd3 = sc.makeRDD(List(1,1,2,3,4,4,5)) rdd3.distinct(2).collect reduceByKey(func, [numTasks])：在一个(K,V)的 RDD 上调用，返回一个 (K,V)的 RDD，使用指定的 reduce 函数， 将相同 key 的值聚合到一起，reduce 任务 的个数可以通过第二个可选的参数来设置 groupByKey：groupByKey 也是对每个 key 进行操作，但只生成 一个 sequence。 sortByKey([ascending], [numTasks])：在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口，返回一个按照 key 进行排序 的(K,V)的 RDD sortBy(func,[ascending], [numTasks])：与 sortByKey 类似，但是更灵活,可以用 func 先对数据进行处理，按照处理后的数 据比较结果排序。 join(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的 RDD 上调用，返 回一个相同 key 对应的所有元素对在一起 的(K,(V,W))的 RDD cogroup(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的 RDD 上调用，返 回一个(K,(Iterable,Iterable))类型 的 RDD cartesian(otherDataset)：笛卡尔积 coalesce(numPartitions)：缩减分区数，用于大数据集过滤后，提高 小数据集的执行效率。 repartition(numPartitions):根据分区数，从新通过网络随机洗牌所有 数据。 glom:将每一个分区形成一个数组，形成新的 RDD 类型时 RDD[Array[T]] subtract:计算差的一种函数去除两个 RDD 中相同的 元素，不同的 RDD 将保留下来 mapValues:针对于(K,V)形式的类型只对 V 进行操作 &nbsp; reduce(func):通过 func 函数聚集 RDD 中的所有元素， 这个功能必须是可交换且可并联的 collect():在驱动程序中，以数组的形式返回数据 集的所有元素 count():返回 RDD 的元素个数 first():返回 RDD 的第一个元素（类似于 take(1)） take(n)；返回一个由数据集的前 n 个元素组成的 数组 takeOrdered(n)：返回前几个的排序 saveAsTextFile(path)：将数据集的元素以 textfile 的形式保存 到 HDFS 文件系统或者其他支持的文件 系统，对于每个元素，Spark 将会调用 toString 方法，将它装换为文件中的文 本 saveAsSequenceFile(path)：将数据集中的元素以 Hadoop sequencefile 的格式保存到指定的目录 下，可以使 HDFS 或者其他 Hadoop 支 持的文件系统。 saveAsObjectFile(path)：用于将 RDD 中的元素序列化成对象， 存储到文件中。 countByKey();针对(K,V)类型的 RDD，返回一个 (K,Int)的 map，表示每一个 key 对应的 元素个数。 数据读取与保存主要方式(Shell) 文本文件输入输出 val rdd1 =sc.textFile(&quot;hdfs://Master:9000/cbeann/README.txt&quot;) rdd.saveAsTextFile(&quot;hdfs://Master:9000/cbeann/README2.txt&quot;) JSON 、CSV文件输入输出(Shell) 先通过文本文件读入，然后通过fastjson等第三方库解析字符串为自定义的类型 先将自定义的类型通过第三方库转换为字符串，在同文本文件的形式保存到RDD中 SequenceFile 文件输入输出(Shell) SequenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对而设计的 一种平面文件(Flat File)。 val data=sc.parallelize(List((2,&quot;aa&quot;),(3,&quot;bb&quot;),(4,&quot;cc&quot;),(5,&quot;dd&quot;),(6,&quot;ee&quot;))) data.saveAsSequenceFile(&quot;hdfs://Master:9000/cbeann/seq&quot;) val sdata = sc.sequenceFile[Int,String](&quot;hdfs://Master:9000/cbeann/seq/p*&quot;) 对象文件输入输出(Shell) val data=sc.parallelize(List((2,&quot;aa&quot;),(3,&quot;bb&quot;),(4,&quot;cc&quot;),(5,&quot;dd&quot;),(6,&quot;ee&quot;))) data.saveAsObjectFile(&quot;hdfs://master01:9000/objfile&quot;) val objrdd:RDD[(Int,String)] = sc.objectFile[(Int,String)](&quot;hdfs://master01:9000/objfile/p*&quot;) Spark SQL(Shell) 启动SparkShell ./bin/spark-shell 读取数据，创建DataFrame 我的hdfs上/cbeann/person.json { &quot;name&quot;: &quot;王小二&quot;, &quot;age&quot;: 15} { &quot;name&quot;: &quot;王小三&quot;, &quot;age&quot;: 25} { &quot;name&quot;: &quot;王小四&quot;, &quot;age&quot;: 35} val df = spark.read.json(&quot;hdfs://Master:9000/cbeann/person.json&quot;) df.show &nbsp; 将数据注册一张表，表名为 people df.createOrReplaceTempView(&quot;people&quot;) 发送SQL spark.sql(&quot;select * from people where age &gt; 16&quot;).show 或者 &nbsp; RDD、DataFrame、DataSet之间的转化(Shell) RDD-》DataFrame val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lisi&quot;,13))) rdd.toDF(&quot;name&quot;,&quot;age&quot;).show 或者&nbsp; val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) case class Person(name:String, age:Int) val df = rdd.map(x=&gt;Person(x._1,x._2.toInt)).toDF &nbsp; DataFrame-》RDD val rdd1 = df.rdd &nbsp; RDD-》DataSet val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) val ds = rdd.toDS 或者 val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) case class Person(name:String, age:Int) rdd.map(x=&gt;Person(x._1,x._2.toInt)).toDS &nbsp; &nbsp; DataSet-》RDD ds.rdd &nbsp; DataFrame》DataSet scala&gt; val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) scala&gt; val df = rdd.toDF(&quot;name&quot;,&quot;age&quot;) scala&gt; case class Person(name:String, age:Int) scala&gt; val ds = df.as[Person] scala&gt; ds.collect &nbsp; DataSet-》DataFrame ds.toDF &nbsp; SparkSQl输入输出(Shell) &nbsp; val personDF= spark.read.format(&quot;json&quot;).load(&quot;hdfs://Master:9000/cbeann/person.json&quot;) 等价于&nbsp; val personDF1= spark.read.json(&quot;hdfs://Master:9000/cbeann/person.json&quot;) 相同的用法还有parquet,csv,text,jdbc &nbsp; personDF1.write.format(&quot;json&quot;).save(&quot;hdfs://Master:9000/cbeann/person&quot;) 等价于与 personDF1.write.json(&quot;hdfs://Master:9000/cbeann/person1&quot;) &nbsp;相同的用法还有parquet,csv,text,jdbc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/08/10/793255.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/10/793255.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-10T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"学习感悟 (1)学习一定要敲，感觉很简单，但是也要敲一敲，不要眼高手低 (2)一定要懂函数式编程，一定，一定 (3)shell中的方法在scala写的项目中也会有对应的方法 (4)sc和spark是程序的入口，直接用 &nbsp; SparkShell 启动SparkShell ./bin/spark-shell &nbsp; WordCount案例 sc.textFile(&quot;hdfs://iZm5ea99qngm2v98asii1aZ:9000/README.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://iZm5ea99qngm2v98asii1aZ:9000/out&quot;) &nbsp; RDD创建(Shell) 从集合中创建RDD parallelize和makeRDD val rdd1246 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)) rdd1246.collect &nbsp; val rdd1617=sc.makeRDD(List(1,List((&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)),(2,List(&quot;d&quot;,&quot;e&quot;,&quot;f&quot;)))) rdd1617.collect 从外部存储创建RDD 由外部存储系统的数据集创建，包括本地文件系统，还有Hadoop支持的数据集，如HDFS，HBase sc.textFile(&quot;hdfs://iZm5ea99qngm2v98asii1aZ:9000/README.txt&quot;) 从其他RDD转换 &nbsp; 常用的Transformation和Action(Shell) map(func):返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成 scala&gt; var rdd1638 = sc.parallelize(1 to 10) scala&gt; rdd1638.collect scala&gt; rdd1638.map(_*2).collect &nbsp; filter(func):返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 scala&gt; var rdd1643 =sc.parallelize(1 to 10) scala&gt; rdd1643.filter(_&gt;5).collect flatMap(func):类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素） 注意：func 必须是将一个数据映射为0或多个输出元素 通俗点说：一个数据通过func函数产生的集合压平 val rdd3=sc.makeRDD(List(&quot;hello1_hello2_hello3&quot;,&quot;hello4_hello5&quot;)) scala&gt; rdd3.flatMap(_.split(&quot;_&quot;)).collect &nbsp; sample(withReplacement, fraction, seed)：以指定的随机种子随机抽样出数量为 fraction 的数据，withReplacement 表示是抽 出的数据是否放回，true 为有放回的抽样， false 为无放回的抽样，seed 用于指定随机 数生成器种子。例子从 RDD 中随机且有放 回的抽出 50%的数据，随机种子值为 3（即 可能以 1 2 3 的其中一个起始值） scala&gt; val rdd5 = sc.makeRDD(List(1,2,3,4,5,6,7)) scala&gt; rdd5.sample(false,0.2,3).collect takeSample：和 Sample 的区别是：takeSample 返回的是最终的结果集合。 union(otherDataset)：对源 RDD 和参数 RDD 求并集后返回一个 新的 RDD intersection(otherDataset)：对源 RDD 和参数 RDD 求交集后返回一个 新的 RDD intersection(otherDataset)：对源 RDD 和参数 RDD 求交集后返回一个 新的 RDD distinct([numTasks]))：对源 RDD 进行去重后返回一个新的 RDD. 默认情况下，只有 8 个并行任务来操作， 但是可以传入一个可选的 numTasks 参数 改变它。 rdd3 = sc.makeRDD(List(1,1,2,3,4,4,5)) rdd3.distinct(2).collect reduceByKey(func, [numTasks])：在一个(K,V)的 RDD 上调用，返回一个 (K,V)的 RDD，使用指定的 reduce 函数， 将相同 key 的值聚合到一起，reduce 任务 的个数可以通过第二个可选的参数来设置 groupByKey：groupByKey 也是对每个 key 进行操作，但只生成 一个 sequence。 sortByKey([ascending], [numTasks])：在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口，返回一个按照 key 进行排序 的(K,V)的 RDD sortBy(func,[ascending], [numTasks])：与 sortByKey 类似，但是更灵活,可以用 func 先对数据进行处理，按照处理后的数 据比较结果排序。 join(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的 RDD 上调用，返 回一个相同 key 对应的所有元素对在一起 的(K,(V,W))的 RDD cogroup(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的 RDD 上调用，返 回一个(K,(Iterable,Iterable))类型 的 RDD cartesian(otherDataset)：笛卡尔积 coalesce(numPartitions)：缩减分区数，用于大数据集过滤后，提高 小数据集的执行效率。 repartition(numPartitions):根据分区数，从新通过网络随机洗牌所有 数据。 glom:将每一个分区形成一个数组，形成新的 RDD 类型时 RDD[Array[T]] subtract:计算差的一种函数去除两个 RDD 中相同的 元素，不同的 RDD 将保留下来 mapValues:针对于(K,V)形式的类型只对 V 进行操作 &nbsp; reduce(func):通过 func 函数聚集 RDD 中的所有元素， 这个功能必须是可交换且可并联的 collect():在驱动程序中，以数组的形式返回数据 集的所有元素 count():返回 RDD 的元素个数 first():返回 RDD 的第一个元素（类似于 take(1)） take(n)；返回一个由数据集的前 n 个元素组成的 数组 takeOrdered(n)：返回前几个的排序 saveAsTextFile(path)：将数据集的元素以 textfile 的形式保存 到 HDFS 文件系统或者其他支持的文件 系统，对于每个元素，Spark 将会调用 toString 方法，将它装换为文件中的文 本 saveAsSequenceFile(path)：将数据集中的元素以 Hadoop sequencefile 的格式保存到指定的目录 下，可以使 HDFS 或者其他 Hadoop 支 持的文件系统。 saveAsObjectFile(path)：用于将 RDD 中的元素序列化成对象， 存储到文件中。 countByKey();针对(K,V)类型的 RDD，返回一个 (K,Int)的 map，表示每一个 key 对应的 元素个数。 数据读取与保存主要方式(Shell) 文本文件输入输出 val rdd1 =sc.textFile(&quot;hdfs://Master:9000/cbeann/README.txt&quot;) rdd.saveAsTextFile(&quot;hdfs://Master:9000/cbeann/README2.txt&quot;) JSON 、CSV文件输入输出(Shell) 先通过文本文件读入，然后通过fastjson等第三方库解析字符串为自定义的类型 先将自定义的类型通过第三方库转换为字符串，在同文本文件的形式保存到RDD中 SequenceFile 文件输入输出(Shell) SequenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对而设计的 一种平面文件(Flat File)。 val data=sc.parallelize(List((2,&quot;aa&quot;),(3,&quot;bb&quot;),(4,&quot;cc&quot;),(5,&quot;dd&quot;),(6,&quot;ee&quot;))) data.saveAsSequenceFile(&quot;hdfs://Master:9000/cbeann/seq&quot;) val sdata = sc.sequenceFile[Int,String](&quot;hdfs://Master:9000/cbeann/seq/p*&quot;) 对象文件输入输出(Shell) val data=sc.parallelize(List((2,&quot;aa&quot;),(3,&quot;bb&quot;),(4,&quot;cc&quot;),(5,&quot;dd&quot;),(6,&quot;ee&quot;))) data.saveAsObjectFile(&quot;hdfs://master01:9000/objfile&quot;) val objrdd:RDD[(Int,String)] = sc.objectFile[(Int,String)](&quot;hdfs://master01:9000/objfile/p*&quot;) Spark SQL(Shell) 启动SparkShell ./bin/spark-shell 读取数据，创建DataFrame 我的hdfs上/cbeann/person.json { &quot;name&quot;: &quot;王小二&quot;, &quot;age&quot;: 15} { &quot;name&quot;: &quot;王小三&quot;, &quot;age&quot;: 25} { &quot;name&quot;: &quot;王小四&quot;, &quot;age&quot;: 35} val df = spark.read.json(&quot;hdfs://Master:9000/cbeann/person.json&quot;) df.show &nbsp; 将数据注册一张表，表名为 people df.createOrReplaceTempView(&quot;people&quot;) 发送SQL spark.sql(&quot;select * from people where age &gt; 16&quot;).show 或者 &nbsp; RDD、DataFrame、DataSet之间的转化(Shell) RDD-》DataFrame val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lisi&quot;,13))) rdd.toDF(&quot;name&quot;,&quot;age&quot;).show 或者&nbsp; val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) case class Person(name:String, age:Int) val df = rdd.map(x=&gt;Person(x._1,x._2.toInt)).toDF &nbsp; DataFrame-》RDD val rdd1 = df.rdd &nbsp; RDD-》DataSet val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) val ds = rdd.toDS 或者 val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) case class Person(name:String, age:Int) rdd.map(x=&gt;Person(x._1,x._2.toInt)).toDS &nbsp; &nbsp; DataSet-》RDD ds.rdd &nbsp; DataFrame》DataSet scala&gt; val rdd = sc.makeRDD(List((&quot;zhangsan&quot;,11),(&quot;lsi&quot;,12),(&quot;wanwu&quot;,16))) scala&gt; val df = rdd.toDF(&quot;name&quot;,&quot;age&quot;) scala&gt; case class Person(name:String, age:Int) scala&gt; val ds = df.as[Person] scala&gt; ds.collect &nbsp; DataSet-》DataFrame ds.toDF &nbsp; SparkSQl输入输出(Shell) &nbsp; val personDF= spark.read.format(&quot;json&quot;).load(&quot;hdfs://Master:9000/cbeann/person.json&quot;) 等价于&nbsp; val personDF1= spark.read.json(&quot;hdfs://Master:9000/cbeann/person.json&quot;) 相同的用法还有parquet,csv,text,jdbc &nbsp; personDF1.write.format(&quot;json&quot;).save(&quot;hdfs://Master:9000/cbeann/person&quot;) 等价于与 personDF1.write.json(&quot;hdfs://Master:9000/cbeann/person1&quot;) &nbsp;相同的用法还有parquet,csv,text,jdbc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/08/10/793255.html","headline":"Spark Shell笔记","dateModified":"2019-08-10T00:00:00+08:00","datePublished":"2019-08-10T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/10/793255.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Spark Shell笔记</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1>学习感悟</h1> 
  <p>(1)学习一定要敲，感觉很简单，但是也要敲一敲，不要眼高手低</p> 
  <p>(2)一定要懂函数式编程，一定，一定</p> 
  <p>(3)shell中的方法在scala写的项目中也会有对应的方法</p> 
  <p>(4)sc和spark是程序的入口，直接用</p> 
  <p><img alt="" class="has" height="220" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190716094136802.png" width="845"></p> 
  <p><img alt="" class="has" height="423" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190717153354862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="854"></p> 
  <p>&nbsp;</p> 
  <h1>SparkShell</h1> 
  <p>启动SparkShell</p> 
  <pre class="has">
<code> ./bin/spark-shell</code></pre> 
  <p>&nbsp;</p> 
  <p>WordCount案例</p> 
  <pre class="has">
<code>sc.textFile("hdfs://iZm5ea99qngm2v98asii1aZ:9000/README.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://iZm5ea99qngm2v98asii1aZ:9000/out")</code></pre> 
  <p>&nbsp;</p> 
  <h1>RDD创建(Shell)</h1> 
  <h3>从集合中创建RDD</h3> 
  <p>parallelize和makeRDD</p> 
  <pre class="has">
<code>val rdd1246 = sc.parallelize(List("a","b","c"))</code></pre> 
  <pre class="has">
<code>rdd1246.collect</code></pre> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" height="200" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190713124739354.png" width="861"></p> 
  <pre class="has">
<code>val rdd1617=sc.makeRDD(List(1,List(("a","b","c")),(2,List("d","e","f"))))</code></pre> 
  <pre class="has">
<code> rdd1617.collect</code></pre> 
  <p><img alt="" class="has" height="243" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190713161847559.png" width="842"></p> 
  <h3>从外部存储创建RDD</h3> 
  <p>由外部存储系统的数据集创建，包括本地文件系统，还有Hadoop支持的数据集，如HDFS，HBase</p> 
  <pre class="has">
<code>sc.textFile("hdfs://iZm5ea99qngm2v98asii1aZ:9000/README.txt")</code></pre> 
  <h3>从其他RDD转换</h3> 
  <p>&nbsp;</p> 
  <h1>常用的Transformation和Action(Shell)</h1> 
  <p style="margin-left:0pt;">map(func):返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</p> 
  <pre class="has">
<code>scala&gt; var rdd1638  = sc.parallelize(1 to 10)

scala&gt; rdd1638.collect

scala&gt; rdd1638.map(_*2).collect
</code></pre> 
  <p style="margin-left:0pt;"><img alt="" class="has" height="207" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190713163924996.png" width="872"></p> 
  <p style="margin-left:0pt;">&nbsp;</p> 
  <p style="margin-left:0pt;">filter(func):返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</p> 
  <pre class="has">
<code>scala&gt; var rdd1643 =sc.parallelize(1 to 10)

scala&gt; rdd1643.filter(_&gt;5).collect</code></pre> 
  <p style="margin-left:0pt;"><img alt="" class="has" height="180" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190713164415453.png" width="850"></p> 
  <p style="margin-left:0pt;">flatMap(func):类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</p> 
  <p style="margin-left:0pt;">注意：func 必须是将一个数据映射为0或多个输出元素</p> 
  <p style="margin-left:0pt;">通俗点说：一个数据通过func函数产生的集合压平</p> 
  <pre class="has">
<code>val rdd3=sc.makeRDD(List("hello1_hello2_hello3","hello4_hello5"))

scala&gt; rdd3.flatMap(_.split("_")).collect</code></pre> 
  <p><img alt="" class="has" height="186" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714154507998.png" width="857"></p> 
  <p>&nbsp;</p> 
  <p>sample(withReplacement, fraction, seed)：以指定的随机种子随机抽样出数量为 fraction 的数据，withReplacement 表示是抽 出的数据是否放回，true 为有放回的抽样， false 为无放回的抽样，seed 用于指定随机 数生成器种子。例子从 RDD 中随机且有放 回的抽出 50%的数据，随机种子值为 3（即 可能以 1 2 3 的其中一个起始值）</p> 
  <pre class="has">
<code>scala&gt; val rdd5 = sc.makeRDD(List(1,2,3,4,5,6,7))

scala&gt; rdd5.sample(false,0.2,3).collect</code></pre> 
  <p><img alt="" class="has" height="205" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714161522123.png" width="841"></p> 
  <p>takeSample：和 Sample 的区别是：takeSample 返回的是最终的结果集合。</p> 
  <p><img alt="" class="has" height="128" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714163317260.png" width="822"></p> 
  <p>union(otherDataset)：对源 RDD 和参数 RDD 求并集后返回一个 新的 RDD</p> 
  <p>intersection(otherDataset)：对源 RDD 和参数 RDD 求交集后返回一个 新的 RDD</p> 
  <p>intersection(otherDataset)：对源 RDD 和参数 RDD 求交集后返回一个 新的 RDD</p> 
  <p><img alt="" class="has" height="322" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714194711282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="839"></p> 
  <p>distinct([numTasks]))：对源 RDD 进行去重后返回一个新的 RDD. 默认情况下，只有 8 个并行任务来操作， 但是可以传入一个可选的 numTasks 参数 改变它。</p> 
  <pre class="has">
<code>rdd3 = sc.makeRDD(List(1,1,2,3,4,4,5))

rdd3.distinct(2).collect
</code></pre> 
  <p><img alt="" class="has" height="192" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714194901270.png" width="855"></p> 
  <p>reduceByKey(func, [numTasks])：在一个(K,V)的 RDD 上调用，返回一个 (K,V)的 RDD，使用指定的 reduce 函数， 将相同 key 的值聚合到一起，reduce 任务 的个数可以通过第二个可选的参数来设置</p> 
  <p><img alt="" class="has" height="278" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714195952557.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="821"></p> 
  <p>groupByKey：groupByKey 也是对每个 key 进行操作，但只生成 一个 sequence。</p> 
  <p><img alt="" class="has" height="176" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714200136874.png" width="843"></p> 
  <p>sortByKey([ascending], [numTasks])：在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口，返回一个按照 key 进行排序 的(K,V)的 RDD</p> 
  <p><img alt="" class="has" height="205" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714201652473.png" width="830"></p> 
  <p>sortBy(func,[ascending], [numTasks])：与 sortByKey 类似，但是更灵活,可以用 func 先对数据进行处理，按照处理后的数 据比较结果排序。</p> 
  <p><img alt="" class="has" height="168" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714204532859.png" width="836"></p> 
  <p>join(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的 RDD 上调用，返 回一个相同 key 对应的所有元素对在一起 的(K,(V,W))的 RDD</p> 
  <p><img alt="" class="has" height="261" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714210920821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="822"></p> 
  <p>cogroup(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的 RDD 上调用，返 回一个(K,(Iterable,Iterable))类型 的 RDD</p> 
  <p><img alt="" class="has" height="293" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019071421363340.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="817"></p> 
  <p>cartesian(otherDataset)：笛卡尔积</p> 
  <p><img alt="" class="has" height="261" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714214057712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="811"></p> 
  <p>coalesce(numPartitions)：缩减分区数，用于大数据集过滤后，提高 小数据集的执行效率。</p> 
  <p>repartition(numPartitions):根据分区数，从新通过网络随机洗牌所有 数据。</p> 
  <p>glom:将每一个分区形成一个数组，形成新的 RDD 类型时 RDD[Array[T]]</p> 
  <p><img alt="" class="has" height="210" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715092228181.png" width="849"></p> 
  <p>subtract:计算差的一种函数去除两个 RDD 中相同的 元素，不同的 RDD 将保留下来</p> 
  <p><img alt="" class="has" height="271" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715093425190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="836"></p> 
  <p>mapValues:针对于(K,V)形式的类型只对 V 进行操作</p> 
  <p><img alt="" class="has" height="292" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715093827664.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="858"></p> 
  <p>&nbsp;</p> 
  <p>reduce(func):通过 func 函数聚集 RDD 中的所有元素， 这个功能必须是可交换且可并联的</p> 
  <p><img alt="" class="has" height="288" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/201907150947253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="845"></p> 
  <p>collect():在驱动程序中，以数组的形式返回数据 集的所有元素</p> 
  <p><img alt="" class="has" height="181" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019071509480989.png" width="822"></p> 
  <p>count():返回 RDD 的元素个数</p> 
  <p>first():返回 RDD 的第一个元素（类似于 take(1)）</p> 
  <p>take(n)；返回一个由数据集的前 n 个元素组成的 数组</p> 
  <p>takeOrdered(n)：返回前几个的排序</p> 
  <p><img alt="" class="has" height="176" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715095233497.png" width="813"></p> 
  <p>saveAsTextFile(path)：将数据集的元素以 textfile 的形式保存 到 HDFS 文件系统或者其他支持的文件 系统，对于每个元素，Spark 将会调用 toString 方法，将它装换为文件中的文 本</p> 
  <p>saveAsSequenceFile(path)：将数据集中的元素以 Hadoop sequencefile 的格式保存到指定的目录 下，可以使 HDFS 或者其他 Hadoop 支 持的文件系统。</p> 
  <p>saveAsObjectFile(path)：用于将 RDD 中的元素序列化成对象， 存储到文件中。</p> 
  <p>countByKey();针对(K,V)类型的 RDD，返回一个 (K,Int)的 map，表示每一个 key 对应的 元素个数。</p> 
  <p><img alt="" class="has" height="319" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715100224657.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="888"></p> 
  <h1>数据读取与保存主要方式(Shell)</h1> 
  <h3>文本文件输入输出</h3> 
  <pre class="has">
<code>val rdd1 =sc.textFile("hdfs://Master:9000/cbeann/README.txt")</code></pre> 
  <pre class="has">
<code> rdd.saveAsTextFile("hdfs://Master:9000/cbeann/README2.txt")</code></pre> 
  <h3>JSON 、CSV文件输入输出(Shell)</h3> 
  <p>先通过文本文件读入，然后通过fastjson等第三方库解析字符串为自定义的类型</p> 
  <p>先将自定义的类型通过第三方库转换为字符串，在同文本文件的形式保存到RDD中</p> 
  <h3>SequenceFile 文件输入输出(Shell)</h3> 
  <p>SequenceFile 文件是 Hadoop 用来存储二进制形式的<em><strong><u> key-value </u></strong></em>对而设计的 一种平面文件(Flat File)。</p> 
  <pre class="has">
<code> val data=sc.parallelize(List((2,"aa"),(3,"bb"),(4,"cc"),(5,"dd"),(6,"ee")))</code></pre> 
  <pre class="has">
<code>data.saveAsSequenceFile("hdfs://Master:9000/cbeann/seq")</code></pre> 
  <pre class="has">
<code>val sdata = sc.sequenceFile[Int,String]("hdfs://Master:9000/cbeann/seq/p*")</code></pre> 
  <h3>对象文件输入输出(Shell)</h3> 
  <pre class="has">
<code>val data=sc.parallelize(List((2,"aa"),(3,"bb"),(4,"cc"),(5,"dd"),(6,"ee")))</code></pre> 
  <pre class="has">
<code>data.saveAsObjectFile("hdfs://master01:9000/objfile")</code></pre> 
  <pre class="has">
<code>val objrdd:RDD[(Int,String)] = sc.objectFile[(Int,String)]("hdfs://master01:9000/objfile/p*")</code></pre> 
  <h1>Spark SQL(Shell)</h1> 
  <p>启动SparkShell</p> 
  <pre class="has">
<code>./bin/spark-shell</code></pre> 
  <p>读取数据，创建DataFrame</p> 
  <p>我的hdfs上/cbeann/person.json</p> 
  <pre class="has">
<code>{  "name": "王小二",   "age": 15}
{  "name": "王小三",   "age": 25}
{  "name": "王小四",   "age": 35}</code></pre> 
  <pre class="has">
<code> val df = spark.read.json("hdfs://Master:9000/cbeann/person.json")</code></pre> 
  <pre class="has">
<code>df.show</code></pre> 
  <p><img alt="" class="has" height="315" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715211621541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="864"></p> 
  <p>&nbsp;</p> 
  <p>将数据注册一张表，表名为 people</p> 
  <pre class="has">
<code>df.createOrReplaceTempView("people")</code></pre> 
  <p><img alt="" class="has" height="109" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715212044906.png" width="834"></p> 
  <p>发送SQL</p> 
  <pre class="has">
<code>spark.sql("select * from people where age &gt; 16").show</code></pre> 
  <p><img alt="" class="has" height="210" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715212229521.png" width="832"></p> 
  <p>或者</p> 
  <p><img alt="" class="has" height="353" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190715215751660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="851"></p> 
  <p>&nbsp;</p> 
  <h1>RDD、DataFrame、DataSet之间的转化(Shell)</h1> 
  <h3>RDD-》DataFrame</h3> 
  <pre class="has">
<code>val rdd = sc.makeRDD(List(("zhangsan",11),("lisi",13)))
rdd.toDF("name","age").show</code></pre> 
  <p>或者&nbsp;</p> 
  <pre class="has">
<code>val rdd = sc.makeRDD(List(("zhangsan",11),("lsi",12),("wanwu",16)))
case class Person(name:String, age:Int)
 val df =  rdd.map(x=&gt;Person(x._1,x._2.toInt)).toDF</code></pre> 
  <p><img alt="" class="has" height="463" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190716084312132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="871"></p> 
  <p><img alt="" class="has" height="304" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190716101134413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="880"></p> 
  <p>&nbsp;</p> 
  <h3>DataFrame-》RDD</h3> 
  <pre class="has">
<code>val rdd1 = df.rdd</code></pre> 
  <p><img alt="" class="has" height="377" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190716090403622.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="850"></p> 
  <p>&nbsp;</p> 
  <h3>RDD-》DataSet</h3> 
  <pre class="has">
<code>val rdd = sc.makeRDD(List(("zhangsan",11),("lsi",12),("wanwu",16)))</code></pre> 
  <pre class="has">
<code>val ds = rdd.toDS</code></pre> 
  <p>或者</p> 
  <pre class="has">
<code>val rdd = sc.makeRDD(List(("zhangsan",11),("lsi",12),("wanwu",16)))</code></pre> 
  <pre class="has">
<code>case class Person(name:String, age:Int)</code></pre> 
  <pre class="has">
<code> rdd.map(x=&gt;Person(x._1,x._2.toInt)).toDS</code></pre> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" height="354" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019071609215836.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="846"></p> 
  <p><img alt="" class="has" height="227" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190716095543761.png" width="849"></p> 
  <p>&nbsp;</p> 
  <h3>DataSet-》RDD</h3> 
  <pre class="has">
<code>ds.rdd</code></pre> 
  <p><img alt="" class="has" height="275" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190716092323465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="824"></p> 
  <p>&nbsp;</p> 
  <h3>DataFrame》DataSet</h3> 
  <pre class="has">
<code>scala&gt; val rdd = sc.makeRDD(List(("zhangsan",11),("lsi",12),("wanwu",16)))

scala&gt; val df = rdd.toDF("name","age")

scala&gt; case class Person(name:String, age:Int)

scala&gt; val ds = df.as[Person]

scala&gt; ds.collect
</code></pre> 
  <p><img alt="" class="has" height="421" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/201907160934354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MTcxMzUz,size_16,color_FFFFFF,t_70" width="879"></p> 
  <p>&nbsp;</p> 
  <h3>DataSet-》DataFrame</h3> 
  <pre class="has">
<code>ds.toDF</code></pre> 
  <p><img alt="" class="has" height="82" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019071609280188.png" width="868"></p> 
  <p>&nbsp;</p> 
  <h1>SparkSQl输入输出(Shell)</h1> 
  <p>&nbsp;</p> 
  <pre class="has">
<code>val personDF= spark.read.format("json").load("hdfs://Master:9000/cbeann/person.json")</code></pre> 
  <p>等价于&nbsp;</p> 
  <pre class="has">
<code> val personDF1= spark.read.json("hdfs://Master:9000/cbeann/person.json")</code></pre> 
  <p>相同的用法还有parquet,csv,text,jdbc</p> 
  <p>&nbsp;</p> 
  <pre class="has">
<code>personDF1.write.format("json").save("hdfs://Master:9000/cbeann/person")</code></pre> 
  <p>等价于与</p> 
  <pre class="has">
<code>personDF1.write.json("hdfs://Master:9000/cbeann/person1")</code></pre> 
  <p>&nbsp;相同的用法还有parquet,csv,text,jdbc</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
