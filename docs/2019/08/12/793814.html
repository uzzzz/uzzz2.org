<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>SparkSQL应用——数据转换器 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="SparkSQL应用——数据转换器" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="前言 场景： 现在我们有一个需求 hive中我们有一个大容量的表 存放了许多属性的属性，我们需要将数据表中的牟烈数据JSon数据转换成为二进制的流对象 供mongoDB使用。 转换的过程可以调用已写好的其他方法转换成为boundary的流对象。 解决方法： 1. 我们可以使用hive来解决，但是hive自带的库函数中没有这样的方法去调用，所以我们可以自己编写一个UDF 来实现； 2. 鉴于hive执行SQL语句 底层是基于mapreduce的过程，我们思考能否使用SparkSQL 来实现，他的优点是低延时 速度快。 我们现有的集群在配置的Spark 1.6的服务，中间调用的转换方法是Java方法所写，虽然scala可以直接调用Java的方法 但这里 为了便于代码方便他人可读，我们在这里使用Java进行代码编写。 sparksql 1.6.0 代码 package com.test.spark import com.alibaba.fastjson.JSONObject; import com.esotericsoftware.kryo.Kryo; import com.esotericsoftware.kryo.serializers.FieldSerializer; import com.huawei.CSCommon.Geo.Boundary; import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.function.Function; import org.apache.spark.serializer.KryoRegistrator; import org.apache.spark.sql.DataFrame; import org.apache.spark.sql.Row; import org.apache.spark.sql.RowFactory; import org.apache.spark.sql.hive.HiveContext; import org.apache.spark.sql.types.DataTypes; import org.apache.spark.sql.types.StructField; import org.apache.spark.sql.types.StructType; import java.io.ByteArrayOutputStream; import java.io.Serializable; import java.util.ArrayList; import java.util.List; /** * 这里进行性能优化： 使用Kryo进行序列化(需要手动注册) * Created by x50005784 on 2019/7/19. */ public class KryoTest { public static class Boun implements Serializable { private byte[] boun; private String area_code; public String getArea_code() { return area_code; } public void setArea_code(String area_code) { this.area_code = area_code; } public byte[] getBoun() { return boun; } public void setBoun(byte[] boun) { this.boun = boun; } } public static class toKryoRegistrator implements KryoRegistrator { @Override public void registerClasses(Kryo kryo) { //在Kryo序列化库中注册自定义的类 kryo.register(Boun.class,new FieldSerializer(kryo,Boun.class)); } } public static void main(String[] args) { SparkConf conf = new SparkConf().setAppName(&quot;Java Spark1.6.0 xjh KryoTest&quot;); conf.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;); //使用Kryo序列化库 conf.set(&quot;spark.kryo.registrator&quot;, toKryoRegistrator.class.getName()); //在Kryo序列化库中注册自定义的类集合 conf.set(&quot;spark.kryoserializer.buffer.max&quot;,&quot;1g&quot;); // 增大spark.kryoserializer.buffer.max参数,默认是64 conf.set(&quot;spark.akka.frameSize&quot;,&quot;256&quot;); //增大spark.akka.frameSize数值，默认是256 // conf.set(&quot;spark.rdd.compress&quot;, &quot;true&quot;); //rdd持久化操作是使用压缩机制 JavaSparkContext sc=new JavaSparkContext(conf); HiveContext sqlContext = new HiveContext(sc); //args[0]=&quot;hdfs://cscloud-rs-hadoop293.huawei.com:8020/user/hive/warehouse/x50005784.db/pm00_base_areaborder_text_10&quot; DataFrame parquetFile = sqlContext.read().parquet(args[0]); parquetFile.registerTempTable(&quot;people&quot;); DataFrame jsonsql = sqlContext.sql(&quot;select * from people&quot;); List&lt;String&gt; geoJsonList = jsonsql.javaRDD().map(new Function&lt;Row, String&gt;() { public String call(Row row) { return new String(row.get(11).toString()); //这里是得到geojson的string形式 } }).collect(); List&lt;String&gt; area_codeList = jsonsql.javaRDD().map(new Function&lt;Row, String&gt;() { public String call(Row row) { return new String(row.get(0).toString()); } }).collect(); System.out.println(&quot;size: &quot;+area_codeList.size()); String schemaString=&quot;area_code boundary&quot;; List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;(); StructField field=null; field= DataTypes.createStructField(&quot;area_code&quot;,DataTypes.StringType,true); fields.add(field); field=DataTypes.createStructField(&quot;boundary&quot;,DataTypes.BinaryType,true); fields.add(field); StructType schema = DataTypes.createStructType(fields); //得到表结构 scmea List&lt;ByteArrayOutputStream&gt; baoList=new ArrayList&lt;ByteArrayOutputStream&gt;(); for (int i=0;i&lt;geoJsonList.size();i++){ Boundary boundary = new Boundary(); System.out.println(&quot;area_code: &quot;+area_codeList.get(i)); JSONObject json = JSONObject.parseObject(geoJsonList.get(i)); boundary.loadFromGeoJson(json); ByteArrayOutputStream baos=new ByteArrayOutputStream(); try { boundary.saveTo(baos); baoList.add(baos); continue; } catch (Exception e) { e.printStackTrace(); } } ArrayList&lt;test.Boun&gt; temp=new ArrayList&lt;&gt;(area_codeList.size()); for (int i=0;i&lt;area_codeList.size();i++){ test.Boun b=new test.Boun(); b.setArea_code(area_codeList.get(i)); b.setBoun(baoList.get(i).toByteArray()); temp.add(b); } JavaRDD&lt;test.Boun&gt; bounrdd=sc.parallelize(temp,10); JavaRDD&lt;Row&gt; rowRDD=bounrdd.map(new Function&lt;test.Boun, Row&gt;() { @Override public Row call(test.Boun v1) throws Exception { String s=v1.getArea_code(); byte[] b=v1.getBoun(); return RowFactory.create(s,b); } }); DataFrame bounDF=sqlContext.createDataFrame(rowRDD,schema); bounDF = bounDF.join(jsonsql,&quot;area_code&quot;); bounDF.write().mode(&quot;overwrite&quot;).saveAsTable(args[1]); sc.stop(); } } 输入数据集： yarn clusrer提交： spark-submit --master yarn --deploy-mode cluster --class com.test.spark.KryoTest --driver-memory 6g --executor-memory 16g --executor-cores 2 --num-executors 20 /home/x50005784/SparkBoundary-1.0-SNAPSHOT.jar hdfs://cscloud-rs-hadoop293.huawei.com:8020/user/hive/warehouse/x50005784.db/pm00_base_areaborder_text_asc x50005784.boundary_all_row_xjh 输出结果： 注意： 在上述的代码中，因为输入源数据集庞大，在这里我们使用Kryo进行序列化减少占内存空间， 并且设置合适的参数以保证程序的运行。在实际应用中，为了保证应用程序能够正常运行，我们需要需要增大kryo buffer的大小， 为了不让executor memory和driver memory在程序运行时不过多浪费，我们可以用Dr.elephant来监控程序作业。 spark处理dataframe数据时，往往遇到&quot;…cannot be cast to …&quot;这种数据不匹配的问题，主要是因为我们代码中spark指定的数据类型和数据源类型不一致。https://blog.csdn.net/qq_35022142/article/details/79800394 sparkSQL输入的hive表中的parquet文件 需要用hive创建的，用impala创建的表会报错(string无法正常解析 他得到的数值变成乱码)。" />
<meta property="og:description" content="前言 场景： 现在我们有一个需求 hive中我们有一个大容量的表 存放了许多属性的属性，我们需要将数据表中的牟烈数据JSon数据转换成为二进制的流对象 供mongoDB使用。 转换的过程可以调用已写好的其他方法转换成为boundary的流对象。 解决方法： 1. 我们可以使用hive来解决，但是hive自带的库函数中没有这样的方法去调用，所以我们可以自己编写一个UDF 来实现； 2. 鉴于hive执行SQL语句 底层是基于mapreduce的过程，我们思考能否使用SparkSQL 来实现，他的优点是低延时 速度快。 我们现有的集群在配置的Spark 1.6的服务，中间调用的转换方法是Java方法所写，虽然scala可以直接调用Java的方法 但这里 为了便于代码方便他人可读，我们在这里使用Java进行代码编写。 sparksql 1.6.0 代码 package com.test.spark import com.alibaba.fastjson.JSONObject; import com.esotericsoftware.kryo.Kryo; import com.esotericsoftware.kryo.serializers.FieldSerializer; import com.huawei.CSCommon.Geo.Boundary; import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.function.Function; import org.apache.spark.serializer.KryoRegistrator; import org.apache.spark.sql.DataFrame; import org.apache.spark.sql.Row; import org.apache.spark.sql.RowFactory; import org.apache.spark.sql.hive.HiveContext; import org.apache.spark.sql.types.DataTypes; import org.apache.spark.sql.types.StructField; import org.apache.spark.sql.types.StructType; import java.io.ByteArrayOutputStream; import java.io.Serializable; import java.util.ArrayList; import java.util.List; /** * 这里进行性能优化： 使用Kryo进行序列化(需要手动注册) * Created by x50005784 on 2019/7/19. */ public class KryoTest { public static class Boun implements Serializable { private byte[] boun; private String area_code; public String getArea_code() { return area_code; } public void setArea_code(String area_code) { this.area_code = area_code; } public byte[] getBoun() { return boun; } public void setBoun(byte[] boun) { this.boun = boun; } } public static class toKryoRegistrator implements KryoRegistrator { @Override public void registerClasses(Kryo kryo) { //在Kryo序列化库中注册自定义的类 kryo.register(Boun.class,new FieldSerializer(kryo,Boun.class)); } } public static void main(String[] args) { SparkConf conf = new SparkConf().setAppName(&quot;Java Spark1.6.0 xjh KryoTest&quot;); conf.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;); //使用Kryo序列化库 conf.set(&quot;spark.kryo.registrator&quot;, toKryoRegistrator.class.getName()); //在Kryo序列化库中注册自定义的类集合 conf.set(&quot;spark.kryoserializer.buffer.max&quot;,&quot;1g&quot;); // 增大spark.kryoserializer.buffer.max参数,默认是64 conf.set(&quot;spark.akka.frameSize&quot;,&quot;256&quot;); //增大spark.akka.frameSize数值，默认是256 // conf.set(&quot;spark.rdd.compress&quot;, &quot;true&quot;); //rdd持久化操作是使用压缩机制 JavaSparkContext sc=new JavaSparkContext(conf); HiveContext sqlContext = new HiveContext(sc); //args[0]=&quot;hdfs://cscloud-rs-hadoop293.huawei.com:8020/user/hive/warehouse/x50005784.db/pm00_base_areaborder_text_10&quot; DataFrame parquetFile = sqlContext.read().parquet(args[0]); parquetFile.registerTempTable(&quot;people&quot;); DataFrame jsonsql = sqlContext.sql(&quot;select * from people&quot;); List&lt;String&gt; geoJsonList = jsonsql.javaRDD().map(new Function&lt;Row, String&gt;() { public String call(Row row) { return new String(row.get(11).toString()); //这里是得到geojson的string形式 } }).collect(); List&lt;String&gt; area_codeList = jsonsql.javaRDD().map(new Function&lt;Row, String&gt;() { public String call(Row row) { return new String(row.get(0).toString()); } }).collect(); System.out.println(&quot;size: &quot;+area_codeList.size()); String schemaString=&quot;area_code boundary&quot;; List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;(); StructField field=null; field= DataTypes.createStructField(&quot;area_code&quot;,DataTypes.StringType,true); fields.add(field); field=DataTypes.createStructField(&quot;boundary&quot;,DataTypes.BinaryType,true); fields.add(field); StructType schema = DataTypes.createStructType(fields); //得到表结构 scmea List&lt;ByteArrayOutputStream&gt; baoList=new ArrayList&lt;ByteArrayOutputStream&gt;(); for (int i=0;i&lt;geoJsonList.size();i++){ Boundary boundary = new Boundary(); System.out.println(&quot;area_code: &quot;+area_codeList.get(i)); JSONObject json = JSONObject.parseObject(geoJsonList.get(i)); boundary.loadFromGeoJson(json); ByteArrayOutputStream baos=new ByteArrayOutputStream(); try { boundary.saveTo(baos); baoList.add(baos); continue; } catch (Exception e) { e.printStackTrace(); } } ArrayList&lt;test.Boun&gt; temp=new ArrayList&lt;&gt;(area_codeList.size()); for (int i=0;i&lt;area_codeList.size();i++){ test.Boun b=new test.Boun(); b.setArea_code(area_codeList.get(i)); b.setBoun(baoList.get(i).toByteArray()); temp.add(b); } JavaRDD&lt;test.Boun&gt; bounrdd=sc.parallelize(temp,10); JavaRDD&lt;Row&gt; rowRDD=bounrdd.map(new Function&lt;test.Boun, Row&gt;() { @Override public Row call(test.Boun v1) throws Exception { String s=v1.getArea_code(); byte[] b=v1.getBoun(); return RowFactory.create(s,b); } }); DataFrame bounDF=sqlContext.createDataFrame(rowRDD,schema); bounDF = bounDF.join(jsonsql,&quot;area_code&quot;); bounDF.write().mode(&quot;overwrite&quot;).saveAsTable(args[1]); sc.stop(); } } 输入数据集： yarn clusrer提交： spark-submit --master yarn --deploy-mode cluster --class com.test.spark.KryoTest --driver-memory 6g --executor-memory 16g --executor-cores 2 --num-executors 20 /home/x50005784/SparkBoundary-1.0-SNAPSHOT.jar hdfs://cscloud-rs-hadoop293.huawei.com:8020/user/hive/warehouse/x50005784.db/pm00_base_areaborder_text_asc x50005784.boundary_all_row_xjh 输出结果： 注意： 在上述的代码中，因为输入源数据集庞大，在这里我们使用Kryo进行序列化减少占内存空间， 并且设置合适的参数以保证程序的运行。在实际应用中，为了保证应用程序能够正常运行，我们需要需要增大kryo buffer的大小， 为了不让executor memory和driver memory在程序运行时不过多浪费，我们可以用Dr.elephant来监控程序作业。 spark处理dataframe数据时，往往遇到&quot;…cannot be cast to …&quot;这种数据不匹配的问题，主要是因为我们代码中spark指定的数据类型和数据源类型不一致。https://blog.csdn.net/qq_35022142/article/details/79800394 sparkSQL输入的hive表中的parquet文件 需要用hive创建的，用impala创建的表会报错(string无法正常解析 他得到的数值变成乱码)。" />
<link rel="canonical" href="https://uzzz.org/2019/08/12/793814.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/12/793814.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-12T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"前言 场景： 现在我们有一个需求 hive中我们有一个大容量的表 存放了许多属性的属性，我们需要将数据表中的牟烈数据JSon数据转换成为二进制的流对象 供mongoDB使用。 转换的过程可以调用已写好的其他方法转换成为boundary的流对象。 解决方法： 1. 我们可以使用hive来解决，但是hive自带的库函数中没有这样的方法去调用，所以我们可以自己编写一个UDF 来实现； 2. 鉴于hive执行SQL语句 底层是基于mapreduce的过程，我们思考能否使用SparkSQL 来实现，他的优点是低延时 速度快。 我们现有的集群在配置的Spark 1.6的服务，中间调用的转换方法是Java方法所写，虽然scala可以直接调用Java的方法 但这里 为了便于代码方便他人可读，我们在这里使用Java进行代码编写。 sparksql 1.6.0 代码 package com.test.spark import com.alibaba.fastjson.JSONObject; import com.esotericsoftware.kryo.Kryo; import com.esotericsoftware.kryo.serializers.FieldSerializer; import com.huawei.CSCommon.Geo.Boundary; import org.apache.spark.SparkConf; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.function.Function; import org.apache.spark.serializer.KryoRegistrator; import org.apache.spark.sql.DataFrame; import org.apache.spark.sql.Row; import org.apache.spark.sql.RowFactory; import org.apache.spark.sql.hive.HiveContext; import org.apache.spark.sql.types.DataTypes; import org.apache.spark.sql.types.StructField; import org.apache.spark.sql.types.StructType; import java.io.ByteArrayOutputStream; import java.io.Serializable; import java.util.ArrayList; import java.util.List; /** * 这里进行性能优化： 使用Kryo进行序列化(需要手动注册) * Created by x50005784 on 2019/7/19. */ public class KryoTest { public static class Boun implements Serializable { private byte[] boun; private String area_code; public String getArea_code() { return area_code; } public void setArea_code(String area_code) { this.area_code = area_code; } public byte[] getBoun() { return boun; } public void setBoun(byte[] boun) { this.boun = boun; } } public static class toKryoRegistrator implements KryoRegistrator { @Override public void registerClasses(Kryo kryo) { //在Kryo序列化库中注册自定义的类 kryo.register(Boun.class,new FieldSerializer(kryo,Boun.class)); } } public static void main(String[] args) { SparkConf conf = new SparkConf().setAppName(&quot;Java Spark1.6.0 xjh KryoTest&quot;); conf.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;); //使用Kryo序列化库 conf.set(&quot;spark.kryo.registrator&quot;, toKryoRegistrator.class.getName()); //在Kryo序列化库中注册自定义的类集合 conf.set(&quot;spark.kryoserializer.buffer.max&quot;,&quot;1g&quot;); // 增大spark.kryoserializer.buffer.max参数,默认是64 conf.set(&quot;spark.akka.frameSize&quot;,&quot;256&quot;); //增大spark.akka.frameSize数值，默认是256 // conf.set(&quot;spark.rdd.compress&quot;, &quot;true&quot;); //rdd持久化操作是使用压缩机制 JavaSparkContext sc=new JavaSparkContext(conf); HiveContext sqlContext = new HiveContext(sc); //args[0]=&quot;hdfs://cscloud-rs-hadoop293.huawei.com:8020/user/hive/warehouse/x50005784.db/pm00_base_areaborder_text_10&quot; DataFrame parquetFile = sqlContext.read().parquet(args[0]); parquetFile.registerTempTable(&quot;people&quot;); DataFrame jsonsql = sqlContext.sql(&quot;select * from people&quot;); List&lt;String&gt; geoJsonList = jsonsql.javaRDD().map(new Function&lt;Row, String&gt;() { public String call(Row row) { return new String(row.get(11).toString()); //这里是得到geojson的string形式 } }).collect(); List&lt;String&gt; area_codeList = jsonsql.javaRDD().map(new Function&lt;Row, String&gt;() { public String call(Row row) { return new String(row.get(0).toString()); } }).collect(); System.out.println(&quot;size: &quot;+area_codeList.size()); String schemaString=&quot;area_code boundary&quot;; List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;(); StructField field=null; field= DataTypes.createStructField(&quot;area_code&quot;,DataTypes.StringType,true); fields.add(field); field=DataTypes.createStructField(&quot;boundary&quot;,DataTypes.BinaryType,true); fields.add(field); StructType schema = DataTypes.createStructType(fields); //得到表结构 scmea List&lt;ByteArrayOutputStream&gt; baoList=new ArrayList&lt;ByteArrayOutputStream&gt;(); for (int i=0;i&lt;geoJsonList.size();i++){ Boundary boundary = new Boundary(); System.out.println(&quot;area_code: &quot;+area_codeList.get(i)); JSONObject json = JSONObject.parseObject(geoJsonList.get(i)); boundary.loadFromGeoJson(json); ByteArrayOutputStream baos=new ByteArrayOutputStream(); try { boundary.saveTo(baos); baoList.add(baos); continue; } catch (Exception e) { e.printStackTrace(); } } ArrayList&lt;test.Boun&gt; temp=new ArrayList&lt;&gt;(area_codeList.size()); for (int i=0;i&lt;area_codeList.size();i++){ test.Boun b=new test.Boun(); b.setArea_code(area_codeList.get(i)); b.setBoun(baoList.get(i).toByteArray()); temp.add(b); } JavaRDD&lt;test.Boun&gt; bounrdd=sc.parallelize(temp,10); JavaRDD&lt;Row&gt; rowRDD=bounrdd.map(new Function&lt;test.Boun, Row&gt;() { @Override public Row call(test.Boun v1) throws Exception { String s=v1.getArea_code(); byte[] b=v1.getBoun(); return RowFactory.create(s,b); } }); DataFrame bounDF=sqlContext.createDataFrame(rowRDD,schema); bounDF = bounDF.join(jsonsql,&quot;area_code&quot;); bounDF.write().mode(&quot;overwrite&quot;).saveAsTable(args[1]); sc.stop(); } } 输入数据集： yarn clusrer提交： spark-submit --master yarn --deploy-mode cluster --class com.test.spark.KryoTest --driver-memory 6g --executor-memory 16g --executor-cores 2 --num-executors 20 /home/x50005784/SparkBoundary-1.0-SNAPSHOT.jar hdfs://cscloud-rs-hadoop293.huawei.com:8020/user/hive/warehouse/x50005784.db/pm00_base_areaborder_text_asc x50005784.boundary_all_row_xjh 输出结果： 注意： 在上述的代码中，因为输入源数据集庞大，在这里我们使用Kryo进行序列化减少占内存空间， 并且设置合适的参数以保证程序的运行。在实际应用中，为了保证应用程序能够正常运行，我们需要需要增大kryo buffer的大小， 为了不让executor memory和driver memory在程序运行时不过多浪费，我们可以用Dr.elephant来监控程序作业。 spark处理dataframe数据时，往往遇到&quot;…cannot be cast to …&quot;这种数据不匹配的问题，主要是因为我们代码中spark指定的数据类型和数据源类型不一致。https://blog.csdn.net/qq_35022142/article/details/79800394 sparkSQL输入的hive表中的parquet文件 需要用hive创建的，用impala创建的表会报错(string无法正常解析 他得到的数值变成乱码)。","@type":"BlogPosting","url":"https://uzzz.org/2019/08/12/793814.html","headline":"SparkSQL应用——数据转换器","dateModified":"2019-08-12T00:00:00+08:00","datePublished":"2019-08-12T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/12/793814.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>SparkSQL应用——数据转换器</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h3><a id="_0"></a>前言</h3> 
  <p>场景： 现在我们有一个需求 hive中我们有一个大容量的表 存放了许多属性的属性，我们需要将数据表中的牟烈数据JSon数据转换成为二进制的流对象 供mongoDB使用。 转换的过程可以调用已写好的其他方法转换成为boundary的流对象。<br> 解决方法： 1. 我们可以使用hive来解决，但是hive自带的库函数中没有这样的方法去调用，所以我们可以自己编写一个UDF 来实现； 2. 鉴于hive执行SQL语句 底层是基于mapreduce的过程，我们思考能否使用SparkSQL 来实现，他的优点是低延时 速度快。</p> 
  <p>我们现有的集群在配置的Spark 1.6的服务，中间调用的转换方法是Java方法所写，虽然scala可以直接调用Java的方法 但这里 为了便于代码方便他人可读，我们在这里使用Java进行代码编写。</p> 
  <h3><a id="sparksql_160__5"></a>sparksql 1.6.0 代码</h3> 
  <pre><code>package com.test.spark
import com.alibaba.fastjson.JSONObject;
import com.esotericsoftware.kryo.Kryo;
import com.esotericsoftware.kryo.serializers.FieldSerializer;
import com.huawei.CSCommon.Geo.Boundary;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.serializer.KryoRegistrator;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.hive.HiveContext;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.io.ByteArrayOutputStream;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.List;

/**
 * 这里进行性能优化： 使用Kryo进行序列化(需要手动注册)
 * Created by x50005784 on 2019/7/19.
 */
public class KryoTest {
    public static class Boun implements Serializable {
        private byte[] boun;
        private String area_code;

        public String getArea_code() {
            return area_code;
        }

        public void setArea_code(String area_code) {
            this.area_code = area_code;
        }

        public byte[] getBoun() {
            return boun;
        }

        public void setBoun(byte[] boun) {
            this.boun = boun;
        }
    }
    public static class toKryoRegistrator implements KryoRegistrator {
        @Override
        public void registerClasses(Kryo kryo) {
            //在Kryo序列化库中注册自定义的类
            kryo.register(Boun.class,new FieldSerializer(kryo,Boun.class));
        }
    }

    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("Java Spark1.6.0 xjh KryoTest");
        conf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer");  //使用Kryo序列化库
        conf.set("spark.kryo.registrator", toKryoRegistrator.class.getName());      //在Kryo序列化库中注册自定义的类集合
        conf.set("spark.kryoserializer.buffer.max","1g");   // 增大spark.kryoserializer.buffer.max参数,默认是64
        conf.set("spark.akka.frameSize","256");             //增大spark.akka.frameSize数值，默认是256
//        conf.set("spark.rdd.compress", "true"); //rdd持久化操作是使用压缩机制
        JavaSparkContext sc=new JavaSparkContext(conf);
        HiveContext sqlContext = new HiveContext(sc);
        //args[0]="hdfs://cscloud-rs-hadoop293.huawei.com:8020/user/hive/warehouse/x50005784.db/pm00_base_areaborder_text_10"
        DataFrame parquetFile = sqlContext.read().parquet(args[0]);
        parquetFile.registerTempTable("people");
        DataFrame jsonsql = sqlContext.sql("select * from people");

        List&lt;String&gt; geoJsonList = jsonsql.javaRDD().map(new Function&lt;Row, String&gt;() {
            public String call(Row row) {
                return new String(row.get(11).toString());   //这里是得到geojson的string形式
            }
        }).collect();
        List&lt;String&gt; area_codeList = jsonsql.javaRDD().map(new Function&lt;Row, String&gt;() {
            public String call(Row row) {
                return new String(row.get(0).toString());
            }
        }).collect();
        System.out.println("size: "+area_codeList.size());

        String schemaString="area_code boundary";
        List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;();
        StructField field=null;
        field= DataTypes.createStructField("area_code",DataTypes.StringType,true);
        fields.add(field);
        field=DataTypes.createStructField("boundary",DataTypes.BinaryType,true);
        fields.add(field);
        StructType schema = DataTypes.createStructType(fields);
        //得到表结构 scmea

        List&lt;ByteArrayOutputStream&gt; baoList=new ArrayList&lt;ByteArrayOutputStream&gt;();
        for (int i=0;i&lt;geoJsonList.size();i++){
            Boundary boundary = new Boundary();
            System.out.println("area_code: "+area_codeList.get(i));

            JSONObject json = JSONObject.parseObject(geoJsonList.get(i));
            boundary.loadFromGeoJson(json);
            ByteArrayOutputStream baos=new ByteArrayOutputStream();
            try {
                boundary.saveTo(baos);
                baoList.add(baos);
                continue;
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        ArrayList&lt;test.Boun&gt; temp=new ArrayList&lt;&gt;(area_codeList.size());
        for (int i=0;i&lt;area_codeList.size();i++){
            test.Boun b=new test.Boun();
            b.setArea_code(area_codeList.get(i));
            b.setBoun(baoList.get(i).toByteArray());
            temp.add(b);
        }
        JavaRDD&lt;test.Boun&gt; bounrdd=sc.parallelize(temp,10);

        JavaRDD&lt;Row&gt; rowRDD=bounrdd.map(new Function&lt;test.Boun, Row&gt;() {
            @Override
            public Row call(test.Boun v1) throws Exception {
                String s=v1.getArea_code();
                byte[] b=v1.getBoun();
                return RowFactory.create(s,b);
            }
        });

        DataFrame bounDF=sqlContext.createDataFrame(rowRDD,schema);

        bounDF = bounDF.join(jsonsql,"area_code");
        bounDF.write().mode("overwrite").saveAsTable(args[1]);
        sc.stop();
    }
}
</code></pre> 
  <p>输入数据集：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722202529825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODA3Mzg4NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722202643835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODA3Mzg4NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> yarn clusrer提交：<br> spark-submit --master yarn --deploy-mode cluster --class com.test.spark.KryoTest --driver-memory 6g --executor-memory 16g --executor-cores 2 --num-executors 20 /home/x50005784/SparkBoundary-1.0-SNAPSHOT.jar hdfs://cscloud-rs-hadoop293.huawei.com:8020/user/hive/warehouse/x50005784.db/pm00_base_areaborder_text_asc x50005784.boundary_all_row_xjh</p> 
  <p>输出结果：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722202720226.png" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722202844910.png" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722203322446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODA3Mzg4NQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 注意：</p> 
  <ol> 
   <li> <p>在上述的代码中，因为输入源数据集庞大，在这里我们使用Kryo进行序列化减少占内存空间， 并且设置合适的参数以保证程序的运行。在实际应用中，为了保证应用程序能够正常运行，我们需要需要增大kryo buffer的大小， 为了不让executor memory和driver memory在程序运行时不过多浪费，我们可以用Dr.elephant来监控程序作业。</p> </li> 
   <li> <p>spark处理dataframe数据时，往往遇到"…cannot be cast to …"这种数据不匹配的问题，主要是因为我们代码中spark指定的数据类型和数据源类型不一致。<a href="https://blog.csdn.net/qq_35022142/article/details/79800394" rel="nofollow" data-token="b98f3c3458fde5acb1e9fe54efa7bdaa">https://blog.csdn.net/qq_35022142/article/details/79800394</a></p> </li> 
   <li> <p>sparkSQL输入的hive表中的parquet文件 需要用hive创建的，用impala创建的表会报错(string无法正常解析 他得到的数值变成乱码)。</p> </li> 
  </ol> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
