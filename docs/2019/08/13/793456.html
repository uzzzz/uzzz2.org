<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>【算法工程师】机器学习面试问题总结 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="【算法工程师】机器学习面试问题总结" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="哎呀，要秋招了啊～ 不禁感叹：How time flys～ 重新出发～ 后期我发现还是需要把相关文章的链接放上来的，方便大家深入理解记忆，如果你没时间就直接看文字，如果有时间记得把链接点开看看哦～都是大佬的精华～ 一切为了暑期实习！！！ 一切为了暑期实习！！！ 一切为了暑期实习！！！ 机器学习 SVM（重点） 1. SVM原理和推导 原理：SVM试图寻找一个超平面使正负样本分开，并使得几何间隔最大。 ⚠️（注意分清以下这三种情况） 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分SVM 当训练样本近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性SVM 当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性分类器，即非线性SVM 推导（十分重要）我自己写的，字很随意啦～ 2. SVM 为什么要引入拉格朗日的优化方法 将拉格朗日对偶性应用到求解原始问题上，通过求解对偶问题进而求得原始问题的最优解，原因有二： 对偶问题往往更容易求解； 自然引入核函数，继而推广到非线性分类问题。 3. 为什么要选择最大间隔分类器？ 从数学上考虑： 误分类次数和几何间隔之间存在下列关系，几何间隔越大，误分类次数越少。 感知机利用误分类最小策略，求得分离超平面，不过此时解有无数个；而线性可分SVM利用间隔最大求得最优分离超平面，求得唯一解，而且此时的模型鲁棒性好，对未知样本泛化能力最强。 4. 样本失衡会对SVM的结果产生影响吗？如何解决SVM样本失衡问题？样本比例失衡时，使用什么指标评价分类器的好坏？ 样本失衡会对结果产生影响，分类超平面会靠近样本少的类别。原因：因为使用软间隔最大化，假设对所有类别使用相同的惩罚因子，而优化目标是最小化惩罚量，所以靠近样本少的类别惩罚量少。 解决SVM样本失衡问题方法： 对不同的类别赋予不同的惩罚因子（C），训练样本越少，C越大。缺点：偏离原始样本的概率分布。 对样本的少的类别，基于某种策略进行采样。 基于核函数解决问题。 当样本比例不均衡时，使用ROC曲线。 5. SVM如何解决多分类问题 https://www.cnblogs.com/CheeseZH/p/5265959.html 直接法：直接修改目标函数，将多个分类面的参数求解合并到一个目标函数上，一次性进行求解。 间接法： One VS One：任意两个样本之间训练一个分类模型，假设有k类，则需要k(k-1)/2个模型。对未知样本进行分类时，得票最多的类别即为未知样本的类别。libsvm使用这个方法。 One VS Rest：训练时依次将某类化为类，将其他所有类别划分为另外一类，共需要训练k个模型。训练时具有最大分类函数值的类别是未知样本的类别。 6. SVM适合处理什么样的数据？ 高维、稀疏、样本少的数据。 7. SVM为什么对缺失数据敏感？（数据缺失某些特征） SVM没有缺失值的处理策略； SVM希望样本在特征空间中线性可分，特征空间的好坏影响SVM性能； 缺失特征数据影响训练结果。 8. sklearn.svm参数 栗子： class sklearn.svm.SVC( C=1.0, kernel=&#39;rbf&#39;, degree=3, gamma=&#39;auto&#39;, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=&#39;ovr&#39;, random_state=None ) 重要参数：(理解含义和对模型的影响) C : float, optional (default=1.0) 误差项的惩罚参数，一般取值为10的n次幂，如10的-5次幂，10的-4次幂…10的0次幂，10，1000,1000，在python中可以使用pow（10，n） n=-5~inf C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱。 C值小，对误分类的惩罚减小，容错能力增强，泛化能力较强。 kernel : string, optional (default=’rbf’) svc中指定的kernel类型。 可以是： ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ 或者自己指定。 默认使用‘rbf’ 。 degree : int, optional (default=3) 当指定kernel为 ‘poly’时，表示选择的多项式的最高次数，默认为三次多项式。 若指定kernel不是‘poly’,则忽略，即该参数只对‘poly’有作用。 gamma : float, optional (default=’auto’) 当kernel为‘rbf’, ‘poly’或‘sigmoid’时的kernel系数。 如果不设置，默认为 ‘auto’ ，此时，kernel系数设置为：1/n_features coef0 : float, optional (default=0.0) kernel函数的常数项。 只有在 kernel为‘poly’或‘sigmoid’时有效，默认为0。 9. SVM的损失函数—Hinge Loss（合页损失函数） https://www.jianshu.com/p/fe14cd066077 hinge loss图像 表达式 10. SMO算法实现SVM(思想、步骤、常见问题)（我这个还不熟悉，mark） 思想：将大的优化问题分解为多个小的优化问题，求解小的优化问题往往更简单，同时顺序求解小问题得出的结果和将他们作为整体求得的结果一致。 步骤：1. 选取一对需要更新的变量ai和aj（阿尔法）2. 固定除ai和aj以外的所有变量，求解对偶问题获得更新ai、aj、b。 常见问题—如何选取ai和aj和b？ 选取违反KKT条件最严重的ai，在针对这个ai选择最有可能获得较大修正步长的aj； b一般选取支持向量求解的平均值。 11. SVM如何解决非线性问题？你所知道的核函数？ 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。 常用核函数（重点sigmoid、RBF（名字一定要记住啦：高斯径向基核函数）、要会写核函数的公式哦～） 12. 线性核与RBF核的区别？ 训练速度：线性核只有惩罚因子一个参数，训练速度快，RBF还需要调节gamma； 训练结果：线性核得到的权重w能反映出特征的重要性，由此进行特征选择，RBF无法解释； 训练数据：线性核适合样本特征&gt;&gt;样本数量的，RBF核相反。（揭示了如何选择核函数） 13. SVM和LR的联系和区别 联系： 都是判别式模型 都是有监督的分类算法 如果不考虑核函数，都是线性分类算法 区别： LR可以输出概率，SVM不可以 损失函数不同，即分类机制不同 SVM通过引入核函数解决非线性问题，LR则不是 原因：LR里每个样本点都要参与核计算，计算复杂度太高，故LR通常不用核函数。 SVM计算复杂，效果比LR好，适用于小数据集；LR计算快，适用于大数据集，用于在线学习 SVM分类只与分类超平面附近的点有关，LR与所有点都有关系 SVM是结构风险最小化，LR则是经验风险最小化 结构风险最小化就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，减小泛化误差。为了达到结构风险最小化的目的，最常用的方法就是添加正则项。 14. SVM如何防止过拟合？ https://www.jianshu.com/p/9b03cac58966 通过引入松弛变量，松弛变量可以容忍异常点的存在。 15. KKT（Karysh-Kuhn-Tucker）条件有哪些，完整描述？ 为什么要从原始问题转换成对偶问题： 对偶问题将原始问题中的约束转为了对偶问题中的等式约束； 方便核函数的引入； 改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关； 求解更高效，因为只用求解alpha系数，而alpha系数只有支持向量才非0，其它全部为0。 集成学习（重点） 1. 决策树和随机森林的区别 决策树 + Bagging + 随机选择特征 = 随机森林，随机森林可以有效防止过拟合。 2. 随机森林里面用的哪种决策树 CART 决策树或其他 3. 随机森林的原理？如何进行调参？树的深度一般如何确定，一般为多少？ 原理：RF是一种集成算法，属于bagging类，它通过集成多个决策树模型，通过对基模型的结果采用投票法或者取平均来获得最终结果，使得最终模型有较高的准确度和泛化性能。 调参： 还是看刘建平老师的这篇：https://www.cnblogs.com/pinard/p/6160412.html RF和GBDT调参过程类似，可以对比记忆： 无参数拟合–&gt;n_estimators调参–&gt;max_depth, min_sample_split–&gt;min_sample_split, min_samples_leaf–&gt;max_features 如何确定树的深度：当训练样本多，数据特征维度多的时候需要限制这个参数，具体取决于数据分布，一般在10-100之间。 3. Bagging 和 Boosting的区别 样本选择：Bagging有放回的选取训练集，并且从原始数据集中选取的各轮训练集之间相互独立；Boosting每次都使用全部数据，只是每个样例的权重不同。 样例权重：Bagging采用均匀采样，每个样例的权重相同；Boosting每轮训练都依据上一轮训练结果更新样例权重，错误率越大的样例，权重越大。 预测函数：Bagging每个基函数的预测结果权重相同；Boosting中预测误差越小的基模型有更大的权重。 偏差和方差：Bagging得出的结果低方差，Boosting低偏差。 并行计算：Bagging可以并行生成基模型，Boosting各个预测函数只能顺序生成，后一轮模型的参数需要前一轮的预测结果。 4. GBDT调参 我觉得啊，一般面试官如果问我们这种题目，一定是要求我们使用过这个算法，如果使用过就要理解记住，没使用过就坦诚的说没用过，大家可以跟着下面这个链接的刘建平老师学习一遍。 具体实例：https://www.cnblogs.com/pinard/p/6143927.html 参数分类： Boosting框架参数：n_estimators, learning_rate, subsample CART回归树参数（与决策树类似）：max_features, max_depth, min_sample_split, min_samples_leaf 大致步骤： 无参数拟合–&gt;固定learning_rate，estimators调参–&gt;max_depth, min_sample_split–&gt;min_sample_split, min_samples_leaf–&gt;拟合查看–&gt;max_features–&gt;subsample–&gt;不断减小learning_rate，加倍estimators来拟合 5. RF、GBDT之间的区别（重要） 此问题充分理解，你需要这些： https://blog.csdn.net/data_scientist/article/details/79022025 https://blog.csdn.net/xwd18280820053/article/details/68927422 https://blog.csdn.net/m510756230/article/details/82051807 相同点：都是由多棵树组成，结果由多棵树共同决定。 不同点： GBDT是回归树，RF可以是回归树也可以是分类树； GBDT对异常值特别敏感，RF则没有； GBDT只能串行生成多个模型，RF可以并行； GBDT的结果有多个结果求和或者加权求和，RF是由投票选出结果； GBDT是通过减少偏差来提高模型性能，RF是通过减少方差； RF对所有训练集一视同仁，GBDT是基于权值的弱分类器。 6. 随机森林的优缺点 优点： 相比于其他算法，在训练速度和预测准确度上有很大的优势； 能够处理很高维的数据，不用选择特征，在模型训练完成后，能够给出特征的重要性； 可以写成并行化方法； 缺点：在噪声较大的分类回归问题上，容易过拟合。 7. GBDT的关键？GBDT中的树是什么树？ 关键：利用损失函数的负梯度方向作为残差的近似值来拟合新的CART回归树。 CART回归树。 8. GBDT和XGB的区别 GBDT以CART为基分类器，XGB则支持多种分类器； GBDT只用到了一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶与二阶导数，并且可以自定义代价函数，只要一阶二阶可导； XGBoost与GDBT都是逐次迭代来提高模型性能，但是XGBoost在选取最佳切分点时可以开启多线程进行，大大提高了运行速度； 新增了Shrinkage和column subsampling，为了防止过拟合； 对缺失值有自动分裂处理（默认归于左子树）； 如何处理的缺失值： Xgboost 在处理带缺失值的特征时，先对非缺失的样本进行排序，对该特征缺失的样本先不处理，然后在遍历每个分裂点时，将这些缺失样本分别划入左子树和右子树来计算损失然后求最优。如果训练样本中没有缺失值，而预测过程中出现了缺失值，那么样本会被默认分到右子树。 xgb损失函数加入正则化，控制模型复杂度，使模型简单，防止过拟合。 9. LGB和XGB的区别（抓住主要区别，理解要有深度） lgb官方文档：http://lightgbm.apachecn.org/#/docs/4 https://www.cnblogs.com/infaraway/p/7890558.html 树的切分策略不同： xgb基于level-wise，对每一层节点进行无差别分裂，造成不必要的开销； lgb基于leaf-wise，在当前所有叶子节点中选择分裂增益最大的节点进行分裂； 实现并行方式不同： xgb使用基于 pre-sorted 决策树算法； lgb使用基于histogram决策树算法，对离散特征进行分裂时，特征的每个取值为一个桶； lgb支持直接输入categorical feature，对类别特征无须进行one-hot处理； 优化通信代价不同：lgb支持特征并行、数据并行。 10. GBDT的算法步骤（我没找到推导呢） 10. 随机森林生成过程 https://www.cnblogs.com/liuwu265/p/4690715.html 样本集选择：从原始样本中有放回的抽取N个训练集，各个训练集之间相互独立，共进行k轮抽取； 决策树生成：假设特征空间共有D个特征，随机选择d个特征，构成新的特征空间，用于训练单棵决策树，共k轮，生成k棵决策树； 模型合成：生成的k棵决策树相互独立，各个基模型之间权重相同，如果是分类问题，则使用投票法决定最终结果，如果是回归问题，则使用平均法； 模型验证：模型验证本身需要验证集，但在此处我们无须额外设置验证集，只需使用原始样本中没有使用过的即可。 11. xgboost如何确定特征和分裂点的？ XGBoost使用了和CART回归树一样的想法，利用贪婪算法。基于目标函数，遍历所有特征的所有特征划分点，具体做法就是分裂后的目标函数值大于分裂之前的就进行分裂。 12. XGB是如何给出特征重要性评分的？ 建议多看下以下链接： https://blog.csdn.net/waitingzby/article/details/81610495 https://blog.csdn.net/sujinhehehe/article/details/84201415 https://www.cnblogs.com/haobang008/p/5929378.html 特征权重（weight）：指的是在所有树中，某特征被用来分裂节点的次数； 如何计算：一个特征对分裂点性能度量（gini或者其他）的提升越大（越靠近根节点）其权重越大，该特征被越多提升树选择来进行分裂，该特征越重要，最终将一个特征在所有提升树中的结果进行加权求和然后求平均即可。 源码片段（加深理解）： 主要是对每个特征进行计数操作： if importance_type == &#39;weight&#39;: # do a simpler tree dump to save time trees = self.get_dump(fmap, with_stats=False) fmap = {} for tree in trees: for line in tree.split(&#39;\n&#39;): # look for the opening square bracket arr = line.split(&#39;[&#39;) # if no opening bracket (leaf node), ignore this line if len(arr) == 1: continue # extract feature name from string between [] fid = arr[1].split(&#39;]&#39;)[0].split(&#39;&lt;&#39;)[0] if fid not in fmap: # if the feature hasn&#39;t been seen yet fmap[fid] = 1 else: fmap[fid] += 1 return fmap 13. XGB如何消除残差的，目标函数是什么？ https://www.cnblogs.com/palantir/p/10671119.html https://blog.csdn.net/guoxinian/article/details/79243307 14. GDBT在处理分类和回归问题时有什么区别？（感觉还有补全的地方） 损失函数不同： 分类：指数、对数； 回归：均方差、绝对值。 XGB如何防止过拟合 Shrinkage and Column Subsampling。 Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型； Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种： 按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点； 随机选择特征，则建树前随机选择一部分特征然后分裂就只遍历这些特征。一般情况下前者效果更好。 EM算法 1. 采用EM算法求解的模型有哪些？为什么不用牛顿法或者梯度下降法？（感觉第二个问题有点错误，mark） 高斯混合模型、协同过滤、KMeans 求和的项随着隐变量的数量随指数上升，梯度计算带来麻烦，而EM是非梯度优化算法。 2. 用EM算法推导解释KMeans KMeans中，每个聚类簇的中心就是隐含变量。 E步：随机初始化k个聚类中心 M步：计算每个样本点最近的质心，并将他聚类到这个质心 重复以上两步，直到聚类中心不发生变化为止。 决策树 1. ID3和C4.5的优缺点 ID3 C4.5 优点 实现简单 1. 可以处理连续型特征 2. 易于理解，准确率高 缺点 1. 只能处理离散特征 2. 倾向于选择取值较多的特征 1. 在构造树的过程中，对数据多次扫描排序，低效 2. 只能够用于驻留在内存中的数据，当数据大到无法在内存容纳时，程序无法执行 2. 决策树处理连续值的方法 连续属性离散化，常用的离散化策略是二分法（C4.5）： 3. 决策树的剪枝策略 目的：简化决策树模型，提高模型泛化能力； 基本思想：减去某些子树和叶节点，将其根节点作为新的叶子节点，实现简化模型。 损失函数： 剪枝策略： 预剪枝：在构造决策树的过程中，在对节点划分之前进行估计，若划分后不能带来决策树性能和泛化能力的提升，则不进行划分，并将此节点作为叶节点。 后剪枝：构造完决策树之后，自底向上搜索，对每个非叶节点进行考察，若将该子树去除变为叶节点能带来决策树泛化性能的提升，则将该节点作为叶节点。 对比：后剪枝的分支比预剪枝的分支要多一些，不容易欠拟合，泛化能力强，但是由于后剪枝在构造完成决策树之后，而且还需要自底向上进行搜索故时间开销大。 4. 决策树的构造过程 https://shuwoom.com/?p=1452 特征选择：在所有特征中选择一个特征，作为当前节点的划分标准：ID3(信息增益)、C4.5(信息增益比)、CART(gini系数)； 决策树生成：依据特征评估标准，从上到下的递归的生成子节点，直到数据集不可分时，停止生长； 剪枝：决策时容易过拟合，通过剪枝，简化模型，降低过拟合。 5. 基于树的模型有必要做标准化吗？ https://blog.csdn.net/answer3lin/article/details/84961694 不必要；概率模型（树模型）只关心变量的分布和变量之间的条件概率。 6. CART回归树和CART决策树的构造过程 特征选择+决策树构造+决策树剪枝 cart回归树 cart回归树对某个特征的每个分量，将数据集划分为大于该分量和小于该分量两部分，同时计算对应y的均值，并计算均方误差函数的值，选择具有最小误差值的分量，并将其作为划分标准，重复该流程知道数据集为空或者前后均方误差下降值小于给定阈值则停止。 cart分类树 选择具有最小的GINI系数的属性和属性值，作为最优分裂属性和最优分裂属性值。 7. 熵 概念：信息所包含不确定大小的度量，一个信息的所包含的不确定性越大，其所含的信息越多。 熵的计算公式： H ( X ) = − ∑ i = 1 N p i l o g 2 p i H(X) = -\sum_ {i=1}^{N}p_i log_2 p_i H(X)=−∑i=1N​pi​log2​pi​ 朴素贝叶斯 1. 朴素贝叶斯的公式 “朴素”的含义：假设各个特征之间相互独立。 2. NB原理及其分类 原理：根据贝叶斯公式，通过某对象的先验概率，计算其后验概率，并选择后验概率最大的类作为该对象所属于的类； 分类：（根据变量的分布不同） NB的伯努利模型，特征是布尔变量，符合0/1分布，在文本分类中，特征就是词是否出现； NB的多项式模型，特征是离散值，符合多项式分布，在文本分类中，特征就是词出现的次数； NB的高斯模型，特征是连续值，符合高斯分布（高斯分布又名正态分布），在文本分类中，特征就是词的TF-IDF值。 3. NB的优缺点 优点： 算法原理简单； 所估计的参数少； 假设条件概率计算是彼此独立的，因此可以用于分布式计算； 属于生成式模型，收敛速度比判别式模型要快； 对缺失数据不太敏感； 天然可以处理多分类问题。 缺点： 假设各个特征之间相互独立这一条件在实际应用中往往是不能成立的； 不能学习到特征之间的相互作用； 对输入数据的表达形式敏感。 4. LR和朴素贝叶斯(NB)之间的区别 https://www.cnblogs.com/wangkundentisy/p/9193217.html 暂时不看。 5. 适用场景 支持大规模数据，并且支持分布式实现； 特征维度可以很高； 可以处理数值型特征和类别型特征； 线性回归与逻辑回归 1. LR推导（重要） 几个小问题： 极大似然的概念：找到参数θ的一个估计值，使得当前样本出现的可能性最大。 为什么极大似然的时候可以相乘：特征之间是独立同分布。 LR的参数计算方法：梯度下降、牛顿法。 2. 逻辑回归和线性回归的区别 线性回归 逻辑回归 对连续值预测 分类 最小二乘法 最大似然估计 拟合函数 预测函数 3. 最小二乘法和最大似然法的区别（没太明白，mark） https://blog.csdn.net/lu597203933/article/details/45032607 4. 为什么用最小二乘而不是最小四乘（没太明白，mark） 5. 介绍一下逻辑回归？它的损失函数是什么？ 一句话介绍逻辑回归：逻辑回归假设数据服从伯努利分布，通过极大化似然函数，利用梯度下降算法来求得参数，实现数据的二分类； 它的损失函数是它的极大似然函数（对数损失函数）： 6. LR 损失函数为什么用极大似然函数？ https://blog.csdn.net/aliceyangxi1987/article/details/80532586 LR的目标是使每个样本的预测都有最大概率，即将所有样本预测后的概率相乘概率最大，这就是极大似然函数； 极大似然函数取对数即为对数损失函数，对数损失函数的训练求解参数比较快，更新速度也稳定； 为什么不用平方损失函数呢？因为平方损失函数的梯度更新速度和sigmoid的梯度有关，而在定义域范围内sigmoid的梯度值&lt;=2.5，训练速度非常慢；而且平方损失函数会导致损失函数是非凸的，不易求解。 7. LR的参数计算方法：梯度下降法，请介绍三种GD的区别 批梯度下降：可以得到全局最优解，缺点是更新每个参数都需要遍历所有数据，计算量大，还有很多冗余计算，在数据非常大的时候，每个参数的更新都是非常慢的； SGD：以高方差频繁更新，使SGD跳到新的或潜在更优的局部解，但是也使得收敛到局部最优解的过程更加复杂； mini-batch SGD：结合了二者的优点，每次选取N个样本，减少了参数更新的次数，可以达到更加稳定的收敛结果。 8. LR的优缺点 http://www.cnblogs.com/ModifyRong/p/7739955.html 优点： 形式简单，模型可解释性非常好，特征权重可以看出不同特征最后结果的影响； 效果不错，可以作为baseline； 占用资源少，特别是内存； 方便输出结果调整； 训练速度快。 缺点： 准确率不高； 对样本不均衡问题无法很好的解决； 对非线性分类问题也是； 本身无法筛选特征，可以和GBDT结合使用。 9. 逻辑斯特回归为什么要对特征进行离散化 稀疏向量做内积乘法运算速度快，计算结果方便存储，易于扩展 离散化后的特征对异常数据有更强的鲁棒性 特征离散化后模型更稳定 LR属于广义线性模型，表达能力有限，特征离散化为N个后，每个变量有自己的权重，相当于引入非线性，表达能力增强，加大拟合 特征离散化后还可以做特征交叉，由M+N个变为M*N个，进一步引入非线性 10. LR为什么用sigmoid函数,这个函数有什么优点和缺点? 为什么： Sigmoid 函数自身的性质 sigmoid 函数连续，单调递增 sigmiod 函数关于（0，0.5） 中心对称 计算sigmoid函数的导数非常的快速 将输入变量的范围从负无穷到正无穷，映射到（0，1），而概率要求正是（0，1）。 逻辑回归认为函数其概率服从伯努利分布，将其写成指数族分布的形式，能够推导出sigmoid函数的形式。 https://blog.csdn.net/a1628864705/article/details/62233395 https://blog.csdn.net/qq_19645269/article/details/79551576 优缺点： 优点： 可以看到sigmoid函数处处连续 -&gt;便于求导； 可以将函数值的范围压缩到[0,1]-&gt;可以压缩数据，且幅度不变； 便于前向传输。 缺点： 在趋向无穷的地方，函数值变化很小，容易缺失梯度，不利于深层神经网络的反馈传输； 幂函数还是比较难算的； 函数均值不为0，当输出大于0时，则梯度方向将大于0，也就是说接下来的反向运算中将会持续正向更新；同理，当输出小于0时，接下来的方向运算将持续负向更新。 11. LR伪代码 读取文件、写入文件以及计算准确率等之前实验做过的或者过于简单的函数功能不列出。 int main() { 读取训练集和测试集，其中训练集每三个样本取前两个作为训练集，第三个作为验证集。 初始化w：for(int i=0;i&lt;Length;i++) w[i]=1; for(int k=0:7) for(int i=0:traincnt)//遍历训练集样本 { CalWeight(i);//计算样本i的权重分数 CalCost(i);//每一维的梯度（代价）计算 Updatew(); //更新w if(i%20==0) //每更新w20次计算一次准确率 { Predict();//预测验证集样本 Cal_acc();//计算准确率 ac[cnt]=accuracy; cnt++; } } output_result();//输出验证集准确率以供调试 output_test_result();//输出测试集预测结果 } void CalWeight(int index){ weight = 当前向量w的转置*样本i向量； } void CalCost(int index){ 计算每一维的梯度，存储在向量数组Cost[]中； } void Updatew(){ 使用w = w - alpha x gradient来更新回归系数（w） } void Predict(){ P = 1/(1+exp(-1* w^T *样本i向量); if(P&gt;0.5) p_label=1; else p_label=0; } PCA 1. PCA原理 具体推导看这里： https://blog.csdn.net/u012421852/article/details/80458340 用于：特征降维，去除冗余和可分性不强的特征； 目标：降维后的各个特征不相关，即特征之间的协方差为0； 原理：基于训练数据X的协方差矩阵的特征向量组成的k阶矩阵U，通过XU得到降维后的k阶矩阵Z； 算法步骤 计算训练样本的协方差矩阵C； 计算C的特征值和特征向量； 将C的特征值降序排列，特征值对应特征向量也依次排列； 假如要得到X的k阶降维矩阵，选取C的前k个特征{u1,u2…uk}，组成降维转换矩阵U； Z = XU，Z即为降维后的矩阵； KMeans 1. K-means 的原理，时间复杂度，优缺点以及改进 原理：对于给定样本集，按照样本之间的距离大小，将样本划分为若干个簇，使簇内距离尽可能小，簇间距离尽可能大； 时间复杂度：O(knd*t) | k:类别，n：样本数，d：计算样本之间距离的时间复杂度，t：迭代次数； 优缺点： 优点：1. 原理易懂、实现简单、收敛速度快、效果好 2. 可解释性强 3. 可调参数只有少，只有k； 缺点：1. 聚类效果受k值影响大 2. 非凸数据集难以收敛 3. 隐含类别不均衡时，效果差 4. 迭代算法，得到的只是局部最优 5. 对噪音和异常数据敏感。 改进：随机初始化K值影响效果 + 计算样本点到质心的距离耗时这两方面优化 KMeans++算法 KMeans随机选取k个点作为聚类中心，而KMeans++采用如下方法： 假设已经选取好n个聚类中心后，再选取第n+1个聚类中心时，距离这n个聚类中心越远的点有越大的概率被选中；选取第一个聚类中心（n=1）时也是需要像KMeans一样随机选取的。 其他 1. 机器学习性能评价指标 Precision （精确率）和Recall（召回率） P = TP / (TP + FP) R = TP / (TP + FN) F1值 F1 = 2PR / (P + R) ROC 和 AUC 2. 奥卡姆剃刀（Occam’s Razor） 如无需要，勿增实体。 简单有效原理。 具体到机器学习上，能够拟合数据的简单模型才是我们需要的。 3. L1范数与L2的作用，区别 在机器学习中，通常损失函数会加上一个额外项，可看作损失函数的惩罚项，是为了限制模型参数，防止过拟合。 （自己注意下图！） L1范数 L2范数 各个参数的绝对值之和 各个参数的平方和的开方 先验分布是拉氏分布 高斯（正态）分布 使参数稀疏化，有特征选择的功能 使参数接近于0，防止过拟合 （模型越简单，越不容易过拟合） Lasso回归 Ridge回归 4. L1正则为什么可以把系数压缩成0？ https://blog.csdn.net/jinping_shi/article/details/52433975 5. 正则化为什么能防止过拟合？添加正则项后依旧过拟合如何调节参数lambda？ 4. 过拟合的原因和防止过拟合的方法 原因：1. 数据有噪声； 2. 训练数据不足，有限的训练数据； 3. 过度训练导致模型复杂。 防止过拟合的方法： 早停止：在模型对训练数据迭代收敛之前停止迭代。 具体做法：在每一个Epoch结束时,计算validation_data的accuracy，当accuracy不再提高时，就停止训练。（注意多次观察，多次精度未提升时则停止迭代） dropout：在训练时，以一定的概率忽略隐层中的某些节点。 插播：为什么dropout能有效防止过拟合，请解释原因？ 详细解答看这个：https://www.cnblogs.com/wuxiangli/p/7309501.html 取平均的作用； 减少神经元之间复杂的共适应关系：因为dropout程序导致两个神经元，不一定每次都在一个网络中出现，避免了有些特征只有在特定特征条件下才有效的情况，迫使网络去学习更加鲁棒的特征。 正则化 数据集扩充：1. 从源头上获取更多数据；2. 数据增强（通过一定规则扩充数据）；3. 根据当前数据估计分布参数，利用该分布获得更多数据。 集成学习 5. 特征选择的方法 什么样的特征是好特征：特征覆盖率高，特征之间相关性小，不能改变原始特征分布 wrapper（根据目标函数，每次选择若干特征，活着排除若干特征） 递归特征消除法：使用基模型进行多轮训练，每轮训练结束后，消除若干权值系数的特征，再使用新的特征进行下一轮训练。 embedded（先使用某些机器学习算法训练模型，得到各个特征的权值系数，再有大到小进行特征选择） 基于惩罚项的特征选择法 基于树的特征选择法 filter（根据发散性活着相关性对各个特征进行评分，设定阈值或者特征个数，选择特征） 卡方检验 互信息 方差选择法：计算每个特征的方差，根据阈值，选择方差大于阈值的特征进行训练； 相关系数 6. 说一说你知道的损失函数 0-1损失函数（感知机） 平方损失函数（线性回归） 绝对值损失函数 指数损失函数（adaBoost） Hinge Loss（SVM） 对数损失函数（逻辑回归） 7. 数据预处理的方法 数据清洗：异常值和缺失值； 数据集成：实体识别，冗余属性识别； 数据转换：简单函数转换，连续特征离散化，规范化，构造属性 数据规约：数值规约，属性规约 8. 偏差和方差是什么，高偏差和高方差说明了什么 偏差：是指预测值和真实值之间的差，偏差越大，预测和真实值之间的差别越大，衡量模型的预测能力。 方差：描述预测值的变化范围和离散程度，方差越大，表示预测值的分布越零散，对象是多个模型，使用不同的训练数据训练出的模型差别有多大。 当训练误差和交叉验证误差或测试误差都很大，且值差不多时，处于高偏差，低方差，欠拟合状态；当训练误差和交叉验证误差差别很大，且测试集误差小，验证集误差大时，处于高方差，低偏差，过拟合状态。 9. 优化算法有那些？ 具体算法原理、优缺点看这个：https://www.cnblogs.com/xinbaby829/p/7289431.html 梯度下降法、牛顿法和拟牛顿法、共轭梯度法、 启发式方法、解决约束优化问题的拉格朗日乘数法 10. 梯度下降算法和牛顿法的区别 https://www.cnblogs.com/lyr2015/p/9010532.html 牛顿法： 通过求解目标函数一阶导数为0的参数来求解目标函数最小值时的参数； 收敛速度快； 迭代过程中，海森矩阵的逆不断减小，相当于逐步减小步长； 海森矩阵的逆，计算复杂，代价较高，因此有了拟牛顿法。 梯度下降算法： 通过梯度方向和步长，直接求得目标函数最小值的参数； 越靠近最优值，步长应该逐渐减小，否则会在最优值附近震荡。 11. 如何解决类别不均衡问题？ https://blog.csdn.net/program_developer/article/details/80287033 https://www.cnblogs.com/zhaokui/p/5101301.html 采样：其中采样又分为上采样（将数量少的类别的数据复制多次），和下采样（将数量多的类别的数据剔除一部分）。 https://www.jianshu.com/p/9a68934d1f56 smote算法： 用途：合成新的少数样本； 基本思路：对每一个少数样本a，从a的k个最近邻中随机挑选一个样本b，从a、b连线上随机选择一个点，作为新合成的少数样本。 步骤： 1）对每一个少数样本a，基于欧式距离，计算它到其他少数样本的距离，找到他的k个最近邻； 2）根据样本不平衡比例设置一个采样比例进而得到采样倍率N，对于每一个少数样本a，从他的最近邻中选择若干样本，假设选择的样本为b； 3）基于以下公式得到新合成的少数样本 c = a * random(0,1)*|b-a|。 数据合成：利用现有的数据的规律生成新的数据。 一分类：当数据样本极不平衡时，将它看作一分类，这样我们的重点就在于将它看成对某种类别进行建模。 对不同的类别给予不同的分错代价。 12. 梯度下降算法的过程 首先我们有一个可微分的函数，这个函数就好像一个山，我们的目标是找到函数的最小值（即山底）。根据经验可知，我们从最陡峭的地方走，可以尽快到达山底。对应到函数就是找到给定点的梯度，并且沿着梯度相反的方向就能让函数值下降最快（因为梯度方向就是函数值变化最快的方向）。重复利用这个方法，反复求取梯度，最后就能到达局部最小值。 13. 为什么我们还是会在训练的过程当中将高度相关的特征去掉？ 使得模型的解释性更强； 大大提高训练速度。 14. 最大似然估计与贝叶斯估计的区别 15. 判别式和生成式的算法各有哪些，区别是什么？ https://blog.csdn.net/amblue/article/details/17023485 https://blog.csdn.net/qq_41853758/article/details/80864072 区别：二者最本质的区别是建模对象的不同。 判别式模型的评估对象是最大化条件概率P(Y|X)并对此进行建模； 生成式模型的评估对象是最大化联合概率P(X,Y)并对此进行建模。 判别式模型：线性回归、决策树、支持向量机SVM、k近邻、神经网络等； 生成式模型：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA。 16. 最大似然估计 https://www.cnblogs.com/zyxu1990/p/3209407.html 条件：假设样本独立同分布； 目标：估计出这个分布中的参数theta； 方法：这一组样本的概率最大时就对应了该模型的参数值。 17. 请用一句话说明AUC的本质和计算规则？AUC高可以理解为精确率高吗？ 本质：一个正例，一个负例，预测为正例的概率值大于预测为负的概率值的可能性； 计算规则：ROC曲线下的面积： AUC = ∫ t = ∞ − ∞ y ( t ) d x ( t ) \text{AUC} = \int_{t=\infty}^{-\infty} y(t) d x(t) AUC=∫t=∞−∞​y(t)dx(t) 不可以，精确率是基于某个阈值进行计算的，AUC是基于所有可能的阈值进行计算的，具有更高的健壮性。AUC不关注某个阈值下的表现如何，综合所有阈值的预测性能，所以精确率高，AUC不一定大，反之亦然。 参考：https://blog.csdn.net/legendavid/article/details/79063044 18. 二分类时，为什么AUC比accuracy更常用？为什么AUC对样本类别比例不敏感？ 19. 如何绘制ROC曲线？ 以真正例率为纵坐标，假正例率为横坐标绘制的曲线。 TPR = TP /（TP + FN）真 FPR = TN / ( TN + FP) 假 20. 梯度下降的改进算法有哪些？梯度消失的概念？ 如何解决梯度下降法陷入局部最优的问题？ 参考 https://blog.csdn.net/maqunfi/article/details/82634529 使用随机梯度下降法替代真正的梯度下降算法； 设置冲量； 使用不同的初始权值进行训练。 21. VC维的理解 这个感觉不太可能会碰到～ https://www.cnblogs.com/wuyuegb2312/archive/2012/12/03/2799893.html 分散：对于一个给定集合S={x1, … ,xd}，如果一个假设类H能够实现集合S中所有元素的任意一种标记方式，则称H能够分散S。 VC维的定义：H的VC维表示为VC(H) ，指能够被H分散的最大集合的大小。若H能分散任意大小的集合，那么VC(H)为无穷大。 22. 判断模型线性与非线性 https://zhuanlan.zhihu.com/p/37866896 只需要判别决策边界是否是直线，也就是是否能用一条直线来划分，如果可以则为线性。" />
<meta property="og:description" content="哎呀，要秋招了啊～ 不禁感叹：How time flys～ 重新出发～ 后期我发现还是需要把相关文章的链接放上来的，方便大家深入理解记忆，如果你没时间就直接看文字，如果有时间记得把链接点开看看哦～都是大佬的精华～ 一切为了暑期实习！！！ 一切为了暑期实习！！！ 一切为了暑期实习！！！ 机器学习 SVM（重点） 1. SVM原理和推导 原理：SVM试图寻找一个超平面使正负样本分开，并使得几何间隔最大。 ⚠️（注意分清以下这三种情况） 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分SVM 当训练样本近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性SVM 当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性分类器，即非线性SVM 推导（十分重要）我自己写的，字很随意啦～ 2. SVM 为什么要引入拉格朗日的优化方法 将拉格朗日对偶性应用到求解原始问题上，通过求解对偶问题进而求得原始问题的最优解，原因有二： 对偶问题往往更容易求解； 自然引入核函数，继而推广到非线性分类问题。 3. 为什么要选择最大间隔分类器？ 从数学上考虑： 误分类次数和几何间隔之间存在下列关系，几何间隔越大，误分类次数越少。 感知机利用误分类最小策略，求得分离超平面，不过此时解有无数个；而线性可分SVM利用间隔最大求得最优分离超平面，求得唯一解，而且此时的模型鲁棒性好，对未知样本泛化能力最强。 4. 样本失衡会对SVM的结果产生影响吗？如何解决SVM样本失衡问题？样本比例失衡时，使用什么指标评价分类器的好坏？ 样本失衡会对结果产生影响，分类超平面会靠近样本少的类别。原因：因为使用软间隔最大化，假设对所有类别使用相同的惩罚因子，而优化目标是最小化惩罚量，所以靠近样本少的类别惩罚量少。 解决SVM样本失衡问题方法： 对不同的类别赋予不同的惩罚因子（C），训练样本越少，C越大。缺点：偏离原始样本的概率分布。 对样本的少的类别，基于某种策略进行采样。 基于核函数解决问题。 当样本比例不均衡时，使用ROC曲线。 5. SVM如何解决多分类问题 https://www.cnblogs.com/CheeseZH/p/5265959.html 直接法：直接修改目标函数，将多个分类面的参数求解合并到一个目标函数上，一次性进行求解。 间接法： One VS One：任意两个样本之间训练一个分类模型，假设有k类，则需要k(k-1)/2个模型。对未知样本进行分类时，得票最多的类别即为未知样本的类别。libsvm使用这个方法。 One VS Rest：训练时依次将某类化为类，将其他所有类别划分为另外一类，共需要训练k个模型。训练时具有最大分类函数值的类别是未知样本的类别。 6. SVM适合处理什么样的数据？ 高维、稀疏、样本少的数据。 7. SVM为什么对缺失数据敏感？（数据缺失某些特征） SVM没有缺失值的处理策略； SVM希望样本在特征空间中线性可分，特征空间的好坏影响SVM性能； 缺失特征数据影响训练结果。 8. sklearn.svm参数 栗子： class sklearn.svm.SVC( C=1.0, kernel=&#39;rbf&#39;, degree=3, gamma=&#39;auto&#39;, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=&#39;ovr&#39;, random_state=None ) 重要参数：(理解含义和对模型的影响) C : float, optional (default=1.0) 误差项的惩罚参数，一般取值为10的n次幂，如10的-5次幂，10的-4次幂…10的0次幂，10，1000,1000，在python中可以使用pow（10，n） n=-5~inf C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱。 C值小，对误分类的惩罚减小，容错能力增强，泛化能力较强。 kernel : string, optional (default=’rbf’) svc中指定的kernel类型。 可以是： ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ 或者自己指定。 默认使用‘rbf’ 。 degree : int, optional (default=3) 当指定kernel为 ‘poly’时，表示选择的多项式的最高次数，默认为三次多项式。 若指定kernel不是‘poly’,则忽略，即该参数只对‘poly’有作用。 gamma : float, optional (default=’auto’) 当kernel为‘rbf’, ‘poly’或‘sigmoid’时的kernel系数。 如果不设置，默认为 ‘auto’ ，此时，kernel系数设置为：1/n_features coef0 : float, optional (default=0.0) kernel函数的常数项。 只有在 kernel为‘poly’或‘sigmoid’时有效，默认为0。 9. SVM的损失函数—Hinge Loss（合页损失函数） https://www.jianshu.com/p/fe14cd066077 hinge loss图像 表达式 10. SMO算法实现SVM(思想、步骤、常见问题)（我这个还不熟悉，mark） 思想：将大的优化问题分解为多个小的优化问题，求解小的优化问题往往更简单，同时顺序求解小问题得出的结果和将他们作为整体求得的结果一致。 步骤：1. 选取一对需要更新的变量ai和aj（阿尔法）2. 固定除ai和aj以外的所有变量，求解对偶问题获得更新ai、aj、b。 常见问题—如何选取ai和aj和b？ 选取违反KKT条件最严重的ai，在针对这个ai选择最有可能获得较大修正步长的aj； b一般选取支持向量求解的平均值。 11. SVM如何解决非线性问题？你所知道的核函数？ 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。 常用核函数（重点sigmoid、RBF（名字一定要记住啦：高斯径向基核函数）、要会写核函数的公式哦～） 12. 线性核与RBF核的区别？ 训练速度：线性核只有惩罚因子一个参数，训练速度快，RBF还需要调节gamma； 训练结果：线性核得到的权重w能反映出特征的重要性，由此进行特征选择，RBF无法解释； 训练数据：线性核适合样本特征&gt;&gt;样本数量的，RBF核相反。（揭示了如何选择核函数） 13. SVM和LR的联系和区别 联系： 都是判别式模型 都是有监督的分类算法 如果不考虑核函数，都是线性分类算法 区别： LR可以输出概率，SVM不可以 损失函数不同，即分类机制不同 SVM通过引入核函数解决非线性问题，LR则不是 原因：LR里每个样本点都要参与核计算，计算复杂度太高，故LR通常不用核函数。 SVM计算复杂，效果比LR好，适用于小数据集；LR计算快，适用于大数据集，用于在线学习 SVM分类只与分类超平面附近的点有关，LR与所有点都有关系 SVM是结构风险最小化，LR则是经验风险最小化 结构风险最小化就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，减小泛化误差。为了达到结构风险最小化的目的，最常用的方法就是添加正则项。 14. SVM如何防止过拟合？ https://www.jianshu.com/p/9b03cac58966 通过引入松弛变量，松弛变量可以容忍异常点的存在。 15. KKT（Karysh-Kuhn-Tucker）条件有哪些，完整描述？ 为什么要从原始问题转换成对偶问题： 对偶问题将原始问题中的约束转为了对偶问题中的等式约束； 方便核函数的引入； 改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关； 求解更高效，因为只用求解alpha系数，而alpha系数只有支持向量才非0，其它全部为0。 集成学习（重点） 1. 决策树和随机森林的区别 决策树 + Bagging + 随机选择特征 = 随机森林，随机森林可以有效防止过拟合。 2. 随机森林里面用的哪种决策树 CART 决策树或其他 3. 随机森林的原理？如何进行调参？树的深度一般如何确定，一般为多少？ 原理：RF是一种集成算法，属于bagging类，它通过集成多个决策树模型，通过对基模型的结果采用投票法或者取平均来获得最终结果，使得最终模型有较高的准确度和泛化性能。 调参： 还是看刘建平老师的这篇：https://www.cnblogs.com/pinard/p/6160412.html RF和GBDT调参过程类似，可以对比记忆： 无参数拟合–&gt;n_estimators调参–&gt;max_depth, min_sample_split–&gt;min_sample_split, min_samples_leaf–&gt;max_features 如何确定树的深度：当训练样本多，数据特征维度多的时候需要限制这个参数，具体取决于数据分布，一般在10-100之间。 3. Bagging 和 Boosting的区别 样本选择：Bagging有放回的选取训练集，并且从原始数据集中选取的各轮训练集之间相互独立；Boosting每次都使用全部数据，只是每个样例的权重不同。 样例权重：Bagging采用均匀采样，每个样例的权重相同；Boosting每轮训练都依据上一轮训练结果更新样例权重，错误率越大的样例，权重越大。 预测函数：Bagging每个基函数的预测结果权重相同；Boosting中预测误差越小的基模型有更大的权重。 偏差和方差：Bagging得出的结果低方差，Boosting低偏差。 并行计算：Bagging可以并行生成基模型，Boosting各个预测函数只能顺序生成，后一轮模型的参数需要前一轮的预测结果。 4. GBDT调参 我觉得啊，一般面试官如果问我们这种题目，一定是要求我们使用过这个算法，如果使用过就要理解记住，没使用过就坦诚的说没用过，大家可以跟着下面这个链接的刘建平老师学习一遍。 具体实例：https://www.cnblogs.com/pinard/p/6143927.html 参数分类： Boosting框架参数：n_estimators, learning_rate, subsample CART回归树参数（与决策树类似）：max_features, max_depth, min_sample_split, min_samples_leaf 大致步骤： 无参数拟合–&gt;固定learning_rate，estimators调参–&gt;max_depth, min_sample_split–&gt;min_sample_split, min_samples_leaf–&gt;拟合查看–&gt;max_features–&gt;subsample–&gt;不断减小learning_rate，加倍estimators来拟合 5. RF、GBDT之间的区别（重要） 此问题充分理解，你需要这些： https://blog.csdn.net/data_scientist/article/details/79022025 https://blog.csdn.net/xwd18280820053/article/details/68927422 https://blog.csdn.net/m510756230/article/details/82051807 相同点：都是由多棵树组成，结果由多棵树共同决定。 不同点： GBDT是回归树，RF可以是回归树也可以是分类树； GBDT对异常值特别敏感，RF则没有； GBDT只能串行生成多个模型，RF可以并行； GBDT的结果有多个结果求和或者加权求和，RF是由投票选出结果； GBDT是通过减少偏差来提高模型性能，RF是通过减少方差； RF对所有训练集一视同仁，GBDT是基于权值的弱分类器。 6. 随机森林的优缺点 优点： 相比于其他算法，在训练速度和预测准确度上有很大的优势； 能够处理很高维的数据，不用选择特征，在模型训练完成后，能够给出特征的重要性； 可以写成并行化方法； 缺点：在噪声较大的分类回归问题上，容易过拟合。 7. GBDT的关键？GBDT中的树是什么树？ 关键：利用损失函数的负梯度方向作为残差的近似值来拟合新的CART回归树。 CART回归树。 8. GBDT和XGB的区别 GBDT以CART为基分类器，XGB则支持多种分类器； GBDT只用到了一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶与二阶导数，并且可以自定义代价函数，只要一阶二阶可导； XGBoost与GDBT都是逐次迭代来提高模型性能，但是XGBoost在选取最佳切分点时可以开启多线程进行，大大提高了运行速度； 新增了Shrinkage和column subsampling，为了防止过拟合； 对缺失值有自动分裂处理（默认归于左子树）； 如何处理的缺失值： Xgboost 在处理带缺失值的特征时，先对非缺失的样本进行排序，对该特征缺失的样本先不处理，然后在遍历每个分裂点时，将这些缺失样本分别划入左子树和右子树来计算损失然后求最优。如果训练样本中没有缺失值，而预测过程中出现了缺失值，那么样本会被默认分到右子树。 xgb损失函数加入正则化，控制模型复杂度，使模型简单，防止过拟合。 9. LGB和XGB的区别（抓住主要区别，理解要有深度） lgb官方文档：http://lightgbm.apachecn.org/#/docs/4 https://www.cnblogs.com/infaraway/p/7890558.html 树的切分策略不同： xgb基于level-wise，对每一层节点进行无差别分裂，造成不必要的开销； lgb基于leaf-wise，在当前所有叶子节点中选择分裂增益最大的节点进行分裂； 实现并行方式不同： xgb使用基于 pre-sorted 决策树算法； lgb使用基于histogram决策树算法，对离散特征进行分裂时，特征的每个取值为一个桶； lgb支持直接输入categorical feature，对类别特征无须进行one-hot处理； 优化通信代价不同：lgb支持特征并行、数据并行。 10. GBDT的算法步骤（我没找到推导呢） 10. 随机森林生成过程 https://www.cnblogs.com/liuwu265/p/4690715.html 样本集选择：从原始样本中有放回的抽取N个训练集，各个训练集之间相互独立，共进行k轮抽取； 决策树生成：假设特征空间共有D个特征，随机选择d个特征，构成新的特征空间，用于训练单棵决策树，共k轮，生成k棵决策树； 模型合成：生成的k棵决策树相互独立，各个基模型之间权重相同，如果是分类问题，则使用投票法决定最终结果，如果是回归问题，则使用平均法； 模型验证：模型验证本身需要验证集，但在此处我们无须额外设置验证集，只需使用原始样本中没有使用过的即可。 11. xgboost如何确定特征和分裂点的？ XGBoost使用了和CART回归树一样的想法，利用贪婪算法。基于目标函数，遍历所有特征的所有特征划分点，具体做法就是分裂后的目标函数值大于分裂之前的就进行分裂。 12. XGB是如何给出特征重要性评分的？ 建议多看下以下链接： https://blog.csdn.net/waitingzby/article/details/81610495 https://blog.csdn.net/sujinhehehe/article/details/84201415 https://www.cnblogs.com/haobang008/p/5929378.html 特征权重（weight）：指的是在所有树中，某特征被用来分裂节点的次数； 如何计算：一个特征对分裂点性能度量（gini或者其他）的提升越大（越靠近根节点）其权重越大，该特征被越多提升树选择来进行分裂，该特征越重要，最终将一个特征在所有提升树中的结果进行加权求和然后求平均即可。 源码片段（加深理解）： 主要是对每个特征进行计数操作： if importance_type == &#39;weight&#39;: # do a simpler tree dump to save time trees = self.get_dump(fmap, with_stats=False) fmap = {} for tree in trees: for line in tree.split(&#39;\n&#39;): # look for the opening square bracket arr = line.split(&#39;[&#39;) # if no opening bracket (leaf node), ignore this line if len(arr) == 1: continue # extract feature name from string between [] fid = arr[1].split(&#39;]&#39;)[0].split(&#39;&lt;&#39;)[0] if fid not in fmap: # if the feature hasn&#39;t been seen yet fmap[fid] = 1 else: fmap[fid] += 1 return fmap 13. XGB如何消除残差的，目标函数是什么？ https://www.cnblogs.com/palantir/p/10671119.html https://blog.csdn.net/guoxinian/article/details/79243307 14. GDBT在处理分类和回归问题时有什么区别？（感觉还有补全的地方） 损失函数不同： 分类：指数、对数； 回归：均方差、绝对值。 XGB如何防止过拟合 Shrinkage and Column Subsampling。 Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型； Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种： 按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点； 随机选择特征，则建树前随机选择一部分特征然后分裂就只遍历这些特征。一般情况下前者效果更好。 EM算法 1. 采用EM算法求解的模型有哪些？为什么不用牛顿法或者梯度下降法？（感觉第二个问题有点错误，mark） 高斯混合模型、协同过滤、KMeans 求和的项随着隐变量的数量随指数上升，梯度计算带来麻烦，而EM是非梯度优化算法。 2. 用EM算法推导解释KMeans KMeans中，每个聚类簇的中心就是隐含变量。 E步：随机初始化k个聚类中心 M步：计算每个样本点最近的质心，并将他聚类到这个质心 重复以上两步，直到聚类中心不发生变化为止。 决策树 1. ID3和C4.5的优缺点 ID3 C4.5 优点 实现简单 1. 可以处理连续型特征 2. 易于理解，准确率高 缺点 1. 只能处理离散特征 2. 倾向于选择取值较多的特征 1. 在构造树的过程中，对数据多次扫描排序，低效 2. 只能够用于驻留在内存中的数据，当数据大到无法在内存容纳时，程序无法执行 2. 决策树处理连续值的方法 连续属性离散化，常用的离散化策略是二分法（C4.5）： 3. 决策树的剪枝策略 目的：简化决策树模型，提高模型泛化能力； 基本思想：减去某些子树和叶节点，将其根节点作为新的叶子节点，实现简化模型。 损失函数： 剪枝策略： 预剪枝：在构造决策树的过程中，在对节点划分之前进行估计，若划分后不能带来决策树性能和泛化能力的提升，则不进行划分，并将此节点作为叶节点。 后剪枝：构造完决策树之后，自底向上搜索，对每个非叶节点进行考察，若将该子树去除变为叶节点能带来决策树泛化性能的提升，则将该节点作为叶节点。 对比：后剪枝的分支比预剪枝的分支要多一些，不容易欠拟合，泛化能力强，但是由于后剪枝在构造完成决策树之后，而且还需要自底向上进行搜索故时间开销大。 4. 决策树的构造过程 https://shuwoom.com/?p=1452 特征选择：在所有特征中选择一个特征，作为当前节点的划分标准：ID3(信息增益)、C4.5(信息增益比)、CART(gini系数)； 决策树生成：依据特征评估标准，从上到下的递归的生成子节点，直到数据集不可分时，停止生长； 剪枝：决策时容易过拟合，通过剪枝，简化模型，降低过拟合。 5. 基于树的模型有必要做标准化吗？ https://blog.csdn.net/answer3lin/article/details/84961694 不必要；概率模型（树模型）只关心变量的分布和变量之间的条件概率。 6. CART回归树和CART决策树的构造过程 特征选择+决策树构造+决策树剪枝 cart回归树 cart回归树对某个特征的每个分量，将数据集划分为大于该分量和小于该分量两部分，同时计算对应y的均值，并计算均方误差函数的值，选择具有最小误差值的分量，并将其作为划分标准，重复该流程知道数据集为空或者前后均方误差下降值小于给定阈值则停止。 cart分类树 选择具有最小的GINI系数的属性和属性值，作为最优分裂属性和最优分裂属性值。 7. 熵 概念：信息所包含不确定大小的度量，一个信息的所包含的不确定性越大，其所含的信息越多。 熵的计算公式： H ( X ) = − ∑ i = 1 N p i l o g 2 p i H(X) = -\sum_ {i=1}^{N}p_i log_2 p_i H(X)=−∑i=1N​pi​log2​pi​ 朴素贝叶斯 1. 朴素贝叶斯的公式 “朴素”的含义：假设各个特征之间相互独立。 2. NB原理及其分类 原理：根据贝叶斯公式，通过某对象的先验概率，计算其后验概率，并选择后验概率最大的类作为该对象所属于的类； 分类：（根据变量的分布不同） NB的伯努利模型，特征是布尔变量，符合0/1分布，在文本分类中，特征就是词是否出现； NB的多项式模型，特征是离散值，符合多项式分布，在文本分类中，特征就是词出现的次数； NB的高斯模型，特征是连续值，符合高斯分布（高斯分布又名正态分布），在文本分类中，特征就是词的TF-IDF值。 3. NB的优缺点 优点： 算法原理简单； 所估计的参数少； 假设条件概率计算是彼此独立的，因此可以用于分布式计算； 属于生成式模型，收敛速度比判别式模型要快； 对缺失数据不太敏感； 天然可以处理多分类问题。 缺点： 假设各个特征之间相互独立这一条件在实际应用中往往是不能成立的； 不能学习到特征之间的相互作用； 对输入数据的表达形式敏感。 4. LR和朴素贝叶斯(NB)之间的区别 https://www.cnblogs.com/wangkundentisy/p/9193217.html 暂时不看。 5. 适用场景 支持大规模数据，并且支持分布式实现； 特征维度可以很高； 可以处理数值型特征和类别型特征； 线性回归与逻辑回归 1. LR推导（重要） 几个小问题： 极大似然的概念：找到参数θ的一个估计值，使得当前样本出现的可能性最大。 为什么极大似然的时候可以相乘：特征之间是独立同分布。 LR的参数计算方法：梯度下降、牛顿法。 2. 逻辑回归和线性回归的区别 线性回归 逻辑回归 对连续值预测 分类 最小二乘法 最大似然估计 拟合函数 预测函数 3. 最小二乘法和最大似然法的区别（没太明白，mark） https://blog.csdn.net/lu597203933/article/details/45032607 4. 为什么用最小二乘而不是最小四乘（没太明白，mark） 5. 介绍一下逻辑回归？它的损失函数是什么？ 一句话介绍逻辑回归：逻辑回归假设数据服从伯努利分布，通过极大化似然函数，利用梯度下降算法来求得参数，实现数据的二分类； 它的损失函数是它的极大似然函数（对数损失函数）： 6. LR 损失函数为什么用极大似然函数？ https://blog.csdn.net/aliceyangxi1987/article/details/80532586 LR的目标是使每个样本的预测都有最大概率，即将所有样本预测后的概率相乘概率最大，这就是极大似然函数； 极大似然函数取对数即为对数损失函数，对数损失函数的训练求解参数比较快，更新速度也稳定； 为什么不用平方损失函数呢？因为平方损失函数的梯度更新速度和sigmoid的梯度有关，而在定义域范围内sigmoid的梯度值&lt;=2.5，训练速度非常慢；而且平方损失函数会导致损失函数是非凸的，不易求解。 7. LR的参数计算方法：梯度下降法，请介绍三种GD的区别 批梯度下降：可以得到全局最优解，缺点是更新每个参数都需要遍历所有数据，计算量大，还有很多冗余计算，在数据非常大的时候，每个参数的更新都是非常慢的； SGD：以高方差频繁更新，使SGD跳到新的或潜在更优的局部解，但是也使得收敛到局部最优解的过程更加复杂； mini-batch SGD：结合了二者的优点，每次选取N个样本，减少了参数更新的次数，可以达到更加稳定的收敛结果。 8. LR的优缺点 http://www.cnblogs.com/ModifyRong/p/7739955.html 优点： 形式简单，模型可解释性非常好，特征权重可以看出不同特征最后结果的影响； 效果不错，可以作为baseline； 占用资源少，特别是内存； 方便输出结果调整； 训练速度快。 缺点： 准确率不高； 对样本不均衡问题无法很好的解决； 对非线性分类问题也是； 本身无法筛选特征，可以和GBDT结合使用。 9. 逻辑斯特回归为什么要对特征进行离散化 稀疏向量做内积乘法运算速度快，计算结果方便存储，易于扩展 离散化后的特征对异常数据有更强的鲁棒性 特征离散化后模型更稳定 LR属于广义线性模型，表达能力有限，特征离散化为N个后，每个变量有自己的权重，相当于引入非线性，表达能力增强，加大拟合 特征离散化后还可以做特征交叉，由M+N个变为M*N个，进一步引入非线性 10. LR为什么用sigmoid函数,这个函数有什么优点和缺点? 为什么： Sigmoid 函数自身的性质 sigmoid 函数连续，单调递增 sigmiod 函数关于（0，0.5） 中心对称 计算sigmoid函数的导数非常的快速 将输入变量的范围从负无穷到正无穷，映射到（0，1），而概率要求正是（0，1）。 逻辑回归认为函数其概率服从伯努利分布，将其写成指数族分布的形式，能够推导出sigmoid函数的形式。 https://blog.csdn.net/a1628864705/article/details/62233395 https://blog.csdn.net/qq_19645269/article/details/79551576 优缺点： 优点： 可以看到sigmoid函数处处连续 -&gt;便于求导； 可以将函数值的范围压缩到[0,1]-&gt;可以压缩数据，且幅度不变； 便于前向传输。 缺点： 在趋向无穷的地方，函数值变化很小，容易缺失梯度，不利于深层神经网络的反馈传输； 幂函数还是比较难算的； 函数均值不为0，当输出大于0时，则梯度方向将大于0，也就是说接下来的反向运算中将会持续正向更新；同理，当输出小于0时，接下来的方向运算将持续负向更新。 11. LR伪代码 读取文件、写入文件以及计算准确率等之前实验做过的或者过于简单的函数功能不列出。 int main() { 读取训练集和测试集，其中训练集每三个样本取前两个作为训练集，第三个作为验证集。 初始化w：for(int i=0;i&lt;Length;i++) w[i]=1; for(int k=0:7) for(int i=0:traincnt)//遍历训练集样本 { CalWeight(i);//计算样本i的权重分数 CalCost(i);//每一维的梯度（代价）计算 Updatew(); //更新w if(i%20==0) //每更新w20次计算一次准确率 { Predict();//预测验证集样本 Cal_acc();//计算准确率 ac[cnt]=accuracy; cnt++; } } output_result();//输出验证集准确率以供调试 output_test_result();//输出测试集预测结果 } void CalWeight(int index){ weight = 当前向量w的转置*样本i向量； } void CalCost(int index){ 计算每一维的梯度，存储在向量数组Cost[]中； } void Updatew(){ 使用w = w - alpha x gradient来更新回归系数（w） } void Predict(){ P = 1/(1+exp(-1* w^T *样本i向量); if(P&gt;0.5) p_label=1; else p_label=0; } PCA 1. PCA原理 具体推导看这里： https://blog.csdn.net/u012421852/article/details/80458340 用于：特征降维，去除冗余和可分性不强的特征； 目标：降维后的各个特征不相关，即特征之间的协方差为0； 原理：基于训练数据X的协方差矩阵的特征向量组成的k阶矩阵U，通过XU得到降维后的k阶矩阵Z； 算法步骤 计算训练样本的协方差矩阵C； 计算C的特征值和特征向量； 将C的特征值降序排列，特征值对应特征向量也依次排列； 假如要得到X的k阶降维矩阵，选取C的前k个特征{u1,u2…uk}，组成降维转换矩阵U； Z = XU，Z即为降维后的矩阵； KMeans 1. K-means 的原理，时间复杂度，优缺点以及改进 原理：对于给定样本集，按照样本之间的距离大小，将样本划分为若干个簇，使簇内距离尽可能小，簇间距离尽可能大； 时间复杂度：O(knd*t) | k:类别，n：样本数，d：计算样本之间距离的时间复杂度，t：迭代次数； 优缺点： 优点：1. 原理易懂、实现简单、收敛速度快、效果好 2. 可解释性强 3. 可调参数只有少，只有k； 缺点：1. 聚类效果受k值影响大 2. 非凸数据集难以收敛 3. 隐含类别不均衡时，效果差 4. 迭代算法，得到的只是局部最优 5. 对噪音和异常数据敏感。 改进：随机初始化K值影响效果 + 计算样本点到质心的距离耗时这两方面优化 KMeans++算法 KMeans随机选取k个点作为聚类中心，而KMeans++采用如下方法： 假设已经选取好n个聚类中心后，再选取第n+1个聚类中心时，距离这n个聚类中心越远的点有越大的概率被选中；选取第一个聚类中心（n=1）时也是需要像KMeans一样随机选取的。 其他 1. 机器学习性能评价指标 Precision （精确率）和Recall（召回率） P = TP / (TP + FP) R = TP / (TP + FN) F1值 F1 = 2PR / (P + R) ROC 和 AUC 2. 奥卡姆剃刀（Occam’s Razor） 如无需要，勿增实体。 简单有效原理。 具体到机器学习上，能够拟合数据的简单模型才是我们需要的。 3. L1范数与L2的作用，区别 在机器学习中，通常损失函数会加上一个额外项，可看作损失函数的惩罚项，是为了限制模型参数，防止过拟合。 （自己注意下图！） L1范数 L2范数 各个参数的绝对值之和 各个参数的平方和的开方 先验分布是拉氏分布 高斯（正态）分布 使参数稀疏化，有特征选择的功能 使参数接近于0，防止过拟合 （模型越简单，越不容易过拟合） Lasso回归 Ridge回归 4. L1正则为什么可以把系数压缩成0？ https://blog.csdn.net/jinping_shi/article/details/52433975 5. 正则化为什么能防止过拟合？添加正则项后依旧过拟合如何调节参数lambda？ 4. 过拟合的原因和防止过拟合的方法 原因：1. 数据有噪声； 2. 训练数据不足，有限的训练数据； 3. 过度训练导致模型复杂。 防止过拟合的方法： 早停止：在模型对训练数据迭代收敛之前停止迭代。 具体做法：在每一个Epoch结束时,计算validation_data的accuracy，当accuracy不再提高时，就停止训练。（注意多次观察，多次精度未提升时则停止迭代） dropout：在训练时，以一定的概率忽略隐层中的某些节点。 插播：为什么dropout能有效防止过拟合，请解释原因？ 详细解答看这个：https://www.cnblogs.com/wuxiangli/p/7309501.html 取平均的作用； 减少神经元之间复杂的共适应关系：因为dropout程序导致两个神经元，不一定每次都在一个网络中出现，避免了有些特征只有在特定特征条件下才有效的情况，迫使网络去学习更加鲁棒的特征。 正则化 数据集扩充：1. 从源头上获取更多数据；2. 数据增强（通过一定规则扩充数据）；3. 根据当前数据估计分布参数，利用该分布获得更多数据。 集成学习 5. 特征选择的方法 什么样的特征是好特征：特征覆盖率高，特征之间相关性小，不能改变原始特征分布 wrapper（根据目标函数，每次选择若干特征，活着排除若干特征） 递归特征消除法：使用基模型进行多轮训练，每轮训练结束后，消除若干权值系数的特征，再使用新的特征进行下一轮训练。 embedded（先使用某些机器学习算法训练模型，得到各个特征的权值系数，再有大到小进行特征选择） 基于惩罚项的特征选择法 基于树的特征选择法 filter（根据发散性活着相关性对各个特征进行评分，设定阈值或者特征个数，选择特征） 卡方检验 互信息 方差选择法：计算每个特征的方差，根据阈值，选择方差大于阈值的特征进行训练； 相关系数 6. 说一说你知道的损失函数 0-1损失函数（感知机） 平方损失函数（线性回归） 绝对值损失函数 指数损失函数（adaBoost） Hinge Loss（SVM） 对数损失函数（逻辑回归） 7. 数据预处理的方法 数据清洗：异常值和缺失值； 数据集成：实体识别，冗余属性识别； 数据转换：简单函数转换，连续特征离散化，规范化，构造属性 数据规约：数值规约，属性规约 8. 偏差和方差是什么，高偏差和高方差说明了什么 偏差：是指预测值和真实值之间的差，偏差越大，预测和真实值之间的差别越大，衡量模型的预测能力。 方差：描述预测值的变化范围和离散程度，方差越大，表示预测值的分布越零散，对象是多个模型，使用不同的训练数据训练出的模型差别有多大。 当训练误差和交叉验证误差或测试误差都很大，且值差不多时，处于高偏差，低方差，欠拟合状态；当训练误差和交叉验证误差差别很大，且测试集误差小，验证集误差大时，处于高方差，低偏差，过拟合状态。 9. 优化算法有那些？ 具体算法原理、优缺点看这个：https://www.cnblogs.com/xinbaby829/p/7289431.html 梯度下降法、牛顿法和拟牛顿法、共轭梯度法、 启发式方法、解决约束优化问题的拉格朗日乘数法 10. 梯度下降算法和牛顿法的区别 https://www.cnblogs.com/lyr2015/p/9010532.html 牛顿法： 通过求解目标函数一阶导数为0的参数来求解目标函数最小值时的参数； 收敛速度快； 迭代过程中，海森矩阵的逆不断减小，相当于逐步减小步长； 海森矩阵的逆，计算复杂，代价较高，因此有了拟牛顿法。 梯度下降算法： 通过梯度方向和步长，直接求得目标函数最小值的参数； 越靠近最优值，步长应该逐渐减小，否则会在最优值附近震荡。 11. 如何解决类别不均衡问题？ https://blog.csdn.net/program_developer/article/details/80287033 https://www.cnblogs.com/zhaokui/p/5101301.html 采样：其中采样又分为上采样（将数量少的类别的数据复制多次），和下采样（将数量多的类别的数据剔除一部分）。 https://www.jianshu.com/p/9a68934d1f56 smote算法： 用途：合成新的少数样本； 基本思路：对每一个少数样本a，从a的k个最近邻中随机挑选一个样本b，从a、b连线上随机选择一个点，作为新合成的少数样本。 步骤： 1）对每一个少数样本a，基于欧式距离，计算它到其他少数样本的距离，找到他的k个最近邻； 2）根据样本不平衡比例设置一个采样比例进而得到采样倍率N，对于每一个少数样本a，从他的最近邻中选择若干样本，假设选择的样本为b； 3）基于以下公式得到新合成的少数样本 c = a * random(0,1)*|b-a|。 数据合成：利用现有的数据的规律生成新的数据。 一分类：当数据样本极不平衡时，将它看作一分类，这样我们的重点就在于将它看成对某种类别进行建模。 对不同的类别给予不同的分错代价。 12. 梯度下降算法的过程 首先我们有一个可微分的函数，这个函数就好像一个山，我们的目标是找到函数的最小值（即山底）。根据经验可知，我们从最陡峭的地方走，可以尽快到达山底。对应到函数就是找到给定点的梯度，并且沿着梯度相反的方向就能让函数值下降最快（因为梯度方向就是函数值变化最快的方向）。重复利用这个方法，反复求取梯度，最后就能到达局部最小值。 13. 为什么我们还是会在训练的过程当中将高度相关的特征去掉？ 使得模型的解释性更强； 大大提高训练速度。 14. 最大似然估计与贝叶斯估计的区别 15. 判别式和生成式的算法各有哪些，区别是什么？ https://blog.csdn.net/amblue/article/details/17023485 https://blog.csdn.net/qq_41853758/article/details/80864072 区别：二者最本质的区别是建模对象的不同。 判别式模型的评估对象是最大化条件概率P(Y|X)并对此进行建模； 生成式模型的评估对象是最大化联合概率P(X,Y)并对此进行建模。 判别式模型：线性回归、决策树、支持向量机SVM、k近邻、神经网络等； 生成式模型：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA。 16. 最大似然估计 https://www.cnblogs.com/zyxu1990/p/3209407.html 条件：假设样本独立同分布； 目标：估计出这个分布中的参数theta； 方法：这一组样本的概率最大时就对应了该模型的参数值。 17. 请用一句话说明AUC的本质和计算规则？AUC高可以理解为精确率高吗？ 本质：一个正例，一个负例，预测为正例的概率值大于预测为负的概率值的可能性； 计算规则：ROC曲线下的面积： AUC = ∫ t = ∞ − ∞ y ( t ) d x ( t ) \text{AUC} = \int_{t=\infty}^{-\infty} y(t) d x(t) AUC=∫t=∞−∞​y(t)dx(t) 不可以，精确率是基于某个阈值进行计算的，AUC是基于所有可能的阈值进行计算的，具有更高的健壮性。AUC不关注某个阈值下的表现如何，综合所有阈值的预测性能，所以精确率高，AUC不一定大，反之亦然。 参考：https://blog.csdn.net/legendavid/article/details/79063044 18. 二分类时，为什么AUC比accuracy更常用？为什么AUC对样本类别比例不敏感？ 19. 如何绘制ROC曲线？ 以真正例率为纵坐标，假正例率为横坐标绘制的曲线。 TPR = TP /（TP + FN）真 FPR = TN / ( TN + FP) 假 20. 梯度下降的改进算法有哪些？梯度消失的概念？ 如何解决梯度下降法陷入局部最优的问题？ 参考 https://blog.csdn.net/maqunfi/article/details/82634529 使用随机梯度下降法替代真正的梯度下降算法； 设置冲量； 使用不同的初始权值进行训练。 21. VC维的理解 这个感觉不太可能会碰到～ https://www.cnblogs.com/wuyuegb2312/archive/2012/12/03/2799893.html 分散：对于一个给定集合S={x1, … ,xd}，如果一个假设类H能够实现集合S中所有元素的任意一种标记方式，则称H能够分散S。 VC维的定义：H的VC维表示为VC(H) ，指能够被H分散的最大集合的大小。若H能分散任意大小的集合，那么VC(H)为无穷大。 22. 判断模型线性与非线性 https://zhuanlan.zhihu.com/p/37866896 只需要判别决策边界是否是直线，也就是是否能用一条直线来划分，如果可以则为线性。" />
<link rel="canonical" href="https://uzzz.org/2019/08/13/793456.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/13/793456.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-13T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"哎呀，要秋招了啊～ 不禁感叹：How time flys～ 重新出发～ 后期我发现还是需要把相关文章的链接放上来的，方便大家深入理解记忆，如果你没时间就直接看文字，如果有时间记得把链接点开看看哦～都是大佬的精华～ 一切为了暑期实习！！！ 一切为了暑期实习！！！ 一切为了暑期实习！！！ 机器学习 SVM（重点） 1. SVM原理和推导 原理：SVM试图寻找一个超平面使正负样本分开，并使得几何间隔最大。 ⚠️（注意分清以下这三种情况） 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分SVM 当训练样本近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性SVM 当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性分类器，即非线性SVM 推导（十分重要）我自己写的，字很随意啦～ 2. SVM 为什么要引入拉格朗日的优化方法 将拉格朗日对偶性应用到求解原始问题上，通过求解对偶问题进而求得原始问题的最优解，原因有二： 对偶问题往往更容易求解； 自然引入核函数，继而推广到非线性分类问题。 3. 为什么要选择最大间隔分类器？ 从数学上考虑： 误分类次数和几何间隔之间存在下列关系，几何间隔越大，误分类次数越少。 感知机利用误分类最小策略，求得分离超平面，不过此时解有无数个；而线性可分SVM利用间隔最大求得最优分离超平面，求得唯一解，而且此时的模型鲁棒性好，对未知样本泛化能力最强。 4. 样本失衡会对SVM的结果产生影响吗？如何解决SVM样本失衡问题？样本比例失衡时，使用什么指标评价分类器的好坏？ 样本失衡会对结果产生影响，分类超平面会靠近样本少的类别。原因：因为使用软间隔最大化，假设对所有类别使用相同的惩罚因子，而优化目标是最小化惩罚量，所以靠近样本少的类别惩罚量少。 解决SVM样本失衡问题方法： 对不同的类别赋予不同的惩罚因子（C），训练样本越少，C越大。缺点：偏离原始样本的概率分布。 对样本的少的类别，基于某种策略进行采样。 基于核函数解决问题。 当样本比例不均衡时，使用ROC曲线。 5. SVM如何解决多分类问题 https://www.cnblogs.com/CheeseZH/p/5265959.html 直接法：直接修改目标函数，将多个分类面的参数求解合并到一个目标函数上，一次性进行求解。 间接法： One VS One：任意两个样本之间训练一个分类模型，假设有k类，则需要k(k-1)/2个模型。对未知样本进行分类时，得票最多的类别即为未知样本的类别。libsvm使用这个方法。 One VS Rest：训练时依次将某类化为类，将其他所有类别划分为另外一类，共需要训练k个模型。训练时具有最大分类函数值的类别是未知样本的类别。 6. SVM适合处理什么样的数据？ 高维、稀疏、样本少的数据。 7. SVM为什么对缺失数据敏感？（数据缺失某些特征） SVM没有缺失值的处理策略； SVM希望样本在特征空间中线性可分，特征空间的好坏影响SVM性能； 缺失特征数据影响训练结果。 8. sklearn.svm参数 栗子： class sklearn.svm.SVC( C=1.0, kernel=&#39;rbf&#39;, degree=3, gamma=&#39;auto&#39;, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=&#39;ovr&#39;, random_state=None ) 重要参数：(理解含义和对模型的影响) C : float, optional (default=1.0) 误差项的惩罚参数，一般取值为10的n次幂，如10的-5次幂，10的-4次幂…10的0次幂，10，1000,1000，在python中可以使用pow（10，n） n=-5~inf C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱。 C值小，对误分类的惩罚减小，容错能力增强，泛化能力较强。 kernel : string, optional (default=’rbf’) svc中指定的kernel类型。 可以是： ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ 或者自己指定。 默认使用‘rbf’ 。 degree : int, optional (default=3) 当指定kernel为 ‘poly’时，表示选择的多项式的最高次数，默认为三次多项式。 若指定kernel不是‘poly’,则忽略，即该参数只对‘poly’有作用。 gamma : float, optional (default=’auto’) 当kernel为‘rbf’, ‘poly’或‘sigmoid’时的kernel系数。 如果不设置，默认为 ‘auto’ ，此时，kernel系数设置为：1/n_features coef0 : float, optional (default=0.0) kernel函数的常数项。 只有在 kernel为‘poly’或‘sigmoid’时有效，默认为0。 9. SVM的损失函数—Hinge Loss（合页损失函数） https://www.jianshu.com/p/fe14cd066077 hinge loss图像 表达式 10. SMO算法实现SVM(思想、步骤、常见问题)（我这个还不熟悉，mark） 思想：将大的优化问题分解为多个小的优化问题，求解小的优化问题往往更简单，同时顺序求解小问题得出的结果和将他们作为整体求得的结果一致。 步骤：1. 选取一对需要更新的变量ai和aj（阿尔法）2. 固定除ai和aj以外的所有变量，求解对偶问题获得更新ai、aj、b。 常见问题—如何选取ai和aj和b？ 选取违反KKT条件最严重的ai，在针对这个ai选择最有可能获得较大修正步长的aj； b一般选取支持向量求解的平均值。 11. SVM如何解决非线性问题？你所知道的核函数？ 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。 常用核函数（重点sigmoid、RBF（名字一定要记住啦：高斯径向基核函数）、要会写核函数的公式哦～） 12. 线性核与RBF核的区别？ 训练速度：线性核只有惩罚因子一个参数，训练速度快，RBF还需要调节gamma； 训练结果：线性核得到的权重w能反映出特征的重要性，由此进行特征选择，RBF无法解释； 训练数据：线性核适合样本特征&gt;&gt;样本数量的，RBF核相反。（揭示了如何选择核函数） 13. SVM和LR的联系和区别 联系： 都是判别式模型 都是有监督的分类算法 如果不考虑核函数，都是线性分类算法 区别： LR可以输出概率，SVM不可以 损失函数不同，即分类机制不同 SVM通过引入核函数解决非线性问题，LR则不是 原因：LR里每个样本点都要参与核计算，计算复杂度太高，故LR通常不用核函数。 SVM计算复杂，效果比LR好，适用于小数据集；LR计算快，适用于大数据集，用于在线学习 SVM分类只与分类超平面附近的点有关，LR与所有点都有关系 SVM是结构风险最小化，LR则是经验风险最小化 结构风险最小化就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，减小泛化误差。为了达到结构风险最小化的目的，最常用的方法就是添加正则项。 14. SVM如何防止过拟合？ https://www.jianshu.com/p/9b03cac58966 通过引入松弛变量，松弛变量可以容忍异常点的存在。 15. KKT（Karysh-Kuhn-Tucker）条件有哪些，完整描述？ 为什么要从原始问题转换成对偶问题： 对偶问题将原始问题中的约束转为了对偶问题中的等式约束； 方便核函数的引入； 改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关； 求解更高效，因为只用求解alpha系数，而alpha系数只有支持向量才非0，其它全部为0。 集成学习（重点） 1. 决策树和随机森林的区别 决策树 + Bagging + 随机选择特征 = 随机森林，随机森林可以有效防止过拟合。 2. 随机森林里面用的哪种决策树 CART 决策树或其他 3. 随机森林的原理？如何进行调参？树的深度一般如何确定，一般为多少？ 原理：RF是一种集成算法，属于bagging类，它通过集成多个决策树模型，通过对基模型的结果采用投票法或者取平均来获得最终结果，使得最终模型有较高的准确度和泛化性能。 调参： 还是看刘建平老师的这篇：https://www.cnblogs.com/pinard/p/6160412.html RF和GBDT调参过程类似，可以对比记忆： 无参数拟合–&gt;n_estimators调参–&gt;max_depth, min_sample_split–&gt;min_sample_split, min_samples_leaf–&gt;max_features 如何确定树的深度：当训练样本多，数据特征维度多的时候需要限制这个参数，具体取决于数据分布，一般在10-100之间。 3. Bagging 和 Boosting的区别 样本选择：Bagging有放回的选取训练集，并且从原始数据集中选取的各轮训练集之间相互独立；Boosting每次都使用全部数据，只是每个样例的权重不同。 样例权重：Bagging采用均匀采样，每个样例的权重相同；Boosting每轮训练都依据上一轮训练结果更新样例权重，错误率越大的样例，权重越大。 预测函数：Bagging每个基函数的预测结果权重相同；Boosting中预测误差越小的基模型有更大的权重。 偏差和方差：Bagging得出的结果低方差，Boosting低偏差。 并行计算：Bagging可以并行生成基模型，Boosting各个预测函数只能顺序生成，后一轮模型的参数需要前一轮的预测结果。 4. GBDT调参 我觉得啊，一般面试官如果问我们这种题目，一定是要求我们使用过这个算法，如果使用过就要理解记住，没使用过就坦诚的说没用过，大家可以跟着下面这个链接的刘建平老师学习一遍。 具体实例：https://www.cnblogs.com/pinard/p/6143927.html 参数分类： Boosting框架参数：n_estimators, learning_rate, subsample CART回归树参数（与决策树类似）：max_features, max_depth, min_sample_split, min_samples_leaf 大致步骤： 无参数拟合–&gt;固定learning_rate，estimators调参–&gt;max_depth, min_sample_split–&gt;min_sample_split, min_samples_leaf–&gt;拟合查看–&gt;max_features–&gt;subsample–&gt;不断减小learning_rate，加倍estimators来拟合 5. RF、GBDT之间的区别（重要） 此问题充分理解，你需要这些： https://blog.csdn.net/data_scientist/article/details/79022025 https://blog.csdn.net/xwd18280820053/article/details/68927422 https://blog.csdn.net/m510756230/article/details/82051807 相同点：都是由多棵树组成，结果由多棵树共同决定。 不同点： GBDT是回归树，RF可以是回归树也可以是分类树； GBDT对异常值特别敏感，RF则没有； GBDT只能串行生成多个模型，RF可以并行； GBDT的结果有多个结果求和或者加权求和，RF是由投票选出结果； GBDT是通过减少偏差来提高模型性能，RF是通过减少方差； RF对所有训练集一视同仁，GBDT是基于权值的弱分类器。 6. 随机森林的优缺点 优点： 相比于其他算法，在训练速度和预测准确度上有很大的优势； 能够处理很高维的数据，不用选择特征，在模型训练完成后，能够给出特征的重要性； 可以写成并行化方法； 缺点：在噪声较大的分类回归问题上，容易过拟合。 7. GBDT的关键？GBDT中的树是什么树？ 关键：利用损失函数的负梯度方向作为残差的近似值来拟合新的CART回归树。 CART回归树。 8. GBDT和XGB的区别 GBDT以CART为基分类器，XGB则支持多种分类器； GBDT只用到了一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶与二阶导数，并且可以自定义代价函数，只要一阶二阶可导； XGBoost与GDBT都是逐次迭代来提高模型性能，但是XGBoost在选取最佳切分点时可以开启多线程进行，大大提高了运行速度； 新增了Shrinkage和column subsampling，为了防止过拟合； 对缺失值有自动分裂处理（默认归于左子树）； 如何处理的缺失值： Xgboost 在处理带缺失值的特征时，先对非缺失的样本进行排序，对该特征缺失的样本先不处理，然后在遍历每个分裂点时，将这些缺失样本分别划入左子树和右子树来计算损失然后求最优。如果训练样本中没有缺失值，而预测过程中出现了缺失值，那么样本会被默认分到右子树。 xgb损失函数加入正则化，控制模型复杂度，使模型简单，防止过拟合。 9. LGB和XGB的区别（抓住主要区别，理解要有深度） lgb官方文档：http://lightgbm.apachecn.org/#/docs/4 https://www.cnblogs.com/infaraway/p/7890558.html 树的切分策略不同： xgb基于level-wise，对每一层节点进行无差别分裂，造成不必要的开销； lgb基于leaf-wise，在当前所有叶子节点中选择分裂增益最大的节点进行分裂； 实现并行方式不同： xgb使用基于 pre-sorted 决策树算法； lgb使用基于histogram决策树算法，对离散特征进行分裂时，特征的每个取值为一个桶； lgb支持直接输入categorical feature，对类别特征无须进行one-hot处理； 优化通信代价不同：lgb支持特征并行、数据并行。 10. GBDT的算法步骤（我没找到推导呢） 10. 随机森林生成过程 https://www.cnblogs.com/liuwu265/p/4690715.html 样本集选择：从原始样本中有放回的抽取N个训练集，各个训练集之间相互独立，共进行k轮抽取； 决策树生成：假设特征空间共有D个特征，随机选择d个特征，构成新的特征空间，用于训练单棵决策树，共k轮，生成k棵决策树； 模型合成：生成的k棵决策树相互独立，各个基模型之间权重相同，如果是分类问题，则使用投票法决定最终结果，如果是回归问题，则使用平均法； 模型验证：模型验证本身需要验证集，但在此处我们无须额外设置验证集，只需使用原始样本中没有使用过的即可。 11. xgboost如何确定特征和分裂点的？ XGBoost使用了和CART回归树一样的想法，利用贪婪算法。基于目标函数，遍历所有特征的所有特征划分点，具体做法就是分裂后的目标函数值大于分裂之前的就进行分裂。 12. XGB是如何给出特征重要性评分的？ 建议多看下以下链接： https://blog.csdn.net/waitingzby/article/details/81610495 https://blog.csdn.net/sujinhehehe/article/details/84201415 https://www.cnblogs.com/haobang008/p/5929378.html 特征权重（weight）：指的是在所有树中，某特征被用来分裂节点的次数； 如何计算：一个特征对分裂点性能度量（gini或者其他）的提升越大（越靠近根节点）其权重越大，该特征被越多提升树选择来进行分裂，该特征越重要，最终将一个特征在所有提升树中的结果进行加权求和然后求平均即可。 源码片段（加深理解）： 主要是对每个特征进行计数操作： if importance_type == &#39;weight&#39;: # do a simpler tree dump to save time trees = self.get_dump(fmap, with_stats=False) fmap = {} for tree in trees: for line in tree.split(&#39;\\n&#39;): # look for the opening square bracket arr = line.split(&#39;[&#39;) # if no opening bracket (leaf node), ignore this line if len(arr) == 1: continue # extract feature name from string between [] fid = arr[1].split(&#39;]&#39;)[0].split(&#39;&lt;&#39;)[0] if fid not in fmap: # if the feature hasn&#39;t been seen yet fmap[fid] = 1 else: fmap[fid] += 1 return fmap 13. XGB如何消除残差的，目标函数是什么？ https://www.cnblogs.com/palantir/p/10671119.html https://blog.csdn.net/guoxinian/article/details/79243307 14. GDBT在处理分类和回归问题时有什么区别？（感觉还有补全的地方） 损失函数不同： 分类：指数、对数； 回归：均方差、绝对值。 XGB如何防止过拟合 Shrinkage and Column Subsampling。 Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型； Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种： 按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点； 随机选择特征，则建树前随机选择一部分特征然后分裂就只遍历这些特征。一般情况下前者效果更好。 EM算法 1. 采用EM算法求解的模型有哪些？为什么不用牛顿法或者梯度下降法？（感觉第二个问题有点错误，mark） 高斯混合模型、协同过滤、KMeans 求和的项随着隐变量的数量随指数上升，梯度计算带来麻烦，而EM是非梯度优化算法。 2. 用EM算法推导解释KMeans KMeans中，每个聚类簇的中心就是隐含变量。 E步：随机初始化k个聚类中心 M步：计算每个样本点最近的质心，并将他聚类到这个质心 重复以上两步，直到聚类中心不发生变化为止。 决策树 1. ID3和C4.5的优缺点 ID3 C4.5 优点 实现简单 1. 可以处理连续型特征 2. 易于理解，准确率高 缺点 1. 只能处理离散特征 2. 倾向于选择取值较多的特征 1. 在构造树的过程中，对数据多次扫描排序，低效 2. 只能够用于驻留在内存中的数据，当数据大到无法在内存容纳时，程序无法执行 2. 决策树处理连续值的方法 连续属性离散化，常用的离散化策略是二分法（C4.5）： 3. 决策树的剪枝策略 目的：简化决策树模型，提高模型泛化能力； 基本思想：减去某些子树和叶节点，将其根节点作为新的叶子节点，实现简化模型。 损失函数： 剪枝策略： 预剪枝：在构造决策树的过程中，在对节点划分之前进行估计，若划分后不能带来决策树性能和泛化能力的提升，则不进行划分，并将此节点作为叶节点。 后剪枝：构造完决策树之后，自底向上搜索，对每个非叶节点进行考察，若将该子树去除变为叶节点能带来决策树泛化性能的提升，则将该节点作为叶节点。 对比：后剪枝的分支比预剪枝的分支要多一些，不容易欠拟合，泛化能力强，但是由于后剪枝在构造完成决策树之后，而且还需要自底向上进行搜索故时间开销大。 4. 决策树的构造过程 https://shuwoom.com/?p=1452 特征选择：在所有特征中选择一个特征，作为当前节点的划分标准：ID3(信息增益)、C4.5(信息增益比)、CART(gini系数)； 决策树生成：依据特征评估标准，从上到下的递归的生成子节点，直到数据集不可分时，停止生长； 剪枝：决策时容易过拟合，通过剪枝，简化模型，降低过拟合。 5. 基于树的模型有必要做标准化吗？ https://blog.csdn.net/answer3lin/article/details/84961694 不必要；概率模型（树模型）只关心变量的分布和变量之间的条件概率。 6. CART回归树和CART决策树的构造过程 特征选择+决策树构造+决策树剪枝 cart回归树 cart回归树对某个特征的每个分量，将数据集划分为大于该分量和小于该分量两部分，同时计算对应y的均值，并计算均方误差函数的值，选择具有最小误差值的分量，并将其作为划分标准，重复该流程知道数据集为空或者前后均方误差下降值小于给定阈值则停止。 cart分类树 选择具有最小的GINI系数的属性和属性值，作为最优分裂属性和最优分裂属性值。 7. 熵 概念：信息所包含不确定大小的度量，一个信息的所包含的不确定性越大，其所含的信息越多。 熵的计算公式： H ( X ) = − ∑ i = 1 N p i l o g 2 p i H(X) = -\\sum_ {i=1}^{N}p_i log_2 p_i H(X)=−∑i=1N​pi​log2​pi​ 朴素贝叶斯 1. 朴素贝叶斯的公式 “朴素”的含义：假设各个特征之间相互独立。 2. NB原理及其分类 原理：根据贝叶斯公式，通过某对象的先验概率，计算其后验概率，并选择后验概率最大的类作为该对象所属于的类； 分类：（根据变量的分布不同） NB的伯努利模型，特征是布尔变量，符合0/1分布，在文本分类中，特征就是词是否出现； NB的多项式模型，特征是离散值，符合多项式分布，在文本分类中，特征就是词出现的次数； NB的高斯模型，特征是连续值，符合高斯分布（高斯分布又名正态分布），在文本分类中，特征就是词的TF-IDF值。 3. NB的优缺点 优点： 算法原理简单； 所估计的参数少； 假设条件概率计算是彼此独立的，因此可以用于分布式计算； 属于生成式模型，收敛速度比判别式模型要快； 对缺失数据不太敏感； 天然可以处理多分类问题。 缺点： 假设各个特征之间相互独立这一条件在实际应用中往往是不能成立的； 不能学习到特征之间的相互作用； 对输入数据的表达形式敏感。 4. LR和朴素贝叶斯(NB)之间的区别 https://www.cnblogs.com/wangkundentisy/p/9193217.html 暂时不看。 5. 适用场景 支持大规模数据，并且支持分布式实现； 特征维度可以很高； 可以处理数值型特征和类别型特征； 线性回归与逻辑回归 1. LR推导（重要） 几个小问题： 极大似然的概念：找到参数θ的一个估计值，使得当前样本出现的可能性最大。 为什么极大似然的时候可以相乘：特征之间是独立同分布。 LR的参数计算方法：梯度下降、牛顿法。 2. 逻辑回归和线性回归的区别 线性回归 逻辑回归 对连续值预测 分类 最小二乘法 最大似然估计 拟合函数 预测函数 3. 最小二乘法和最大似然法的区别（没太明白，mark） https://blog.csdn.net/lu597203933/article/details/45032607 4. 为什么用最小二乘而不是最小四乘（没太明白，mark） 5. 介绍一下逻辑回归？它的损失函数是什么？ 一句话介绍逻辑回归：逻辑回归假设数据服从伯努利分布，通过极大化似然函数，利用梯度下降算法来求得参数，实现数据的二分类； 它的损失函数是它的极大似然函数（对数损失函数）： 6. LR 损失函数为什么用极大似然函数？ https://blog.csdn.net/aliceyangxi1987/article/details/80532586 LR的目标是使每个样本的预测都有最大概率，即将所有样本预测后的概率相乘概率最大，这就是极大似然函数； 极大似然函数取对数即为对数损失函数，对数损失函数的训练求解参数比较快，更新速度也稳定； 为什么不用平方损失函数呢？因为平方损失函数的梯度更新速度和sigmoid的梯度有关，而在定义域范围内sigmoid的梯度值&lt;=2.5，训练速度非常慢；而且平方损失函数会导致损失函数是非凸的，不易求解。 7. LR的参数计算方法：梯度下降法，请介绍三种GD的区别 批梯度下降：可以得到全局最优解，缺点是更新每个参数都需要遍历所有数据，计算量大，还有很多冗余计算，在数据非常大的时候，每个参数的更新都是非常慢的； SGD：以高方差频繁更新，使SGD跳到新的或潜在更优的局部解，但是也使得收敛到局部最优解的过程更加复杂； mini-batch SGD：结合了二者的优点，每次选取N个样本，减少了参数更新的次数，可以达到更加稳定的收敛结果。 8. LR的优缺点 http://www.cnblogs.com/ModifyRong/p/7739955.html 优点： 形式简单，模型可解释性非常好，特征权重可以看出不同特征最后结果的影响； 效果不错，可以作为baseline； 占用资源少，特别是内存； 方便输出结果调整； 训练速度快。 缺点： 准确率不高； 对样本不均衡问题无法很好的解决； 对非线性分类问题也是； 本身无法筛选特征，可以和GBDT结合使用。 9. 逻辑斯特回归为什么要对特征进行离散化 稀疏向量做内积乘法运算速度快，计算结果方便存储，易于扩展 离散化后的特征对异常数据有更强的鲁棒性 特征离散化后模型更稳定 LR属于广义线性模型，表达能力有限，特征离散化为N个后，每个变量有自己的权重，相当于引入非线性，表达能力增强，加大拟合 特征离散化后还可以做特征交叉，由M+N个变为M*N个，进一步引入非线性 10. LR为什么用sigmoid函数,这个函数有什么优点和缺点? 为什么： Sigmoid 函数自身的性质 sigmoid 函数连续，单调递增 sigmiod 函数关于（0，0.5） 中心对称 计算sigmoid函数的导数非常的快速 将输入变量的范围从负无穷到正无穷，映射到（0，1），而概率要求正是（0，1）。 逻辑回归认为函数其概率服从伯努利分布，将其写成指数族分布的形式，能够推导出sigmoid函数的形式。 https://blog.csdn.net/a1628864705/article/details/62233395 https://blog.csdn.net/qq_19645269/article/details/79551576 优缺点： 优点： 可以看到sigmoid函数处处连续 -&gt;便于求导； 可以将函数值的范围压缩到[0,1]-&gt;可以压缩数据，且幅度不变； 便于前向传输。 缺点： 在趋向无穷的地方，函数值变化很小，容易缺失梯度，不利于深层神经网络的反馈传输； 幂函数还是比较难算的； 函数均值不为0，当输出大于0时，则梯度方向将大于0，也就是说接下来的反向运算中将会持续正向更新；同理，当输出小于0时，接下来的方向运算将持续负向更新。 11. LR伪代码 读取文件、写入文件以及计算准确率等之前实验做过的或者过于简单的函数功能不列出。 int main() { 读取训练集和测试集，其中训练集每三个样本取前两个作为训练集，第三个作为验证集。 初始化w：for(int i=0;i&lt;Length;i++) w[i]=1; for(int k=0:7) for(int i=0:traincnt)//遍历训练集样本 { CalWeight(i);//计算样本i的权重分数 CalCost(i);//每一维的梯度（代价）计算 Updatew(); //更新w if(i%20==0) //每更新w20次计算一次准确率 { Predict();//预测验证集样本 Cal_acc();//计算准确率 ac[cnt]=accuracy; cnt++; } } output_result();//输出验证集准确率以供调试 output_test_result();//输出测试集预测结果 } void CalWeight(int index){ weight = 当前向量w的转置*样本i向量； } void CalCost(int index){ 计算每一维的梯度，存储在向量数组Cost[]中； } void Updatew(){ 使用w = w - alpha x gradient来更新回归系数（w） } void Predict(){ P = 1/(1+exp(-1* w^T *样本i向量); if(P&gt;0.5) p_label=1; else p_label=0; } PCA 1. PCA原理 具体推导看这里： https://blog.csdn.net/u012421852/article/details/80458340 用于：特征降维，去除冗余和可分性不强的特征； 目标：降维后的各个特征不相关，即特征之间的协方差为0； 原理：基于训练数据X的协方差矩阵的特征向量组成的k阶矩阵U，通过XU得到降维后的k阶矩阵Z； 算法步骤 计算训练样本的协方差矩阵C； 计算C的特征值和特征向量； 将C的特征值降序排列，特征值对应特征向量也依次排列； 假如要得到X的k阶降维矩阵，选取C的前k个特征{u1,u2…uk}，组成降维转换矩阵U； Z = XU，Z即为降维后的矩阵； KMeans 1. K-means 的原理，时间复杂度，优缺点以及改进 原理：对于给定样本集，按照样本之间的距离大小，将样本划分为若干个簇，使簇内距离尽可能小，簇间距离尽可能大； 时间复杂度：O(knd*t) | k:类别，n：样本数，d：计算样本之间距离的时间复杂度，t：迭代次数； 优缺点： 优点：1. 原理易懂、实现简单、收敛速度快、效果好 2. 可解释性强 3. 可调参数只有少，只有k； 缺点：1. 聚类效果受k值影响大 2. 非凸数据集难以收敛 3. 隐含类别不均衡时，效果差 4. 迭代算法，得到的只是局部最优 5. 对噪音和异常数据敏感。 改进：随机初始化K值影响效果 + 计算样本点到质心的距离耗时这两方面优化 KMeans++算法 KMeans随机选取k个点作为聚类中心，而KMeans++采用如下方法： 假设已经选取好n个聚类中心后，再选取第n+1个聚类中心时，距离这n个聚类中心越远的点有越大的概率被选中；选取第一个聚类中心（n=1）时也是需要像KMeans一样随机选取的。 其他 1. 机器学习性能评价指标 Precision （精确率）和Recall（召回率） P = TP / (TP + FP) R = TP / (TP + FN) F1值 F1 = 2PR / (P + R) ROC 和 AUC 2. 奥卡姆剃刀（Occam’s Razor） 如无需要，勿增实体。 简单有效原理。 具体到机器学习上，能够拟合数据的简单模型才是我们需要的。 3. L1范数与L2的作用，区别 在机器学习中，通常损失函数会加上一个额外项，可看作损失函数的惩罚项，是为了限制模型参数，防止过拟合。 （自己注意下图！） L1范数 L2范数 各个参数的绝对值之和 各个参数的平方和的开方 先验分布是拉氏分布 高斯（正态）分布 使参数稀疏化，有特征选择的功能 使参数接近于0，防止过拟合 （模型越简单，越不容易过拟合） Lasso回归 Ridge回归 4. L1正则为什么可以把系数压缩成0？ https://blog.csdn.net/jinping_shi/article/details/52433975 5. 正则化为什么能防止过拟合？添加正则项后依旧过拟合如何调节参数lambda？ 4. 过拟合的原因和防止过拟合的方法 原因：1. 数据有噪声； 2. 训练数据不足，有限的训练数据； 3. 过度训练导致模型复杂。 防止过拟合的方法： 早停止：在模型对训练数据迭代收敛之前停止迭代。 具体做法：在每一个Epoch结束时,计算validation_data的accuracy，当accuracy不再提高时，就停止训练。（注意多次观察，多次精度未提升时则停止迭代） dropout：在训练时，以一定的概率忽略隐层中的某些节点。 插播：为什么dropout能有效防止过拟合，请解释原因？ 详细解答看这个：https://www.cnblogs.com/wuxiangli/p/7309501.html 取平均的作用； 减少神经元之间复杂的共适应关系：因为dropout程序导致两个神经元，不一定每次都在一个网络中出现，避免了有些特征只有在特定特征条件下才有效的情况，迫使网络去学习更加鲁棒的特征。 正则化 数据集扩充：1. 从源头上获取更多数据；2. 数据增强（通过一定规则扩充数据）；3. 根据当前数据估计分布参数，利用该分布获得更多数据。 集成学习 5. 特征选择的方法 什么样的特征是好特征：特征覆盖率高，特征之间相关性小，不能改变原始特征分布 wrapper（根据目标函数，每次选择若干特征，活着排除若干特征） 递归特征消除法：使用基模型进行多轮训练，每轮训练结束后，消除若干权值系数的特征，再使用新的特征进行下一轮训练。 embedded（先使用某些机器学习算法训练模型，得到各个特征的权值系数，再有大到小进行特征选择） 基于惩罚项的特征选择法 基于树的特征选择法 filter（根据发散性活着相关性对各个特征进行评分，设定阈值或者特征个数，选择特征） 卡方检验 互信息 方差选择法：计算每个特征的方差，根据阈值，选择方差大于阈值的特征进行训练； 相关系数 6. 说一说你知道的损失函数 0-1损失函数（感知机） 平方损失函数（线性回归） 绝对值损失函数 指数损失函数（adaBoost） Hinge Loss（SVM） 对数损失函数（逻辑回归） 7. 数据预处理的方法 数据清洗：异常值和缺失值； 数据集成：实体识别，冗余属性识别； 数据转换：简单函数转换，连续特征离散化，规范化，构造属性 数据规约：数值规约，属性规约 8. 偏差和方差是什么，高偏差和高方差说明了什么 偏差：是指预测值和真实值之间的差，偏差越大，预测和真实值之间的差别越大，衡量模型的预测能力。 方差：描述预测值的变化范围和离散程度，方差越大，表示预测值的分布越零散，对象是多个模型，使用不同的训练数据训练出的模型差别有多大。 当训练误差和交叉验证误差或测试误差都很大，且值差不多时，处于高偏差，低方差，欠拟合状态；当训练误差和交叉验证误差差别很大，且测试集误差小，验证集误差大时，处于高方差，低偏差，过拟合状态。 9. 优化算法有那些？ 具体算法原理、优缺点看这个：https://www.cnblogs.com/xinbaby829/p/7289431.html 梯度下降法、牛顿法和拟牛顿法、共轭梯度法、 启发式方法、解决约束优化问题的拉格朗日乘数法 10. 梯度下降算法和牛顿法的区别 https://www.cnblogs.com/lyr2015/p/9010532.html 牛顿法： 通过求解目标函数一阶导数为0的参数来求解目标函数最小值时的参数； 收敛速度快； 迭代过程中，海森矩阵的逆不断减小，相当于逐步减小步长； 海森矩阵的逆，计算复杂，代价较高，因此有了拟牛顿法。 梯度下降算法： 通过梯度方向和步长，直接求得目标函数最小值的参数； 越靠近最优值，步长应该逐渐减小，否则会在最优值附近震荡。 11. 如何解决类别不均衡问题？ https://blog.csdn.net/program_developer/article/details/80287033 https://www.cnblogs.com/zhaokui/p/5101301.html 采样：其中采样又分为上采样（将数量少的类别的数据复制多次），和下采样（将数量多的类别的数据剔除一部分）。 https://www.jianshu.com/p/9a68934d1f56 smote算法： 用途：合成新的少数样本； 基本思路：对每一个少数样本a，从a的k个最近邻中随机挑选一个样本b，从a、b连线上随机选择一个点，作为新合成的少数样本。 步骤： 1）对每一个少数样本a，基于欧式距离，计算它到其他少数样本的距离，找到他的k个最近邻； 2）根据样本不平衡比例设置一个采样比例进而得到采样倍率N，对于每一个少数样本a，从他的最近邻中选择若干样本，假设选择的样本为b； 3）基于以下公式得到新合成的少数样本 c = a * random(0,1)*|b-a|。 数据合成：利用现有的数据的规律生成新的数据。 一分类：当数据样本极不平衡时，将它看作一分类，这样我们的重点就在于将它看成对某种类别进行建模。 对不同的类别给予不同的分错代价。 12. 梯度下降算法的过程 首先我们有一个可微分的函数，这个函数就好像一个山，我们的目标是找到函数的最小值（即山底）。根据经验可知，我们从最陡峭的地方走，可以尽快到达山底。对应到函数就是找到给定点的梯度，并且沿着梯度相反的方向就能让函数值下降最快（因为梯度方向就是函数值变化最快的方向）。重复利用这个方法，反复求取梯度，最后就能到达局部最小值。 13. 为什么我们还是会在训练的过程当中将高度相关的特征去掉？ 使得模型的解释性更强； 大大提高训练速度。 14. 最大似然估计与贝叶斯估计的区别 15. 判别式和生成式的算法各有哪些，区别是什么？ https://blog.csdn.net/amblue/article/details/17023485 https://blog.csdn.net/qq_41853758/article/details/80864072 区别：二者最本质的区别是建模对象的不同。 判别式模型的评估对象是最大化条件概率P(Y|X)并对此进行建模； 生成式模型的评估对象是最大化联合概率P(X,Y)并对此进行建模。 判别式模型：线性回归、决策树、支持向量机SVM、k近邻、神经网络等； 生成式模型：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA。 16. 最大似然估计 https://www.cnblogs.com/zyxu1990/p/3209407.html 条件：假设样本独立同分布； 目标：估计出这个分布中的参数theta； 方法：这一组样本的概率最大时就对应了该模型的参数值。 17. 请用一句话说明AUC的本质和计算规则？AUC高可以理解为精确率高吗？ 本质：一个正例，一个负例，预测为正例的概率值大于预测为负的概率值的可能性； 计算规则：ROC曲线下的面积： AUC = ∫ t = ∞ − ∞ y ( t ) d x ( t ) \\text{AUC} = \\int_{t=\\infty}^{-\\infty} y(t) d x(t) AUC=∫t=∞−∞​y(t)dx(t) 不可以，精确率是基于某个阈值进行计算的，AUC是基于所有可能的阈值进行计算的，具有更高的健壮性。AUC不关注某个阈值下的表现如何，综合所有阈值的预测性能，所以精确率高，AUC不一定大，反之亦然。 参考：https://blog.csdn.net/legendavid/article/details/79063044 18. 二分类时，为什么AUC比accuracy更常用？为什么AUC对样本类别比例不敏感？ 19. 如何绘制ROC曲线？ 以真正例率为纵坐标，假正例率为横坐标绘制的曲线。 TPR = TP /（TP + FN）真 FPR = TN / ( TN + FP) 假 20. 梯度下降的改进算法有哪些？梯度消失的概念？ 如何解决梯度下降法陷入局部最优的问题？ 参考 https://blog.csdn.net/maqunfi/article/details/82634529 使用随机梯度下降法替代真正的梯度下降算法； 设置冲量； 使用不同的初始权值进行训练。 21. VC维的理解 这个感觉不太可能会碰到～ https://www.cnblogs.com/wuyuegb2312/archive/2012/12/03/2799893.html 分散：对于一个给定集合S={x1, … ,xd}，如果一个假设类H能够实现集合S中所有元素的任意一种标记方式，则称H能够分散S。 VC维的定义：H的VC维表示为VC(H) ，指能够被H分散的最大集合的大小。若H能分散任意大小的集合，那么VC(H)为无穷大。 22. 判断模型线性与非线性 https://zhuanlan.zhihu.com/p/37866896 只需要判别决策边界是否是直线，也就是是否能用一条直线来划分，如果可以则为线性。","@type":"BlogPosting","url":"https://uzzz.org/2019/08/13/793456.html","headline":"【算法工程师】机器学习面试问题总结","dateModified":"2019-08-13T00:00:00+08:00","datePublished":"2019-08-13T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/13/793456.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>【算法工程师】机器学习面试问题总结</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-tomorrow-night-eighties"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>哎呀，要秋招了啊～<br> 不禁感叹：How time flys～<br> 重新出发～</p> 
  <hr> 
  <p>后期我发现还是需要把相关文章的链接放上来的，方便大家深入理解记忆，如果你没时间就直接看文字，如果有时间记得把链接点开看看哦～都是大佬的精华～</p> 
  <p><strong>一切为了暑期实习！！！<br> 一切为了暑期实习！！！<br> 一切为了暑期实习！！！</strong></p> 
  <hr> 
  <h1><a id="_13"></a>机器学习</h1> 
  <h2><a id="SVM_14"></a>SVM（重点）</h2> 
  <h3><a id="1__SVM_15"></a>1. SVM原理和推导</h3> 
  <ol> 
   <li>原理：SVM试图寻找一个超平面使正负样本分开，并使得几何间隔最大。</li> 
  </ol> 
  <blockquote> 
   <p>⚠️（注意分清以下这三种情况）<br> 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分SVM<br> 当训练样本近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性SVM<br> 当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性分类器，即非线性SVM</p> 
  </blockquote> 
  <ol start="2"> 
   <li>推导（十分重要）我自己写的，字很随意啦～<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190313162157660.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
  </ol> 
  <h3><a id="2_SVM__24"></a>2. SVM 为什么要引入拉格朗日的优化方法</h3> 
  <p>将拉格朗日对偶性应用到求解原始问题上，通过求解对偶问题进而求得原始问题的最优解，原因有二：</p> 
  <ol> 
   <li>对偶问题往往更容易求解；</li> 
   <li>自然引入核函数，继而推广到非线性分类问题。</li> 
  </ol> 
  <h3><a id="3__28"></a>3. 为什么要选择最大间隔分类器？</h3> 
  <ol> 
   <li>从数学上考虑：<br> 误分类次数和几何间隔之间存在下列关系，几何间隔越大，误分类次数越少。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190319103314583.png" alt="在这里插入图片描述"></li> 
   <li>感知机利用误分类最小策略，求得分离超平面，不过此时解有无数个；而线性可分SVM利用间隔最大求得最优分离超平面，求得唯一解，而且此时的模型鲁棒性好，对未知样本泛化能力最强。</li> 
  </ol> 
  <h3><a id="4_SVMSVM_33"></a>4. 样本失衡会对SVM的结果产生影响吗？如何解决SVM样本失衡问题？样本比例失衡时，使用什么指标评价分类器的好坏？</h3> 
  <ol> 
   <li>样本失衡会对结果产生影响，分类超平面会靠近样本少的类别。原因：因为使用软间隔最大化，假设对所有类别使用相同的惩罚因子，而优化目标是最小化惩罚量，所以靠近样本少的类别惩罚量少。</li> 
   <li>解决SVM样本失衡问题方法： 
    <ol> 
     <li>对不同的类别赋予不同的惩罚因子（C），训练样本越少，C越大。缺点：偏离原始样本的概率分布。</li> 
     <li>对样本的少的类别，基于某种策略进行采样。</li> 
     <li>基于核函数解决问题。</li> 
    </ol> </li> 
   <li>当样本比例不均衡时，使用ROC曲线。</li> 
  </ol> 
  <h3><a id="5_SVM_40"></a>5. SVM如何解决多分类问题</h3> 
  <blockquote> 
   <p><a href="https://www.cnblogs.com/CheeseZH/p/5265959.html" rel="nofollow">https://www.cnblogs.com/CheeseZH/p/5265959.html</a></p> 
  </blockquote> 
  <ol> 
   <li>直接法：直接修改目标函数，将多个分类面的参数求解合并到一个目标函数上，一次性进行求解。</li> 
   <li>间接法： 
    <ol> 
     <li>One VS One：任意两个样本之间训练一个分类模型，假设有k类，则需要k(k-1)/2个模型。对未知样本进行分类时，得票最多的类别即为未知样本的类别。libsvm使用这个方法。</li> 
     <li>One VS Rest：训练时依次将某类化为类，将其他所有类别划分为另外一类，共需要训练k个模型。训练时具有最大分类函数值的类别是未知样本的类别。</li> 
    </ol> </li> 
  </ol> 
  <h3><a id="6_SVM_46"></a>6. SVM适合处理什么样的数据？</h3> 
  <p>高维、稀疏、样本少的数据。</p> 
  <h3><a id="7_SVM_48"></a>7. SVM为什么对缺失数据敏感？（数据缺失某些特征）</h3> 
  <ol> 
   <li>SVM没有缺失值的处理策略；</li> 
   <li>SVM希望样本在特征空间中线性可分，特征空间的好坏影响SVM性能；</li> 
   <li>缺失特征数据影响训练结果。</li> 
  </ol> 
  <h3><a id="8_sklearnsvm_52"></a>8. sklearn.svm参数</h3> 
  <ol> 
   <li>栗子：</li> 
  </ol> 
  <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span>
            C<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> 
            kernel<span class="token operator">=</span><span class="token string">'rbf'</span><span class="token punctuation">,</span> 
            degree<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> 
            gamma<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">,</span> 
            coef0<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> 
            shrinking<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> 
            probability<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
            tol<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> 
            cache_size<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> 
            class_weight<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
            verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
            max_iter<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> 
            decision_function_shape<span class="token operator">=</span><span class="token string">'ovr'</span><span class="token punctuation">,</span> 
            random_state<span class="token operator">=</span><span class="token boolean">None</span>
            <span class="token punctuation">)</span>
</code></pre> 
  <ol start="2"> 
   <li>重要参数：<strong>(理解含义和对模型的影响)</strong> 
    <ol> 
     <li> <p><code>C : float, optional (default=1.0)</code></p> <p>误差项的惩罚参数，一般取值为10的n次幂，如10的-5次幂，10的-4次幂…10的0次幂，10，1000,1000，在python中可以使用pow（10，n） n=-5~inf<br> <strong>C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱。<br> C值小，对误分类的惩罚减小，容错能力增强，泛化能力较强。</strong></p> </li> 
     <li> <p><code>kernel : string, optional (default=’rbf’)</code><br> svc中指定的kernel类型。<br> 可以是： ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ 或者自己指定。 默认使用‘rbf’ 。</p> </li> 
     <li> <p><code>degree : int, optional (default=3)</code><br> 当指定kernel为 ‘poly’时，表示选择的多项式的最高次数，默认为三次多项式。<br> 若指定kernel不是‘poly’,则忽略，<strong>即该参数只对‘poly’有作用。</strong></p> </li> 
     <li> <p><code>gamma : float, optional (default=’auto’)</code><br> 当kernel为‘rbf’, ‘poly’或‘sigmoid’时的kernel系数。<br> 如果不设置，默认为 ‘auto’ ，此时，kernel系数设置为：1/n_features</p> </li> 
     <li> <p><code>coef0 : float, optional (default=0.0)</code><br> kernel函数的常数项。<br> 只有在 kernel为‘poly’或‘sigmoid’时有效，默认为0。</p> </li> 
    </ol> </li> 
  </ol> 
  <h3><a id="9_SVMHinge_Loss_90"></a>9. SVM的损失函数—Hinge Loss（合页损失函数）</h3> 
  <blockquote> 
   <p><a href="https://www.jianshu.com/p/fe14cd066077" rel="nofollow" data-token="ea257c1830c67f48c4a90107801e22d3">https://www.jianshu.com/p/fe14cd066077</a></p> 
  </blockquote> 
  <ol> 
   <li>hinge loss图像<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190403210753871.png" alt="在这里插入图片描述"></li> 
   <li>表达式<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190403210804776.png" alt="在这里插入图片描述"></li> 
  </ol> 
  <h3><a id="10_SMOSVMmark_96"></a>10. SMO算法实现SVM(思想、步骤、常见问题)（我这个还不熟悉，mark）</h3> 
  <ol> 
   <li><strong>思想</strong>：将大的优化问题分解为多个小的优化问题，求解小的优化问题往往更简单，同时顺序求解小问题得出的结果和将他们作为整体求得的结果一致。</li> 
   <li><strong>步骤</strong>：1. 选取一对需要更新的变量ai和aj（阿尔法）2. 固定除ai和aj以外的所有变量，求解对偶问题获得更新ai、aj、b。</li> 
   <li><strong>常见问题—如何选取ai和aj和b？</strong> 
    <ol> 
     <li>选取违反KKT条件最严重的ai，在针对这个ai选择最有可能获得较大修正步长的aj；</li> 
     <li>b一般选取支持向量求解的平均值。</li> 
    </ol> </li> 
  </ol> 
  <h3><a id="11_SVM_102"></a>11. SVM如何解决非线性问题？你所知道的核函数？</h3> 
  <ol> 
   <li>当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。</li> 
   <li>常用核函数（重点sigmoid、RBF（名字一定要记住啦：<strong>高斯径向基核函数</strong>）、要会写核函数的公式哦～）<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190313163633355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
  </ol> 
  <h3><a id="12_RBF_106"></a>12. 线性核与RBF核的区别？</h3> 
  <ol> 
   <li><strong>训练速度</strong>：线性核只有惩罚因子一个参数，训练速度快，RBF还需要调节gamma；</li> 
   <li><strong>训练结果</strong>：线性核得到的权重w能反映出特征的重要性，由此进行特征选择，RBF无法解释；</li> 
   <li><strong>训练数据</strong>：线性核适合样本特征&gt;&gt;样本数量的，RBF核相反。<strong>（揭示了如何选择核函数）</strong></li> 
  </ol> 
  <h3><a id="13_SVMLR_110"></a>13. SVM和LR的联系和区别</h3> 
  <ol> 
   <li>联系： 
    <ol> 
     <li>都是判别式模型</li> 
     <li>都是有监督的分类算法</li> 
     <li>如果不考虑核函数，都是线性分类算法</li> 
    </ol> </li> 
   <li>区别： 
    <ol> 
     <li>LR可以输出概率，SVM不可以</li> 
     <li>损失函数不同，即分类机制不同</li> 
     <li>SVM通过引入核函数解决非线性问题，LR则不是</li> 
    </ol> 
    <blockquote> 
     <p>原因：LR里每个样本点都要参与核计算，计算复杂度太高，故LR通常不用核函数。</p> 
    </blockquote> 
    <ol start="4"> 
     <li>SVM计算复杂，效果比LR好，适用于小数据集；LR计算快，适用于大数据集，用于在线学习</li> 
     <li>SVM分类只与分类超平面附近的点有关，LR与所有点都有关系</li> 
     <li>SVM是结构风险最小化，LR则是经验风险最小化</li> 
    </ol> 
    <blockquote> 
     <p>结构风险最小化就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，减小泛化误差。为了达到结构风险最小化的目的，最常用的方法就是添加正则项。</p> 
    </blockquote> </li> 
  </ol> 
  <h3><a id="14_SVM_124"></a>14. SVM如何防止过拟合？</h3> 
  <blockquote> 
   <p><a href="https://www.jianshu.com/p/9b03cac58966" rel="nofollow">https://www.jianshu.com/p/9b03cac58966</a></p> 
  </blockquote> 
  <p>通过引入<strong>松弛变量</strong>，松弛变量可以容忍异常点的存在。</p> 
  <h3><a id="15_KKTKaryshKuhnTucker_129"></a>15. KKT（Karysh-Kuhn-Tucker）条件有哪些，完整描述？</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190322191559281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="_131"></a>为什么要从原始问题转换成对偶问题：</h3> 
  <ol> 
   <li>对偶问题将原始问题中的约束转为了对偶问题中的等式约束；</li> 
   <li>方便核函数的引入；</li> 
   <li>改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关；</li> 
   <li>求解更高效，因为只用求解alpha系数，而alpha系数只有支持向量才非0，其它全部为0。</li> 
  </ol> 
  <hr> 
  <h2><a id="_138"></a>集成学习（重点）</h2> 
  <h3><a id="1__139"></a>1. 决策树和随机森林的区别</h3> 
  <p>决策树 + Bagging + 随机选择特征 = 随机森林，随机森林可以有效防止过拟合。</p> 
  <h3><a id="2___141"></a>2. 随机森林里面用的哪种决策树</h3> 
  <p>CART 决策树或其他</p> 
  <h3><a id="3__143"></a>3. 随机森林的原理？如何进行调参？树的深度一般如何确定，一般为多少？</h3> 
  <ol> 
   <li><strong>原理</strong>：RF是一种集成算法，属于bagging类，它通过集成多个决策树模型，通过对基模型的结果采用投票法或者取平均来获得最终结果，使得最终模型有较高的准确度和泛化性能。</li> 
   <li><strong>调参</strong>：</li> 
  </ol> 
  <blockquote> 
   <p>还是看刘建平老师的这篇：<a href="https://www.cnblogs.com/pinard/p/6160412.html" rel="nofollow" data-token="32ee15b8a2d9fd86a1b240dca9c5099f">https://www.cnblogs.com/pinard/p/6160412.html</a></p> 
  </blockquote> 
  <p>RF和GBDT调参过程类似，可以对比记忆：<br> <strong>无参数拟合</strong>–&gt;<strong>n_estimators调参</strong>–&gt;<strong>max_depth, min_sample_split</strong>–&gt;<strong>min_sample_split, min_samples_leaf</strong>–&gt;<strong>max_features</strong></p> 
  <ol start="3"> 
   <li><strong>如何确定树的深度</strong>：当训练样本多，数据特征维度多的时候需要限制这个参数，具体取决于数据分布，一般在10-100之间。</li> 
  </ol> 
  <h3><a id="3_Bagging__Boosting_152"></a>3. Bagging 和 Boosting的区别</h3> 
  <ol> 
   <li><strong>样本选择</strong>：Bagging有放回的选取训练集，并且从原始数据集中选取的各轮训练集之间相互独立；Boosting每次都使用全部数据，只是每个样例的权重不同。</li> 
   <li><strong>样例权重</strong>：Bagging采用均匀采样，每个样例的权重相同；Boosting每轮训练都依据上一轮训练结果更新样例权重，错误率越大的样例，权重越大。</li> 
   <li><strong>预测函数</strong>：Bagging每个基函数的预测结果权重相同；Boosting中预测误差越小的基模型有更大的权重。</li> 
   <li><strong>偏差和方差</strong>：Bagging得出的结果低方差，Boosting低偏差。</li> 
   <li><strong>并行计算</strong>：Bagging可以并行生成基模型，Boosting各个预测函数只能顺序生成，后一轮模型的参数需要前一轮的预测结果。</li> 
  </ol> 
  <h3><a id="4_GBDT_158"></a>4. GBDT调参</h3> 
  <p>我觉得啊，一般面试官如果问我们这种题目，一定是要求我们使用过这个算法，如果使用过就要理解记住，没使用过就坦诚的说没用过，大家可以跟着下面这个链接的刘建平老师学习一遍。</p> 
  <blockquote> 
   <p>具体实例：<a href="https://www.cnblogs.com/pinard/p/6143927.html" rel="nofollow" data-token="37ae226d52eb49bba1ca350278b7e7c2">https://www.cnblogs.com/pinard/p/6143927.html</a></p> 
  </blockquote> 
  <ol> 
   <li>参数分类： 
    <ol> 
     <li><strong>Boosting框架参数</strong>：n_estimators, learning_rate, subsample</li> 
     <li><strong>CART回归树参数（与决策树类似）</strong>：max_features, max_depth, min_sample_split, min_samples_leaf</li> 
    </ol> </li> 
   <li>大致步骤：<br> <strong>无参数拟合</strong>–&gt;<strong>固定learning_rate，estimators调参</strong>–&gt;<strong>max_depth, min_sample_split</strong>–&gt;<strong>min_sample_split, min_samples_leaf</strong>–&gt;<strong>拟合查看</strong>–&gt;<strong>max_features–&gt;subsample</strong>–&gt;<strong>不断减小learning_rate，加倍estimators来拟合</strong></li> 
  </ol> 
  <h3><a id="5_RFGBDT_167"></a>5. RF、GBDT之间的区别（重要）</h3> 
  <blockquote> 
   <p>此问题充分理解，你需要这些：<br> <a href="https://blog.csdn.net/data_scientist/article/details/79022025" rel="nofollow">https://blog.csdn.net/data_scientist/article/details/79022025</a><br> <a href="https://blog.csdn.net/xwd18280820053/article/details/68927422" rel="nofollow" data-token="3d0c55c387bd5082884448b4eb203a52">https://blog.csdn.net/xwd18280820053/article/details/68927422</a><br> <a href="https://blog.csdn.net/m510756230/article/details/82051807" rel="nofollow" data-token="178dfc4af47d0c82591be1db068883cc">https://blog.csdn.net/m510756230/article/details/82051807</a></p> 
  </blockquote> 
  <ol> 
   <li>相同点：都是由多棵树组成，结果由多棵树共同决定。</li> 
   <li>不同点： 
    <ol> 
     <li>GBDT是回归树，RF可以是回归树也可以是分类树；</li> 
     <li>GBDT对异常值特别敏感，RF则没有；</li> 
     <li>GBDT只能串行生成多个模型，RF可以并行；</li> 
     <li>GBDT的结果有多个结果求和或者加权求和，RF是由投票选出结果；</li> 
     <li>GBDT是通过减少偏差来提高模型性能，RF是通过减少方差；</li> 
     <li>RF对所有训练集一视同仁，GBDT是基于权值的弱分类器。</li> 
    </ol> </li> 
  </ol> 
  <h3><a id="6__181"></a>6. 随机森林的优缺点</h3> 
  <ol> 
   <li>优点： 
    <ol> 
     <li>相比于其他算法，在训练速度和预测准确度上有很大的优势；</li> 
     <li>能够处理很高维的数据，不用选择特征，在模型训练完成后，能够给出特征的重要性；</li> 
     <li>可以写成并行化方法；</li> 
    </ol> </li> 
   <li>缺点：在噪声较大的分类回归问题上，容易过拟合。</li> 
  </ol> 
  <h3><a id="7_GBDTGBDT_187"></a>7. GBDT的关键？GBDT中的树是什么树？</h3> 
  <ol> 
   <li>关键：利用损失函数的负梯度方向作为残差的近似值来拟合新的CART回归树。</li> 
   <li>CART回归树。</li> 
  </ol> 
  <h3><a id="8_GBDTXGB_190"></a>8. GBDT和XGB的区别</h3> 
  <ol> 
   <li>GBDT以CART为基分类器，XGB则支持多种分类器；</li> 
   <li>GBDT只用到了一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶与二阶导数，并且可以自定义代价函数，只要一阶二阶可导；</li> 
   <li>XGBoost与GDBT都是逐次迭代来提高模型性能，但是XGBoost在选取最佳切分点时可以开启多线程进行，大大提高了运行速度；</li> 
   <li>新增了Shrinkage和column subsampling，为了防止过拟合；</li> 
   <li>对缺失值有自动分裂处理（默认归于左子树）；</li> 
  </ol> 
  <blockquote> 
   <p>如何处理的缺失值：<br> Xgboost 在处理带缺失值的特征时，先对非缺失的样本进行排序，对该特征缺失的样本先不处理，然后在遍历每个分裂点时，将这些缺失样本分别划入左子树和右子树来计算损失然后求最优。如果训练样本中没有缺失值，而预测过程中出现了缺失值，那么样本会被默认分到右子树。</p> 
  </blockquote> 
  <ol start="6"> 
   <li>xgb损失函数加入正则化，控制模型复杂度，使模型简单，防止过拟合。</li> 
  </ol> 
  <h3><a id="9_LGBXGB_199"></a>9. LGB和XGB的区别（抓住主要区别，理解要有深度）</h3> 
  <blockquote> 
   <p>lgb官方文档：<a href="http://lightgbm.apachecn.org/#/docs/4" rel="nofollow" data-token="8c76ca74db9ef493a31542cb1a2e8e69">http://lightgbm.apachecn.org/#/docs/4</a><br> <a href="https://www.cnblogs.com/infaraway/p/7890558.html" rel="nofollow" data-token="aa6e29dca465630520a8692a261379cd">https://www.cnblogs.com/infaraway/p/7890558.html</a></p> 
  </blockquote> 
  <ol> 
   <li> <p><strong>树的切分策略不同</strong>：</p> 
    <ol> 
     <li><strong>xgb基于level-wise</strong>，对每一层节点进行无差别分裂，造成不必要的开销；</li> 
     <li><strong>lgb基于leaf-wise</strong>，在当前所有叶子节点中选择分裂增益最大的节点进行分裂；</li> 
    </ol> </li> 
   <li> <p><strong>实现并行方式不同</strong>：</p> 
    <ol> 
     <li><strong>xgb使用基于 pre-sorted 决策树算法</strong>；</li> 
     <li><strong>lgb使用基于histogram决策树算法</strong>，对离散特征进行分裂时，特征的每个取值为一个桶；</li> 
    </ol> </li> 
   <li> <p><strong>lgb支持直接输入categorical feature</strong>，对类别特征无须进行one-hot处理；</p> </li> 
   <li> <p><strong>优化通信代价不同</strong>：lgb支持特征并行、数据并行。</p> </li> 
  </ol> 
  <h3><a id="10_GBDT_211"></a>10. GBDT的算法步骤（我没找到推导呢）</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190326165400913.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="10__213"></a>10. 随机森林生成过程</h3> 
  <blockquote> 
   <p><a href="https://www.cnblogs.com/liuwu265/p/4690715.html" rel="nofollow" data-token="1708dc81fd13464c3689045f5f876f16">https://www.cnblogs.com/liuwu265/p/4690715.html</a></p> 
  </blockquote> 
  <ol> 
   <li><strong>样本集选择</strong>：从原始样本中有放回的抽取N个训练集，各个训练集之间相互独立，共进行k轮抽取；</li> 
   <li><strong>决策树生成</strong>：假设特征空间共有D个特征，随机选择d个特征，构成新的特征空间，用于训练单棵决策树，共k轮，生成k棵决策树；</li> 
   <li><strong>模型合成</strong>：生成的k棵决策树相互独立，各个基模型之间权重相同，如果是分类问题，则使用投票法决定最终结果，如果是回归问题，则使用平均法；</li> 
   <li><strong>模型验证</strong>：模型验证本身需要验证集，但在此处我们无须额外设置验证集，只需使用原始样本中没有使用过的即可。</li> 
  </ol> 
  <h3><a id="11_xgboost_219"></a>11. xgboost如何确定特征和分裂点的？</h3> 
  <p>XGBoost使用了和CART回归树一样的想法，利用贪婪算法。基于目标函数，遍历所有特征的所有特征划分点，具体做法就是分裂后的目标函数值大于分裂之前的就进行分裂。</p> 
  <h3><a id="12_XGB_222"></a>12. XGB是如何给出特征重要性评分的？</h3> 
  <p>建议多看下以下链接：</p> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/waitingzby/article/details/81610495" rel="nofollow" data-token="995c35db4b0d8cc8106275e7398c5cd7">https://blog.csdn.net/waitingzby/article/details/81610495</a><br> <a href="https://blog.csdn.net/sujinhehehe/article/details/84201415" rel="nofollow" data-token="5bb611963388552e9dee1b05fa3e081a">https://blog.csdn.net/sujinhehehe/article/details/84201415</a><br> <a href="https://www.cnblogs.com/haobang008/p/5929378.html" rel="nofollow" data-token="59b94b6e6918ce54aabdae2184e6ed83">https://www.cnblogs.com/haobang008/p/5929378.html</a></p> 
  </blockquote> 
  <ol> 
   <li>特征权重（weight）：指的是在所有树中，某特征被用来分裂节点的次数；</li> 
   <li>如何计算：一个特征对分裂点性能度量（gini或者其他）的提升越大（越靠近根节点）其权重越大，该特征被越多提升树选择来进行分裂，该特征越重要，最终将一个特征在所有提升树中的结果进行加权求和然后求平均即可。</li> 
   <li>源码片段（加深理解）：<br> 主要是对每个特征进行<strong>计数</strong>操作：<pre><code class="prism language-python">		<span class="token keyword">if</span> importance_type <span class="token operator">==</span> <span class="token string">'weight'</span><span class="token punctuation">:</span>
           <span class="token comment"># do a simpler tree dump to save time</span>
           trees <span class="token operator">=</span> self<span class="token punctuation">.</span>get_dump<span class="token punctuation">(</span>fmap<span class="token punctuation">,</span> with_stats<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
 
           fmap <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
           <span class="token keyword">for</span> tree <span class="token keyword">in</span> trees<span class="token punctuation">:</span>
               <span class="token keyword">for</span> line <span class="token keyword">in</span> tree<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                   <span class="token comment"># look for the opening square bracket</span>
                   arr <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'['</span><span class="token punctuation">)</span>
                   <span class="token comment"># if no opening bracket (leaf node), ignore this line</span>
                   <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                       <span class="token keyword">continue</span>
 
                   <span class="token comment"># extract feature name from string between []</span>
                   fid <span class="token operator">=</span> arr<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">']'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'&lt;'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
 
                   <span class="token keyword">if</span> fid <span class="token operator">not</span> <span class="token keyword">in</span> fmap<span class="token punctuation">:</span>
                       <span class="token comment"># if the feature hasn't been seen yet</span>
                       fmap<span class="token punctuation">[</span>fid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
                   <span class="token keyword">else</span><span class="token punctuation">:</span>
                       fmap<span class="token punctuation">[</span>fid<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
 
           <span class="token keyword">return</span> fmap
</code></pre> </li> 
  </ol> 
  <h3><a id="13_XGB_257"></a>13. XGB如何消除残差的，目标函数是什么？</h3> 
  <blockquote> 
   <p><a href="https://www.cnblogs.com/palantir/p/10671119.html" rel="nofollow">https://www.cnblogs.com/palantir/p/10671119.html</a><br> <a href="https://blog.csdn.net/guoxinian/article/details/79243307" rel="nofollow" data-token="ee0224284b4129390e2ccf45f8a3f88f">https://blog.csdn.net/guoxinian/article/details/79243307</a></p> 
  </blockquote> 
  <h3><a id="14_GDBT_260"></a>14. GDBT在处理分类和回归问题时有什么区别？（感觉还有补全的地方）</h3> 
  <p>损失函数不同：</p> 
  <ol> 
   <li>分类：指数、对数；</li> 
   <li>回归：均方差、绝对值。</li> 
  </ol> 
  <h3><a id="XGB_264"></a>XGB如何防止过拟合</h3> 
  <p><strong>Shrinkage and Column Subsampling。</strong></p> 
  <ol> 
   <li>Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型；</li> 
   <li>Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种： 
    <ol> 
     <li>按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点；</li> 
     <li>随机选择特征，则建树前随机选择一部分特征然后分裂就只遍历这些特征。一般情况下前者效果更好。</li> 
    </ol> </li> 
  </ol> 
  <hr> 
  <h2><a id="EM_272"></a>EM算法</h2> 
  <h3><a id="1_EMmark_273"></a>1. 采用EM算法求解的模型有哪些？为什么不用牛顿法或者梯度下降法？（感觉第二个问题有点错误，mark）</h3> 
  <ol> 
   <li>高斯混合模型、协同过滤、KMeans</li> 
   <li>求和的项随着隐变量的数量随指数上升，梯度计算带来麻烦，而EM是非梯度优化算法。</li> 
  </ol> 
  <h3><a id="2__EMKMeans_276"></a>2. 用EM算法推导解释KMeans</h3> 
  <p>KMeans中，每个聚类簇的中心就是隐含变量。<br> E步：随机初始化k个聚类中心<br> M步：计算每个样本点最近的质心，并将他聚类到这个质心<br> 重复以上两步，直到聚类中心不发生变化为止。</p> 
  <hr> 
  <h2><a id="_283"></a>决策树</h2> 
  <h3><a id="1_ID3C45_284"></a>1. ID3和C4.5的优缺点</h3> 
  <table> 
   <thead> 
    <tr> 
     <th></th> 
     <th>ID3</th> 
     <th>C4.5</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>优点</td> 
     <td>实现简单</td> 
     <td>1. 可以处理连续型特征 2. 易于理解，准确率高</td> 
    </tr> 
    <tr> 
     <td>缺点</td> 
     <td>1. 只能处理离散特征 2. 倾向于选择取值较多的特征</td> 
     <td>1. 在构造树的过程中，对数据多次扫描排序，低效 2. 只能够用于驻留在内存中的数据，当数据大到无法在内存容纳时，程序无法执行</td> 
    </tr> 
   </tbody> 
  </table>
  <h3><a id="2__289"></a>2. 决策树处理连续值的方法</h3> 
  <p>连续属性离散化，常用的离散化策略是<strong>二分法</strong>（C4.5）：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190313144329390.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="3__292"></a>3. 决策树的剪枝策略</h3> 
  <ol> 
   <li><strong>目的</strong>：简化决策树模型，提高模型泛化能力；</li> 
   <li><strong>基本思想</strong>：减去某些子树和叶节点，将其根节点作为新的叶子节点，实现简化模型。</li> 
   <li><strong>损失函数</strong>：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190319165909146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
   <li><strong>剪枝策略</strong>： 
    <ol> 
     <li><strong>预剪枝</strong>：在构造决策树的过程中，在对节点划分之前进行估计，若划分后不能带来决策树性能和泛化能力的提升，则不进行划分，并将此节点作为叶节点。</li> 
     <li><strong>后剪枝</strong>：构造完决策树之后，自底向上搜索，对每个非叶节点进行考察，若将该子树去除变为叶节点能带来决策树泛化性能的提升，则将该节点作为叶节点。</li> 
     <li><strong>对比</strong>：后剪枝的分支比预剪枝的分支要多一些，不容易欠拟合，泛化能力强，但是由于后剪枝在构造完成决策树之后，而且还需要自底向上进行搜索故时间开销大。</li> 
    </ol> </li> 
  </ol> 
  <h3><a id="4__301"></a>4. 决策树的构造过程</h3> 
  <blockquote> 
   <p><a href="https://shuwoom.com/?p=1452" rel="nofollow" data-token="f0ec51142df0d41a33184abf95468672">https://shuwoom.com/?p=1452</a></p> 
  </blockquote> 
  <ol> 
   <li><strong>特征选择</strong>：在所有特征中选择一个特征，作为当前节点的划分标准：ID3(信息增益)、C4.5(信息增益比)、CART(gini系数)；</li> 
   <li><strong>决策树生成</strong>：依据特征评估标准，从上到下的递归的生成子节点，直到数据集不可分时，停止生长；</li> 
   <li><strong>剪枝</strong>：决策时容易过拟合，通过剪枝，简化模型，降低过拟合。</li> 
  </ol> 
  <h3><a id="5__306"></a>5. 基于树的模型有必要做标准化吗？</h3> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/answer3lin/article/details/84961694" rel="nofollow">https://blog.csdn.net/answer3lin/article/details/84961694</a></p> 
  </blockquote> 
  <p>不必要；概率模型（树模型）只关心变量的分布和变量之间的条件概率。</p> 
  <h3><a id="6_CARTCART_310"></a>6. CART回归树和CART决策树的构造过程</h3> 
  <p><strong>特征选择+决策树构造+决策树剪枝</strong></p> 
  <ol> 
   <li>cart回归树<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190419230116477.png" alt="在这里插入图片描述"><br> cart回归树对某个特征的每个分量，将数据集划分为大于该分量和小于该分量两部分，同时计算对应y的均值，并计算均方误差函数的值，选择具有最小误差值的分量，并将其作为划分标准，重复该流程知道数据集为空或者前后均方误差下降值小于给定阈值则停止。</li> 
   <li>cart分类树<br> 选择具有<strong>最小的GINI系数</strong>的属性和属性值，作为最优分裂属性和最优分裂属性值。</li> 
  </ol> 
  <h3><a id="7__317"></a>7. 熵</h3> 
  <ol> 
   <li>概念：信息所包含不确定大小的度量，一个信息的所包含的不确定性越大，其所含的信息越多。</li> 
   <li>熵的计算公式：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       <math>
        <semantics>
         <mrow>
          <mi>
           H
          </mi>
          <mo>
           (
          </mo>
          <mi>
           X
          </mi>
          <mo>
           )
          </mo>
          <mo>
           =
          </mo>
          <mo>
           −
          </mo>
          <msubsup>
           <mo>
            ∑
           </mo>
           <mrow>
            <mi>
             i
            </mi>
            <mo>
             =
            </mo>
            <mn>
             1
            </mn>
           </mrow>
           <mi>
            N
           </mi>
          </msubsup>
          <msub>
           <mi>
            p
           </mi>
           <mi>
            i
           </mi>
          </msub>
          <mi>
           l
          </mi>
          <mi>
           o
          </mi>
          <msub>
           <mi>
            g
           </mi>
           <mn>
            2
           </mn>
          </msub>
          <msub>
           <mi>
            p
           </mi>
           <mi>
            i
           </mi>
          </msub>
         </mrow>
         <annotation encoding="application/x-tex">
          H(X) = -\sum_ {i=1}^{N}p_i log_2 p_i
         </annotation>
        </semantics>
       </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.08125em;">H</span><span class="mopen">(</span><span class="mord mathit" style="margin-right: 0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.28094em; vertical-align: -0.29971em;"></span><span class="mord">−</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.981231em;"><span class="" style="top: -2.40029em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.29971em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="mord mathit">o</span><span class="mord"><span class="mord mathit" style="margin-right: 0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathit">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></li> 
  </ol> 
  <hr> 
  <h2><a id="_322"></a>朴素贝叶斯</h2> 
  <h3><a id="1__323"></a>1. 朴素贝叶斯的公式</h3> 
  <p><strong>“朴素”的含义</strong>：假设各个特征之间相互独立。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190319161813931.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="2_NB_326"></a>2. NB原理及其分类</h3> 
  <ol> 
   <li><strong>原理</strong>：根据贝叶斯公式，通过某对象的先验概率，计算其后验概率，并选择后验概率最大的类作为该对象所属于的类；</li> 
   <li><strong>分类</strong>：（根据变量的分布不同） 
    <ol> 
     <li>NB的<strong>伯努利模型</strong>，特征是布尔变量，符合0/1分布，在文本分类中，特征就是<strong>词是否出现</strong>；</li> 
     <li>NB的<strong>多项式模型</strong>，特征是离散值，符合多项式分布，在文本分类中，特征就是<strong>词出现的次数</strong>；</li> 
     <li>NB的<strong>高斯模型</strong>，特征是连续值，符合高斯分布（高斯分布又名正态分布），在文本分类中，特征就是<strong>词的TF-IDF值</strong>。</li> 
    </ol> </li> 
  </ol> 
  <h3><a id="3_NB_333"></a>3. NB的优缺点</h3> 
  <ol> 
   <li>优点： 
    <ol> 
     <li>算法原理简单；</li> 
     <li>所估计的参数少；</li> 
     <li>假设条件概率计算是彼此独立的，因此可以用于分布式计算；</li> 
     <li>属于生成式模型，收敛速度比判别式模型要快；</li> 
     <li>对缺失数据不太敏感；</li> 
     <li>天然可以处理多分类问题。</li> 
    </ol> </li> 
   <li>缺点： 
    <ol> 
     <li>假设各个特征之间相互独立这一条件在实际应用中往往是不能成立的；</li> 
     <li>不能学习到特征之间的相互作用；</li> 
     <li>对输入数据的表达形式敏感。</li> 
    </ol> </li> 
  </ol> 
  <h3><a id="4_LRNB_345"></a>4. LR和朴素贝叶斯(NB)之间的区别</h3> 
  <blockquote> 
   <p><a href="https://www.cnblogs.com/wangkundentisy/p/9193217.html" rel="nofollow">https://www.cnblogs.com/wangkundentisy/p/9193217.html</a><br> 暂时不看。</p> 
  </blockquote> 
  <h3><a id="5__349"></a>5. 适用场景</h3> 
  <ol> 
   <li>支持大规模数据，并且支持分布式实现；</li> 
   <li>特征维度可以很高；</li> 
   <li>可以处理数值型特征和类别型特征；</li> 
  </ol> 
  <hr> 
  <h2><a id="_355"></a>线性回归与逻辑回归</h2> 
  <h3><a id="1_LR_356"></a>1. LR推导（重要）</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190322183224161.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 几个小问题：</p> 
  <ol> 
   <li><strong>极大似然的概念</strong>：找到参数θ的一个估计值，使得当前样本出现的可能性最大。</li> 
   <li><strong>为什么极大似然的时候可以相乘</strong>：特征之间是独立同分布。</li> 
   <li><strong>LR的参数计算方法</strong>：梯度下降、牛顿法。</li> 
  </ol> 
  <h3><a id="2___362"></a>2. 逻辑回归和线性回归的区别</h3> 
  <table> 
   <thead> 
    <tr> 
     <th>线性回归</th> 
     <th>逻辑回归</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>对连续值预测</td> 
     <td>分类</td> 
    </tr> 
    <tr> 
     <td>最小二乘法</td> 
     <td>最大似然估计</td> 
    </tr> 
    <tr> 
     <td>拟合函数</td> 
     <td>预测函数</td> 
    </tr> 
   </tbody> 
  </table>
  <h3><a id="3_mark_368"></a>3. 最小二乘法和最大似然法的区别（没太明白，mark）</h3> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/lu597203933/article/details/45032607" rel="nofollow">https://blog.csdn.net/lu597203933/article/details/45032607</a></p> 
  </blockquote> 
  <h3><a id="4_mark_370"></a>4. 为什么用最小二乘而不是最小四乘（没太明白，mark）</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190320112436782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="5__372"></a>5. 介绍一下逻辑回归？它的损失函数是什么？</h3> 
  <ol> 
   <li>一句话介绍逻辑回归：逻辑回归假设数据服从伯努利分布，通过极大化似然函数，利用梯度下降算法来求得参数，实现数据的二分类；</li> 
   <li>它的损失函数是它的极大似然函数（对数损失函数）：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190322141142700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
  </ol> 
  <h3><a id="6_LR__376"></a>6. LR 损失函数为什么用极大似然函数？</h3> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/aliceyangxi1987/article/details/80532586" rel="nofollow">https://blog.csdn.net/aliceyangxi1987/article/details/80532586</a></p> 
  </blockquote> 
  <ol> 
   <li>LR的目标是使每个样本的预测都有最大概率，即将所有样本预测后的概率相乘概率最大，这就是极大似然函数；</li> 
   <li>极大似然函数取对数即为对数损失函数，对数损失函数的训练求解参数比较快，更新速度也稳定；</li> 
   <li>为什么不用平方损失函数呢？因为平方损失函数的梯度更新速度和sigmoid的梯度有关，而在定义域范围内sigmoid的梯度值&lt;=2.5，训练速度非常慢；而且平方损失函数会导致损失函数是非凸的，不易求解。</li> 
  </ol> 
  <h3><a id="7_LRGD_381"></a>7. LR的参数计算方法：梯度下降法，请介绍三种GD的区别</h3> 
  <ol> 
   <li>批梯度下降：可以得到全局最优解，缺点是更新每个参数都需要遍历所有数据，计算量大，还有很多冗余计算，在数据非常大的时候，每个参数的更新都是非常慢的；</li> 
   <li>SGD：以高方差频繁更新，使SGD跳到新的或潜在更优的局部解，但是也使得收敛到局部最优解的过程更加复杂；</li> 
   <li>mini-batch SGD：结合了二者的优点，每次选取N个样本，减少了参数更新的次数，可以达到更加稳定的收敛结果。</li> 
  </ol> 
  <h3><a id="8_LR_385"></a>8. LR的优缺点</h3> 
  <blockquote> 
   <p><a href="http://www.cnblogs.com/ModifyRong/p/7739955.html" rel="nofollow" data-token="7c199cdc664942a663856cff47dc9460">http://www.cnblogs.com/ModifyRong/p/7739955.html</a></p> 
  </blockquote> 
  <ol> 
   <li> <p>优点：</p> 
    <ol> 
     <li>形式简单，模型可解释性非常好，特征权重可以看出不同特征最后结果的影响；</li> 
     <li>效果不错，可以作为baseline；</li> 
     <li>占用资源少，特别是内存；</li> 
     <li>方便输出结果调整；</li> 
     <li>训练速度快。</li> 
    </ol> </li> 
   <li> <p>缺点：</p> 
    <ol> 
     <li>准确率不高；</li> 
     <li>对样本不均衡问题无法很好的解决；</li> 
     <li>对非线性分类问题也是；</li> 
     <li>本身无法筛选特征，可以和GBDT结合使用。</li> 
    </ol> </li> 
  </ol> 
  <h3><a id="9__400"></a>9. 逻辑斯特回归为什么要对特征进行离散化</h3> 
  <ol> 
   <li>稀疏向量做内积乘法运算速度快，计算结果方便存储，易于扩展</li> 
   <li>离散化后的特征对异常数据有更强的鲁棒性</li> 
   <li>特征离散化后模型更稳定</li> 
   <li>LR属于广义线性模型，表达能力有限，特征离散化为N个后，每个变量有自己的权重，相当于引入非线性，表达能力增强，加大拟合</li> 
   <li>特征离散化后还可以做特征交叉，由M+N个变为M*N个，进一步引入非线性</li> 
  </ol> 
  <h3><a id="10_LRsigmoid_406"></a>10. LR为什么用sigmoid函数,这个函数有什么优点和缺点?</h3> 
  <ol> 
   <li>为什么： 
    <ol> 
     <li>Sigmoid 函数自身的性质 
      <ol> 
       <li>sigmoid 函数连续，单调递增</li> 
       <li>sigmiod 函数关于（0，0.5） 中心对称</li> 
       <li>计算sigmoid函数的导数非常的快速</li> 
       <li>将输入变量的范围从负无穷到正无穷，映射到（0，1），而概率要求正是（0，1）。</li> 
      </ol> </li> 
     <li>逻辑回归认为函数其概率服从伯努利分布，将其写成指数族分布的形式，能够推导出sigmoid函数的形式。</li> 
    </ol> </li> 
  </ol> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/a1628864705/article/details/62233395" rel="nofollow" data-token="5c6edd753b2ea70cb91a5a075d313beb">https://blog.csdn.net/a1628864705/article/details/62233395</a><br> <a href="https://blog.csdn.net/qq_19645269/article/details/79551576" rel="nofollow" data-token="8bece83d3cfdd79c50ee497b70c7f0b8">https://blog.csdn.net/qq_19645269/article/details/79551576</a></p> 
  </blockquote> 
  <ol start="2"> 
   <li>优缺点： 
    <ol> 
     <li>优点： 
      <ol> 
       <li>可以看到sigmoid函数处处连续 -&gt;便于求导；</li> 
       <li>可以将函数值的范围压缩到[0,1]-&gt;可以压缩数据，且幅度不变；</li> 
       <li>便于前向传输。</li> 
      </ol> </li> 
     <li>缺点： 
      <ol> 
       <li>在趋向无穷的地方，函数值变化很小，容易缺失梯度，不利于深层神经网络的反馈传输；</li> 
       <li>幂函数还是比较难算的；</li> 
       <li>函数均值不为0，当输出大于0时，则梯度方向将大于0，也就是说接下来的反向运算中将会持续正向更新；同理，当输出小于0时，接下来的方向运算将持续负向更新。</li> 
      </ol> </li> 
    </ol> </li> 
  </ol> 
  <h3><a id="11_LR_427"></a>11. LR伪代码</h3> 
  <pre><code class="prism language-python">读取文件、写入文件以及计算准确率等之前实验做过的或者过于简单的函数功能不列出。
<span class="token builtin">int</span> main<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{</span>
	读取训练集和测试集，其中训练集每三个样本取前两个作为训练集，第三个作为验证集。
	初始化w：<span class="token keyword">for</span><span class="token punctuation">(</span><span class="token builtin">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>Length<span class="token punctuation">;</span>i<span class="token operator">+</span><span class="token operator">+</span><span class="token punctuation">)</span> w<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>
	<span class="token keyword">for</span><span class="token punctuation">(</span><span class="token builtin">int</span> k<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">)</span>
		<span class="token keyword">for</span><span class="token punctuation">(</span><span class="token builtin">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">:</span>traincnt<span class="token punctuation">)</span><span class="token operator">//</span>遍历训练集样本
		<span class="token punctuation">{</span>
			CalWeight<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">//</span>计算样本i的权重分数
			CalCost<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">//</span>每一维的梯度（代价）计算
			Updatew<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>	<span class="token operator">//</span>更新w
			<span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">%</span><span class="token number">20</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span>	<span class="token operator">//</span>每更新w20次计算一次准确率
			<span class="token punctuation">{</span>
				Predict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">//</span>预测验证集样本
				Cal_acc<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">//</span>计算准确率 
				ac<span class="token punctuation">[</span>cnt<span class="token punctuation">]</span><span class="token operator">=</span>accuracy<span class="token punctuation">;</span>				
				cnt<span class="token operator">+</span><span class="token operator">+</span><span class="token punctuation">;</span>
			<span class="token punctuation">}</span>										
		<span class="token punctuation">}</span>
	output_result<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">//</span>输出验证集准确率以供调试
	output_test_result<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">//</span>输出测试集预测结果
<span class="token punctuation">}</span>
void CalWeight<span class="token punctuation">(</span><span class="token builtin">int</span> index<span class="token punctuation">)</span><span class="token punctuation">{</span>
	weight <span class="token operator">=</span> 当前向量w的转置<span class="token operator">*</span>样本i向量；
<span class="token punctuation">}</span>
void CalCost<span class="token punctuation">(</span><span class="token builtin">int</span> index<span class="token punctuation">)</span><span class="token punctuation">{</span>
	计算每一维的梯度，存储在向量数组Cost<span class="token punctuation">[</span><span class="token punctuation">]</span>中；
<span class="token punctuation">}</span>
void Updatew<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
	使用w <span class="token operator">=</span> w <span class="token operator">-</span> alpha x gradient来更新回归系数（w）
<span class="token punctuation">}</span>
void Predict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
	P <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>exp<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token operator">*</span> w<span class="token operator">^</span>T <span class="token operator">*</span>样本i向量<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">if</span><span class="token punctuation">(</span>P<span class="token operator">&gt;</span><span class="token number">0.5</span><span class="token punctuation">)</span> p_label<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>
	<span class="token keyword">else</span> p_label<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
  <hr> 
  <h2><a id="PCA_468"></a>PCA</h2> 
  <h3><a id="1_PCA_469"></a>1. PCA原理</h3> 
  <blockquote> 
   <p>具体推导看这里： <a href="https://blog.csdn.net/u012421852/article/details/80458340" rel="nofollow">https://blog.csdn.net/u012421852/article/details/80458340</a></p> 
  </blockquote> 
  <ol> 
   <li><strong>用于</strong>：特征降维，去除冗余和可分性不强的特征；</li> 
   <li><strong>目标</strong>：降维后的各个特征不相关，即特征之间的协方差为0；</li> 
   <li><strong>原理</strong>：基于训练数据X的协方差矩阵的特征向量组成的k阶矩阵U，通过XU得到降维后的k阶矩阵Z；</li> 
   <li><strong>算法步骤</strong> 
    <ol> 
     <li>计算训练样本的协方差矩阵C；</li> 
     <li>计算C的特征值和特征向量；</li> 
     <li>将C的特征值降序排列，特征值对应特征向量也依次排列；</li> 
     <li>假如要得到X的k阶降维矩阵，选取C的前k个特征{u1,u2…uk}，组成降维转换矩阵U；</li> 
     <li>Z = XU，Z即为降维后的矩阵；<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190320094709260.png" alt="在这里插入图片描述"></li> 
    </ol> </li> 
  </ol> 
  <hr> 
  <h2><a id="KMeans_482"></a>KMeans</h2> 
  <h3><a id="1_Kmeans__483"></a>1. K-means 的原理，时间复杂度，优缺点以及改进</h3> 
  <ol> 
   <li><strong>原理</strong>：对于给定样本集，按照样本之间的距离大小，将样本划分为若干个簇，使簇内距离尽可能小，簇间距离尽可能大；</li> 
   <li><strong>时间复杂度</strong>：O(k<em>n</em>d*t) | k:类别，n：样本数，d：计算样本之间距离的时间复杂度，t：迭代次数；</li> 
   <li><strong>优缺点</strong>： 
    <ol> 
     <li>优点：1. 原理易懂、实现简单、收敛速度快、效果好 2. 可解释性强 3. 可调参数只有少，只有k；</li> 
     <li>缺点：1. 聚类效果受k值影响大 2. 非凸数据集难以收敛 3. 隐含类别不均衡时，效果差 4. 迭代算法，得到的只是局部最优 5. 对噪音和异常数据敏感。</li> 
    </ol> </li> 
   <li><strong>改进</strong>：<strong>随机初始化K值影响效果</strong> + <strong>计算样本点到质心的距离耗时</strong>这两方面优化<br> <strong>KMeans++算法</strong><br> KMeans随机选取k个点作为聚类中心，而KMeans++采用如下方法：<br> 假设已经选取好n个聚类中心后，再选取第n+1个聚类中心时，距离这n个聚类中心越远的点有越大的概率被选中；选取第一个聚类中心（n=1）时也是需要像KMeans一样随机选取的。</li> 
  </ol> 
  <hr> 
  <h2><a id="_494"></a>其他</h2> 
  <h3><a id="1__495"></a>1. 机器学习性能评价指标</h3> 
  <ol> 
   <li><strong>Precision （精确率）和Recall（召回率）</strong><br> P = TP / (TP + FP)<br> R = TP / (TP + FN)</li> 
   <li><strong>F1值</strong><br> F1 = 2PR / (P + R)</li> 
   <li><strong>ROC 和 AUC</strong><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190313163456961.png" alt="在这里插入图片描述"></li> 
  </ol> 
  <h3><a id="2_Occams_Razor_504"></a>2. 奥卡姆剃刀（Occam’s Razor）</h3> 
  <p>如无需要，勿增实体。<br> 简单有效原理。<br> 具体到机器学习上，能够拟合数据的简单模型才是我们需要的。</p> 
  <h3><a id="3_L1L2_508"></a>3. L1范数与L2的作用，区别</h3> 
  <p>在机器学习中，通常损失函数会加上一个额外项，可看作损失函数的惩罚项，是为了限制模型参数，防止过拟合。<br> <strong>（自己注意下图！）</strong><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190313162413835.gif" alt="在这里插入图片描述"></p> 
  <table> 
   <thead> 
    <tr> 
     <th>L1范数</th> 
     <th>L2范数</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>各个参数的绝对值之和</td> 
     <td>各个参数的平方和的开方</td> 
    </tr> 
    <tr> 
     <td>先验分布是拉氏分布</td> 
     <td>高斯（正态）分布</td> 
    </tr> 
    <tr> 
     <td>使参数稀疏化，有特征选择的功能</td> 
     <td>使参数接近于0，防止过拟合 （模型越简单，越不容易过拟合）</td> 
    </tr> 
    <tr> 
     <td>Lasso回归</td> 
     <td>Ridge回归</td> 
    </tr> 
   </tbody> 
  </table>
  <h3><a id="4_L10_518"></a>4. L1正则为什么可以把系数压缩成0？</h3> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/jinping_shi/article/details/52433975" rel="nofollow">https://blog.csdn.net/jinping_shi/article/details/52433975</a></p> 
  </blockquote> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190813103726620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="5_lambda_522"></a>5. 正则化为什么能防止过拟合？添加正则项后依旧过拟合如何调节参数lambda？</h3> 
  <h3><a id="4__524"></a>4. 过拟合的原因和防止过拟合的方法</h3> 
  <ol> 
   <li><strong>原因</strong>：1. 数据有噪声； 2. 训练数据不足，有限的训练数据； 3. 过度训练导致模型复杂。</li> 
   <li><strong>防止过拟合的方法</strong>： 
    <ul> 
     <li>早停止：在模型对训练数据迭代收敛之前停止迭代。 
      <blockquote> 
       <p>具体做法：在每一个Epoch结束时,计算validation_data的accuracy，当accuracy不再提高时，就停止训练。（注意多次观察，多次精度未提升时则停止迭代）</p> 
      </blockquote> </li> 
     <li>dropout：在训练时，以一定的概率忽略隐层中的某些节点。 
      <blockquote> 
       <p>插播：为什么dropout能有效防止过拟合，请解释原因？<br> 详细解答看这个：<a href="https://www.cnblogs.com/wuxiangli/p/7309501.html" rel="nofollow" data-token="7f130e513f926bfab262c2fe6e49b278">https://www.cnblogs.com/wuxiangli/p/7309501.html</a></p> 
       <ol> 
        <li>取平均的作用；</li> 
        <li>减少神经元之间复杂的共适应关系：因为dropout程序导致两个神经元，不一定每次都在一个网络中出现，避免了有些特征只有在特定特征条件下才有效的情况，迫使网络去学习更加鲁棒的特征。</li> 
       </ol> 
      </blockquote> </li> 
     <li>正则化</li> 
     <li>数据集扩充：1. 从源头上获取更多数据；2. 数据增强（通过一定规则扩充数据）；3. 根据当前数据估计分布参数，利用该分布获得更多数据。</li> 
     <li>集成学习</li> 
    </ul> </li> 
  </ol> 
  <h3><a id="5___537"></a>5. 特征选择的方法</h3> 
  <blockquote> 
   <p>什么样的特征是好特征：特征覆盖率高，特征之间相关性小，不能改变原始特征分布</p> 
  </blockquote> 
  <ol> 
   <li>wrapper（根据目标函数，每次选择若干特征，活着排除若干特征）<br> <strong>递归特征消除法</strong>：使用基模型进行多轮训练，每轮训练结束后，消除若干权值系数的特征，再使用新的特征进行下一轮训练。</li> 
   <li>embedded（先使用某些机器学习算法训练模型，得到各个特征的权值系数，再有大到小进行特征选择） 
    <ul> 
     <li><strong>基于惩罚项的特征选择法</strong></li> 
     <li><strong>基于树的特征选择法</strong></li> 
    </ul> </li> 
   <li>filter（根据发散性活着相关性对各个特征进行评分，设定阈值或者特征个数，选择特征） 
    <ul> 
     <li><strong>卡方检验</strong></li> 
     <li><strong>互信息</strong></li> 
     <li><strong>方差选择法</strong>：计算每个特征的方差，根据阈值，选择方差大于阈值的特征进行训练；</li> 
     <li><strong>相关系数</strong></li> 
    </ul> </li> 
  </ol> 
  <h3><a id="6__550"></a>6. 说一说你知道的损失函数</h3> 
  <ol> 
   <li>0-1损失函数（感知机）<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312165140885.png" alt="在这里插入图片描述"></li> 
   <li>平方损失函数（线性回归）<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312165214730.png" alt="在这里插入图片描述"></li> 
   <li>绝对值损失函数<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019031216515511.png" alt="在这里插入图片描述"></li> 
   <li>指数损失函数（adaBoost）<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312165229216.png" alt="在这里插入图片描述"></li> 
   <li>Hinge Loss（SVM）<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312165239682.png" alt="在这里插入图片描述"></li> 
   <li>对数损失函数（逻辑回归）<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312165253223.png" alt="在这里插入图片描述"></li> 
  </ol> 
  <h3><a id="7__563"></a>7. 数据预处理的方法</h3> 
  <ol> 
   <li>数据清洗：异常值和缺失值；</li> 
   <li>数据集成：实体识别，冗余属性识别；</li> 
   <li>数据转换：简单函数转换，连续特征离散化，规范化，构造属性</li> 
   <li>数据规约：数值规约，属性规约</li> 
  </ol> 
  <h3><a id="8__568"></a>8. 偏差和方差是什么，高偏差和高方差说明了什么</h3> 
  <ol> 
   <li><strong>偏差</strong>：是指预测值和真实值之间的差，偏差越大，预测和真实值之间的差别越大，衡量模型的预测能力。</li> 
   <li><strong>方差</strong>：描述预测值的变化范围和离散程度，方差越大，表示预测值的分布越零散，对象是多个模型，使用不同的训练数据训练出的模型差别有多大。</li> 
   <li>当训练误差和交叉验证误差或测试误差都很大，且值差不多时，处于<strong>高偏差，低方差，欠拟合</strong>状态；当训练误差和交叉验证误差差别很大，且测试集误差小，验证集误差大时，处于<strong>高方差，低偏差，过拟合</strong>状态。</li> 
  </ol> 
  <h3><a id="9__572"></a>9. 优化算法有那些？</h3> 
  <blockquote> 
   <p>具体算法原理、优缺点看这个：<a href="https://www.cnblogs.com/xinbaby829/p/7289431.html" rel="nofollow">https://www.cnblogs.com/xinbaby829/p/7289431.html</a></p> 
  </blockquote> 
  <p>梯度下降法、牛顿法和拟牛顿法、共轭梯度法、<br> 启发式方法、解决约束优化问题的拉格朗日乘数法</p> 
  <h3><a id="10__578"></a>10. 梯度下降算法和牛顿法的区别</h3> 
  <blockquote> 
   <p><a href="https://www.cnblogs.com/lyr2015/p/9010532.html" rel="nofollow">https://www.cnblogs.com/lyr2015/p/9010532.html</a></p> 
  </blockquote> 
  <ol> 
   <li>牛顿法： 
    <ol> 
     <li>通过求解目标函数一阶导数为0的参数来求解目标函数最小值时的参数；</li> 
     <li>收敛速度快；</li> 
     <li>迭代过程中，海森矩阵的逆不断减小，相当于逐步减小步长；</li> 
     <li>海森矩阵的逆，计算复杂，代价较高，因此有了拟牛顿法。</li> 
    </ol> </li> 
   <li>梯度下降算法： 
    <ol> 
     <li>通过梯度方向和步长，直接求得目标函数最小值的参数；</li> 
     <li>越靠近最优值，步长应该逐渐减小，否则会在最优值附近震荡。</li> 
    </ol> </li> 
  </ol> 
  <h3><a id="11__588"></a>11. 如何解决类别不均衡问题？</h3> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/program_developer/article/details/80287033" rel="nofollow">https://blog.csdn.net/program_developer/article/details/80287033</a><br> <a href="https://www.cnblogs.com/zhaokui/p/5101301.html" rel="nofollow" data-token="b5cc65cf3d4775ef627f644120ab57e3">https://www.cnblogs.com/zhaokui/p/5101301.html</a></p> 
  </blockquote> 
  <ol> 
   <li><strong>采样</strong>：其中采样又分为上采样（将数量少的类别的数据复制多次），和下采样（将数量多的类别的数据剔除一部分）。</li> 
  </ol> 
  <blockquote> 
   <p><a href="https://www.jianshu.com/p/9a68934d1f56" rel="nofollow" data-token="60e3f372a1647dccfa98f4e574518172">https://www.jianshu.com/p/9a68934d1f56</a><br> smote算法：</p> 
   <ol> 
    <li><strong>用途</strong>：合成新的少数样本；</li> 
    <li><strong>基本思路</strong>：对每一个少数样本a，从a的k个最近邻中随机挑选一个样本b，从a、b连线上随机选择一个点，作为新合成的少数样本。</li> 
    <li><strong>步骤</strong>：<br> 1）对每一个少数样本a，基于欧式距离，计算它到其他少数样本的距离，找到他的k个最近邻；<br> 2）根据样本不平衡比例设置一个采样比例进而得到采样倍率N，对于每一个少数样本a，从他的最近邻中选择若干样本，假设选择的样本为b；<br> 3）基于以下公式得到新合成的少数样本 c = a * random(0,1)*|b-a|。</li> 
   </ol> 
  </blockquote> 
  <ol start="2"> 
   <li><strong>数据合成</strong>：利用现有的数据的规律生成新的数据。</li> 
   <li><strong>一分类</strong>：当数据样本极不平衡时，将它看作一分类，这样我们的重点就在于将它看成对某种类别进行建模。</li> 
   <li>对不同的类别给予不同的分错代价。</li> 
  </ol> 
  <h3><a id="12__603"></a>12. 梯度下降算法的过程</h3> 
  <p>首先我们有一个可微分的函数，这个函数就好像一个山，我们的目标是找到函数的最小值（即山底）。根据经验可知，我们从最陡峭的地方走，可以尽快到达山底。对应到函数就是找到给定点的梯度，并且沿着梯度相反的方向就能让函数值下降最快（因为梯度方向就是函数值变化最快的方向）。重复利用这个方法，反复求取梯度，最后就能到达局部最小值。</p> 
  <h3><a id="13__606"></a>13. 为什么我们还是会在训练的过程当中将高度相关的特征去掉？</h3> 
  <ol> 
   <li>使得模型的解释性更强；</li> 
   <li>大大提高训练速度。</li> 
  </ol> 
  <h3><a id="14__610"></a>14. 最大似然估计与贝叶斯估计的区别</h3> 
  <h3><a id="15__611"></a>15. 判别式和生成式的算法各有哪些，区别是什么？</h3> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/amblue/article/details/17023485" rel="nofollow" data-token="5aea07e97afa90dd120a27b1e32cf0b3">https://blog.csdn.net/amblue/article/details/17023485</a><br> <a href="https://blog.csdn.net/qq_41853758/article/details/80864072" rel="nofollow" data-token="7e63bde7b11ff0941a91f3c930ab90ae">https://blog.csdn.net/qq_41853758/article/details/80864072</a></p> 
  </blockquote> 
  <ol> 
   <li>区别：二者最本质的区别是<strong>建模对象的不同</strong>。<br> 判别式模型的评估对象是最大化条件概率<code>P(Y|X)</code>并对此进行建模；<br> 生成式模型的评估对象是最大化联合概率<code>P(X,Y)</code>并对此进行建模。</li> 
   <li>判别式模型：线性回归、决策树、支持向量机SVM、k近邻、神经网络等；</li> 
   <li>生成式模型：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA。</li> 
  </ol> 
  <h3><a id="16__619"></a>16. 最大似然估计</h3> 
  <blockquote> 
   <p><a href="https://www.cnblogs.com/zyxu1990/p/3209407.html" rel="nofollow">https://www.cnblogs.com/zyxu1990/p/3209407.html</a></p> 
  </blockquote> 
  <ol> 
   <li><strong>条件</strong>：假设样本独立同分布；</li> 
   <li><strong>目标</strong>：估计出这个分布中的参数theta；</li> 
   <li><strong>方法</strong>：这一组样本的概率最大时就对应了该模型的参数值。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190402184815216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
  </ol> 
  <h3><a id="17_AUCAUC_626"></a>17. 请用一句话说明AUC的本质和计算规则？AUC高可以理解为精确率高吗？</h3> 
  <ol> 
   <li><strong>本质</strong>：一个正例，一个负例，预测为正例的概率值大于预测为负的概率值的可能性；<br> <strong>计算规则</strong>：ROC曲线下的面积：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
        <math>
         <semantics>
          <mrow>
           <mtext>
            AUC
           </mtext>
           <mo>
            =
           </mo>
           <msubsup>
            <mo>
             ∫
            </mo>
            <mrow>
             <mi>
              t
             </mi>
             <mo>
              =
             </mo>
             <mi mathvariant="normal">
              ∞
             </mi>
            </mrow>
            <mrow>
             <mo>
              −
             </mo>
             <mi mathvariant="normal">
              ∞
             </mi>
            </mrow>
           </msubsup>
           <mi>
            y
           </mi>
           <mo>
            (
           </mo>
           <mi>
            t
           </mi>
           <mo>
            )
           </mo>
           <mi>
            d
           </mi>
           <mi>
            x
           </mi>
           <mo>
            (
           </mo>
           <mi>
            t
           </mi>
           <mo>
            )
           </mo>
          </mrow>
          <annotation encoding="application/x-tex">
            \text{AUC} = \int_{t=\infty}^{-\infty} y(t) d x(t) 
          </annotation>
         </semantics>
        </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord text"><span class="mord">AUC</span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.43318em; vertical-align: -0.91195em;"></span><span class="mop"><span class="mop op-symbol large-op" style="margin-right: 0.44445em; position: relative; top: -0.001125em;">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.52123em;"><span class="" style="top: -1.78805em; margin-left: -0.44445em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">∞</span></span></span></span><span class="" style="top: -3.8129em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">∞</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.91195em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mopen">(</span><span class="mord mathit">t</span><span class="mclose">)</span><span class="mord mathit">d</span><span class="mord mathit">x</span><span class="mopen">(</span><span class="mord mathit">t</span><span class="mclose">)</span></span></span></span></span></span></li> 
   <li>不可以，精确率是基于某个阈值进行计算的，AUC是基于所有可能的阈值进行计算的，具有更高的健壮性。AUC不关注某个阈值下的表现如何，综合所有阈值的预测性能，所以精确率高，AUC不一定大，反之亦然。 
    <blockquote> 
     <p>参考：<a href="https://blog.csdn.net/legendavid/article/details/79063044" rel="nofollow">https://blog.csdn.net/legendavid/article/details/79063044</a></p> 
    </blockquote> </li> 
  </ol> 
  <h3><a id="18_AUCaccuracyAUC_634"></a>18. 二分类时，为什么AUC比accuracy更常用？为什么AUC对样本类别比例不敏感？</h3> 
  <h3><a id="19_ROC_635"></a>19. 如何绘制ROC曲线？</h3> 
  <p>以真正例率为纵坐标，假正例率为横坐标绘制的曲线。<br> TPR = TP /（TP + FN）真<br> FPR = TN / ( TN + FP) 假</p> 
  <h3><a id="20__640"></a>20. 梯度下降的改进算法有哪些？梯度消失的概念？</h3> 
  <h3><a id="_641"></a>如何解决梯度下降法陷入局部最优的问题？</h3> 
  <blockquote> 
   <p>参考 <a href="https://blog.csdn.net/maqunfi/article/details/82634529" rel="nofollow" data-token="3b091ef1b5bde7ea8c976245015ff93d">https://blog.csdn.net/maqunfi/article/details/82634529</a></p> 
  </blockquote> 
  <ol> 
   <li>使用随机梯度下降法替代真正的梯度下降算法；</li> 
   <li>设置冲量；</li> 
   <li>使用不同的初始权值进行训练。</li> 
  </ol> 
  <h3><a id="21_VC_646"></a>21. VC维的理解</h3> 
  <p>这个感觉不太可能会碰到～</p> 
  <blockquote> 
   <p><a href="https://www.cnblogs.com/wuyuegb2312/archive/2012/12/03/2799893.html" rel="nofollow">https://www.cnblogs.com/wuyuegb2312/archive/2012/12/03/2799893.html</a></p> 
  </blockquote> 
  <ol> 
   <li>分散：对于一个给定集合S={x1, … ,xd}，如果一个假设类H能够实现集合S中所有元素的任意一种标记方式，则称H能够分散S。</li> 
   <li>VC维的定义：H的VC维表示为VC(H) ，指能够被H分散的最大集合的大小。若H能分散任意大小的集合，那么VC(H)为无穷大。</li> 
  </ol> 
  <h3><a id="22__652"></a>22. 判断模型线性与非线性</h3> 
  <blockquote> 
   <p><a href="https://zhuanlan.zhihu.com/p/37866896" rel="nofollow">https://zhuanlan.zhihu.com/p/37866896</a></p> 
  </blockquote> 
  <p>只需要判别决策边界是否是直线，也就是是否能用一条直线来划分，如果可以则为线性。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
