<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Retina U-Net论文简析 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Retina U-Net论文简析" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Contents Links Title Abstract 1.Introduction 2.Related work 3.Methods 3.1.Retina U-Net Retina Net Adding Semantic Segmentation Supervision 逐像素交叉熵损失（pixel-wide cross entropy） Dice系数损失（Dice coefficient loss） Weighted Box Clustering 4.Experiments 4.1.Backbone&amp;Baselines Retina Net. Mask R-CNN. Faster R-CNN+. U-Faster R-CNN+. DetU-Net. 4.2. Training&amp;Inference Setup 4.3.Evaluation 4.4.Lung nodule detection and categorization 4.4.1 Utilized data set 4.4.2 Results 4.5.Breast lesion detection and categorization 4.5.1 Utilized data set 4.5.2 Results 4.6.Toy Experiments 1.Distinguishing object shapes: 2.Learning discriminative object patterns: 3.Distinguishing object scales: 4.6.1 Utilized data set 4.6.2 Results 5.Conclusion Links paper: https://arxiv.org/pdf/1811.08661.pdf github: https://github.com/pfjaeger/medicaldetectiontoolkit Title 直译过来是说这个网络是对监督学习下医学目标检测的分割的简单开发，但是还有一个词embarrassingly（令人尴尬的），这就有点令人费解了。那就先读文章慢慢理解吧。 Abstract 作者比较了语义分割（semantic segmentation）和state-of-the-art detectors。 semantic segmentation：通过预测pixel-level scores，再由特殊的启发式算法，映射到object-level scores。 state-of-the-art：end-to-end（端到端）的单个物体scoring，但这是利用逐像素监督标注得到的。这点对医学图像分割尤其不利，因为医学的数据集都是建立在像素级别的。 作者提出的Retina U-Net：自然地融合了Retina Net这个one-stage detector和U-Net 结构，广泛用于医学图像语义分割。这个网络通过辅助的语义分割补充丢失的监督标记，而且不会引入之前提出的two-stage detectors的复杂性。作者还说这个网络强大的检测性能只有更复杂的two-stage同类才能与其媲美。 1.Introduction 主要提到了U-Net，object-level，粗略水平的检测，两种应用于object-level预测的方法（two-stage detectors和one-stage detectors）及作者的做法。 U-Net网络是一种语义分割算法，目前在临床中已形成了一种标准，这归功于医学图像通常是用像素级别的标注标识感兴趣目标的结果，它们也很好地应用于逐像素监督学习的分割场景。不仅如此，MRI和CT成像还能捕获3D空间的影像，包括空间中的物体分割，因此不需要区分（重叠的）实例。 尽管像放疗或肿瘤生长态势探测在临床上需要像素级别的检测，在大多情况下，粗略水平下的的检测和知道要探测的东西存在是有联系的。在评估object-level大小的模型时，这种联系在研究设计中得到了最佳反映。为了弥合（bridge）语义分割pixel-level的检测和object-level的评估，必须要引入特殊的启发式算法或是其他模型。 通过从粗略水平级别提取预测信息到实现end-to-end的object scoring，大多目前的目标检测都基于FPN（feature pyramid network）。 现有两种方法应用于object-level检测，一是two-stage detectors：无论第一种物体是什么类都与背景相区别，随着边界框回归生成大小可变的候选框，然后候选框重采样为大小固定的网格以确定分类比例的不变性。二是one-stage detectors：在粗略水平可立即分类出物体的类别。但这种在粗略水平检测会导致信息的丢失，这又与医学领域相对level较小的数据集的数据得到有效训练的需求相悖。 作者表明，充分利用可用的语义分割标注可以使医学图像上的检测性能显著提升。此外，作者认为two-stage detectors中的重采样操作在医学领域中并无帮助，因为与自然图像不同，是语义编码信息描述了比例的变化，而非物体和相机间的距离变化。 作者的做法是通过高分辨率补充FPN自下而上的部分（以学习语义分割），通过在U-Net的decoder部分的粗略水平上的两个子网络来改进U-Net（以允许end-to-end的object-scoring）。 作者通过两个数据集评估自己的模型。一个是LIDC-IDRI【Lung-CT】（一个公开的数据集），另一个是Diffusion-MRI（一个内部的数据集）。 这篇论文有以下的contributions： 一种简单有效且侧重于医学图像中的应用，可用于通过语义分割训练识别标注。 对流行的object detectors（无论在2D还是3D）进行了深入分析。 一种名为weighted box clustering的算法，可用于在2D和3D中合并同一图像的不同检测结果。 这是一个全面的框架。 2.Related work 许多two-stage detectors学习通过基于proposal的分割，但是，作者质疑这种做法，因为它没有充分地利用语义分割监督。有如下原因： 仅在裁剪的候选区上评估mask loss，也就是说，周围区域的前后梯度不反向传播。 proposal region（候选区）和ground truth mask通常被重采样为大小固定的网格（RoIAlign）。 只有正例的候选区被用于mask loss，这会导致对候选区表现的依赖。 mask loss的梯度不会流过整个模型，而只从相应的pyramid（金字塔）级向上流动。 而在one-stage领域，Uhrig等人在SSD上进行语义分割并用于实例分割，这与作者的做法很类似，其中分割输出在后处理中分配给候选框。Zhang等人也提出了类似的结构，但是是使用边界框标注创建的masks，以弱监督方式学习分割。 与用于特征提取的自下而上的主干网络相反，作者遵循了FPN的方法，即有自上而下（decoder）的路径使得不同尺度的语义都能丰富地展示。 作者还用一些例子强调了额外的语义分割监督的重要性。 3.Methods 3.1.Retina U-Net Retina Net 如上图所示，Retina Net是一个基于FPN的、简单的one-stage的检测网络。其中两个sub-networks分别在金字塔等级P3-P6上进行分类和边界框回归。这里Pj表示第j个解码器级别的特征映射，其中j随着分辨率的降低而增加。由于考虑到医学图像中小对象的存在，作者将sub-networks操作向一个金字塔级别转移到P2-P5，因为在更高分辨率的级别中产生了大量密集位置。 这样的结构类似于对称的U-Net结构，作者把它称作U-FPN。 作者在sub-networks的cl（分类）中把sigmoid non-linearity换成softmax（可解决由于3D图像中的非重叠目标而导致的类别互斥性），对于3D的实现，第一个网络的feature maps数量降低到64（可降低GPU内存开销）。 Adding Semantic Segmentation Supervision 添加了语义分割监督。在图中，可以看到绿色的箭头就是将Semantic Seg.的Signals通过P1和P0添加到top-down的path中，而且Signals还会跳接到bottom-up path中。 在图中的灰色箭头表示会在该层预测，而P0、P1这两层不预测，这使得在理论时间内参数的数量不变。 segmentation loss是根据P0logits计算的，它用到了逐像素交叉熵损失（分别检查每个像素，将类预测与热编码目标向量进行比较），还用到了Dice loss（在类别极不平衡的分割任务中能很稳定地训练）。（这两个都是医学分割中常用的损失函数） 其中，u是网络的softmax输出，v是ground truth的一个one hot编码。u和v都是大小为I x K的矩阵（i∈I是training batch中的像素数量，k∈K是类别）。 逐像素交叉熵损失（pixel-wide cross entropy） 逐像素交叉熵损失**单独评估每个像素矢量的类预测，然后对所有像素求均值，**我们可以认为图像中的像素被平等的学习了。但是由于医学图像的高分辨率，常出现类别不平衡的问题，所以，训练会被像素较多的类主导，对于较小的物体很难学到它的特征，网络的有效性会被降低。 Dice系数损失（Dice coefficient loss） dice系数源于二分类，本质上是为了衡量两个样本的重叠部分。范围为[0,1]，1表示完全重叠。它的计算公式是： D i c e = 2 ∣ A ∩ B ∣ ∣ A ∣ + ∣ B ∣ Dice=\frac{2\left | A\cap B \right |}{\left | A \right |+\left | B \right | } Dice=∣A∣+∣B∣2∣A∩B∣​ 其中，|A∩B|表示集合A、B之间的共同元素，|A|、|B|分别表示A、B集合中的元素个数。其中，分子的系数为2是由于分母存在重复计算A和B之间共同元素。 Dice loss： 为了形成可以最小化的损失函数，我们将简单地使用1-Dice。这种损失函数被称为 soft dice loss，因为我们直接使用预测概率而不是使用阈值或将它们转换为二进制mask。 dice loss比较适用于样本极度不均的情况，一般的情况下，使用 dice loss 会对反向传播造成不利的影响，容易使训练变得不稳定。 Weighted Box Clustering 由于医学图像的高分辨率及3D成像（MRI），需要对patch crops进行训练，从而需要在可用GPU内存限制与batch size和patch size之间权衡。 为了合并对目标检测的预测结果，作者提出了weighted box clustering（WBC），加权框聚类：这个算法与非极大值抑制算法（NMS）类似，根据IoU阈值进行聚类的预测，而非选择得分最高的候选框。 其中，os表示每个预测框的加权置信分数，oc表示每个坐标的加权平均值，i是聚类的下标，s是置信度分数，c是坐标。n missing表示对集群没有贡献一个框的视图的权重。 w是加权因子，包含： 重叠因子f：预测框与得分最高的框（softmax confidence）之间的重叠权重。 区域a：表明较大的框有较高的权重。 patch中心因子p：以patch中心的正态分布密度分配分数。 4.Experiments 4.1.Backbone&amp;Baselines 在这项研究中，作者将Retina U-Net与一组one-stage和two-stage物体检测器比较。为了公平地比较，所有的模型都用同一个框架（Pytorch）实现。都使用基于ResNet50的FPN作为共同的特征提取器。由于医学图像的目标相对较小，因此使用anchor都以4为因子，即对应金字塔{P2，P3，P4，P5}的anchor大小为{4²，8²，16²，32²}（2D）。而在3D中，anchor cube的大小设为{1，2，4，8}（沿z轴通常有较低的分辨率）。 Retina Net. 图c）Retina Net的实现与Retina U-Net中的相同。 Mask R-CNN. 图a）需要对3D进行微调：在RPN中的feature map降到64（可降低GPU内存开销），3D-RoIAlign的poolsize对classification head设置为（7，7，3），对mask head设置为（14，14，5）。positive候选框的IoU降到0.3。 Faster R-CNN+. 图b）为了挑出从Mask R-CNN中通过分割监督获得的性能增益，作者对toy data sets进行ablation（消融），并禁用mask-loss。从而有效地将模型减少到更快的R-CNN结构（除了RoIAlign操作，用+表示）。 因为Mask R-CNN大致是在Faster R-CNN上增加了分割。消融实验类似于控制变量。因为作者提出了一种方案，同时改变了多个条件/参数，他在接下去的消融实验中，会一一控制一个条件/参数不变，来看看结果，看到底是哪个条件/参数对结果的影响更大。 U-Faster R-CNN+. 图d）通过在U-FPN上部署Faster R-CNN来探索two-stage中额外语义分割的性能。 DetU-Net. 图e）使用UFPN实现类似U-Net的baseline，通过用1x1的卷积从P0中提取softmax预测，用于识别所有前景类的连通分量，通过连通分量画边界框（体），并将每个分量和类别的最高sofxmax概率指定为目标的得分。为了降低噪声，每个图像仅考虑5个（3D15个）最大分量。 4.2. Training&amp;Inference Setup 2D：slice-wise处理 2Dc：±3张相邻切片作为附加输入 3D：体积卷积 softmax probability解决分类损失的类不平衡。 Adam优化、0.0001的learning rate、5折交叉验证（train60%、val20%、test20%）、2Dbatchsize为20（3D为8）。 为防止过拟合，2D、3D都采用数据增强。 为了补偿小数据集的不稳定数据，通过执行测试时镜像以及根据验证度选出5个最高得分的多个模型进行测试。 通过对得分和坐标进行加权聚类，来合并来自全部成员（ensemble-members）和重叠区的预测框。 应用非极大值抑制（NMS）的自适应来合并从2D到3D框的预测：将所有切片的框投影到一个平面中，同时保留切片原点信息。应用NMS时，只有与最高得分框的切片直接或间接相连的框才被视为匹配。将所有匹配的最小最大切片编号指定为预测立方体的z坐标的结果。 4.3.Evaluation 使用mAP（平均精确度）评估实验。在IoU=0.1的阈值处的相对较低匹配交叉点处确定mAP。 4.4.Lung nodule detection and categorization 肺结节的检测与分类，检测病变是良性还是恶性。细粒度分类有望在不断增长的数据集和图像分辨率的背景下获得相关性。 4.4.1 Utilized data set 使用公开数据集LIDC-ICRI（1035个肺部CT扫描）。使用数字1-5表示恶性的可能性等级。良性标记（1-2，n=1319）、恶性标记（3-5，n=494）。CT扫描重采样为0.7x0.7x1.25mm，大致对应于数据集的平均分辨率。训练时，patches大小为288x288（2D），128x128x64（3D）。 4.4.2 Results 4.5.Breast lesion detection and categorization 乳房病变检测。 4.5.1 Utilized data set 使用的数据集是在331名患者的扩散MRI内部数据集上进行的，这些患者在之前的乳房X线照相术中有可能患病的风险。良性标注n=141，恶性标注n=190。图像重采样为1.25x1.25x3mm。训练时，patches大小为160x160（2D），160x160x56（3D）。 4.5.2 Results 4.6.Toy Experiments 这一系列的toy experiments主要是来处理医学上对象分类的子任务，如区分尺度，形状和强度。更具体是为了研究在有限的训练数据下完全分割监督的重要性。这里有三个子任务，且每个任务逐渐减少训练数据量: 1.Distinguishing object shapes: 检测和区分两类物体的形状。a）图：第一类是由直径为20像素的圆形组成，第二类是由20像素的圆形和直径为4像素的中心组成，类似于甜甜圈状。预计完全语义监督将在此任务中产生显着的性能提升，特别是在小数据中。 2.Learning discriminative object patterns: 这个任务与前一任务相同，图a）除了中心孔没有从甜甜圈的segmentation masks中切除。这需要模型拾取判别模式（孔），而不是通过mask的形状明确指出它。 在医学图像的背景下，这种设置可以被认为是更真实的。 3.Distinguishing object scales: 检测和区分两类物体的尺度。图b）第一类由直径为19像素的圆组成，第二类由直径为20像素的圆组成。类信息完全以对象比例编码，因此在目标框坐标中编码。预计语义监督不会带来重大收益。 4.6.1 Utilized data set 使用的两个数据集均为由人工生成的320x320的2D图像组成，1000个用于训练，500个用于验证，另外1000个用于测试。 4.6.2 Results 5.Conclusion 1.输入维度上利用语义分割的重要性，并与流行的目标检测模型进行了细致的比较，特别强调了有上下联系的有限训练数据。 2.在公开可用的LIDC-IDRI肺CT数据集以及内部的乳房病变MRI数据集中，Retina U-Net产生的检测性能优于没有完全分割监督的模型。 3.正如标题所说，Retina U-Net，在其令人尴尬的简单的架构中利用监督的语义分割来解决医学图像较大而目标较小的问题。" />
<meta property="og:description" content="Contents Links Title Abstract 1.Introduction 2.Related work 3.Methods 3.1.Retina U-Net Retina Net Adding Semantic Segmentation Supervision 逐像素交叉熵损失（pixel-wide cross entropy） Dice系数损失（Dice coefficient loss） Weighted Box Clustering 4.Experiments 4.1.Backbone&amp;Baselines Retina Net. Mask R-CNN. Faster R-CNN+. U-Faster R-CNN+. DetU-Net. 4.2. Training&amp;Inference Setup 4.3.Evaluation 4.4.Lung nodule detection and categorization 4.4.1 Utilized data set 4.4.2 Results 4.5.Breast lesion detection and categorization 4.5.1 Utilized data set 4.5.2 Results 4.6.Toy Experiments 1.Distinguishing object shapes: 2.Learning discriminative object patterns: 3.Distinguishing object scales: 4.6.1 Utilized data set 4.6.2 Results 5.Conclusion Links paper: https://arxiv.org/pdf/1811.08661.pdf github: https://github.com/pfjaeger/medicaldetectiontoolkit Title 直译过来是说这个网络是对监督学习下医学目标检测的分割的简单开发，但是还有一个词embarrassingly（令人尴尬的），这就有点令人费解了。那就先读文章慢慢理解吧。 Abstract 作者比较了语义分割（semantic segmentation）和state-of-the-art detectors。 semantic segmentation：通过预测pixel-level scores，再由特殊的启发式算法，映射到object-level scores。 state-of-the-art：end-to-end（端到端）的单个物体scoring，但这是利用逐像素监督标注得到的。这点对医学图像分割尤其不利，因为医学的数据集都是建立在像素级别的。 作者提出的Retina U-Net：自然地融合了Retina Net这个one-stage detector和U-Net 结构，广泛用于医学图像语义分割。这个网络通过辅助的语义分割补充丢失的监督标记，而且不会引入之前提出的two-stage detectors的复杂性。作者还说这个网络强大的检测性能只有更复杂的two-stage同类才能与其媲美。 1.Introduction 主要提到了U-Net，object-level，粗略水平的检测，两种应用于object-level预测的方法（two-stage detectors和one-stage detectors）及作者的做法。 U-Net网络是一种语义分割算法，目前在临床中已形成了一种标准，这归功于医学图像通常是用像素级别的标注标识感兴趣目标的结果，它们也很好地应用于逐像素监督学习的分割场景。不仅如此，MRI和CT成像还能捕获3D空间的影像，包括空间中的物体分割，因此不需要区分（重叠的）实例。 尽管像放疗或肿瘤生长态势探测在临床上需要像素级别的检测，在大多情况下，粗略水平下的的检测和知道要探测的东西存在是有联系的。在评估object-level大小的模型时，这种联系在研究设计中得到了最佳反映。为了弥合（bridge）语义分割pixel-level的检测和object-level的评估，必须要引入特殊的启发式算法或是其他模型。 通过从粗略水平级别提取预测信息到实现end-to-end的object scoring，大多目前的目标检测都基于FPN（feature pyramid network）。 现有两种方法应用于object-level检测，一是two-stage detectors：无论第一种物体是什么类都与背景相区别，随着边界框回归生成大小可变的候选框，然后候选框重采样为大小固定的网格以确定分类比例的不变性。二是one-stage detectors：在粗略水平可立即分类出物体的类别。但这种在粗略水平检测会导致信息的丢失，这又与医学领域相对level较小的数据集的数据得到有效训练的需求相悖。 作者表明，充分利用可用的语义分割标注可以使医学图像上的检测性能显著提升。此外，作者认为two-stage detectors中的重采样操作在医学领域中并无帮助，因为与自然图像不同，是语义编码信息描述了比例的变化，而非物体和相机间的距离变化。 作者的做法是通过高分辨率补充FPN自下而上的部分（以学习语义分割），通过在U-Net的decoder部分的粗略水平上的两个子网络来改进U-Net（以允许end-to-end的object-scoring）。 作者通过两个数据集评估自己的模型。一个是LIDC-IDRI【Lung-CT】（一个公开的数据集），另一个是Diffusion-MRI（一个内部的数据集）。 这篇论文有以下的contributions： 一种简单有效且侧重于医学图像中的应用，可用于通过语义分割训练识别标注。 对流行的object detectors（无论在2D还是3D）进行了深入分析。 一种名为weighted box clustering的算法，可用于在2D和3D中合并同一图像的不同检测结果。 这是一个全面的框架。 2.Related work 许多two-stage detectors学习通过基于proposal的分割，但是，作者质疑这种做法，因为它没有充分地利用语义分割监督。有如下原因： 仅在裁剪的候选区上评估mask loss，也就是说，周围区域的前后梯度不反向传播。 proposal region（候选区）和ground truth mask通常被重采样为大小固定的网格（RoIAlign）。 只有正例的候选区被用于mask loss，这会导致对候选区表现的依赖。 mask loss的梯度不会流过整个模型，而只从相应的pyramid（金字塔）级向上流动。 而在one-stage领域，Uhrig等人在SSD上进行语义分割并用于实例分割，这与作者的做法很类似，其中分割输出在后处理中分配给候选框。Zhang等人也提出了类似的结构，但是是使用边界框标注创建的masks，以弱监督方式学习分割。 与用于特征提取的自下而上的主干网络相反，作者遵循了FPN的方法，即有自上而下（decoder）的路径使得不同尺度的语义都能丰富地展示。 作者还用一些例子强调了额外的语义分割监督的重要性。 3.Methods 3.1.Retina U-Net Retina Net 如上图所示，Retina Net是一个基于FPN的、简单的one-stage的检测网络。其中两个sub-networks分别在金字塔等级P3-P6上进行分类和边界框回归。这里Pj表示第j个解码器级别的特征映射，其中j随着分辨率的降低而增加。由于考虑到医学图像中小对象的存在，作者将sub-networks操作向一个金字塔级别转移到P2-P5，因为在更高分辨率的级别中产生了大量密集位置。 这样的结构类似于对称的U-Net结构，作者把它称作U-FPN。 作者在sub-networks的cl（分类）中把sigmoid non-linearity换成softmax（可解决由于3D图像中的非重叠目标而导致的类别互斥性），对于3D的实现，第一个网络的feature maps数量降低到64（可降低GPU内存开销）。 Adding Semantic Segmentation Supervision 添加了语义分割监督。在图中，可以看到绿色的箭头就是将Semantic Seg.的Signals通过P1和P0添加到top-down的path中，而且Signals还会跳接到bottom-up path中。 在图中的灰色箭头表示会在该层预测，而P0、P1这两层不预测，这使得在理论时间内参数的数量不变。 segmentation loss是根据P0logits计算的，它用到了逐像素交叉熵损失（分别检查每个像素，将类预测与热编码目标向量进行比较），还用到了Dice loss（在类别极不平衡的分割任务中能很稳定地训练）。（这两个都是医学分割中常用的损失函数） 其中，u是网络的softmax输出，v是ground truth的一个one hot编码。u和v都是大小为I x K的矩阵（i∈I是training batch中的像素数量，k∈K是类别）。 逐像素交叉熵损失（pixel-wide cross entropy） 逐像素交叉熵损失**单独评估每个像素矢量的类预测，然后对所有像素求均值，**我们可以认为图像中的像素被平等的学习了。但是由于医学图像的高分辨率，常出现类别不平衡的问题，所以，训练会被像素较多的类主导，对于较小的物体很难学到它的特征，网络的有效性会被降低。 Dice系数损失（Dice coefficient loss） dice系数源于二分类，本质上是为了衡量两个样本的重叠部分。范围为[0,1]，1表示完全重叠。它的计算公式是： D i c e = 2 ∣ A ∩ B ∣ ∣ A ∣ + ∣ B ∣ Dice=\frac{2\left | A\cap B \right |}{\left | A \right |+\left | B \right | } Dice=∣A∣+∣B∣2∣A∩B∣​ 其中，|A∩B|表示集合A、B之间的共同元素，|A|、|B|分别表示A、B集合中的元素个数。其中，分子的系数为2是由于分母存在重复计算A和B之间共同元素。 Dice loss： 为了形成可以最小化的损失函数，我们将简单地使用1-Dice。这种损失函数被称为 soft dice loss，因为我们直接使用预测概率而不是使用阈值或将它们转换为二进制mask。 dice loss比较适用于样本极度不均的情况，一般的情况下，使用 dice loss 会对反向传播造成不利的影响，容易使训练变得不稳定。 Weighted Box Clustering 由于医学图像的高分辨率及3D成像（MRI），需要对patch crops进行训练，从而需要在可用GPU内存限制与batch size和patch size之间权衡。 为了合并对目标检测的预测结果，作者提出了weighted box clustering（WBC），加权框聚类：这个算法与非极大值抑制算法（NMS）类似，根据IoU阈值进行聚类的预测，而非选择得分最高的候选框。 其中，os表示每个预测框的加权置信分数，oc表示每个坐标的加权平均值，i是聚类的下标，s是置信度分数，c是坐标。n missing表示对集群没有贡献一个框的视图的权重。 w是加权因子，包含： 重叠因子f：预测框与得分最高的框（softmax confidence）之间的重叠权重。 区域a：表明较大的框有较高的权重。 patch中心因子p：以patch中心的正态分布密度分配分数。 4.Experiments 4.1.Backbone&amp;Baselines 在这项研究中，作者将Retina U-Net与一组one-stage和two-stage物体检测器比较。为了公平地比较，所有的模型都用同一个框架（Pytorch）实现。都使用基于ResNet50的FPN作为共同的特征提取器。由于医学图像的目标相对较小，因此使用anchor都以4为因子，即对应金字塔{P2，P3，P4，P5}的anchor大小为{4²，8²，16²，32²}（2D）。而在3D中，anchor cube的大小设为{1，2，4，8}（沿z轴通常有较低的分辨率）。 Retina Net. 图c）Retina Net的实现与Retina U-Net中的相同。 Mask R-CNN. 图a）需要对3D进行微调：在RPN中的feature map降到64（可降低GPU内存开销），3D-RoIAlign的poolsize对classification head设置为（7，7，3），对mask head设置为（14，14，5）。positive候选框的IoU降到0.3。 Faster R-CNN+. 图b）为了挑出从Mask R-CNN中通过分割监督获得的性能增益，作者对toy data sets进行ablation（消融），并禁用mask-loss。从而有效地将模型减少到更快的R-CNN结构（除了RoIAlign操作，用+表示）。 因为Mask R-CNN大致是在Faster R-CNN上增加了分割。消融实验类似于控制变量。因为作者提出了一种方案，同时改变了多个条件/参数，他在接下去的消融实验中，会一一控制一个条件/参数不变，来看看结果，看到底是哪个条件/参数对结果的影响更大。 U-Faster R-CNN+. 图d）通过在U-FPN上部署Faster R-CNN来探索two-stage中额外语义分割的性能。 DetU-Net. 图e）使用UFPN实现类似U-Net的baseline，通过用1x1的卷积从P0中提取softmax预测，用于识别所有前景类的连通分量，通过连通分量画边界框（体），并将每个分量和类别的最高sofxmax概率指定为目标的得分。为了降低噪声，每个图像仅考虑5个（3D15个）最大分量。 4.2. Training&amp;Inference Setup 2D：slice-wise处理 2Dc：±3张相邻切片作为附加输入 3D：体积卷积 softmax probability解决分类损失的类不平衡。 Adam优化、0.0001的learning rate、5折交叉验证（train60%、val20%、test20%）、2Dbatchsize为20（3D为8）。 为防止过拟合，2D、3D都采用数据增强。 为了补偿小数据集的不稳定数据，通过执行测试时镜像以及根据验证度选出5个最高得分的多个模型进行测试。 通过对得分和坐标进行加权聚类，来合并来自全部成员（ensemble-members）和重叠区的预测框。 应用非极大值抑制（NMS）的自适应来合并从2D到3D框的预测：将所有切片的框投影到一个平面中，同时保留切片原点信息。应用NMS时，只有与最高得分框的切片直接或间接相连的框才被视为匹配。将所有匹配的最小最大切片编号指定为预测立方体的z坐标的结果。 4.3.Evaluation 使用mAP（平均精确度）评估实验。在IoU=0.1的阈值处的相对较低匹配交叉点处确定mAP。 4.4.Lung nodule detection and categorization 肺结节的检测与分类，检测病变是良性还是恶性。细粒度分类有望在不断增长的数据集和图像分辨率的背景下获得相关性。 4.4.1 Utilized data set 使用公开数据集LIDC-ICRI（1035个肺部CT扫描）。使用数字1-5表示恶性的可能性等级。良性标记（1-2，n=1319）、恶性标记（3-5，n=494）。CT扫描重采样为0.7x0.7x1.25mm，大致对应于数据集的平均分辨率。训练时，patches大小为288x288（2D），128x128x64（3D）。 4.4.2 Results 4.5.Breast lesion detection and categorization 乳房病变检测。 4.5.1 Utilized data set 使用的数据集是在331名患者的扩散MRI内部数据集上进行的，这些患者在之前的乳房X线照相术中有可能患病的风险。良性标注n=141，恶性标注n=190。图像重采样为1.25x1.25x3mm。训练时，patches大小为160x160（2D），160x160x56（3D）。 4.5.2 Results 4.6.Toy Experiments 这一系列的toy experiments主要是来处理医学上对象分类的子任务，如区分尺度，形状和强度。更具体是为了研究在有限的训练数据下完全分割监督的重要性。这里有三个子任务，且每个任务逐渐减少训练数据量: 1.Distinguishing object shapes: 检测和区分两类物体的形状。a）图：第一类是由直径为20像素的圆形组成，第二类是由20像素的圆形和直径为4像素的中心组成，类似于甜甜圈状。预计完全语义监督将在此任务中产生显着的性能提升，特别是在小数据中。 2.Learning discriminative object patterns: 这个任务与前一任务相同，图a）除了中心孔没有从甜甜圈的segmentation masks中切除。这需要模型拾取判别模式（孔），而不是通过mask的形状明确指出它。 在医学图像的背景下，这种设置可以被认为是更真实的。 3.Distinguishing object scales: 检测和区分两类物体的尺度。图b）第一类由直径为19像素的圆组成，第二类由直径为20像素的圆组成。类信息完全以对象比例编码，因此在目标框坐标中编码。预计语义监督不会带来重大收益。 4.6.1 Utilized data set 使用的两个数据集均为由人工生成的320x320的2D图像组成，1000个用于训练，500个用于验证，另外1000个用于测试。 4.6.2 Results 5.Conclusion 1.输入维度上利用语义分割的重要性，并与流行的目标检测模型进行了细致的比较，特别强调了有上下联系的有限训练数据。 2.在公开可用的LIDC-IDRI肺CT数据集以及内部的乳房病变MRI数据集中，Retina U-Net产生的检测性能优于没有完全分割监督的模型。 3.正如标题所说，Retina U-Net，在其令人尴尬的简单的架构中利用监督的语义分割来解决医学图像较大而目标较小的问题。" />
<link rel="canonical" href="https://uzzz.org/2019/08/17/794022.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/17/794022.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-17T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Contents Links Title Abstract 1.Introduction 2.Related work 3.Methods 3.1.Retina U-Net Retina Net Adding Semantic Segmentation Supervision 逐像素交叉熵损失（pixel-wide cross entropy） Dice系数损失（Dice coefficient loss） Weighted Box Clustering 4.Experiments 4.1.Backbone&amp;Baselines Retina Net. Mask R-CNN. Faster R-CNN+. U-Faster R-CNN+. DetU-Net. 4.2. Training&amp;Inference Setup 4.3.Evaluation 4.4.Lung nodule detection and categorization 4.4.1 Utilized data set 4.4.2 Results 4.5.Breast lesion detection and categorization 4.5.1 Utilized data set 4.5.2 Results 4.6.Toy Experiments 1.Distinguishing object shapes: 2.Learning discriminative object patterns: 3.Distinguishing object scales: 4.6.1 Utilized data set 4.6.2 Results 5.Conclusion Links paper: https://arxiv.org/pdf/1811.08661.pdf github: https://github.com/pfjaeger/medicaldetectiontoolkit Title 直译过来是说这个网络是对监督学习下医学目标检测的分割的简单开发，但是还有一个词embarrassingly（令人尴尬的），这就有点令人费解了。那就先读文章慢慢理解吧。 Abstract 作者比较了语义分割（semantic segmentation）和state-of-the-art detectors。 semantic segmentation：通过预测pixel-level scores，再由特殊的启发式算法，映射到object-level scores。 state-of-the-art：end-to-end（端到端）的单个物体scoring，但这是利用逐像素监督标注得到的。这点对医学图像分割尤其不利，因为医学的数据集都是建立在像素级别的。 作者提出的Retina U-Net：自然地融合了Retina Net这个one-stage detector和U-Net 结构，广泛用于医学图像语义分割。这个网络通过辅助的语义分割补充丢失的监督标记，而且不会引入之前提出的two-stage detectors的复杂性。作者还说这个网络强大的检测性能只有更复杂的two-stage同类才能与其媲美。 1.Introduction 主要提到了U-Net，object-level，粗略水平的检测，两种应用于object-level预测的方法（two-stage detectors和one-stage detectors）及作者的做法。 U-Net网络是一种语义分割算法，目前在临床中已形成了一种标准，这归功于医学图像通常是用像素级别的标注标识感兴趣目标的结果，它们也很好地应用于逐像素监督学习的分割场景。不仅如此，MRI和CT成像还能捕获3D空间的影像，包括空间中的物体分割，因此不需要区分（重叠的）实例。 尽管像放疗或肿瘤生长态势探测在临床上需要像素级别的检测，在大多情况下，粗略水平下的的检测和知道要探测的东西存在是有联系的。在评估object-level大小的模型时，这种联系在研究设计中得到了最佳反映。为了弥合（bridge）语义分割pixel-level的检测和object-level的评估，必须要引入特殊的启发式算法或是其他模型。 通过从粗略水平级别提取预测信息到实现end-to-end的object scoring，大多目前的目标检测都基于FPN（feature pyramid network）。 现有两种方法应用于object-level检测，一是two-stage detectors：无论第一种物体是什么类都与背景相区别，随着边界框回归生成大小可变的候选框，然后候选框重采样为大小固定的网格以确定分类比例的不变性。二是one-stage detectors：在粗略水平可立即分类出物体的类别。但这种在粗略水平检测会导致信息的丢失，这又与医学领域相对level较小的数据集的数据得到有效训练的需求相悖。 作者表明，充分利用可用的语义分割标注可以使医学图像上的检测性能显著提升。此外，作者认为two-stage detectors中的重采样操作在医学领域中并无帮助，因为与自然图像不同，是语义编码信息描述了比例的变化，而非物体和相机间的距离变化。 作者的做法是通过高分辨率补充FPN自下而上的部分（以学习语义分割），通过在U-Net的decoder部分的粗略水平上的两个子网络来改进U-Net（以允许end-to-end的object-scoring）。 作者通过两个数据集评估自己的模型。一个是LIDC-IDRI【Lung-CT】（一个公开的数据集），另一个是Diffusion-MRI（一个内部的数据集）。 这篇论文有以下的contributions： 一种简单有效且侧重于医学图像中的应用，可用于通过语义分割训练识别标注。 对流行的object detectors（无论在2D还是3D）进行了深入分析。 一种名为weighted box clustering的算法，可用于在2D和3D中合并同一图像的不同检测结果。 这是一个全面的框架。 2.Related work 许多two-stage detectors学习通过基于proposal的分割，但是，作者质疑这种做法，因为它没有充分地利用语义分割监督。有如下原因： 仅在裁剪的候选区上评估mask loss，也就是说，周围区域的前后梯度不反向传播。 proposal region（候选区）和ground truth mask通常被重采样为大小固定的网格（RoIAlign）。 只有正例的候选区被用于mask loss，这会导致对候选区表现的依赖。 mask loss的梯度不会流过整个模型，而只从相应的pyramid（金字塔）级向上流动。 而在one-stage领域，Uhrig等人在SSD上进行语义分割并用于实例分割，这与作者的做法很类似，其中分割输出在后处理中分配给候选框。Zhang等人也提出了类似的结构，但是是使用边界框标注创建的masks，以弱监督方式学习分割。 与用于特征提取的自下而上的主干网络相反，作者遵循了FPN的方法，即有自上而下（decoder）的路径使得不同尺度的语义都能丰富地展示。 作者还用一些例子强调了额外的语义分割监督的重要性。 3.Methods 3.1.Retina U-Net Retina Net 如上图所示，Retina Net是一个基于FPN的、简单的one-stage的检测网络。其中两个sub-networks分别在金字塔等级P3-P6上进行分类和边界框回归。这里Pj表示第j个解码器级别的特征映射，其中j随着分辨率的降低而增加。由于考虑到医学图像中小对象的存在，作者将sub-networks操作向一个金字塔级别转移到P2-P5，因为在更高分辨率的级别中产生了大量密集位置。 这样的结构类似于对称的U-Net结构，作者把它称作U-FPN。 作者在sub-networks的cl（分类）中把sigmoid non-linearity换成softmax（可解决由于3D图像中的非重叠目标而导致的类别互斥性），对于3D的实现，第一个网络的feature maps数量降低到64（可降低GPU内存开销）。 Adding Semantic Segmentation Supervision 添加了语义分割监督。在图中，可以看到绿色的箭头就是将Semantic Seg.的Signals通过P1和P0添加到top-down的path中，而且Signals还会跳接到bottom-up path中。 在图中的灰色箭头表示会在该层预测，而P0、P1这两层不预测，这使得在理论时间内参数的数量不变。 segmentation loss是根据P0logits计算的，它用到了逐像素交叉熵损失（分别检查每个像素，将类预测与热编码目标向量进行比较），还用到了Dice loss（在类别极不平衡的分割任务中能很稳定地训练）。（这两个都是医学分割中常用的损失函数） 其中，u是网络的softmax输出，v是ground truth的一个one hot编码。u和v都是大小为I x K的矩阵（i∈I是training batch中的像素数量，k∈K是类别）。 逐像素交叉熵损失（pixel-wide cross entropy） 逐像素交叉熵损失**单独评估每个像素矢量的类预测，然后对所有像素求均值，**我们可以认为图像中的像素被平等的学习了。但是由于医学图像的高分辨率，常出现类别不平衡的问题，所以，训练会被像素较多的类主导，对于较小的物体很难学到它的特征，网络的有效性会被降低。 Dice系数损失（Dice coefficient loss） dice系数源于二分类，本质上是为了衡量两个样本的重叠部分。范围为[0,1]，1表示完全重叠。它的计算公式是： D i c e = 2 ∣ A ∩ B ∣ ∣ A ∣ + ∣ B ∣ Dice=\\frac{2\\left | A\\cap B \\right |}{\\left | A \\right |+\\left | B \\right | } Dice=∣A∣+∣B∣2∣A∩B∣​ 其中，|A∩B|表示集合A、B之间的共同元素，|A|、|B|分别表示A、B集合中的元素个数。其中，分子的系数为2是由于分母存在重复计算A和B之间共同元素。 Dice loss： 为了形成可以最小化的损失函数，我们将简单地使用1-Dice。这种损失函数被称为 soft dice loss，因为我们直接使用预测概率而不是使用阈值或将它们转换为二进制mask。 dice loss比较适用于样本极度不均的情况，一般的情况下，使用 dice loss 会对反向传播造成不利的影响，容易使训练变得不稳定。 Weighted Box Clustering 由于医学图像的高分辨率及3D成像（MRI），需要对patch crops进行训练，从而需要在可用GPU内存限制与batch size和patch size之间权衡。 为了合并对目标检测的预测结果，作者提出了weighted box clustering（WBC），加权框聚类：这个算法与非极大值抑制算法（NMS）类似，根据IoU阈值进行聚类的预测，而非选择得分最高的候选框。 其中，os表示每个预测框的加权置信分数，oc表示每个坐标的加权平均值，i是聚类的下标，s是置信度分数，c是坐标。n missing表示对集群没有贡献一个框的视图的权重。 w是加权因子，包含： 重叠因子f：预测框与得分最高的框（softmax confidence）之间的重叠权重。 区域a：表明较大的框有较高的权重。 patch中心因子p：以patch中心的正态分布密度分配分数。 4.Experiments 4.1.Backbone&amp;Baselines 在这项研究中，作者将Retina U-Net与一组one-stage和two-stage物体检测器比较。为了公平地比较，所有的模型都用同一个框架（Pytorch）实现。都使用基于ResNet50的FPN作为共同的特征提取器。由于医学图像的目标相对较小，因此使用anchor都以4为因子，即对应金字塔{P2，P3，P4，P5}的anchor大小为{4²，8²，16²，32²}（2D）。而在3D中，anchor cube的大小设为{1，2，4，8}（沿z轴通常有较低的分辨率）。 Retina Net. 图c）Retina Net的实现与Retina U-Net中的相同。 Mask R-CNN. 图a）需要对3D进行微调：在RPN中的feature map降到64（可降低GPU内存开销），3D-RoIAlign的poolsize对classification head设置为（7，7，3），对mask head设置为（14，14，5）。positive候选框的IoU降到0.3。 Faster R-CNN+. 图b）为了挑出从Mask R-CNN中通过分割监督获得的性能增益，作者对toy data sets进行ablation（消融），并禁用mask-loss。从而有效地将模型减少到更快的R-CNN结构（除了RoIAlign操作，用+表示）。 因为Mask R-CNN大致是在Faster R-CNN上增加了分割。消融实验类似于控制变量。因为作者提出了一种方案，同时改变了多个条件/参数，他在接下去的消融实验中，会一一控制一个条件/参数不变，来看看结果，看到底是哪个条件/参数对结果的影响更大。 U-Faster R-CNN+. 图d）通过在U-FPN上部署Faster R-CNN来探索two-stage中额外语义分割的性能。 DetU-Net. 图e）使用UFPN实现类似U-Net的baseline，通过用1x1的卷积从P0中提取softmax预测，用于识别所有前景类的连通分量，通过连通分量画边界框（体），并将每个分量和类别的最高sofxmax概率指定为目标的得分。为了降低噪声，每个图像仅考虑5个（3D15个）最大分量。 4.2. Training&amp;Inference Setup 2D：slice-wise处理 2Dc：±3张相邻切片作为附加输入 3D：体积卷积 softmax probability解决分类损失的类不平衡。 Adam优化、0.0001的learning rate、5折交叉验证（train60%、val20%、test20%）、2Dbatchsize为20（3D为8）。 为防止过拟合，2D、3D都采用数据增强。 为了补偿小数据集的不稳定数据，通过执行测试时镜像以及根据验证度选出5个最高得分的多个模型进行测试。 通过对得分和坐标进行加权聚类，来合并来自全部成员（ensemble-members）和重叠区的预测框。 应用非极大值抑制（NMS）的自适应来合并从2D到3D框的预测：将所有切片的框投影到一个平面中，同时保留切片原点信息。应用NMS时，只有与最高得分框的切片直接或间接相连的框才被视为匹配。将所有匹配的最小最大切片编号指定为预测立方体的z坐标的结果。 4.3.Evaluation 使用mAP（平均精确度）评估实验。在IoU=0.1的阈值处的相对较低匹配交叉点处确定mAP。 4.4.Lung nodule detection and categorization 肺结节的检测与分类，检测病变是良性还是恶性。细粒度分类有望在不断增长的数据集和图像分辨率的背景下获得相关性。 4.4.1 Utilized data set 使用公开数据集LIDC-ICRI（1035个肺部CT扫描）。使用数字1-5表示恶性的可能性等级。良性标记（1-2，n=1319）、恶性标记（3-5，n=494）。CT扫描重采样为0.7x0.7x1.25mm，大致对应于数据集的平均分辨率。训练时，patches大小为288x288（2D），128x128x64（3D）。 4.4.2 Results 4.5.Breast lesion detection and categorization 乳房病变检测。 4.5.1 Utilized data set 使用的数据集是在331名患者的扩散MRI内部数据集上进行的，这些患者在之前的乳房X线照相术中有可能患病的风险。良性标注n=141，恶性标注n=190。图像重采样为1.25x1.25x3mm。训练时，patches大小为160x160（2D），160x160x56（3D）。 4.5.2 Results 4.6.Toy Experiments 这一系列的toy experiments主要是来处理医学上对象分类的子任务，如区分尺度，形状和强度。更具体是为了研究在有限的训练数据下完全分割监督的重要性。这里有三个子任务，且每个任务逐渐减少训练数据量: 1.Distinguishing object shapes: 检测和区分两类物体的形状。a）图：第一类是由直径为20像素的圆形组成，第二类是由20像素的圆形和直径为4像素的中心组成，类似于甜甜圈状。预计完全语义监督将在此任务中产生显着的性能提升，特别是在小数据中。 2.Learning discriminative object patterns: 这个任务与前一任务相同，图a）除了中心孔没有从甜甜圈的segmentation masks中切除。这需要模型拾取判别模式（孔），而不是通过mask的形状明确指出它。 在医学图像的背景下，这种设置可以被认为是更真实的。 3.Distinguishing object scales: 检测和区分两类物体的尺度。图b）第一类由直径为19像素的圆组成，第二类由直径为20像素的圆组成。类信息完全以对象比例编码，因此在目标框坐标中编码。预计语义监督不会带来重大收益。 4.6.1 Utilized data set 使用的两个数据集均为由人工生成的320x320的2D图像组成，1000个用于训练，500个用于验证，另外1000个用于测试。 4.6.2 Results 5.Conclusion 1.输入维度上利用语义分割的重要性，并与流行的目标检测模型进行了细致的比较，特别强调了有上下联系的有限训练数据。 2.在公开可用的LIDC-IDRI肺CT数据集以及内部的乳房病变MRI数据集中，Retina U-Net产生的检测性能优于没有完全分割监督的模型。 3.正如标题所说，Retina U-Net，在其令人尴尬的简单的架构中利用监督的语义分割来解决医学图像较大而目标较小的问题。","@type":"BlogPosting","url":"https://uzzz.org/2019/08/17/794022.html","headline":"Retina U-Net论文简析","dateModified":"2019-08-17T00:00:00+08:00","datePublished":"2019-08-17T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/17/794022.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Retina U-Net论文简析</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>Contents</h3>
   <ul>
    <li><a href="#Links_1" rel="nofollow" data-token="9d99b4dec390d54b6455ef510a2abd39">Links</a></li>
    <li><a href="#Title_4" rel="nofollow" data-token="b320d47b9de6a5ccc044a85276fd5185">Title</a></li>
    <li><a href="#Abstract_9" rel="nofollow" data-token="beb2b016fabd2e6c2279a56f3f33d578">Abstract</a></li>
    <li><a href="#1Introduction_18" rel="nofollow" data-token="0d18677700815ccd02c0ac5a4b46edf5">1.Introduction</a></li>
    <li><a href="#2Related_work_46" rel="nofollow" data-token="187908086b15dc97a415c385134e299c">2.Related work</a></li>
    <li><a href="#3Methods_64" rel="nofollow" data-token="5de17c4f5a04114cfd976ddd999acd5c">3.Methods</a></li>
    <ul>
     <li><a href="#31Retina_UNet_66" rel="nofollow" data-token="d77ffb61a2b16b051d1c152b9a27be41">3.1.Retina U-Net</a></li>
     <ul>
      <li><a href="#Retina_Net_68" rel="nofollow" data-token="632753a356a4ee06fa68ea9a6de86935">Retina Net</a></li>
      <li><a href="#Adding_Semantic_Segmentation_Supervision_78" rel="nofollow" data-token="635f89d5b302dee9201b94e155b33274">Adding Semantic Segmentation Supervision</a></li>
      <ul>
       <li><a href="#pixelwide_cross_entropy_90" rel="nofollow" data-token="defa4b22bfdec31b446f8596e8d6970a">逐像素交叉熵损失（pixel-wide cross entropy）</a></li>
       <li><a href="#DiceDice_coefficient_loss_96" rel="nofollow" data-token="2c67bcbf1c2fe23aac6886742e432045">Dice系数损失（Dice coefficient loss）</a></li>
      </ul>
      <li><a href="#Weighted_Box_Clustering_115" rel="nofollow" data-token="cac0085dc0dfd96987438cef611bedf3">Weighted Box Clustering</a></li>
     </ul>
    </ul>
    <li><a href="#4Experiments_135" rel="nofollow" data-token="7f54852bdb100c195cd54b6366ada438">4.Experiments</a></li>
    <ul>
     <li><a href="#41BackboneBaselines_137" rel="nofollow" data-token="345ee92e62b6bf11064a7b25a75bfa13">4.1.Backbone&amp;Baselines</a></li>
     <ul>
      <li><a href="#Retina_Net_143" rel="nofollow" data-token="54d1d3b70ac3ae65bbb6c4a6964279ef">Retina Net.</a></li>
      <li><a href="#Mask_RCNN_147" rel="nofollow" data-token="03736d05d44373a55bb348dfff69bfc6">Mask R-CNN.</a></li>
      <li><a href="#Faster_RCNN_151" rel="nofollow" data-token="e8f6b9697d701b830f0d9756a4002ada">Faster R-CNN+.</a></li>
      <li><a href="#UFaster_RCNN_157" rel="nofollow" data-token="552a80198636d999216af1e4a5ebb951">U-Faster R-CNN+.</a></li>
      <li><a href="#DetUNet_161" rel="nofollow" data-token="b46272f7bc980b876ed02357a41a7cfa">DetU-Net.</a></li>
     </ul>
     <li><a href="#42_TrainingInference_Setup_165" rel="nofollow" data-token="cce911bae83d4c7b05892f6cca409361">4.2. Training&amp;Inference Setup</a></li>
     <li><a href="#43Evaluation_177" rel="nofollow" data-token="fc6508e7525c1dbadc07becc4ad63bfe">4.3.Evaluation</a></li>
     <li><a href="#44Lung_nodule_detection_and_categorization_181" rel="nofollow" data-token="2e41fcfe438ada2b71e2721bcef2fe9b">4.4.Lung nodule detection and categorization</a></li>
     <ul>
      <li><a href="#441_Utilized_data_set_185" rel="nofollow" data-token="115319a3510cf17fd93f0b282719e826">4.4.1 Utilized data set</a></li>
      <li><a href="#442_Results_189" rel="nofollow" data-token="586d697eefaee1e4f06af2c58e15cbd3">4.4.2 Results</a></li>
     </ul>
     <li><a href="#45Breast_lesion_detection_and_categorization_193" rel="nofollow" data-token="150bcd6c4d6b103870a5ae220a37e31d">4.5.Breast lesion detection and categorization</a></li>
     <ul>
      <li><a href="#451_Utilized_data_set_197" rel="nofollow" data-token="96a402b950da7c8e6df8f08c9c748d85">4.5.1 Utilized data set</a></li>
      <li><a href="#452_Results_201" rel="nofollow" data-token="88bdd1ba88fe23f89e910d292d18c380">4.5.2 Results</a></li>
     </ul>
     <li><a href="#46Toy_Experiments_205" rel="nofollow" data-token="880ae7ecebc0ae58a31047f0d6e405df">4.6.Toy Experiments</a></li>
     <ul>
      <ul>
       <li><a href="#1Distinguishing_object_shapes_209" rel="nofollow" data-token="b88df278e0a13a2da4e22a3ab32c33c8">1.Distinguishing object shapes:</a></li>
       <li><a href="#2Learning_discriminative_object_patterns_215" rel="nofollow" data-token="0e878216775abaf261459e8758bfacca">2.Learning discriminative object patterns:</a></li>
       <li><a href="#3Distinguishing_object_scales_221" rel="nofollow" data-token="4cf3a380991b6f2d49f24270e2223b95">3.Distinguishing object scales:</a></li>
      </ul>
      <li><a href="#461_Utilized_data_set_225" rel="nofollow" data-token="067680e7bef5e19b49f3f4aa91fdc2cf">4.6.1 Utilized data set</a></li>
      <li><a href="#462_Results_229" rel="nofollow" data-token="b04b7b8d9bfc42784463dee78c2cba0b">4.6.2 Results</a></li>
     </ul>
    </ul>
    <li><a href="#5Conclusion_233" rel="nofollow" data-token="7f726c743e07de601acfdeefd254dd24">5.Conclusion</a></li>
   </ul>
  </div>
  <p></p> 
  <h1><a id="Links_1"></a>Links</h1> 
  <p>paper: <a href="https://arxiv.org/pdf/1811.08661.pdf" rel="nofollow" data-token="3fbb2894a4a7d90593784576291053ad">https://arxiv.org/pdf/1811.08661.pdf</a><br> github: <a href="https://github.com/pfjaeger/medicaldetectiontoolkit" rel="nofollow" data-token="393927f95eaf2b92dafc70e3210b7f53">https://github.com/pfjaeger/medicaldetectiontoolkit</a></p> 
  <h1><a id="Title_4"></a>Title</h1> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190727154334129.png" alt="Retina U-Net：Embarrassingly Simple Exploitation of Segmentation Supervision for Medical Object Detection"></p> 
  <p>直译过来是说这个网络是对监督学习下医学目标检测的分割的简单开发，但是还有一个词<strong>embarrassingly</strong>（令人尴尬的），这就有点令人费解了。那就先读文章慢慢理解吧。</p> 
  <h1><a id="Abstract_9"></a>Abstract</h1> 
  <p>作者比较了语义分割（semantic segmentation）和state-of-the-art detectors。</p> 
  <ul> 
   <li><strong>semantic segmentation</strong>：通过预测pixel-level scores，再由特殊的启发式算法，映射到object-level scores。</li> 
   <li><strong>state-of-the-art</strong>：end-to-end（端到端）的单个物体scoring，但这是利用逐像素监督标注得到的。这点对医学图像分割尤其不利，因为医学的数据集都是建立在像素级别的。</li> 
  </ul> 
  <p>作者提出的<strong>Retina U-Net</strong>：自然地<strong>融合了Retina Net这个one-stage detector和U-Net 结构</strong>，广泛用于医学图像语义分割。这个网络通过辅助的语义分割补充丢失的监督标记，而且不会引入之前提出的two-stage detectors的复杂性。作者还说这个网络强大的检测性能只有更复杂的two-stage同类才能与其媲美。</p> 
  <h1><a id="1Introduction_18"></a>1.Introduction</h1> 
  <p>主要提到了U-Net，object-level，粗略水平的检测，两种应用于object-level预测的方法（two-stage detectors和one-stage detectors）及作者的做法。</p> 
  <ul> 
   <li> <p><strong>U-Net</strong>网络是一种<strong>语义分割算法</strong>，目前在临床中已形成了一种标准，这归功于医学图像通常是用像素级别的标注标识感兴趣目标的结果，它们也很好地应用于逐像素监督学习的分割场景。不仅如此，MRI和CT成像还能捕获3D空间的影像，包括空间中的物体分割，因此不需要区分（重叠的）实例。</p> </li> 
   <li> <p>尽管像放疗或肿瘤生长态势探测在临床上需要像素级别的检测，在大多情况下，<strong>粗略水平下的的检测</strong>和知道要探测的东西存在是有联系的。在评估object-level大小的模型时，这种联系在研究设计中得到了最佳反映。<strong>为了弥合（bridge）语义分割pixel-level的检测和object-level的评估，必须要引入特殊的启发式算法或是其他模型</strong>。</p> </li> 
  </ul> 
  <p>通过<strong>从粗略水平级别提取预测信息</strong>到实现end-to-end的object scoring，大多目前的目标检测都基于<strong>FPN（feature pyramid network）</strong>。</p> 
  <ul> 
   <li>现有两种方法应用于object-level检测，一是<strong>two-stage detectors</strong>：无论第一种物体是什么类都与背景相区别，随着边界框回归生成大小可变的候选框，然后候选框重采样为大小固定的网格以确定分类比例的不变性。二是<strong>one-stage detectors</strong>：在粗略水平可立即分类出物体的类别。但这种在粗略水平检测会导致信息的丢失，这又与医学领域相对level较小的数据集的数据得到有效训练的需求相悖。</li> 
  </ul> 
  <p>作者表明，充分利用可用的语义分割标注可以使医学图像上的检测性能显著提升。此外，作者认为two-stage detectors中的重采样操作在医学领域中并无帮助，因为<strong>与自然图像不同，是语义编码信息描述了比例的变化，而非物体和相机间的距离变化。</strong></p> 
  <p>作者的做法是<strong>通过高分辨率补充FPN自下而上的部分（以学习语义分割），通过在U-Net的decoder部分的粗略水平上的两个子网络来改进U-Net</strong>（以允许end-to-end的object-scoring）。</p> 
  <p>作者通过两个数据集评估自己的模型。一个是<strong>LIDC-IDRI【Lung-CT】（一个公开的数据集）</strong>，另一个是<strong>Diffusion-MRI（一个内部的数据集）</strong>。</p> 
  <p>这篇论文有以下的<strong>contributions</strong>：</p> 
  <ul> 
   <li> <p>一种简单有效且侧重于医学图像中的应用，可用于通过语义分割训练识别标注。</p> </li> 
   <li> <p>对流行的object detectors（无论在2D还是3D）进行了深入分析。</p> </li> 
   <li> <p>一种名为weighted box clustering的算法，可用于在2D和3D中合并同一图像的不同检测结果。</p> </li> 
   <li> <p>这是一个全面的框架。</p> </li> 
  </ul> 
  <h1><a id="2Related_work_46"></a>2.Related work</h1> 
  <p>许多two-stage detectors学习通过基于proposal的分割，但是，作者质疑这种做法，因为它没有充分地利用语义分割监督。有如下<strong>原因</strong>：</p> 
  <ul> 
   <li> <p>仅在裁剪的候选区上评估mask loss，也就是说，周围区域的前后梯度不反向传播。</p> </li> 
   <li> <p>proposal region（候选区）和ground truth mask通常被重采样为大小固定的网格（RoIAlign）。</p> </li> 
   <li> <p>只有正例的候选区被用于mask loss，这会导致对候选区表现的依赖。</p> </li> 
   <li> <p>mask loss的梯度不会流过整个模型，而只从相应的pyramid（金字塔）级向上流动。</p> </li> 
  </ul> 
  <p>而在one-stage领域，Uhrig等人在SSD上进行语义分割并用于实例分割，这与作者的做法很类似，其中分割输出在后处理中分配给候选框。Zhang等人也提出了类似的结构，但是是使用边界框标注创建的masks，以弱监督方式学习分割。</p> 
  <p>与用于特征提取的自下而上的主干网络相反，作者遵循了FPN的方法，即有自上而下（decoder）的路径使得不同尺度的语义都能丰富地展示。</p> 
  <p>作者还用一些例子强调了<strong>额外的语义分割监督的重要性</strong>。</p> 
  <h1><a id="3Methods_64"></a>3.Methods</h1> 
  <h2><a id="31Retina_UNet_66"></a>3.1.Retina U-Net</h2> 
  <h3><a id="Retina_Net_68"></a>Retina Net</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190727205015645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDg0NzU2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>如上图所示，<strong>Retina Net是一个基于FPN的、简单的one-stage的检测网络</strong>。其中<strong>两个sub-networks分别在金字塔等级P3-P6上进行分类和边界框回归</strong>。这里Pj表示第j个解码器级别的特征映射，其中j随着分辨率的降低而增加。由于考虑到医学图像中小对象的存在，作者将<strong>sub-networks操作向一个金字塔级别转移到P2-P5</strong>，因为在更高分辨率的级别中产生了大量密集位置。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190727205147101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDg0NzU2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 这样的<strong>结构类似于对称的U-Net结构</strong>，作者把它称作U-FPN。</p> 
  <p>作者<strong>在sub-networks的cl（分类）中把sigmoid non-linearity换成softmax</strong>（可解决由于3D图像中的非重叠目标而导致的类别互斥性），对于3D的实现，第一个网络的feature maps数量降低到64（可降低GPU内存开销）。</p> 
  <h3><a id="Adding_Semantic_Segmentation_Supervision_78"></a>Adding Semantic Segmentation Supervision</h3> 
  <p><strong>添加了语义分割监督。<strong>在图中，可以看到</strong>绿色的箭头就是将Semantic Seg.的Signals通过P1和P0添加到top-down的path中，而且Signals还会跳接到bottom-up path中。</strong></p> 
  <p>在图中的<strong>灰色箭头表示会在该层预测，而P0、P1这两层不预测，这使得在理论时间内参数的数量不变。</strong></p> 
  <p><strong>segmentation loss是根据P0logits计算的</strong>，它用到了<strong>逐像素交叉熵损失</strong>（分别检查每个像素，将类预测与热编码目标向量进行比较），还用到了<strong>Dice loss</strong>（在类别极不平衡的分割任务中能很稳定地训练）。（这两个都是医学分割中常用的损失函数）</p> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy50b25ndGlhbnRhLnNpdGUvcGFwZXJfaW1hZ2UvMTEzMGFhYTQtYTdhYS0xMWU5LTgwYzQtMDAxNjNlMDhiYjg2L1BBUkFHUkFQSExBVEVYMjIwLmpwZw" alt="img"></p> 
  <p>其中，<strong>u是网络的softmax输出，v是ground truth的一个one hot编码。u和v都是大小为I x K的矩阵（i∈I是training batch中的像素数量，k∈K是类别）。</strong></p> 
  <h4><a id="pixelwide_cross_entropy_90"></a>逐像素交叉熵损失（pixel-wide cross entropy）</h4> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190721165836349.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDg0NzU2,size_16,color_FFFFFF,t_70" alt="[(img-NPMZlxGJ-1563699455544)(https://www.jeremyjordan.me/content/images/2018/05/Screen-Shot-2018-05-24-at-10.46.16-PM.png)]"></p> 
  <p>逐像素交叉熵损失**单独评估每个像素矢量的类预测，然后对所有像素求均值，**我们可以认为图像中的像素被平等的学习了。但是由于医学图像的高分辨率，常出现类别不平衡的问题，所以，训练会被像素较多的类主导，对于较小的物体很难学到它的特征，网络的有效性会被降低。</p> 
  <h4><a id="DiceDice_coefficient_loss_96"></a>Dice系数损失（Dice coefficient loss）</h4> 
  <p>dice系数源于二分类，<strong>本质上是为了衡量两个样本的重叠部分</strong>。范围为[0,1]，1表示完全重叠。它的计算公式是：</p> 
  <p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
       <math>
        <semantics>
         <mrow>
          <mi>
           D
          </mi>
          <mi>
           i
          </mi>
          <mi>
           c
          </mi>
          <mi>
           e
          </mi>
          <mo>
           =
          </mo>
          <mfrac>
           <mrow>
            <mn>
             2
            </mn>
            <mrow>
             <mo fence="true">
              ∣
             </mo>
             <mi>
              A
             </mi>
             <mo>
              ∩
             </mo>
             <mi>
              B
             </mi>
             <mo fence="true">
              ∣
             </mo>
            </mrow>
           </mrow>
           <mrow>
            <mrow>
             <mo fence="true">
              ∣
             </mo>
             <mi>
              A
             </mi>
             <mo fence="true">
              ∣
             </mo>
            </mrow>
            <mo>
             +
            </mo>
            <mrow>
             <mo fence="true">
              ∣
             </mo>
             <mi>
              B
             </mi>
             <mo fence="true">
              ∣
             </mo>
            </mrow>
           </mrow>
          </mfrac>
         </mrow>
         <annotation encoding="application/x-tex">
           Dice=\frac{2\left | A\cap B \right |}{\left | A \right |+\left | B \right | } 
         </annotation>
        </semantics>
       </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.02778em;">D</span><span class="mord mathit">i</span><span class="mord mathit">c</span><span class="mord mathit">e</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.363em; vertical-align: -0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.427em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="minner"><span class="mopen delimcenter" style="top: 0em;">∣</span><span class="mord mathit">A</span><span class="mclose delimcenter" style="top: 0em;">∣</span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">∣</span><span class="mord mathit" style="margin-right: 0.05017em;">B</span><span class="mclose delimcenter" style="top: 0em;">∣</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">2</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">∣</span><span class="mord mathit">A</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathit" style="margin-right: 0.05017em;">B</span><span class="mclose delimcenter" style="top: 0em;">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.936em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
  <p>其中，|A∩B|表示集合A、B之间的共同元素，|A|、|B|分别表示A、B集合中的元素个数。其中，分子的系数为2是由于分母存在重复计算A和B之间共同元素。</p> 
  <p><strong>Dice loss</strong>：</p> 
  <p>为了形成<strong>可以最小化的损失函数</strong>，我们将简单地使用<strong>1-Dice</strong>。这种损失函数被称为 <strong>soft dice loss</strong>，因为我们<strong>直接使用预测概率</strong>而不是使用阈值或将它们转换为二进制mask。</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/201907211700091.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMDg0NzU2,size_16,color_FFFFFF,t_70" alt="["></p> 
  <p>dice loss比较<strong>适用于样本极度不均的情况</strong>，一般的情况下，使用 dice loss 会对反向传播造成不利的影响，容易使训练变得不稳定。</p> 
  <h3><a id="Weighted_Box_Clustering_115"></a>Weighted Box Clustering</h3> 
  <p>由于<strong>医学图像的高分辨率及3D成像（MRI）</strong>，需要<strong>对patch crops进行训练</strong>，从而需要<strong>在可用GPU内存限制与batch size和patch size之间权衡</strong>。</p> 
  <p>为了<strong>合并对目标检测的预测结果</strong>，作者提出了<strong>weighted box clustering（WBC），加权框聚类</strong>：这个算法与非极大值抑制算法（NMS）类似，<strong>根据IoU阈值进行聚类的预测</strong>，而非选择得分最高的候选框。</p> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy50b25ndGlhbnRhLnNpdGUvcGFwZXJfaW1hZ2UvMTEzMGFhYTQtYTdhYS0xMWU5LTgwYzQtMDAxNjNlMDhiYjg2L1BBUkFHUkFQSExBVEVYMzEwLmpwZw" alt="img"></p> 
  <p>其中，<strong>os表示每个预测框的加权置信分数，oc表示每个坐标的加权平均值，i是聚类的下标，s是置信度分数，c是坐标。n missing表示对集群没有贡献一个框的视图的权重。</strong></p> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy50b25ndGlhbnRhLnNpdGUvcGFwZXJfaW1hZ2UvMTEzMGFhYTQtYTdhYS0xMWU5LTgwYzQtMDAxNjNlMDhiYjg2L0lOTkVSTEFURVhUMzE0LmpwZw" alt="img"></p> 
  <p><strong>w是加权因子</strong>，包含：</p> 
  <p><strong>重叠因子f</strong>：预测框与得分最高的框（softmax confidence）之间的重叠权重。</p> 
  <p><strong>区域a</strong>：表明较大的框有较高的权重。</p> 
  <p><strong>patch中心因子p</strong>：以patch中心的正态分布密度分配分数。</p> 
  <h1><a id="4Experiments_135"></a>4.Experiments</h1> 
  <h2><a id="41BackboneBaselines_137"></a>4.1.Backbone&amp;Baselines</h2> 
  <p>在这项研究中，作者将Retina U-Net与一组one-stage和two-stage物体检测器比较。为了公平地比较，<strong>所有的模型都用同一个框架（Pytorch）实现</strong>。都<strong>使用基于ResNet50的FPN作为共同的特征提取器</strong>。由于医学图像的目标相对较小，因此使用<strong>anchor都以4为因子，即对应金字塔{P2，P3，P4，P5}的anchor大小为{4²，8²，16²，32²}（2D）。而在3D中，anchor cube的大小设为{1，2，4，8}</strong>（沿z轴通常有较低的分辨率）。</p> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy50b25ndGlhbnRhLnNpdGUvcGFwZXJfaW1hZ2UvMTEzMGFhYTQtYTdhYS0xMWU5LTgwYzQtMDAxNjNlMDhiYjg2L2ltYWdlXzRfMF8wLmpwZw" alt="img"></p> 
  <h3><a id="Retina_Net_143"></a>Retina Net.</h3> 
  <p>图c）Retina Net的实现与Retina U-Net中的相同。</p> 
  <h3><a id="Mask_RCNN_147"></a>Mask R-CNN.</h3> 
  <p>图a）需要<strong>对3D进行微调</strong>：在RPN中的feature map降到64（可降低GPU内存开销），3D-RoIAlign的poolsize对classification head设置为（7，7，3），对mask head设置为（14，14，5）。positive候选框的IoU降到0.3。</p> 
  <h3><a id="Faster_RCNN_151"></a>Faster R-CNN+.</h3> 
  <p>图b）为了挑出从Mask R-CNN中通过分割监督获得的性能增益，作者对toy data sets进行ablation（消融），并<strong>禁用mask-loss</strong>。从而有效地将模型减少到更快的R-CNN结构（除了RoIAlign操作，用+表示）。</p> 
  <p>因为Mask R-CNN大致是在Faster R-CNN上增加了分割。<strong>消融实验</strong>类似于控制变量。因为作者提出了一种方案，同时改变了多个条件/参数，他在接下去的消融实验中，会一一控制一个条件/参数不变，来看看结果，看到底是哪个条件/参数对结果的影响更大。</p> 
  <h3><a id="UFaster_RCNN_157"></a>U-Faster R-CNN+.</h3> 
  <p>图d）通过在U-FPN上部署Faster R-CNN来探索two-stage中额外语义分割的性能。</p> 
  <h3><a id="DetUNet_161"></a>DetU-Net.</h3> 
  <p>图e）使用UFPN实现类似U-Net的baseline，通过用1x1的卷积从P0中提取softmax预测，用于识别所有前景类的连通分量，通过连通分量画边界框（体），并将每个分量和类别的最高sofxmax概率指定为目标的得分。为了降低噪声，每个图像仅考虑5个（3D15个）最大分量。</p> 
  <h2><a id="42_TrainingInference_Setup_165"></a>4.2. Training&amp;Inference Setup</h2> 
  <ul> 
   <li>2D：slice-wise处理</li> 
   <li><strong>2Dc：±3张相邻切片作为附加输入</strong></li> 
   <li>3D：体积卷积</li> 
   <li>softmax probability解决分类损失的类不平衡。</li> 
   <li>Adam优化、0.0001的learning rate、5折交叉验证（train60%、val20%、test20%）、<strong>2Dbatchsize为20（3D为8）</strong>。</li> 
   <li>为防止过拟合，2D、3D都采用<strong>数据增强</strong>。</li> 
   <li>为了补偿小数据集的不稳定数据，通过执行测试时镜像以及根据验证度选出5个最高得分的多个模型进行测试。</li> 
   <li>通过对得分和坐标进行加权聚类，来<strong>合并来自全部成员（ensemble-members）和重叠区的预测框</strong>。</li> 
   <li>应用<strong>非极大值抑制</strong>（NMS）的自适应来合并从2D到3D框的预测：将所有切片的框投影到一个平面中，同时保留切片原点信息。<strong>应用NMS时，只有与最高得分框的切片直接或间接相连的框才被视为匹配</strong>。将所有匹配的最小最大切片编号指定为预测立方体的z坐标的结果。</li> 
  </ul> 
  <h2><a id="43Evaluation_177"></a>4.3.Evaluation</h2> 
  <p>使用<strong>mAP（平均精确度）评估实验</strong>。在<strong>IoU=0.1的阈值</strong>处的相对较低匹配交叉点处确定mAP。</p> 
  <h2><a id="44Lung_nodule_detection_and_categorization_181"></a>4.4.Lung nodule detection and categorization</h2> 
  <p>肺结节的检测与分类，检测病变是良性还是恶性。细粒度分类有望在不断增长的数据集和图像分辨率的背景下获得相关性。</p> 
  <h3><a id="441_Utilized_data_set_185"></a>4.4.1 Utilized data set</h3> 
  <p>使用公开数据集<strong>LIDC-ICRI（1035个肺部CT扫描）</strong>。使用<strong>数字1-5表示恶性的可能性等级</strong>。良性标记（1-2，n=1319）、恶性标记（3-5，n=494）。<strong>CT扫描重采样为0.7x0.7x1.25mm</strong>，大致对应于数据集的平均分辨率。<strong>训练时，patches大小为288x288（2D），128x128x64（3D）。</strong></p> 
  <h3><a id="442_Results_189"></a>4.4.2 Results</h3> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy50b25ndGlhbnRhLnNpdGUvcGFwZXJfaW1hZ2UvMTEzMGFhYTQtYTdhYS0xMWU5LTgwYzQtMDAxNjNlMDhiYjg2L2xpbmV0YWJsZV80XzJfMC5qcGc" alt="img"></p> 
  <h2><a id="45Breast_lesion_detection_and_categorization_193"></a>4.5.Breast lesion detection and categorization</h2> 
  <p>乳房病变检测。</p> 
  <h3><a id="451_Utilized_data_set_197"></a>4.5.1 Utilized data set</h3> 
  <p>使用的数据集是在331名患者的<strong>扩散MRI内部数据集</strong>上进行的，这些患者在之前的乳房X线照相术中有可能患病的风险。良性标注n=141，恶性标注n=190。<strong>图像重采样为1.25x1.25x3mm</strong>。<strong>训练时，patches大小为160x160（2D），160x160x56（3D）</strong>。</p> 
  <h3><a id="452_Results_201"></a>4.5.2 Results</h3> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy50b25ndGlhbnRhLnNpdGUvcGFwZXJfaW1hZ2UvMTEzMGFhYTQtYTdhYS0xMWU5LTgwYzQtMDAxNjNlMDhiYjg2L2xpbmV0YWJsZV81XzFfMC5qcGc" alt="img"></p> 
  <h2><a id="46Toy_Experiments_205"></a>4.6.Toy Experiments</h2> 
  <p>这一系列的toy experiments主要是来处理医学上对象分类的子任务，如区分尺度，形状和强度。更具体是为了研究在有限的训练数据下完全分割监督的重要性。这里有三个子任务，且<strong>每个任务逐渐减少训练数据量</strong>:</p> 
  <h4><a id="1Distinguishing_object_shapes_209"></a>1.Distinguishing object shapes:</h4> 
  <p><strong>检测和区分两类物体的形状</strong>。a）图：第一类是由直径为20像素的圆形组成，第二类是由20像素的圆形和直径为4像素的中心组成，类似于甜甜圈状。<strong>预计完全语义监督将在此任务中产生显着的性能提升，特别是在小数据中</strong>。</p> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy50b25ndGlhbnRhLnNpdGUvcGFwZXJfaW1hZ2UvMTEzMGFhYTQtYTdhYS0xMWU5LTgwYzQtMDAxNjNlMDhiYjg2L2ltYWdlXzdfMl8wLmpwZw" alt="img"></p> 
  <h4><a id="2Learning_discriminative_object_patterns_215"></a>2.Learning discriminative object patterns:</h4> 
  <p>这个任务与前一任务相同，图a）除了中心孔没有从甜甜圈的segmentation masks中切除。这需要<strong>模型拾取判别模式（孔）</strong>，而不是通过mask的形状明确指出它。</p> 
  <p>在医学图像的背景下，这种设置可以被认为是<strong>更真实</strong>的。</p> 
  <h4><a id="3Distinguishing_object_scales_221"></a>3.Distinguishing object scales:</h4> 
  <p><strong>检测和区分两类物体的尺度</strong>。图b）第一类由直径为19像素的圆组成，第二类由直径为20像素的圆组成。类信息完全以对象比例编码，因此在目标框坐标中编码。<strong>预计语义监督不会带来重大收益。</strong></p> 
  <h3><a id="461_Utilized_data_set_225"></a>4.6.1 Utilized data set</h3> 
  <p>使用的<strong>两个数据集均为由人工生成的320x320的2D图像组成，1000个用于训练，500个用于验证，另外1000个用于测试。</strong></p> 
  <h3><a id="462_Results_229"></a>4.6.2 Results</h3> 
  <p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy50b25ndGlhbnRhLnNpdGUvcGFwZXJfaW1hZ2UvMTEzMGFhYTQtYTdhYS0xMWU5LTgwYzQtMDAxNjNlMDhiYjg2L2ltYWdlXzdfMF8wLmpwZw" alt="img"></p> 
  <h1><a id="5Conclusion_233"></a>5.Conclusion</h1> 
  <p>1.输入维度上利用<strong>语义分割的重要性</strong>，并与流行的目标检测模型进行了细致的比较，特别强调了<strong>有上下联系的有限训练数据</strong>。</p> 
  <p>2.在公开可用的LIDC-IDRI肺CT数据集以及内部的乳房病变MRI数据集中，Retina U-Net产生的检测性能优于没有完全分割监督的模型。</p> 
  <p>3.正如标题所说，Retina U-Net，在其<strong>令人尴尬的简单的架构中利用监督的语义分割</strong>来解决医学图像较大而目标较小的问题。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
