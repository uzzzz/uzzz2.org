<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>2019移动广告反欺诈算法挑战赛baseline | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="2019移动广告反欺诈算法挑战赛baseline" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="前言： 分享这个baseline之前，首先先感谢一下我的好朋友油菜花一朵给予的一些帮助。然后呢介绍一下最近比赛中碰到的几个问题，以及解释。如果有可能的话，明天分享一个94.47左右的baseline吧，初赛之前设置为粉丝可见，初赛后在设置所有人可见吧。本来想分享47的baseline的，但是后来发现版本找不到了。就把自己的想法融合了一下，也不知道多少分。比赛名次不重要学到东西才重要。 &nbsp; 第一：为什么使用kaggle上的方案效果不好 因为科大讯飞的移动广告反欺诈算法挑战赛和之前kaggle上的‘TalkingData AdTracking Fraud Detection Challenge’比赛题目是一样的，和kaggle上不同的是科大讯飞增加了一些属性。所以有很多人都是仿照kaggle上的比赛进行的，但是发现效果不是很好。这个原因是由于kaggle上的广告欺诈是最后的标签是由TalkingData公司自己的算法生成的，最后公司也说了他们的算法中加上了时间信息。所以这也是为什么几乎每个开源代码都统计了很多关于时间的特征。 &nbsp; 第二：catboost的baseline抖动特别厉害 训练集的数据100万条，测试集仅仅只有10万条。这可能是由于训练集和测试集的数据都太少了导致的。所以同样的模型，相同的随机种子最后生成的结果差距也是有的。当跑的代码不是很好的时候，可以尝试一下再跑一次。 &nbsp; 第三： 为什么数据清洗之后会更差 我们发现最后训练的模型线上结果好的时候，往往模型中单个属性是比较强的特征。所以我想会不会科大讯飞官方的label也是由自己模型生成的，而且训练的时候没有对数据做过多的处理。如果我们要数据清洗的话，我们清理之后的属性一定要比之前的强。 &nbsp; 第四： 一些强特征加上关于label的统计之后线下会很好，但是线上提分不是很高 这个我一直很好奇，在我的模型中得到一些强特征之后我统计一下，我尝试使用统计这些特征关于label的均值和方差，最后显示特征的重要程度的时候，这些也都是不错的特征，但是最后线上提交的时候却发现增加分数不是很明显。这个可能是catboost的特征对于label而言比较敏感，所以可以换成其他的模型例如lgb试一下。 &nbsp; 第五： 加上时间数据之后线下很好但是线上不好 之前我统计同一个model前一次点击时间，和之后的点击时间差，但是同样是线下有提升，但是线上效果不好，我之前是以为数据泄露，但是按照网上的一些教程修改之后效果依旧存在同样的问题。暂时没有想到解释的方案。 &nbsp; 第六：介绍一下我这个baseline的特点吧 1、基本架构使用的是catboost的baseline 2、增加了一些关于强特征关于label的统计特征 例如均值 方差 出现次数 3、使用make的均值填充 h, w, ppi 4、对于一些强特征进行组合，统计组合特征的出现次数count()，以及累积计数cumcount() 5、为了节约内存，优化训练过程，删除一些不必要的,输出信息 6、删除了catboost模型中一些不必要的特征 7、使用相同的特征，但是加上xgb, lgb, catboost进行stacking， 最后使用logist进行回归分析 &nbsp; 代码： # -*- coding: utf-8 -*- # @Time : 2019/8/18 9:28 # @Author : YYLin # @Email : 854280599@qq.com # @File : A_Simple_Stacking_Model.py # 特征部分选择使用之前简单的特征 加上lgb catboost xgb进行stacking操作 分数大约46 import numpy as np import pandas as pd import gc from tqdm import tqdm from sklearn.model_selection import KFold from sklearn.preprocessing import LabelEncoder from datetime import timedelta import catboost as cbt import lightgbm as lgb import xgboost as xgb from sklearn.metrics import f1_score from sklearn.linear_model import LogisticRegression import warnings warnings.filterwarnings(&#39;ignore&#39;) from scipy import stats test = pd.read_table(&quot;../A_Data/testdata.txt&quot;, nrows=10000) train = pd.read_table(&quot;../A_Data/traindata.txt&quot;, nrows=10000) all_data = pd.concat([train, test], ignore_index=True) all_data[&#39;time&#39;] = pd.to_datetime(all_data[&#39;nginxtime&#39;] * 1e+6) + timedelta(hours=8) # 转换为北京时间24小时格式 all_data[&#39;day&#39;] = all_data[&#39;time&#39;].dt.dayofyear # 今年的第几天 all_data[&#39;hour&#39;] = all_data[&#39;time&#39;].dt.hour # 每天的第几小时 all_data[&#39;model&#39;].replace(&#39;PACM00&#39;, &quot;OPPO R15&quot;, inplace=True) # all_data[&#39;model&#39;].replace(&#39;PBAM00&#39;, &quot;OPPO A5&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PBEM00&#39;, &quot;OPPO R17&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PADM00&#39;, &quot;OPPO A3&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PBBM00&#39;, &quot;OPPO A7&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PAAM00&#39;, &quot;OPPO R15_1&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PACT00&#39;, &quot;OPPO R15_2&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PABT00&#39;, &quot;OPPO A5_1&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PBCM10&#39;, &quot;OPPO R15x&quot;, inplace=True) # for fea in [&#39;model&#39;, &#39;make&#39;, &#39;lan&#39;, &#39;new_make&#39;, &#39;new_model&#39;]: for fea in [&#39;model&#39;, &#39;make&#39;, &#39;lan&#39;]: all_data[fea] = all_data[fea].astype(&#39;str&#39;) all_data[fea] = all_data[fea].map(lambda x: x.upper()) # .upper()字符转大写 from urllib.parse import unquote def url_clean(x): x = unquote(x, &#39;utf-8&#39;).replace(&#39;%2B&#39;, &#39; &#39;).replace(&#39;%20&#39;, &#39; &#39;).replace(&#39;%2F&#39;, &#39;/&#39;).replace(&#39;%3F&#39;, &#39;?&#39;).replace( &#39;%25&#39;, &#39;%&#39;).replace(&#39;%23&#39;, &#39;#&#39;).replace(&quot;.&quot;, &#39; &#39;).replace(&#39;??&#39;, &#39; &#39;). \ replace(&#39;%26&#39;, &#39; &#39;).replace(&quot;%3D&quot;, &#39;=&#39;).replace(&#39;%22&#39;, &#39;&#39;).replace(&#39;_&#39;, &#39; &#39;).replace(&#39;+&#39;, &#39; &#39;).replace(&#39;-&#39;, &#39; &#39;).replace( &#39;__&#39;, &#39; &#39;).replace(&#39; &#39;, &#39; &#39;).replace(&#39;,&#39;, &#39; &#39;) if (x[0] == &#39;V&#39;) &amp; (x[-1] == &#39;A&#39;): return &quot;VIVO {}&quot;.format(x) elif (x[0] == &#39;P&#39;) &amp; (x[-1] == &#39;0&#39;): return &quot;OPPO {}&quot;.format(x) elif (len(x) == 5) &amp; (x[0] == &#39;O&#39;): return &quot;Smartisan {}&quot;.format(x) elif (&#39;AL00&#39; in x): return &quot;HW {}&quot;.format(x) else: return x all_data[fea] = all_data[fea].map(url_clean) all_data[&#39;big_model&#39;] = all_data[&#39;model&#39;].map(lambda x: x.split(&#39; &#39;)[0]) all_data[&#39;model_equal_make&#39;] = (all_data[&#39;big_model&#39;] == all_data[&#39;make&#39;]).astype(int) # 处理 ntt 的数据特征 但是不删除之前的特征 将其归为新的一列数据 all_data[&#39;new_ntt&#39;] = all_data[&#39;ntt&#39;] all_data.new_ntt[(all_data.new_ntt == 0) | (all_data.new_ntt == 7)] = 0 all_data.new_ntt[(all_data.new_ntt == 1) | (all_data.new_ntt == 2)] = 1 all_data.new_ntt[all_data.new_ntt == 3] = 2 all_data.new_ntt[(all_data.new_ntt &gt;= 4) &amp; (all_data.new_ntt &lt;= 6)] = 3 # 使用make填充 h w ppi值为0.0的数据 all_data[&#39;h&#39;].replace(0.0, np.nan, inplace=True) all_data[&#39;w&#39;].replace(0.0, np.nan, inplace=True) # all_data[&#39;ppi&#39;].replace(0.0, np.nan, inplace=True) # cols = [&#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;] cols = [&#39;h&#39;, &#39;w&#39;] gp_col = &#39;make&#39; for col in tqdm(cols): na_series = all_data[col].isna() names = list(all_data.loc[na_series, gp_col]) # 使用均值 或者众数进行填充缺失值 # df_fill = all_data.groupby(gp_col)[col].mean() df_fill = all_data.groupby(gp_col)[col].agg(lambda x: stats.mode(x)[0][0]) t = df_fill.loc[names] t.index = all_data.loc[na_series, col].index # 相同的index进行赋值 all_data.loc[na_series, col] = t all_data[col].fillna(0.0, inplace=True) del df_fill gc.collect() # H, W, PPI all_data[&#39;size&#39;] = (np.sqrt(all_data[&#39;h&#39;] ** 2 + all_data[&#39;w&#39;] ** 2) / 2.54) / 1000 all_data[&#39;ratio&#39;] = all_data[&#39;h&#39;] / all_data[&#39;w&#39;] all_data[&#39;px&#39;] = all_data[&#39;ppi&#39;] * all_data[&#39;size&#39;] all_data[&#39;mj&#39;] = all_data[&#39;h&#39;] * all_data[&#39;w&#39;] # 强特征进行组合 Fusion_attributes = [&#39;make_adunitshowid&#39;, &#39;adunitshowid_model&#39;, &#39;adunitshowid_ratio&#39;, &#39;make_model&#39;, &#39;make_osv&#39;, &#39;make_ratio&#39;, &#39;model_osv&#39;, &#39;model_ratio&#39;, &#39;model_h&#39;, &#39;ratio_osv&#39;] for attribute in tqdm(Fusion_attributes): name = &quot;Fusion_attr_&quot; + attribute dummy = &#39;label&#39; cols = attribute.split(&quot;_&quot;) cols_with_dummy = cols.copy() cols_with_dummy.append(dummy) gp = all_data[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: name}) all_data = all_data.merge(gp, on=cols, how=&#39;left&#39;) # 对ip地址和reqrealip地址进行分割 定义一个machine的关键字 all_data[&#39;ip2&#39;] = all_data[&#39;ip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:2])) all_data[&#39;ip3&#39;] = all_data[&#39;ip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:3])) all_data[&#39;reqrealip2&#39;] = all_data[&#39;reqrealip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:2])) all_data[&#39;reqrealip3&#39;] = all_data[&#39;reqrealip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:3])) all_data[&#39;machine&#39;] = 1000 * all_data[&#39;model&#39;] + all_data[&#39;make&#39;] var_mean_attributes = [&#39;adunitshowid&#39;, &#39;make&#39;, &#39;model&#39;, &#39;ver&#39;] for attr in tqdm(var_mean_attributes): # 统计关于ratio的方差和均值特征 var_label = &#39;ratio&#39; var_name = &#39;var_&#39; + attr + &#39;_&#39; + var_label gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str, columns={var_label: var_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[var_name] = all_data[var_name].fillna(0).astype(int) mean_label = &#39;ratio&#39; mean_name = &#39;mean_&#39; + attr + &#39;_&#39; + mean_label gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={ mean_label: mean_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[mean_name] = all_data[mean_name].fillna(0).astype(int) # 统计关于h的方差和均值特征 var_label = &#39;h&#39; var_name = &#39;var_&#39; + attr + &#39;_&#39; + var_label gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str, columns={var_label: var_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[var_name] = all_data[var_name].fillna(0).astype(int) mean_label = &#39;h&#39; mean_name = &#39;mean_&#39; + attr + &#39;_&#39; + mean_label gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={ mean_label: mean_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[mean_name] = all_data[mean_name].fillna(0).astype(int) # 统计关于h的方差和均值特征 var_label = &#39;w&#39; var_name = &#39;var_&#39; + attr + &#39;_&#39; + var_label gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str, columns={var_label: var_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[var_name] = all_data[var_name].fillna(0).astype(int) mean_label = &#39;w&#39; mean_name = &#39;mean_&#39; + attr + &#39;_&#39; + mean_label gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={ mean_label: mean_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[mean_name] = all_data[mean_name].fillna(0).astype(int) del gp gc.collect() cat_col = [i for i in all_data.select_dtypes(object).columns if i not in [&#39;sid&#39;, &#39;label&#39;]] for i in tqdm(cat_col): lbl = LabelEncoder() all_data[&#39;count_&#39; + i] = all_data.groupby([i])[i].transform(&#39;count&#39;) all_data[i] = lbl.fit_transform(all_data[i].astype(str)) for i in tqdm([&#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;, &#39;ratio&#39;]): all_data[&#39;{}_count&#39;.format(i)] = all_data.groupby([&#39;{}&#39;.format(i)])[&#39;sid&#39;].transform(&#39;count&#39;) feature_name = [i for i in all_data.columns if i not in [&#39;sid&#39;, &#39;label&#39;, &#39;time&#39;]] print(feature_name) print(&#39;all_data.info:&#39;, all_data.info()) # cat_list = [&#39;pkgname&#39;, &#39;ver&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;apptype&#39;, &#39;ip&#39;, &#39;city&#39;, &#39;province&#39;, &#39;reqrealip&#39;, # &#39;adidmd5&#39;, # &#39;imeimd5&#39;, &#39;idfamd5&#39;, &#39;openudidmd5&#39;, &#39;macmd5&#39;, &#39;dvctype&#39;, &#39;model&#39;, &#39;make&#39;, &#39;ntt&#39;, &#39;carrier&#39;, &#39;os&#39;, &#39;osv&#39;, # &#39;orientation&#39;, &#39;lan&#39;, &#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;, &#39;ip2&#39;, &#39;new_make&#39;, &#39;new_model&#39;, &#39;country&#39;, &#39;new_province&#39;, # &#39;new_city&#39;, # &#39;ip3&#39;, &#39;reqrealip2&#39;, &#39;reqrealip3&#39;] cat_list = [&#39;pkgname&#39;, &#39;ver&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;apptype&#39;, &#39;ip&#39;, &#39;city&#39;, &#39;province&#39;, &#39;reqrealip&#39;, &#39;adidmd5&#39;, &#39;imeimd5&#39;, &#39;idfamd5&#39;, &#39;openudidmd5&#39;, &#39;macmd5&#39;, &#39;dvctype&#39;, &#39;model&#39;, &#39;make&#39;, &#39;ntt&#39;, &#39;carrier&#39;, &#39;os&#39;, &#39;osv&#39;, &#39;orientation&#39;, &#39;lan&#39;, &#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;, &#39;ip2&#39;, &#39;ip3&#39;, &#39;reqrealip2&#39;, &#39;reqrealip3&#39;] tr_index = ~all_data[&#39;label&#39;].isnull() X_train = all_data[tr_index][list(set(feature_name))].reset_index(drop=True) y = all_data[tr_index][&#39;label&#39;].reset_index(drop=True).astype(int) X_test = all_data[~tr_index][list(set(feature_name))].reset_index(drop=True) print(X_train.shape, X_test.shape) # 节约一下内存 del all_data gc.collect() # 以下代码是5折交叉验证的结果 + lgb catboost xgb 最后使用logist进行回归预测 def get_stacking(clf, x_train, y_train, x_test, feature_name, n_folds=5): print(&#39;len_x_train:&#39;, len(x_train)) train_num, test_num = x_train.shape[0], x_test.shape[0] second_level_train_set = np.zeros((train_num,)) second_level_test_set = np.zeros((test_num,)) test_nfolds_sets = np.zeros((test_num, n_folds)) kf = KFold(n_splits=n_folds) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tra, y_tra = x_train[feature_name].iloc[train_index], y_train[train_index] x_tst, y_tst = x_train[feature_name].iloc[test_index], y_train[test_index] clf.fit(x_tra[feature_name], y_tra, eval_set=[(x_tst[feature_name], y_tst)]) second_level_train_set[test_index] = clf.predict(x_tst[feature_name]) test_nfolds_sets[:, i] = clf.predict(x_test[feature_name]) second_level_test_set[:] = test_nfolds_sets.mean(axis=1) return second_level_train_set, second_level_test_set def lgb_f1(labels, preds): score = f1_score(labels, np.round(preds)) return &#39;f1&#39;, score, True lgb_model = lgb.LGBMClassifier(random_seed=2019, n_jobs=-1, objective=&#39;binary&#39;, learning_rate=0.05, n_estimators=3000, num_leaves=31, max_depth=-1, min_child_samples=50, min_child_weight=9, subsample_freq=1, subsample=0.7, colsample_bytree=0.7, reg_alpha=1, reg_lambda=5, early_stopping_rounds=400) xgb_model = xgb.XGBClassifier(objective=&#39;binary:logistic&#39;, eval_metric=&#39;auc&#39;, learning_rate=0.02, max_depth=6, early_stopping_rounds=400, feval=lgb_f1) cbt_model = cbt.CatBoostClassifier(iterations=3000, learning_rate=0.05, max_depth=11, l2_leaf_reg=1, verbose=10, early_stopping_rounds=400, task_type=&#39;GPU&#39;, eval_metric=&#39;F1&#39;) train_sets = [] test_sets = [] for clf in [xgb_model, cbt_model, lgb_model]: print(&#39;begin train clf:&#39;, clf) train_set, test_set = get_stacking(clf, X_train, y, X_test, feature_name) train_sets.append(train_set) test_sets.append(test_set) meta_train = np.concatenate([result_set.reshape(-1, 1) for result_set in train_sets], axis=1) meta_test = np.concatenate([y_test_set.reshape(-1, 1) for y_test_set in test_sets], axis=1) # 使用逻辑回归作为第二层模型 bclf = LogisticRegression() bclf.fit(meta_train, y) test_pred = bclf.predict_proba(meta_test)[:, 1] # 提交结果 submit = test[[&#39;sid&#39;]] submit[&#39;label&#39;] = (test_pred &gt;= 0.5).astype(int) print(submit[&#39;label&#39;].value_counts()) submit.to_csv(&quot;A_Simple_Stacking_Model.csv&quot;, index=False) # 打印预测地概率 方便以后使用融合模型 df_sub = pd.concat([test[&#39;sid&#39;], pd.Series(test_pred)], axis=1) df_sub.columns = [&#39;sid&#39;, &#39;label&#39;] df_sub.to_csv(&#39;A_Simple_Stacking_Model_proba.csv&#39;, sep=&#39;,&#39;, index=False) &nbsp; XGB输出结果： &nbsp; lgb输出的结果： &nbsp; &nbsp; &nbsp;catboost输出的结果： &nbsp;" />
<meta property="og:description" content="前言： 分享这个baseline之前，首先先感谢一下我的好朋友油菜花一朵给予的一些帮助。然后呢介绍一下最近比赛中碰到的几个问题，以及解释。如果有可能的话，明天分享一个94.47左右的baseline吧，初赛之前设置为粉丝可见，初赛后在设置所有人可见吧。本来想分享47的baseline的，但是后来发现版本找不到了。就把自己的想法融合了一下，也不知道多少分。比赛名次不重要学到东西才重要。 &nbsp; 第一：为什么使用kaggle上的方案效果不好 因为科大讯飞的移动广告反欺诈算法挑战赛和之前kaggle上的‘TalkingData AdTracking Fraud Detection Challenge’比赛题目是一样的，和kaggle上不同的是科大讯飞增加了一些属性。所以有很多人都是仿照kaggle上的比赛进行的，但是发现效果不是很好。这个原因是由于kaggle上的广告欺诈是最后的标签是由TalkingData公司自己的算法生成的，最后公司也说了他们的算法中加上了时间信息。所以这也是为什么几乎每个开源代码都统计了很多关于时间的特征。 &nbsp; 第二：catboost的baseline抖动特别厉害 训练集的数据100万条，测试集仅仅只有10万条。这可能是由于训练集和测试集的数据都太少了导致的。所以同样的模型，相同的随机种子最后生成的结果差距也是有的。当跑的代码不是很好的时候，可以尝试一下再跑一次。 &nbsp; 第三： 为什么数据清洗之后会更差 我们发现最后训练的模型线上结果好的时候，往往模型中单个属性是比较强的特征。所以我想会不会科大讯飞官方的label也是由自己模型生成的，而且训练的时候没有对数据做过多的处理。如果我们要数据清洗的话，我们清理之后的属性一定要比之前的强。 &nbsp; 第四： 一些强特征加上关于label的统计之后线下会很好，但是线上提分不是很高 这个我一直很好奇，在我的模型中得到一些强特征之后我统计一下，我尝试使用统计这些特征关于label的均值和方差，最后显示特征的重要程度的时候，这些也都是不错的特征，但是最后线上提交的时候却发现增加分数不是很明显。这个可能是catboost的特征对于label而言比较敏感，所以可以换成其他的模型例如lgb试一下。 &nbsp; 第五： 加上时间数据之后线下很好但是线上不好 之前我统计同一个model前一次点击时间，和之后的点击时间差，但是同样是线下有提升，但是线上效果不好，我之前是以为数据泄露，但是按照网上的一些教程修改之后效果依旧存在同样的问题。暂时没有想到解释的方案。 &nbsp; 第六：介绍一下我这个baseline的特点吧 1、基本架构使用的是catboost的baseline 2、增加了一些关于强特征关于label的统计特征 例如均值 方差 出现次数 3、使用make的均值填充 h, w, ppi 4、对于一些强特征进行组合，统计组合特征的出现次数count()，以及累积计数cumcount() 5、为了节约内存，优化训练过程，删除一些不必要的,输出信息 6、删除了catboost模型中一些不必要的特征 7、使用相同的特征，但是加上xgb, lgb, catboost进行stacking， 最后使用logist进行回归分析 &nbsp; 代码： # -*- coding: utf-8 -*- # @Time : 2019/8/18 9:28 # @Author : YYLin # @Email : 854280599@qq.com # @File : A_Simple_Stacking_Model.py # 特征部分选择使用之前简单的特征 加上lgb catboost xgb进行stacking操作 分数大约46 import numpy as np import pandas as pd import gc from tqdm import tqdm from sklearn.model_selection import KFold from sklearn.preprocessing import LabelEncoder from datetime import timedelta import catboost as cbt import lightgbm as lgb import xgboost as xgb from sklearn.metrics import f1_score from sklearn.linear_model import LogisticRegression import warnings warnings.filterwarnings(&#39;ignore&#39;) from scipy import stats test = pd.read_table(&quot;../A_Data/testdata.txt&quot;, nrows=10000) train = pd.read_table(&quot;../A_Data/traindata.txt&quot;, nrows=10000) all_data = pd.concat([train, test], ignore_index=True) all_data[&#39;time&#39;] = pd.to_datetime(all_data[&#39;nginxtime&#39;] * 1e+6) + timedelta(hours=8) # 转换为北京时间24小时格式 all_data[&#39;day&#39;] = all_data[&#39;time&#39;].dt.dayofyear # 今年的第几天 all_data[&#39;hour&#39;] = all_data[&#39;time&#39;].dt.hour # 每天的第几小时 all_data[&#39;model&#39;].replace(&#39;PACM00&#39;, &quot;OPPO R15&quot;, inplace=True) # all_data[&#39;model&#39;].replace(&#39;PBAM00&#39;, &quot;OPPO A5&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PBEM00&#39;, &quot;OPPO R17&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PADM00&#39;, &quot;OPPO A3&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PBBM00&#39;, &quot;OPPO A7&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PAAM00&#39;, &quot;OPPO R15_1&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PACT00&#39;, &quot;OPPO R15_2&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PABT00&#39;, &quot;OPPO A5_1&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PBCM10&#39;, &quot;OPPO R15x&quot;, inplace=True) # for fea in [&#39;model&#39;, &#39;make&#39;, &#39;lan&#39;, &#39;new_make&#39;, &#39;new_model&#39;]: for fea in [&#39;model&#39;, &#39;make&#39;, &#39;lan&#39;]: all_data[fea] = all_data[fea].astype(&#39;str&#39;) all_data[fea] = all_data[fea].map(lambda x: x.upper()) # .upper()字符转大写 from urllib.parse import unquote def url_clean(x): x = unquote(x, &#39;utf-8&#39;).replace(&#39;%2B&#39;, &#39; &#39;).replace(&#39;%20&#39;, &#39; &#39;).replace(&#39;%2F&#39;, &#39;/&#39;).replace(&#39;%3F&#39;, &#39;?&#39;).replace( &#39;%25&#39;, &#39;%&#39;).replace(&#39;%23&#39;, &#39;#&#39;).replace(&quot;.&quot;, &#39; &#39;).replace(&#39;??&#39;, &#39; &#39;). \ replace(&#39;%26&#39;, &#39; &#39;).replace(&quot;%3D&quot;, &#39;=&#39;).replace(&#39;%22&#39;, &#39;&#39;).replace(&#39;_&#39;, &#39; &#39;).replace(&#39;+&#39;, &#39; &#39;).replace(&#39;-&#39;, &#39; &#39;).replace( &#39;__&#39;, &#39; &#39;).replace(&#39; &#39;, &#39; &#39;).replace(&#39;,&#39;, &#39; &#39;) if (x[0] == &#39;V&#39;) &amp; (x[-1] == &#39;A&#39;): return &quot;VIVO {}&quot;.format(x) elif (x[0] == &#39;P&#39;) &amp; (x[-1] == &#39;0&#39;): return &quot;OPPO {}&quot;.format(x) elif (len(x) == 5) &amp; (x[0] == &#39;O&#39;): return &quot;Smartisan {}&quot;.format(x) elif (&#39;AL00&#39; in x): return &quot;HW {}&quot;.format(x) else: return x all_data[fea] = all_data[fea].map(url_clean) all_data[&#39;big_model&#39;] = all_data[&#39;model&#39;].map(lambda x: x.split(&#39; &#39;)[0]) all_data[&#39;model_equal_make&#39;] = (all_data[&#39;big_model&#39;] == all_data[&#39;make&#39;]).astype(int) # 处理 ntt 的数据特征 但是不删除之前的特征 将其归为新的一列数据 all_data[&#39;new_ntt&#39;] = all_data[&#39;ntt&#39;] all_data.new_ntt[(all_data.new_ntt == 0) | (all_data.new_ntt == 7)] = 0 all_data.new_ntt[(all_data.new_ntt == 1) | (all_data.new_ntt == 2)] = 1 all_data.new_ntt[all_data.new_ntt == 3] = 2 all_data.new_ntt[(all_data.new_ntt &gt;= 4) &amp; (all_data.new_ntt &lt;= 6)] = 3 # 使用make填充 h w ppi值为0.0的数据 all_data[&#39;h&#39;].replace(0.0, np.nan, inplace=True) all_data[&#39;w&#39;].replace(0.0, np.nan, inplace=True) # all_data[&#39;ppi&#39;].replace(0.0, np.nan, inplace=True) # cols = [&#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;] cols = [&#39;h&#39;, &#39;w&#39;] gp_col = &#39;make&#39; for col in tqdm(cols): na_series = all_data[col].isna() names = list(all_data.loc[na_series, gp_col]) # 使用均值 或者众数进行填充缺失值 # df_fill = all_data.groupby(gp_col)[col].mean() df_fill = all_data.groupby(gp_col)[col].agg(lambda x: stats.mode(x)[0][0]) t = df_fill.loc[names] t.index = all_data.loc[na_series, col].index # 相同的index进行赋值 all_data.loc[na_series, col] = t all_data[col].fillna(0.0, inplace=True) del df_fill gc.collect() # H, W, PPI all_data[&#39;size&#39;] = (np.sqrt(all_data[&#39;h&#39;] ** 2 + all_data[&#39;w&#39;] ** 2) / 2.54) / 1000 all_data[&#39;ratio&#39;] = all_data[&#39;h&#39;] / all_data[&#39;w&#39;] all_data[&#39;px&#39;] = all_data[&#39;ppi&#39;] * all_data[&#39;size&#39;] all_data[&#39;mj&#39;] = all_data[&#39;h&#39;] * all_data[&#39;w&#39;] # 强特征进行组合 Fusion_attributes = [&#39;make_adunitshowid&#39;, &#39;adunitshowid_model&#39;, &#39;adunitshowid_ratio&#39;, &#39;make_model&#39;, &#39;make_osv&#39;, &#39;make_ratio&#39;, &#39;model_osv&#39;, &#39;model_ratio&#39;, &#39;model_h&#39;, &#39;ratio_osv&#39;] for attribute in tqdm(Fusion_attributes): name = &quot;Fusion_attr_&quot; + attribute dummy = &#39;label&#39; cols = attribute.split(&quot;_&quot;) cols_with_dummy = cols.copy() cols_with_dummy.append(dummy) gp = all_data[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: name}) all_data = all_data.merge(gp, on=cols, how=&#39;left&#39;) # 对ip地址和reqrealip地址进行分割 定义一个machine的关键字 all_data[&#39;ip2&#39;] = all_data[&#39;ip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:2])) all_data[&#39;ip3&#39;] = all_data[&#39;ip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:3])) all_data[&#39;reqrealip2&#39;] = all_data[&#39;reqrealip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:2])) all_data[&#39;reqrealip3&#39;] = all_data[&#39;reqrealip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:3])) all_data[&#39;machine&#39;] = 1000 * all_data[&#39;model&#39;] + all_data[&#39;make&#39;] var_mean_attributes = [&#39;adunitshowid&#39;, &#39;make&#39;, &#39;model&#39;, &#39;ver&#39;] for attr in tqdm(var_mean_attributes): # 统计关于ratio的方差和均值特征 var_label = &#39;ratio&#39; var_name = &#39;var_&#39; + attr + &#39;_&#39; + var_label gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str, columns={var_label: var_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[var_name] = all_data[var_name].fillna(0).astype(int) mean_label = &#39;ratio&#39; mean_name = &#39;mean_&#39; + attr + &#39;_&#39; + mean_label gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={ mean_label: mean_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[mean_name] = all_data[mean_name].fillna(0).astype(int) # 统计关于h的方差和均值特征 var_label = &#39;h&#39; var_name = &#39;var_&#39; + attr + &#39;_&#39; + var_label gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str, columns={var_label: var_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[var_name] = all_data[var_name].fillna(0).astype(int) mean_label = &#39;h&#39; mean_name = &#39;mean_&#39; + attr + &#39;_&#39; + mean_label gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={ mean_label: mean_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[mean_name] = all_data[mean_name].fillna(0).astype(int) # 统计关于h的方差和均值特征 var_label = &#39;w&#39; var_name = &#39;var_&#39; + attr + &#39;_&#39; + var_label gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str, columns={var_label: var_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[var_name] = all_data[var_name].fillna(0).astype(int) mean_label = &#39;w&#39; mean_name = &#39;mean_&#39; + attr + &#39;_&#39; + mean_label gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={ mean_label: mean_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[mean_name] = all_data[mean_name].fillna(0).astype(int) del gp gc.collect() cat_col = [i for i in all_data.select_dtypes(object).columns if i not in [&#39;sid&#39;, &#39;label&#39;]] for i in tqdm(cat_col): lbl = LabelEncoder() all_data[&#39;count_&#39; + i] = all_data.groupby([i])[i].transform(&#39;count&#39;) all_data[i] = lbl.fit_transform(all_data[i].astype(str)) for i in tqdm([&#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;, &#39;ratio&#39;]): all_data[&#39;{}_count&#39;.format(i)] = all_data.groupby([&#39;{}&#39;.format(i)])[&#39;sid&#39;].transform(&#39;count&#39;) feature_name = [i for i in all_data.columns if i not in [&#39;sid&#39;, &#39;label&#39;, &#39;time&#39;]] print(feature_name) print(&#39;all_data.info:&#39;, all_data.info()) # cat_list = [&#39;pkgname&#39;, &#39;ver&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;apptype&#39;, &#39;ip&#39;, &#39;city&#39;, &#39;province&#39;, &#39;reqrealip&#39;, # &#39;adidmd5&#39;, # &#39;imeimd5&#39;, &#39;idfamd5&#39;, &#39;openudidmd5&#39;, &#39;macmd5&#39;, &#39;dvctype&#39;, &#39;model&#39;, &#39;make&#39;, &#39;ntt&#39;, &#39;carrier&#39;, &#39;os&#39;, &#39;osv&#39;, # &#39;orientation&#39;, &#39;lan&#39;, &#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;, &#39;ip2&#39;, &#39;new_make&#39;, &#39;new_model&#39;, &#39;country&#39;, &#39;new_province&#39;, # &#39;new_city&#39;, # &#39;ip3&#39;, &#39;reqrealip2&#39;, &#39;reqrealip3&#39;] cat_list = [&#39;pkgname&#39;, &#39;ver&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;apptype&#39;, &#39;ip&#39;, &#39;city&#39;, &#39;province&#39;, &#39;reqrealip&#39;, &#39;adidmd5&#39;, &#39;imeimd5&#39;, &#39;idfamd5&#39;, &#39;openudidmd5&#39;, &#39;macmd5&#39;, &#39;dvctype&#39;, &#39;model&#39;, &#39;make&#39;, &#39;ntt&#39;, &#39;carrier&#39;, &#39;os&#39;, &#39;osv&#39;, &#39;orientation&#39;, &#39;lan&#39;, &#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;, &#39;ip2&#39;, &#39;ip3&#39;, &#39;reqrealip2&#39;, &#39;reqrealip3&#39;] tr_index = ~all_data[&#39;label&#39;].isnull() X_train = all_data[tr_index][list(set(feature_name))].reset_index(drop=True) y = all_data[tr_index][&#39;label&#39;].reset_index(drop=True).astype(int) X_test = all_data[~tr_index][list(set(feature_name))].reset_index(drop=True) print(X_train.shape, X_test.shape) # 节约一下内存 del all_data gc.collect() # 以下代码是5折交叉验证的结果 + lgb catboost xgb 最后使用logist进行回归预测 def get_stacking(clf, x_train, y_train, x_test, feature_name, n_folds=5): print(&#39;len_x_train:&#39;, len(x_train)) train_num, test_num = x_train.shape[0], x_test.shape[0] second_level_train_set = np.zeros((train_num,)) second_level_test_set = np.zeros((test_num,)) test_nfolds_sets = np.zeros((test_num, n_folds)) kf = KFold(n_splits=n_folds) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tra, y_tra = x_train[feature_name].iloc[train_index], y_train[train_index] x_tst, y_tst = x_train[feature_name].iloc[test_index], y_train[test_index] clf.fit(x_tra[feature_name], y_tra, eval_set=[(x_tst[feature_name], y_tst)]) second_level_train_set[test_index] = clf.predict(x_tst[feature_name]) test_nfolds_sets[:, i] = clf.predict(x_test[feature_name]) second_level_test_set[:] = test_nfolds_sets.mean(axis=1) return second_level_train_set, second_level_test_set def lgb_f1(labels, preds): score = f1_score(labels, np.round(preds)) return &#39;f1&#39;, score, True lgb_model = lgb.LGBMClassifier(random_seed=2019, n_jobs=-1, objective=&#39;binary&#39;, learning_rate=0.05, n_estimators=3000, num_leaves=31, max_depth=-1, min_child_samples=50, min_child_weight=9, subsample_freq=1, subsample=0.7, colsample_bytree=0.7, reg_alpha=1, reg_lambda=5, early_stopping_rounds=400) xgb_model = xgb.XGBClassifier(objective=&#39;binary:logistic&#39;, eval_metric=&#39;auc&#39;, learning_rate=0.02, max_depth=6, early_stopping_rounds=400, feval=lgb_f1) cbt_model = cbt.CatBoostClassifier(iterations=3000, learning_rate=0.05, max_depth=11, l2_leaf_reg=1, verbose=10, early_stopping_rounds=400, task_type=&#39;GPU&#39;, eval_metric=&#39;F1&#39;) train_sets = [] test_sets = [] for clf in [xgb_model, cbt_model, lgb_model]: print(&#39;begin train clf:&#39;, clf) train_set, test_set = get_stacking(clf, X_train, y, X_test, feature_name) train_sets.append(train_set) test_sets.append(test_set) meta_train = np.concatenate([result_set.reshape(-1, 1) for result_set in train_sets], axis=1) meta_test = np.concatenate([y_test_set.reshape(-1, 1) for y_test_set in test_sets], axis=1) # 使用逻辑回归作为第二层模型 bclf = LogisticRegression() bclf.fit(meta_train, y) test_pred = bclf.predict_proba(meta_test)[:, 1] # 提交结果 submit = test[[&#39;sid&#39;]] submit[&#39;label&#39;] = (test_pred &gt;= 0.5).astype(int) print(submit[&#39;label&#39;].value_counts()) submit.to_csv(&quot;A_Simple_Stacking_Model.csv&quot;, index=False) # 打印预测地概率 方便以后使用融合模型 df_sub = pd.concat([test[&#39;sid&#39;], pd.Series(test_pred)], axis=1) df_sub.columns = [&#39;sid&#39;, &#39;label&#39;] df_sub.to_csv(&#39;A_Simple_Stacking_Model_proba.csv&#39;, sep=&#39;,&#39;, index=False) &nbsp; XGB输出结果： &nbsp; lgb输出的结果： &nbsp; &nbsp; &nbsp;catboost输出的结果： &nbsp;" />
<link rel="canonical" href="https://uzzz.org/2019/08/18/794113.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/18/794113.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-18T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"前言： 分享这个baseline之前，首先先感谢一下我的好朋友油菜花一朵给予的一些帮助。然后呢介绍一下最近比赛中碰到的几个问题，以及解释。如果有可能的话，明天分享一个94.47左右的baseline吧，初赛之前设置为粉丝可见，初赛后在设置所有人可见吧。本来想分享47的baseline的，但是后来发现版本找不到了。就把自己的想法融合了一下，也不知道多少分。比赛名次不重要学到东西才重要。 &nbsp; 第一：为什么使用kaggle上的方案效果不好 因为科大讯飞的移动广告反欺诈算法挑战赛和之前kaggle上的‘TalkingData AdTracking Fraud Detection Challenge’比赛题目是一样的，和kaggle上不同的是科大讯飞增加了一些属性。所以有很多人都是仿照kaggle上的比赛进行的，但是发现效果不是很好。这个原因是由于kaggle上的广告欺诈是最后的标签是由TalkingData公司自己的算法生成的，最后公司也说了他们的算法中加上了时间信息。所以这也是为什么几乎每个开源代码都统计了很多关于时间的特征。 &nbsp; 第二：catboost的baseline抖动特别厉害 训练集的数据100万条，测试集仅仅只有10万条。这可能是由于训练集和测试集的数据都太少了导致的。所以同样的模型，相同的随机种子最后生成的结果差距也是有的。当跑的代码不是很好的时候，可以尝试一下再跑一次。 &nbsp; 第三： 为什么数据清洗之后会更差 我们发现最后训练的模型线上结果好的时候，往往模型中单个属性是比较强的特征。所以我想会不会科大讯飞官方的label也是由自己模型生成的，而且训练的时候没有对数据做过多的处理。如果我们要数据清洗的话，我们清理之后的属性一定要比之前的强。 &nbsp; 第四： 一些强特征加上关于label的统计之后线下会很好，但是线上提分不是很高 这个我一直很好奇，在我的模型中得到一些强特征之后我统计一下，我尝试使用统计这些特征关于label的均值和方差，最后显示特征的重要程度的时候，这些也都是不错的特征，但是最后线上提交的时候却发现增加分数不是很明显。这个可能是catboost的特征对于label而言比较敏感，所以可以换成其他的模型例如lgb试一下。 &nbsp; 第五： 加上时间数据之后线下很好但是线上不好 之前我统计同一个model前一次点击时间，和之后的点击时间差，但是同样是线下有提升，但是线上效果不好，我之前是以为数据泄露，但是按照网上的一些教程修改之后效果依旧存在同样的问题。暂时没有想到解释的方案。 &nbsp; 第六：介绍一下我这个baseline的特点吧 1、基本架构使用的是catboost的baseline 2、增加了一些关于强特征关于label的统计特征 例如均值 方差 出现次数 3、使用make的均值填充 h, w, ppi 4、对于一些强特征进行组合，统计组合特征的出现次数count()，以及累积计数cumcount() 5、为了节约内存，优化训练过程，删除一些不必要的,输出信息 6、删除了catboost模型中一些不必要的特征 7、使用相同的特征，但是加上xgb, lgb, catboost进行stacking， 最后使用logist进行回归分析 &nbsp; 代码： # -*- coding: utf-8 -*- # @Time : 2019/8/18 9:28 # @Author : YYLin # @Email : 854280599@qq.com # @File : A_Simple_Stacking_Model.py # 特征部分选择使用之前简单的特征 加上lgb catboost xgb进行stacking操作 分数大约46 import numpy as np import pandas as pd import gc from tqdm import tqdm from sklearn.model_selection import KFold from sklearn.preprocessing import LabelEncoder from datetime import timedelta import catboost as cbt import lightgbm as lgb import xgboost as xgb from sklearn.metrics import f1_score from sklearn.linear_model import LogisticRegression import warnings warnings.filterwarnings(&#39;ignore&#39;) from scipy import stats test = pd.read_table(&quot;../A_Data/testdata.txt&quot;, nrows=10000) train = pd.read_table(&quot;../A_Data/traindata.txt&quot;, nrows=10000) all_data = pd.concat([train, test], ignore_index=True) all_data[&#39;time&#39;] = pd.to_datetime(all_data[&#39;nginxtime&#39;] * 1e+6) + timedelta(hours=8) # 转换为北京时间24小时格式 all_data[&#39;day&#39;] = all_data[&#39;time&#39;].dt.dayofyear # 今年的第几天 all_data[&#39;hour&#39;] = all_data[&#39;time&#39;].dt.hour # 每天的第几小时 all_data[&#39;model&#39;].replace(&#39;PACM00&#39;, &quot;OPPO R15&quot;, inplace=True) # all_data[&#39;model&#39;].replace(&#39;PBAM00&#39;, &quot;OPPO A5&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PBEM00&#39;, &quot;OPPO R17&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PADM00&#39;, &quot;OPPO A3&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PBBM00&#39;, &quot;OPPO A7&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PAAM00&#39;, &quot;OPPO R15_1&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PACT00&#39;, &quot;OPPO R15_2&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PABT00&#39;, &quot;OPPO A5_1&quot;, inplace=True) all_data[&#39;model&#39;].replace(&#39;PBCM10&#39;, &quot;OPPO R15x&quot;, inplace=True) # for fea in [&#39;model&#39;, &#39;make&#39;, &#39;lan&#39;, &#39;new_make&#39;, &#39;new_model&#39;]: for fea in [&#39;model&#39;, &#39;make&#39;, &#39;lan&#39;]: all_data[fea] = all_data[fea].astype(&#39;str&#39;) all_data[fea] = all_data[fea].map(lambda x: x.upper()) # .upper()字符转大写 from urllib.parse import unquote def url_clean(x): x = unquote(x, &#39;utf-8&#39;).replace(&#39;%2B&#39;, &#39; &#39;).replace(&#39;%20&#39;, &#39; &#39;).replace(&#39;%2F&#39;, &#39;/&#39;).replace(&#39;%3F&#39;, &#39;?&#39;).replace( &#39;%25&#39;, &#39;%&#39;).replace(&#39;%23&#39;, &#39;#&#39;).replace(&quot;.&quot;, &#39; &#39;).replace(&#39;??&#39;, &#39; &#39;). \\ replace(&#39;%26&#39;, &#39; &#39;).replace(&quot;%3D&quot;, &#39;=&#39;).replace(&#39;%22&#39;, &#39;&#39;).replace(&#39;_&#39;, &#39; &#39;).replace(&#39;+&#39;, &#39; &#39;).replace(&#39;-&#39;, &#39; &#39;).replace( &#39;__&#39;, &#39; &#39;).replace(&#39; &#39;, &#39; &#39;).replace(&#39;,&#39;, &#39; &#39;) if (x[0] == &#39;V&#39;) &amp; (x[-1] == &#39;A&#39;): return &quot;VIVO {}&quot;.format(x) elif (x[0] == &#39;P&#39;) &amp; (x[-1] == &#39;0&#39;): return &quot;OPPO {}&quot;.format(x) elif (len(x) == 5) &amp; (x[0] == &#39;O&#39;): return &quot;Smartisan {}&quot;.format(x) elif (&#39;AL00&#39; in x): return &quot;HW {}&quot;.format(x) else: return x all_data[fea] = all_data[fea].map(url_clean) all_data[&#39;big_model&#39;] = all_data[&#39;model&#39;].map(lambda x: x.split(&#39; &#39;)[0]) all_data[&#39;model_equal_make&#39;] = (all_data[&#39;big_model&#39;] == all_data[&#39;make&#39;]).astype(int) # 处理 ntt 的数据特征 但是不删除之前的特征 将其归为新的一列数据 all_data[&#39;new_ntt&#39;] = all_data[&#39;ntt&#39;] all_data.new_ntt[(all_data.new_ntt == 0) | (all_data.new_ntt == 7)] = 0 all_data.new_ntt[(all_data.new_ntt == 1) | (all_data.new_ntt == 2)] = 1 all_data.new_ntt[all_data.new_ntt == 3] = 2 all_data.new_ntt[(all_data.new_ntt &gt;= 4) &amp; (all_data.new_ntt &lt;= 6)] = 3 # 使用make填充 h w ppi值为0.0的数据 all_data[&#39;h&#39;].replace(0.0, np.nan, inplace=True) all_data[&#39;w&#39;].replace(0.0, np.nan, inplace=True) # all_data[&#39;ppi&#39;].replace(0.0, np.nan, inplace=True) # cols = [&#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;] cols = [&#39;h&#39;, &#39;w&#39;] gp_col = &#39;make&#39; for col in tqdm(cols): na_series = all_data[col].isna() names = list(all_data.loc[na_series, gp_col]) # 使用均值 或者众数进行填充缺失值 # df_fill = all_data.groupby(gp_col)[col].mean() df_fill = all_data.groupby(gp_col)[col].agg(lambda x: stats.mode(x)[0][0]) t = df_fill.loc[names] t.index = all_data.loc[na_series, col].index # 相同的index进行赋值 all_data.loc[na_series, col] = t all_data[col].fillna(0.0, inplace=True) del df_fill gc.collect() # H, W, PPI all_data[&#39;size&#39;] = (np.sqrt(all_data[&#39;h&#39;] ** 2 + all_data[&#39;w&#39;] ** 2) / 2.54) / 1000 all_data[&#39;ratio&#39;] = all_data[&#39;h&#39;] / all_data[&#39;w&#39;] all_data[&#39;px&#39;] = all_data[&#39;ppi&#39;] * all_data[&#39;size&#39;] all_data[&#39;mj&#39;] = all_data[&#39;h&#39;] * all_data[&#39;w&#39;] # 强特征进行组合 Fusion_attributes = [&#39;make_adunitshowid&#39;, &#39;adunitshowid_model&#39;, &#39;adunitshowid_ratio&#39;, &#39;make_model&#39;, &#39;make_osv&#39;, &#39;make_ratio&#39;, &#39;model_osv&#39;, &#39;model_ratio&#39;, &#39;model_h&#39;, &#39;ratio_osv&#39;] for attribute in tqdm(Fusion_attributes): name = &quot;Fusion_attr_&quot; + attribute dummy = &#39;label&#39; cols = attribute.split(&quot;_&quot;) cols_with_dummy = cols.copy() cols_with_dummy.append(dummy) gp = all_data[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: name}) all_data = all_data.merge(gp, on=cols, how=&#39;left&#39;) # 对ip地址和reqrealip地址进行分割 定义一个machine的关键字 all_data[&#39;ip2&#39;] = all_data[&#39;ip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:2])) all_data[&#39;ip3&#39;] = all_data[&#39;ip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:3])) all_data[&#39;reqrealip2&#39;] = all_data[&#39;reqrealip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:2])) all_data[&#39;reqrealip3&#39;] = all_data[&#39;reqrealip&#39;].apply(lambda x: &#39;.&#39;.join(x.split(&#39;.&#39;)[0:3])) all_data[&#39;machine&#39;] = 1000 * all_data[&#39;model&#39;] + all_data[&#39;make&#39;] var_mean_attributes = [&#39;adunitshowid&#39;, &#39;make&#39;, &#39;model&#39;, &#39;ver&#39;] for attr in tqdm(var_mean_attributes): # 统计关于ratio的方差和均值特征 var_label = &#39;ratio&#39; var_name = &#39;var_&#39; + attr + &#39;_&#39; + var_label gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str, columns={var_label: var_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[var_name] = all_data[var_name].fillna(0).astype(int) mean_label = &#39;ratio&#39; mean_name = &#39;mean_&#39; + attr + &#39;_&#39; + mean_label gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={ mean_label: mean_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[mean_name] = all_data[mean_name].fillna(0).astype(int) # 统计关于h的方差和均值特征 var_label = &#39;h&#39; var_name = &#39;var_&#39; + attr + &#39;_&#39; + var_label gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str, columns={var_label: var_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[var_name] = all_data[var_name].fillna(0).astype(int) mean_label = &#39;h&#39; mean_name = &#39;mean_&#39; + attr + &#39;_&#39; + mean_label gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={ mean_label: mean_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[mean_name] = all_data[mean_name].fillna(0).astype(int) # 统计关于h的方差和均值特征 var_label = &#39;w&#39; var_name = &#39;var_&#39; + attr + &#39;_&#39; + var_label gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str, columns={var_label: var_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[var_name] = all_data[var_name].fillna(0).astype(int) mean_label = &#39;w&#39; mean_name = &#39;mean_&#39; + attr + &#39;_&#39; + mean_label gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={ mean_label: mean_name}) all_data = all_data.merge(gp, on=attr, how=&#39;left&#39;) all_data[mean_name] = all_data[mean_name].fillna(0).astype(int) del gp gc.collect() cat_col = [i for i in all_data.select_dtypes(object).columns if i not in [&#39;sid&#39;, &#39;label&#39;]] for i in tqdm(cat_col): lbl = LabelEncoder() all_data[&#39;count_&#39; + i] = all_data.groupby([i])[i].transform(&#39;count&#39;) all_data[i] = lbl.fit_transform(all_data[i].astype(str)) for i in tqdm([&#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;, &#39;ratio&#39;]): all_data[&#39;{}_count&#39;.format(i)] = all_data.groupby([&#39;{}&#39;.format(i)])[&#39;sid&#39;].transform(&#39;count&#39;) feature_name = [i for i in all_data.columns if i not in [&#39;sid&#39;, &#39;label&#39;, &#39;time&#39;]] print(feature_name) print(&#39;all_data.info:&#39;, all_data.info()) # cat_list = [&#39;pkgname&#39;, &#39;ver&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;apptype&#39;, &#39;ip&#39;, &#39;city&#39;, &#39;province&#39;, &#39;reqrealip&#39;, # &#39;adidmd5&#39;, # &#39;imeimd5&#39;, &#39;idfamd5&#39;, &#39;openudidmd5&#39;, &#39;macmd5&#39;, &#39;dvctype&#39;, &#39;model&#39;, &#39;make&#39;, &#39;ntt&#39;, &#39;carrier&#39;, &#39;os&#39;, &#39;osv&#39;, # &#39;orientation&#39;, &#39;lan&#39;, &#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;, &#39;ip2&#39;, &#39;new_make&#39;, &#39;new_model&#39;, &#39;country&#39;, &#39;new_province&#39;, # &#39;new_city&#39;, # &#39;ip3&#39;, &#39;reqrealip2&#39;, &#39;reqrealip3&#39;] cat_list = [&#39;pkgname&#39;, &#39;ver&#39;, &#39;adunitshowid&#39;, &#39;mediashowid&#39;, &#39;apptype&#39;, &#39;ip&#39;, &#39;city&#39;, &#39;province&#39;, &#39;reqrealip&#39;, &#39;adidmd5&#39;, &#39;imeimd5&#39;, &#39;idfamd5&#39;, &#39;openudidmd5&#39;, &#39;macmd5&#39;, &#39;dvctype&#39;, &#39;model&#39;, &#39;make&#39;, &#39;ntt&#39;, &#39;carrier&#39;, &#39;os&#39;, &#39;osv&#39;, &#39;orientation&#39;, &#39;lan&#39;, &#39;h&#39;, &#39;w&#39;, &#39;ppi&#39;, &#39;ip2&#39;, &#39;ip3&#39;, &#39;reqrealip2&#39;, &#39;reqrealip3&#39;] tr_index = ~all_data[&#39;label&#39;].isnull() X_train = all_data[tr_index][list(set(feature_name))].reset_index(drop=True) y = all_data[tr_index][&#39;label&#39;].reset_index(drop=True).astype(int) X_test = all_data[~tr_index][list(set(feature_name))].reset_index(drop=True) print(X_train.shape, X_test.shape) # 节约一下内存 del all_data gc.collect() # 以下代码是5折交叉验证的结果 + lgb catboost xgb 最后使用logist进行回归预测 def get_stacking(clf, x_train, y_train, x_test, feature_name, n_folds=5): print(&#39;len_x_train:&#39;, len(x_train)) train_num, test_num = x_train.shape[0], x_test.shape[0] second_level_train_set = np.zeros((train_num,)) second_level_test_set = np.zeros((test_num,)) test_nfolds_sets = np.zeros((test_num, n_folds)) kf = KFold(n_splits=n_folds) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tra, y_tra = x_train[feature_name].iloc[train_index], y_train[train_index] x_tst, y_tst = x_train[feature_name].iloc[test_index], y_train[test_index] clf.fit(x_tra[feature_name], y_tra, eval_set=[(x_tst[feature_name], y_tst)]) second_level_train_set[test_index] = clf.predict(x_tst[feature_name]) test_nfolds_sets[:, i] = clf.predict(x_test[feature_name]) second_level_test_set[:] = test_nfolds_sets.mean(axis=1) return second_level_train_set, second_level_test_set def lgb_f1(labels, preds): score = f1_score(labels, np.round(preds)) return &#39;f1&#39;, score, True lgb_model = lgb.LGBMClassifier(random_seed=2019, n_jobs=-1, objective=&#39;binary&#39;, learning_rate=0.05, n_estimators=3000, num_leaves=31, max_depth=-1, min_child_samples=50, min_child_weight=9, subsample_freq=1, subsample=0.7, colsample_bytree=0.7, reg_alpha=1, reg_lambda=5, early_stopping_rounds=400) xgb_model = xgb.XGBClassifier(objective=&#39;binary:logistic&#39;, eval_metric=&#39;auc&#39;, learning_rate=0.02, max_depth=6, early_stopping_rounds=400, feval=lgb_f1) cbt_model = cbt.CatBoostClassifier(iterations=3000, learning_rate=0.05, max_depth=11, l2_leaf_reg=1, verbose=10, early_stopping_rounds=400, task_type=&#39;GPU&#39;, eval_metric=&#39;F1&#39;) train_sets = [] test_sets = [] for clf in [xgb_model, cbt_model, lgb_model]: print(&#39;begin train clf:&#39;, clf) train_set, test_set = get_stacking(clf, X_train, y, X_test, feature_name) train_sets.append(train_set) test_sets.append(test_set) meta_train = np.concatenate([result_set.reshape(-1, 1) for result_set in train_sets], axis=1) meta_test = np.concatenate([y_test_set.reshape(-1, 1) for y_test_set in test_sets], axis=1) # 使用逻辑回归作为第二层模型 bclf = LogisticRegression() bclf.fit(meta_train, y) test_pred = bclf.predict_proba(meta_test)[:, 1] # 提交结果 submit = test[[&#39;sid&#39;]] submit[&#39;label&#39;] = (test_pred &gt;= 0.5).astype(int) print(submit[&#39;label&#39;].value_counts()) submit.to_csv(&quot;A_Simple_Stacking_Model.csv&quot;, index=False) # 打印预测地概率 方便以后使用融合模型 df_sub = pd.concat([test[&#39;sid&#39;], pd.Series(test_pred)], axis=1) df_sub.columns = [&#39;sid&#39;, &#39;label&#39;] df_sub.to_csv(&#39;A_Simple_Stacking_Model_proba.csv&#39;, sep=&#39;,&#39;, index=False) &nbsp; XGB输出结果： &nbsp; lgb输出的结果： &nbsp; &nbsp; &nbsp;catboost输出的结果： &nbsp;","@type":"BlogPosting","url":"https://uzzz.org/2019/08/18/794113.html","headline":"2019移动广告反欺诈算法挑战赛baseline","dateModified":"2019-08-18T00:00:00+08:00","datePublished":"2019-08-18T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/18/794113.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>2019移动广告反欺诈算法挑战赛baseline</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1>前言：</h1> 
  <p>分享这个baseline之前，首先先感谢一下我的好朋友<span style="color:#f33b45;">油菜花一朵</span>给予的一些帮助。然后呢介绍一下最近比赛中碰到的几个问题，以及解释。如果有可能的话，明天分享一个94.47左右的baseline吧，初赛之前设置为粉丝可见，初赛后在设置所有人可见吧。本来想分享47的baseline的，但是后来发现版本找不到了。就把自己的想法融合了一下，也不知道多少分。比赛名次不重要学到东西才重要。</p> 
  <p>&nbsp;</p> 
  <h3>第一：为什么使用kaggle上的方案效果不好</h3> 
  <p>因为科大讯飞的移动广告反欺诈算法挑战赛和之前kaggle上的‘TalkingData AdTracking Fraud Detection Challenge’比赛题目是一样的，和kaggle上不同的是科大讯飞增加了一些属性。所以有很多人都是仿照kaggle上的比赛进行的，但是发现效果不是很好。这个原因是由于kaggle上的广告欺诈是最后的标签是由TalkingData公司自己的算法生成的，最后公司也说了他们的算法中加上了时间信息。所以这也是为什么几乎每个开源代码都统计了很多关于时间的特征。</p> 
  <p>&nbsp;</p> 
  <h3>第二：catboost的baseline抖动特别厉害</h3> 
  <p>训练集的数据100万条，测试集仅仅只有10万条。这可能是由于训练集和测试集的数据都太少了导致的。所以同样的模型，相同的随机种子最后生成的结果差距也是有的。当跑的代码不是很好的时候，可以尝试一下再跑一次。</p> 
  <p>&nbsp;</p> 
  <h3>第三： 为什么数据清洗之后会更差</h3> 
  <p>我们发现最后训练的模型线上结果好的时候，往往模型中单个属性是比较强的特征。所以我想会不会科大讯飞官方的label也是由自己模型生成的，而且训练的时候没有对数据做过多的处理。如果我们要数据清洗的话，我们清理之后的属性一定要比之前的强。</p> 
  <p>&nbsp;</p> 
  <h3>第四： 一些强特征加上关于label的统计之后线下会很好，但是线上提分不是很高</h3> 
  <p>这个我一直很好奇，在我的模型中得到一些强特征之后我统计一下，我尝试使用统计这些特征关于label的均值和方差，最后显示特征的重要程度的时候，这些也都是不错的特征，但是最后线上提交的时候却发现增加分数不是很明显。这个可能是catboost的特征对于label而言比较敏感，所以可以换成其他的模型例如lgb试一下。</p> 
  <p>&nbsp;</p> 
  <h3>第五： 加上时间数据之后线下很好但是线上不好</h3> 
  <p>之前我统计同一个model前一次点击时间，和之后的点击时间差，但是同样是线下有提升，但是线上效果不好，我之前是以为数据泄露，但是按照网上的一些教程修改之后效果依旧存在同样的问题。暂时没有想到解释的方案。</p> 
  <p>&nbsp;</p> 
  <h3>第六：介绍一下我这个baseline的特点吧</h3> 
  <p>1、基本架构使用的是catboost的baseline</p> 
  <p>2、增加了一些关于强特征关于label的统计特征 例如均值 方差 出现次数</p> 
  <p>3、使用make的均值填充 h, w, ppi</p> 
  <p>4、对于一些强特征进行组合，统计组合特征的出现次数count()，以及累积计数cumcount()</p> 
  <p>5、为了节约内存，优化训练过程，删除一些不必要的,输出信息</p> 
  <p>6、删除了catboost模型中一些不必要的特征</p> 
  <p>7、使用相同的特征，但是加上xgb, lgb, catboost进行stacking， 最后使用logist进行回归分析</p> 
  <p>&nbsp;</p> 
  <h1>代码：</h1> 
  <pre class="has">
<code class="language-python"># -*- coding: utf-8 -*-
# @Time    : 2019/8/18 9:28
# @Author  : YYLin
# @Email   : 854280599@qq.com
# @File    : A_Simple_Stacking_Model.py
# 特征部分选择使用之前简单的特征 加上lgb catboost xgb进行stacking操作 分数大约46

import numpy as np
import pandas as pd
import gc
from tqdm import tqdm
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from datetime import timedelta
import catboost as cbt
import lightgbm as lgb
import xgboost as xgb
from sklearn.metrics import f1_score
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')
from scipy import stats


test = pd.read_table("../A_Data/testdata.txt", nrows=10000)
train = pd.read_table("../A_Data/traindata.txt", nrows=10000)
all_data = pd.concat([train, test], ignore_index=True)

all_data['time'] = pd.to_datetime(all_data['nginxtime'] * 1e+6) + timedelta(hours=8)  # 转换为北京时间24小时格式
all_data['day'] = all_data['time'].dt.dayofyear  # 今年的第几天
all_data['hour'] = all_data['time'].dt.hour  # 每天的第几小时

all_data['model'].replace('PACM00', "OPPO R15", inplace=True)  #
all_data['model'].replace('PBAM00', "OPPO A5", inplace=True)
all_data['model'].replace('PBEM00', "OPPO R17", inplace=True)
all_data['model'].replace('PADM00', "OPPO A3", inplace=True)
all_data['model'].replace('PBBM00', "OPPO A7", inplace=True)
all_data['model'].replace('PAAM00', "OPPO R15_1", inplace=True)
all_data['model'].replace('PACT00', "OPPO R15_2", inplace=True)
all_data['model'].replace('PABT00', "OPPO A5_1", inplace=True)
all_data['model'].replace('PBCM10', "OPPO R15x", inplace=True)

# for fea in ['model', 'make', 'lan', 'new_make', 'new_model']:
for fea in ['model', 'make', 'lan']:
    all_data[fea] = all_data[fea].astype('str')
    all_data[fea] = all_data[fea].map(lambda x: x.upper())  # .upper()字符转大写

    from urllib.parse import unquote


    def url_clean(x):
        x = unquote(x, 'utf-8').replace('%2B', ' ').replace('%20', ' ').replace('%2F', '/').replace('%3F', '?').replace(
            '%25', '%').replace('%23', '#').replace(".", ' ').replace('??', ' '). \
            replace('%26', ' ').replace("%3D", '=').replace('%22', '').replace('_', ' ').replace('+', ' ').replace('-',
                                                                                                                   ' ').replace(
            '__', ' ').replace('  ', ' ').replace(',', ' ')

        if (x[0] == 'V') &amp; (x[-1] == 'A'):
            return "VIVO {}".format(x)
        elif (x[0] == 'P') &amp; (x[-1] == '0'):
            return "OPPO {}".format(x)
        elif (len(x) == 5) &amp; (x[0] == 'O'):
            return "Smartisan {}".format(x)
        elif ('AL00' in x):
            return "HW {}".format(x)
        else:
            return x


    all_data[fea] = all_data[fea].map(url_clean)

all_data['big_model'] = all_data['model'].map(lambda x: x.split(' ')[0])
all_data['model_equal_make'] = (all_data['big_model'] == all_data['make']).astype(int)

# 处理 ntt 的数据特征 但是不删除之前的特征 将其归为新的一列数据
all_data['new_ntt'] = all_data['ntt']
all_data.new_ntt[(all_data.new_ntt == 0) | (all_data.new_ntt == 7)] = 0
all_data.new_ntt[(all_data.new_ntt == 1) | (all_data.new_ntt == 2)] = 1
all_data.new_ntt[all_data.new_ntt == 3] = 2
all_data.new_ntt[(all_data.new_ntt &gt;= 4) &amp; (all_data.new_ntt &lt;= 6)] = 3

# 使用make填充 h w ppi值为0.0的数据
all_data['h'].replace(0.0, np.nan, inplace=True)
all_data['w'].replace(0.0, np.nan, inplace=True)
# all_data['ppi'].replace(0.0, np.nan, inplace=True)
# cols = ['h', 'w', 'ppi']
cols = ['h', 'w']
gp_col = 'make'
for col in tqdm(cols):
    na_series = all_data[col].isna()
    names = list(all_data.loc[na_series, gp_col])
    # 使用均值 或者众数进行填充缺失值
    # df_fill = all_data.groupby(gp_col)[col].mean()
    df_fill = all_data.groupby(gp_col)[col].agg(lambda x: stats.mode(x)[0][0])
    t = df_fill.loc[names]
    t.index = all_data.loc[na_series, col].index
    # 相同的index进行赋值
    all_data.loc[na_series, col] = t
    all_data[col].fillna(0.0, inplace=True)
    del df_fill
    gc.collect()

# H, W, PPI
all_data['size'] = (np.sqrt(all_data['h'] ** 2 + all_data['w'] ** 2) / 2.54) / 1000
all_data['ratio'] = all_data['h'] / all_data['w']
all_data['px'] = all_data['ppi'] * all_data['size']
all_data['mj'] = all_data['h'] * all_data['w']

# 强特征进行组合
Fusion_attributes = ['make_adunitshowid', 'adunitshowid_model', 'adunitshowid_ratio', 'make_model',
                     'make_osv', 'make_ratio', 'model_osv', 'model_ratio', 'model_h', 'ratio_osv']

for attribute in tqdm(Fusion_attributes):
    name = "Fusion_attr_" + attribute
    dummy = 'label'
    cols = attribute.split("_")
    cols_with_dummy = cols.copy()
    cols_with_dummy.append(dummy)
    gp = all_data[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str,
                                                                                          columns={dummy: name})
    all_data = all_data.merge(gp, on=cols, how='left')

# 对ip地址和reqrealip地址进行分割 定义一个machine的关键字
all_data['ip2'] = all_data['ip'].apply(lambda x: '.'.join(x.split('.')[0:2]))
all_data['ip3'] = all_data['ip'].apply(lambda x: '.'.join(x.split('.')[0:3]))
all_data['reqrealip2'] = all_data['reqrealip'].apply(lambda x: '.'.join(x.split('.')[0:2]))
all_data['reqrealip3'] = all_data['reqrealip'].apply(lambda x: '.'.join(x.split('.')[0:3]))
all_data['machine'] = 1000 * all_data['model'] + all_data['make']

var_mean_attributes = ['adunitshowid', 'make', 'model', 'ver']
for attr in tqdm(var_mean_attributes):
    # 统计关于ratio的方差和均值特征
    var_label = 'ratio'
    var_name = 'var_' + attr + '_' + var_label
    gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str,
                                                                                         columns={var_label: var_name})
    all_data = all_data.merge(gp, on=attr, how='left')
    all_data[var_name] = all_data[var_name].fillna(0).astype(int)

    mean_label = 'ratio'
    mean_name = 'mean_' + attr + '_' + mean_label
    gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={
        mean_label: mean_name})
    all_data = all_data.merge(gp, on=attr, how='left')
    all_data[mean_name] = all_data[mean_name].fillna(0).astype(int)

    # 统计关于h的方差和均值特征
    var_label = 'h'
    var_name = 'var_' + attr + '_' + var_label
    gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str,
                                                                                         columns={var_label: var_name})
    all_data = all_data.merge(gp, on=attr, how='left')
    all_data[var_name] = all_data[var_name].fillna(0).astype(int)

    mean_label = 'h'
    mean_name = 'mean_' + attr + '_' + mean_label
    gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={
        mean_label: mean_name})
    all_data = all_data.merge(gp, on=attr, how='left')
    all_data[mean_name] = all_data[mean_name].fillna(0).astype(int)

    # 统计关于h的方差和均值特征
    var_label = 'w'
    var_name = 'var_' + attr + '_' + var_label
    gp = all_data[[attr, var_label]].groupby(attr)[var_label].var().reset_index().rename(index=str,
                                                                                         columns={var_label: var_name})
    all_data = all_data.merge(gp, on=attr, how='left')
    all_data[var_name] = all_data[var_name].fillna(0).astype(int)

    mean_label = 'w'
    mean_name = 'mean_' + attr + '_' + mean_label
    gp = all_data[[attr, mean_label]].groupby(attr)[mean_label].mean().reset_index().rename(index=str, columns={
        mean_label: mean_name})
    all_data = all_data.merge(gp, on=attr, how='left')
    all_data[mean_name] = all_data[mean_name].fillna(0).astype(int)

    del gp
    gc.collect()

cat_col = [i for i in all_data.select_dtypes(object).columns if i not in ['sid', 'label']]
for i in tqdm(cat_col):
    lbl = LabelEncoder()
    all_data['count_' + i] = all_data.groupby([i])[i].transform('count')
    all_data[i] = lbl.fit_transform(all_data[i].astype(str))

for i in tqdm(['h', 'w', 'ppi', 'ratio']):
    all_data['{}_count'.format(i)] = all_data.groupby(['{}'.format(i)])['sid'].transform('count')

feature_name = [i for i in all_data.columns if i not in ['sid', 'label', 'time']]
print(feature_name)
print('all_data.info:', all_data.info())

# cat_list = ['pkgname', 'ver', 'adunitshowid', 'mediashowid', 'apptype', 'ip', 'city', 'province', 'reqrealip',
#             'adidmd5',
#             'imeimd5', 'idfamd5', 'openudidmd5', 'macmd5', 'dvctype', 'model', 'make', 'ntt', 'carrier', 'os', 'osv',
#             'orientation', 'lan', 'h', 'w', 'ppi', 'ip2', 'new_make', 'new_model', 'country', 'new_province',
#             'new_city',
#             'ip3', 'reqrealip2', 'reqrealip3']
cat_list = ['pkgname', 'ver', 'adunitshowid', 'mediashowid', 'apptype', 'ip', 'city', 'province', 'reqrealip',
            'adidmd5',
            'imeimd5', 'idfamd5', 'openudidmd5', 'macmd5', 'dvctype', 'model', 'make', 'ntt', 'carrier', 'os', 'osv',
            'orientation', 'lan', 'h', 'w', 'ppi', 'ip2',
            'ip3', 'reqrealip2', 'reqrealip3']

tr_index = ~all_data['label'].isnull()
X_train = all_data[tr_index][list(set(feature_name))].reset_index(drop=True)
y = all_data[tr_index]['label'].reset_index(drop=True).astype(int)
X_test = all_data[~tr_index][list(set(feature_name))].reset_index(drop=True)
print(X_train.shape, X_test.shape)
# 节约一下内存
del all_data
gc.collect()


# 以下代码是5折交叉验证的结果 + lgb catboost xgb 最后使用logist进行回归预测
def get_stacking(clf, x_train, y_train, x_test, feature_name, n_folds=5):
    print('len_x_train:', len(x_train))

    train_num, test_num = x_train.shape[0], x_test.shape[0]
    second_level_train_set = np.zeros((train_num,))
    second_level_test_set = np.zeros((test_num,))
    test_nfolds_sets = np.zeros((test_num, n_folds))
    kf = KFold(n_splits=n_folds)

    for i, (train_index, test_index) in enumerate(kf.split(x_train)):
        x_tra, y_tra = x_train[feature_name].iloc[train_index], y_train[train_index]
        x_tst, y_tst = x_train[feature_name].iloc[test_index], y_train[test_index]

        clf.fit(x_tra[feature_name], y_tra, eval_set=[(x_tst[feature_name], y_tst)])

        second_level_train_set[test_index] = clf.predict(x_tst[feature_name])
        test_nfolds_sets[:, i] = clf.predict(x_test[feature_name])

    second_level_test_set[:] = test_nfolds_sets.mean(axis=1)
    return second_level_train_set, second_level_test_set


def lgb_f1(labels, preds):
    score = f1_score(labels, np.round(preds))
    return 'f1', score, True


lgb_model = lgb.LGBMClassifier(random_seed=2019, n_jobs=-1, objective='binary', learning_rate=0.05, n_estimators=3000,
                               num_leaves=31, max_depth=-1, min_child_samples=50, min_child_weight=9, subsample_freq=1,
                               subsample=0.7, colsample_bytree=0.7, reg_alpha=1, reg_lambda=5,
                               early_stopping_rounds=400)


xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='auc', learning_rate=0.02, max_depth=6,
                              early_stopping_rounds=400, feval=lgb_f1)


cbt_model = cbt.CatBoostClassifier(iterations=3000, learning_rate=0.05, max_depth=11, l2_leaf_reg=1, verbose=10,
                                   early_stopping_rounds=400, task_type='GPU', eval_metric='F1')

train_sets = []
test_sets = []
for clf in [xgb_model, cbt_model, lgb_model]:
    print('begin train clf:', clf)
    train_set, test_set = get_stacking(clf, X_train, y, X_test, feature_name)
    train_sets.append(train_set)
    test_sets.append(test_set)

meta_train = np.concatenate([result_set.reshape(-1, 1) for result_set in train_sets], axis=1)
meta_test = np.concatenate([y_test_set.reshape(-1, 1) for y_test_set in test_sets], axis=1)

# 使用逻辑回归作为第二层模型
bclf = LogisticRegression()
bclf.fit(meta_train, y)
test_pred = bclf.predict_proba(meta_test)[:, 1]

# 提交结果
submit = test[['sid']]
submit['label'] = (test_pred &gt;= 0.5).astype(int)
print(submit['label'].value_counts())
submit.to_csv("A_Simple_Stacking_Model.csv", index=False)

# 打印预测地概率 方便以后使用融合模型
df_sub = pd.concat([test['sid'], pd.Series(test_pred)], axis=1)
df_sub.columns = ['sid', 'label']
df_sub.to_csv('A_Simple_Stacking_Model_proba.csv', sep=',', index=False)

</code></pre> 
  <p>&nbsp;</p> 
  <p>XGB输出结果：</p> 
  <p><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190819200022126.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzc2Nzgx,size_16,color_FFFFFF,t_70"></p> 
  <p>&nbsp;</p> 
  <p>lgb输出的结果：</p> 
  <p>&nbsp;<img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190819200327376.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzc2Nzgx,size_16,color_FFFFFF,t_70"></p> 
  <p>&nbsp;</p> 
  <p>&nbsp;catboost输出的结果：</p> 
  <p><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190819200546744.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzc2Nzgx,size_16,color_FFFFFF,t_70"></p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
