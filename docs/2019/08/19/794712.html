<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Kubernetes笔记（二十六）－－Helm安装和使用 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Kubernetes笔记（二十六）－－Helm安装和使用" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Helm安装使用 Helm这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套测试环境，对于单体服务，部署一套测试环境我相信还是非常快的，但是对于微服务架构的应用，要部署一套新的环境，就有点折磨人了，微服务越多、你就会越绝望的。虽然我们线上和测试环境已经都迁移到了kubernetes环境，但是每个微服务也得维护一套yaml文件，而且每个环境下的配置文件也不太一样，部署一套新的环境成本是真的很高。如果我们能使用类似于yum的工具来安装我们的应用的话是不是就很爽歪歪了啊？Helm就相当于kubernetes环境下的yum包管理工具。 用途 做为 Kubernetes 的一个包管理工具，Helm具有如下功能： 创建新的 chart chart 打包成 tgz 格式 上传 chart 到 chart 仓库或从仓库中下载 chart 在Kubernetes集群中安装或卸载 chart 管理用Helm安装的 chart 的发布周期 重要概念 Helm 有三个重要概念： chart：包含了创建Kubernetes的一个应用实例的必要信息 config：包含了应用发布配置信息 release：是一个 chart 及其配置的一个运行实例 Helm组件 Helm 有以下两个组成部分： 图 1 图 2 Helm Client 是用户命令行工具，其主要负责如下： 本地 chart 开发 仓库管理 与 Tiller sever 交互 发送预安装的 chart 查询 release 信息 要求升级或卸载已存在的 release Tiller Server是一个部署在Kubernetes集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下： 监听来自 Helm client 的请求 通过 chart 及其配置构建一次发布 安装 chart 到Kubernetes集群，并跟踪随后的发布 通过与Kubernetes交互升级或卸载 chart 简单的说，client 管理 charts，而 server 管理发布 release 安装 我们可以在Helm Realese(https://github.com/helm/helm/releases)页面下载二进制文件，这里下载的v2.10.0版本，解压后将可执行文件helm拷贝到/usr/local/bin目录下即可，这样Helm客户端就在这台机器上安装完成了。 现在我们可以使用Helm命令查看版本了，会提示无法连接到服务端Tiller： $ helm version Client: &amp;version.Version{SemVer:&quot;v2.10.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;} Error: could not find tiller 要安装 Helm 的服务端程序，我们需要使用到kubectl工具，所以先确保kubectl工具能够正常的访问 kubernetes 集群的apiserver哦。 然后我们在命令行中执行初始化操作： $ helm init 由于 Helm 默认会去gcr.io拉取镜像，所以如果你当前执行的机器没有配置科学上网的话可以实现下面的命令代替： $ helm init --upgrade --tiller-image cnych/tiller:v2.10.0 $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure &#39;allow unauthenticated users&#39; policy. To prevent this, run `helm init` with the --tiller-tls-verify flag. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! 如果一直卡住或者报 google api 之类的错误，可以使用下面的命令进行初始化： $ helm init --upgrade --tiller-image cnych/tiller:v2.10.0 --stable-repo-url https://cnych.github.io/kube-charts-mirror/ 这个命令会把默认的 google 的仓库地址替换成我同步的一个镜像地址。 如果在安装过程中遇到了一些其他问题，比如初始化的时候出现了如下错误： E0125 14:03:19.093131 56246 portforward.go:331] an error occurred forwarding 55943 -&gt; 44134: error forwarding port 44134 to pod d01941068c9dfea1c9e46127578994d1cf8bc34c971ff109dc6faa4c05043a6e, uid : unable to do port forwarding: socat not found. 2018/01/25 14:03:19 (0xc420476210) (0xc4203ae1e0) Stream removed, broadcasting: 3 2018/01/25 14:03:19 (0xc4203ae1e0) (3) Writing data frame 2018/01/25 14:03:19 (0xc420476210) (0xc4200c3900) Create stream 2018/01/25 14:03:19 (0xc420476210) (0xc4200c3900) Stream added, broadcasting: 5 Error: cannot connect to Tiller 解决方案：在节点上安装socat可以解决 $ sudo yum install -y socat Helm 服务端正常安装完成后，Tiller默认被部署在kubernetes集群的kube-system命名空间下： $ kubectl get pod -n kube-system -l app=helm NAME READY STATUS RESTARTS AGE tiller-deploy-86b844d8c6-44fpq 1/1 Running 0 7m 此时，我们查看 Helm 版本就都正常了： $ helm version Client: &amp;version.Version{SemVer:&quot;v2.10.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;} Server: &amp;version.Version{SemVer:&quot;v2.10.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;} 另外一个值得注意的问题是RBAC，我们的 kubernetes 集群是1.10.0版本的，默认开启了RBAC访问控制，所以我们需要为Tiller创建一个ServiceAccount，让他拥有执行的权限，详细内容可以查看 Helm 文档中的Role-based Access Control。 创建rbac.yaml文件： apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system 然后使用kubectl创建： $ kubectl create -f rbac-config.yaml serviceaccount &quot;tiller&quot; created clusterrolebinding.rbac.authorization.k8s.io &quot;tiller&quot; created 创建了tiller的 ServceAccount 后还没完，因为我们的 Tiller 之前已经就部署成功了，而且是没有指定 ServiceAccount 的，所以我们需要给 Tiller 打上一个 ServiceAccount 的补丁： $ kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;serviceAccount&quot;:&quot;tiller&quot;}}}}&#39; 上面这一步非常重要，不然后面在使用 Helm 的过程中可能出现Error: no available release name found的错误信息。 至此, Helm客户端和服务端都配置完成了，接下来我们看看如何使用吧。 使用 我们现在了尝试创建一个 Chart： $ helm create hello-helm Creating hello-helm $ tree hello-helm hello-helm ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── NOTES.txt │ └── service.yaml └── values.yaml 2 directories, 7 files 我们通过查看templates目录下面的deployment.yaml文件可以看出默认创建的 Chart 是一个 nginx 服务，具体的每个文件是干什么用的，我们可以前往 Helm 官方文档进行查看，后面会和大家详细讲解的。比如这里我们来安装 1.7.9 这个版本的 nginx，则我们更改 value.yaml 文件下面的 image tag 即可，将默认的 stable 更改为 1.7.9，为了测试方便，我们把 Service 的类型也改成 NodePort ... image: repository: nginx tag: 1.7.9 pullPolicy: IfNotPresent nameOverride: &quot;&quot; fullnameOverride: &quot;&quot; service: type: NodePort port: 80 ... 现在我们来尝试安装下这个 Chart : $ helm install ./hello-helm NAME: iced-ferret LAST DEPLOYED: Thu Aug 30 23:39:45 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==&gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iced-ferret-hello-helm ClusterIP 10.100.118.77 &lt;none&gt; 80/TCP 0s ==&gt; v1beta2/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE iced-ferret-hello-helm 1 0 0 0 0s ==&gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE iced-ferret-hello-helm-58cb69d5bb-s9f2m 0/1 Pending 0 0s NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=hello-helm,release=iced-ferret&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;) echo &quot;Visit http://127.0.0.1:8080 to use your application&quot; kubectl port-forward $POD_NAME 8080:80 $ kubectl get pods -l app=hello-helm(我用grep helm 查找的） NAME READY STATUS RESTARTS AGE iced-ferret-hello-helm-58cb69d5bb-s9f2m 1/1 Running 0 2m $ kubectl get svc -l app=hello-helm NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iced-ferret-hello-helm NodePort 10.104.127.141 &lt;none&gt; 80:31236/TCP 3m 等到 Pod 创建完成后，我们可以根据创建的 Service 的 NodePort 来访问该服务了，然后在浏览器中打开http://k8s.haimaxy.com:31236就可以正常的访问我们刚刚部署的 nginx 应用了。 查看release： $ helm list NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE winning-zebra 1 Thu Aug 30 23:50:29 2018 DEPLOYED hello-helm-0.1.0 1.0 default 打包chart： $ helm package hello-helm Successfully packaged chart and saved it to: /root/course/kubeadm/helm/hello-helm-0.1.0.tgz 然后我们就可以将打包的tgz文件分发到任意的服务器上，通过helm fetch就可以获取到该 Chart 了。 删除release： $ helm delete winning-zebra release &quot;winning-zebra&quot; deleted 然后我们看到kubernetes集群上的该 nginx 服务也已经被删除了。 $ kubectl get pods -l app=hello-helm No resources found. 还有更多关于Helm的使用命令，我们可以前往官方文档查看。下节课我们再来和大家详细讲解。" />
<meta property="og:description" content="Helm安装使用 Helm这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套测试环境，对于单体服务，部署一套测试环境我相信还是非常快的，但是对于微服务架构的应用，要部署一套新的环境，就有点折磨人了，微服务越多、你就会越绝望的。虽然我们线上和测试环境已经都迁移到了kubernetes环境，但是每个微服务也得维护一套yaml文件，而且每个环境下的配置文件也不太一样，部署一套新的环境成本是真的很高。如果我们能使用类似于yum的工具来安装我们的应用的话是不是就很爽歪歪了啊？Helm就相当于kubernetes环境下的yum包管理工具。 用途 做为 Kubernetes 的一个包管理工具，Helm具有如下功能： 创建新的 chart chart 打包成 tgz 格式 上传 chart 到 chart 仓库或从仓库中下载 chart 在Kubernetes集群中安装或卸载 chart 管理用Helm安装的 chart 的发布周期 重要概念 Helm 有三个重要概念： chart：包含了创建Kubernetes的一个应用实例的必要信息 config：包含了应用发布配置信息 release：是一个 chart 及其配置的一个运行实例 Helm组件 Helm 有以下两个组成部分： 图 1 图 2 Helm Client 是用户命令行工具，其主要负责如下： 本地 chart 开发 仓库管理 与 Tiller sever 交互 发送预安装的 chart 查询 release 信息 要求升级或卸载已存在的 release Tiller Server是一个部署在Kubernetes集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下： 监听来自 Helm client 的请求 通过 chart 及其配置构建一次发布 安装 chart 到Kubernetes集群，并跟踪随后的发布 通过与Kubernetes交互升级或卸载 chart 简单的说，client 管理 charts，而 server 管理发布 release 安装 我们可以在Helm Realese(https://github.com/helm/helm/releases)页面下载二进制文件，这里下载的v2.10.0版本，解压后将可执行文件helm拷贝到/usr/local/bin目录下即可，这样Helm客户端就在这台机器上安装完成了。 现在我们可以使用Helm命令查看版本了，会提示无法连接到服务端Tiller： $ helm version Client: &amp;version.Version{SemVer:&quot;v2.10.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;} Error: could not find tiller 要安装 Helm 的服务端程序，我们需要使用到kubectl工具，所以先确保kubectl工具能够正常的访问 kubernetes 集群的apiserver哦。 然后我们在命令行中执行初始化操作： $ helm init 由于 Helm 默认会去gcr.io拉取镜像，所以如果你当前执行的机器没有配置科学上网的话可以实现下面的命令代替： $ helm init --upgrade --tiller-image cnych/tiller:v2.10.0 $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure &#39;allow unauthenticated users&#39; policy. To prevent this, run `helm init` with the --tiller-tls-verify flag. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! 如果一直卡住或者报 google api 之类的错误，可以使用下面的命令进行初始化： $ helm init --upgrade --tiller-image cnych/tiller:v2.10.0 --stable-repo-url https://cnych.github.io/kube-charts-mirror/ 这个命令会把默认的 google 的仓库地址替换成我同步的一个镜像地址。 如果在安装过程中遇到了一些其他问题，比如初始化的时候出现了如下错误： E0125 14:03:19.093131 56246 portforward.go:331] an error occurred forwarding 55943 -&gt; 44134: error forwarding port 44134 to pod d01941068c9dfea1c9e46127578994d1cf8bc34c971ff109dc6faa4c05043a6e, uid : unable to do port forwarding: socat not found. 2018/01/25 14:03:19 (0xc420476210) (0xc4203ae1e0) Stream removed, broadcasting: 3 2018/01/25 14:03:19 (0xc4203ae1e0) (3) Writing data frame 2018/01/25 14:03:19 (0xc420476210) (0xc4200c3900) Create stream 2018/01/25 14:03:19 (0xc420476210) (0xc4200c3900) Stream added, broadcasting: 5 Error: cannot connect to Tiller 解决方案：在节点上安装socat可以解决 $ sudo yum install -y socat Helm 服务端正常安装完成后，Tiller默认被部署在kubernetes集群的kube-system命名空间下： $ kubectl get pod -n kube-system -l app=helm NAME READY STATUS RESTARTS AGE tiller-deploy-86b844d8c6-44fpq 1/1 Running 0 7m 此时，我们查看 Helm 版本就都正常了： $ helm version Client: &amp;version.Version{SemVer:&quot;v2.10.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;} Server: &amp;version.Version{SemVer:&quot;v2.10.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;} 另外一个值得注意的问题是RBAC，我们的 kubernetes 集群是1.10.0版本的，默认开启了RBAC访问控制，所以我们需要为Tiller创建一个ServiceAccount，让他拥有执行的权限，详细内容可以查看 Helm 文档中的Role-based Access Control。 创建rbac.yaml文件： apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system 然后使用kubectl创建： $ kubectl create -f rbac-config.yaml serviceaccount &quot;tiller&quot; created clusterrolebinding.rbac.authorization.k8s.io &quot;tiller&quot; created 创建了tiller的 ServceAccount 后还没完，因为我们的 Tiller 之前已经就部署成功了，而且是没有指定 ServiceAccount 的，所以我们需要给 Tiller 打上一个 ServiceAccount 的补丁： $ kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;serviceAccount&quot;:&quot;tiller&quot;}}}}&#39; 上面这一步非常重要，不然后面在使用 Helm 的过程中可能出现Error: no available release name found的错误信息。 至此, Helm客户端和服务端都配置完成了，接下来我们看看如何使用吧。 使用 我们现在了尝试创建一个 Chart： $ helm create hello-helm Creating hello-helm $ tree hello-helm hello-helm ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── NOTES.txt │ └── service.yaml └── values.yaml 2 directories, 7 files 我们通过查看templates目录下面的deployment.yaml文件可以看出默认创建的 Chart 是一个 nginx 服务，具体的每个文件是干什么用的，我们可以前往 Helm 官方文档进行查看，后面会和大家详细讲解的。比如这里我们来安装 1.7.9 这个版本的 nginx，则我们更改 value.yaml 文件下面的 image tag 即可，将默认的 stable 更改为 1.7.9，为了测试方便，我们把 Service 的类型也改成 NodePort ... image: repository: nginx tag: 1.7.9 pullPolicy: IfNotPresent nameOverride: &quot;&quot; fullnameOverride: &quot;&quot; service: type: NodePort port: 80 ... 现在我们来尝试安装下这个 Chart : $ helm install ./hello-helm NAME: iced-ferret LAST DEPLOYED: Thu Aug 30 23:39:45 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==&gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iced-ferret-hello-helm ClusterIP 10.100.118.77 &lt;none&gt; 80/TCP 0s ==&gt; v1beta2/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE iced-ferret-hello-helm 1 0 0 0 0s ==&gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE iced-ferret-hello-helm-58cb69d5bb-s9f2m 0/1 Pending 0 0s NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=hello-helm,release=iced-ferret&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;) echo &quot;Visit http://127.0.0.1:8080 to use your application&quot; kubectl port-forward $POD_NAME 8080:80 $ kubectl get pods -l app=hello-helm(我用grep helm 查找的） NAME READY STATUS RESTARTS AGE iced-ferret-hello-helm-58cb69d5bb-s9f2m 1/1 Running 0 2m $ kubectl get svc -l app=hello-helm NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iced-ferret-hello-helm NodePort 10.104.127.141 &lt;none&gt; 80:31236/TCP 3m 等到 Pod 创建完成后，我们可以根据创建的 Service 的 NodePort 来访问该服务了，然后在浏览器中打开http://k8s.haimaxy.com:31236就可以正常的访问我们刚刚部署的 nginx 应用了。 查看release： $ helm list NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE winning-zebra 1 Thu Aug 30 23:50:29 2018 DEPLOYED hello-helm-0.1.0 1.0 default 打包chart： $ helm package hello-helm Successfully packaged chart and saved it to: /root/course/kubeadm/helm/hello-helm-0.1.0.tgz 然后我们就可以将打包的tgz文件分发到任意的服务器上，通过helm fetch就可以获取到该 Chart 了。 删除release： $ helm delete winning-zebra release &quot;winning-zebra&quot; deleted 然后我们看到kubernetes集群上的该 nginx 服务也已经被删除了。 $ kubectl get pods -l app=hello-helm No resources found. 还有更多关于Helm的使用命令，我们可以前往官方文档查看。下节课我们再来和大家详细讲解。" />
<link rel="canonical" href="https://uzzz.org/2019/08/19/794712.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/19/794712.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-19T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Helm安装使用 Helm这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套测试环境，对于单体服务，部署一套测试环境我相信还是非常快的，但是对于微服务架构的应用，要部署一套新的环境，就有点折磨人了，微服务越多、你就会越绝望的。虽然我们线上和测试环境已经都迁移到了kubernetes环境，但是每个微服务也得维护一套yaml文件，而且每个环境下的配置文件也不太一样，部署一套新的环境成本是真的很高。如果我们能使用类似于yum的工具来安装我们的应用的话是不是就很爽歪歪了啊？Helm就相当于kubernetes环境下的yum包管理工具。 用途 做为 Kubernetes 的一个包管理工具，Helm具有如下功能： 创建新的 chart chart 打包成 tgz 格式 上传 chart 到 chart 仓库或从仓库中下载 chart 在Kubernetes集群中安装或卸载 chart 管理用Helm安装的 chart 的发布周期 重要概念 Helm 有三个重要概念： chart：包含了创建Kubernetes的一个应用实例的必要信息 config：包含了应用发布配置信息 release：是一个 chart 及其配置的一个运行实例 Helm组件 Helm 有以下两个组成部分： 图 1 图 2 Helm Client 是用户命令行工具，其主要负责如下： 本地 chart 开发 仓库管理 与 Tiller sever 交互 发送预安装的 chart 查询 release 信息 要求升级或卸载已存在的 release Tiller Server是一个部署在Kubernetes集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下： 监听来自 Helm client 的请求 通过 chart 及其配置构建一次发布 安装 chart 到Kubernetes集群，并跟踪随后的发布 通过与Kubernetes交互升级或卸载 chart 简单的说，client 管理 charts，而 server 管理发布 release 安装 我们可以在Helm Realese(https://github.com/helm/helm/releases)页面下载二进制文件，这里下载的v2.10.0版本，解压后将可执行文件helm拷贝到/usr/local/bin目录下即可，这样Helm客户端就在这台机器上安装完成了。 现在我们可以使用Helm命令查看版本了，会提示无法连接到服务端Tiller： $ helm version Client: &amp;version.Version{SemVer:&quot;v2.10.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;} Error: could not find tiller 要安装 Helm 的服务端程序，我们需要使用到kubectl工具，所以先确保kubectl工具能够正常的访问 kubernetes 集群的apiserver哦。 然后我们在命令行中执行初始化操作： $ helm init 由于 Helm 默认会去gcr.io拉取镜像，所以如果你当前执行的机器没有配置科学上网的话可以实现下面的命令代替： $ helm init --upgrade --tiller-image cnych/tiller:v2.10.0 $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure &#39;allow unauthenticated users&#39; policy. To prevent this, run `helm init` with the --tiller-tls-verify flag. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! 如果一直卡住或者报 google api 之类的错误，可以使用下面的命令进行初始化： $ helm init --upgrade --tiller-image cnych/tiller:v2.10.0 --stable-repo-url https://cnych.github.io/kube-charts-mirror/ 这个命令会把默认的 google 的仓库地址替换成我同步的一个镜像地址。 如果在安装过程中遇到了一些其他问题，比如初始化的时候出现了如下错误： E0125 14:03:19.093131 56246 portforward.go:331] an error occurred forwarding 55943 -&gt; 44134: error forwarding port 44134 to pod d01941068c9dfea1c9e46127578994d1cf8bc34c971ff109dc6faa4c05043a6e, uid : unable to do port forwarding: socat not found. 2018/01/25 14:03:19 (0xc420476210) (0xc4203ae1e0) Stream removed, broadcasting: 3 2018/01/25 14:03:19 (0xc4203ae1e0) (3) Writing data frame 2018/01/25 14:03:19 (0xc420476210) (0xc4200c3900) Create stream 2018/01/25 14:03:19 (0xc420476210) (0xc4200c3900) Stream added, broadcasting: 5 Error: cannot connect to Tiller 解决方案：在节点上安装socat可以解决 $ sudo yum install -y socat Helm 服务端正常安装完成后，Tiller默认被部署在kubernetes集群的kube-system命名空间下： $ kubectl get pod -n kube-system -l app=helm NAME READY STATUS RESTARTS AGE tiller-deploy-86b844d8c6-44fpq 1/1 Running 0 7m 此时，我们查看 Helm 版本就都正常了： $ helm version Client: &amp;version.Version{SemVer:&quot;v2.10.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;} Server: &amp;version.Version{SemVer:&quot;v2.10.0&quot;, GitCommit:&quot;9ad53aac42165a5fadc6c87be0dea6b115f93090&quot;, GitTreeState:&quot;clean&quot;} 另外一个值得注意的问题是RBAC，我们的 kubernetes 集群是1.10.0版本的，默认开启了RBAC访问控制，所以我们需要为Tiller创建一个ServiceAccount，让他拥有执行的权限，详细内容可以查看 Helm 文档中的Role-based Access Control。 创建rbac.yaml文件： apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system 然后使用kubectl创建： $ kubectl create -f rbac-config.yaml serviceaccount &quot;tiller&quot; created clusterrolebinding.rbac.authorization.k8s.io &quot;tiller&quot; created 创建了tiller的 ServceAccount 后还没完，因为我们的 Tiller 之前已经就部署成功了，而且是没有指定 ServiceAccount 的，所以我们需要给 Tiller 打上一个 ServiceAccount 的补丁： $ kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;serviceAccount&quot;:&quot;tiller&quot;}}}}&#39; 上面这一步非常重要，不然后面在使用 Helm 的过程中可能出现Error: no available release name found的错误信息。 至此, Helm客户端和服务端都配置完成了，接下来我们看看如何使用吧。 使用 我们现在了尝试创建一个 Chart： $ helm create hello-helm Creating hello-helm $ tree hello-helm hello-helm ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── NOTES.txt │ └── service.yaml └── values.yaml 2 directories, 7 files 我们通过查看templates目录下面的deployment.yaml文件可以看出默认创建的 Chart 是一个 nginx 服务，具体的每个文件是干什么用的，我们可以前往 Helm 官方文档进行查看，后面会和大家详细讲解的。比如这里我们来安装 1.7.9 这个版本的 nginx，则我们更改 value.yaml 文件下面的 image tag 即可，将默认的 stable 更改为 1.7.9，为了测试方便，我们把 Service 的类型也改成 NodePort ... image: repository: nginx tag: 1.7.9 pullPolicy: IfNotPresent nameOverride: &quot;&quot; fullnameOverride: &quot;&quot; service: type: NodePort port: 80 ... 现在我们来尝试安装下这个 Chart : $ helm install ./hello-helm NAME: iced-ferret LAST DEPLOYED: Thu Aug 30 23:39:45 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==&gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iced-ferret-hello-helm ClusterIP 10.100.118.77 &lt;none&gt; 80/TCP 0s ==&gt; v1beta2/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE iced-ferret-hello-helm 1 0 0 0 0s ==&gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE iced-ferret-hello-helm-58cb69d5bb-s9f2m 0/1 Pending 0 0s NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=hello-helm,release=iced-ferret&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;) echo &quot;Visit http://127.0.0.1:8080 to use your application&quot; kubectl port-forward $POD_NAME 8080:80 $ kubectl get pods -l app=hello-helm(我用grep helm 查找的） NAME READY STATUS RESTARTS AGE iced-ferret-hello-helm-58cb69d5bb-s9f2m 1/1 Running 0 2m $ kubectl get svc -l app=hello-helm NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iced-ferret-hello-helm NodePort 10.104.127.141 &lt;none&gt; 80:31236/TCP 3m 等到 Pod 创建完成后，我们可以根据创建的 Service 的 NodePort 来访问该服务了，然后在浏览器中打开http://k8s.haimaxy.com:31236就可以正常的访问我们刚刚部署的 nginx 应用了。 查看release： $ helm list NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE winning-zebra 1 Thu Aug 30 23:50:29 2018 DEPLOYED hello-helm-0.1.0 1.0 default 打包chart： $ helm package hello-helm Successfully packaged chart and saved it to: /root/course/kubeadm/helm/hello-helm-0.1.0.tgz 然后我们就可以将打包的tgz文件分发到任意的服务器上，通过helm fetch就可以获取到该 Chart 了。 删除release： $ helm delete winning-zebra release &quot;winning-zebra&quot; deleted 然后我们看到kubernetes集群上的该 nginx 服务也已经被删除了。 $ kubectl get pods -l app=hello-helm No resources found. 还有更多关于Helm的使用命令，我们可以前往官方文档查看。下节课我们再来和大家详细讲解。","@type":"BlogPosting","url":"https://uzzz.org/2019/08/19/794712.html","headline":"Kubernetes笔记（二十六）－－Helm安装和使用","dateModified":"2019-08-19T00:00:00+08:00","datePublished":"2019-08-19T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/19/794712.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Kubernetes笔记（二十六）－－Helm安装和使用</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-tomorrow-night"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h3><a id="Helm_0"></a>Helm安装使用</h3> 
  <p>Helm这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套测试环境，对于单体服务，部署一套测试环境我相信还是非常快的，但是对于微服务架构的应用，要部署一套新的环境，就有点折磨人了，微服务越多、你就会越绝望的。虽然我们线上和测试环境已经都迁移到了kubernetes环境，但是每个微服务也得维护一套yaml文件，而且每个环境下的配置文件也不太一样，部署一套新的环境成本是真的很高。如果我们能使用类似于yum的工具来安装我们的应用的话是不是就很爽歪歪了啊？Helm就相当于kubernetes环境下的yum包管理工具。</p> 
  <h4><a id="_4"></a>用途</h4> 
  <p>做为 Kubernetes 的一个包管理工具，Helm具有如下功能：</p> 
  <pre><code>创建新的 chart
chart 打包成 tgz 格式
上传 chart 到 chart 仓库或从仓库中下载 chart
在Kubernetes集群中安装或卸载 chart
管理用Helm安装的 chart 的发布周期
</code></pre> 
  <h4><a id="_13"></a>重要概念</h4> 
  <p>Helm 有三个重要概念：</p> 
  <pre><code>chart：包含了创建Kubernetes的一个应用实例的必要信息
config：包含了应用发布配置信息
release：是一个 chart 及其配置的一个运行实例
</code></pre> 
  <h4><a id="Helm_20"></a>Helm组件</h4> 
  <p>Helm 有以下两个组成部分：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725102349611.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JpZ0RhdGFfTWluaW5n,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 图 1<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190826165227797.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JpZ0RhdGFfTWluaW5n,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 图 2<br> Helm Client 是用户命令行工具，其主要负责如下：</p> 
  <pre><code>本地 chart 开发
仓库管理
与 Tiller sever 交互
发送预安装的 chart
查询 release 信息
要求升级或卸载已存在的 release
</code></pre> 
  <p>Tiller Server是一个部署在Kubernetes集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下：</p> 
  <pre><code>监听来自 Helm client 的请求
通过 chart 及其配置构建一次发布
安装 chart 到Kubernetes集群，并跟踪随后的发布
通过与Kubernetes交互升级或卸载 chart
简单的说，client 管理 charts，而 server 管理发布 release
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190826164800389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JpZ0RhdGFfTWluaW5n,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h4><a id="_43"></a>安装</h4> 
  <p>我们可以在Helm Realese(https://github.com/helm/helm/releases)页面下载二进制文件，这里下载的v2.10.0版本，解压后将可执行文件helm拷贝到/usr/local/bin目录下即可，这样Helm客户端就在这台机器上安装完成了。</p> 
  <p>现在我们可以使用Helm命令查看版本了，会提示无法连接到服务端Tiller：</p> 
  <pre><code>$ helm version
Client: &amp;version.Version{SemVer:"v2.10.0", GitCommit:"9ad53aac42165a5fadc6c87be0dea6b115f93090", GitTreeState:"clean"}
Error: could not find tiller
</code></pre> 
  <p>要安装 Helm 的服务端程序，我们需要使用到kubectl工具，所以先确保kubectl工具能够正常的访问 kubernetes 集群的apiserver哦。</p> 
  <p>然后我们在命令行中执行初始化操作：</p> 
  <pre><code>$ helm init
</code></pre> 
  <p>由于 Helm 默认会去gcr.io拉取镜像，所以如果你当前执行的机器没有配置科学上网的话可以实现下面的命令代替：</p> 
  <pre><code>$ helm init --upgrade --tiller-image cnych/tiller:v2.10.0
$HELM_HOME has been configured at /root/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.

Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.
To prevent this, run `helm init` with the --tiller-tls-verify flag.
For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation
Happy Helming!
</code></pre> 
  <p>如果一直卡住或者报 google api 之类的错误，可以使用下面的命令进行初始化：</p> 
  <pre><code>$ helm init --upgrade --tiller-image cnych/tiller:v2.10.0 --stable-repo-url https://cnych.github.io/kube-charts-mirror/
</code></pre> 
  <p>这个命令会把默认的 google 的仓库地址替换成我同步的一个镜像地址。</p> 
  <p>如果在安装过程中遇到了一些其他问题，比如初始化的时候出现了如下错误：</p> 
  <pre><code>E0125 14:03:19.093131   56246 portforward.go:331] an error occurred forwarding 55943 -&gt; 44134: error forwarding port 44134 to pod d01941068c9dfea1c9e46127578994d1cf8bc34c971ff109dc6faa4c05043a6e, uid : unable to do port forwarding: socat not found.
2018/01/25 14:03:19 (0xc420476210) (0xc4203ae1e0) Stream removed, broadcasting: 3
2018/01/25 14:03:19 (0xc4203ae1e0) (3) Writing data frame
2018/01/25 14:03:19 (0xc420476210) (0xc4200c3900) Create stream
2018/01/25 14:03:19 (0xc420476210) (0xc4200c3900) Stream added, broadcasting: 5
Error: cannot connect to Tiller
</code></pre> 
  <p>解决方案：在节点上安装socat可以解决</p> 
  <pre><code>$ sudo yum install -y socat
</code></pre> 
  <p>Helm 服务端正常安装完成后，Tiller默认被部署在kubernetes集群的kube-system命名空间下：</p> 
  <pre><code>$ kubectl get pod -n kube-system -l app=helm
NAME                             READY     STATUS    RESTARTS   AGE
tiller-deploy-86b844d8c6-44fpq   1/1       Running   0          7m
</code></pre> 
  <p>此时，我们查看 Helm 版本就都正常了：</p> 
  <pre><code>$ helm version
Client: &amp;version.Version{SemVer:"v2.10.0", GitCommit:"9ad53aac42165a5fadc6c87be0dea6b115f93090", GitTreeState:"clean"}
Server: &amp;version.Version{SemVer:"v2.10.0", GitCommit:"9ad53aac42165a5fadc6c87be0dea6b115f93090", GitTreeState:"clean"}
</code></pre> 
  <p>另外一个值得注意的问题是RBAC，我们的 kubernetes 集群是1.10.0版本的，默认开启了RBAC访问控制，所以我们需要为Tiller创建一个ServiceAccount，让他拥有执行的权限，详细内容可以查看 Helm 文档中的Role-based Access Control。 创建rbac.yaml文件：</p> 
  <pre><code>apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
</code></pre> 
  <p>然后使用kubectl创建：</p> 
  <pre><code>$ kubectl create -f rbac-config.yaml
serviceaccount "tiller" created
clusterrolebinding.rbac.authorization.k8s.io "tiller" created
</code></pre> 
  <p>创建了tiller的 ServceAccount 后还没完，因为我们的 Tiller 之前已经就部署成功了，而且是没有指定 ServiceAccount 的，所以我们需要给 Tiller 打上一个 ServiceAccount 的补丁：</p> 
  <pre><code>$ kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
</code></pre> 
  <pre><code>上面这一步非常重要，不然后面在使用 Helm 的过程中可能出现Error: no available release name found的错误信息。
</code></pre> 
  <p>至此, Helm客户端和服务端都配置完成了，接下来我们看看如何使用吧。<br> 使用</p> 
  <p>我们现在了尝试创建一个 Chart：</p> 
  <pre><code>$ helm create hello-helm
Creating hello-helm
$ tree hello-helm
hello-helm
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml

2 directories, 7 files
</code></pre> 
  <p>我们通过查看templates目录下面的deployment.yaml文件可以看出默认创建的 Chart 是一个 nginx 服务，具体的每个文件是干什么用的，我们可以前往 Helm 官方文档进行查看，后面会和大家详细讲解的。比如这里我们来安装 1.7.9 这个版本的 nginx，则我们更改 value.yaml 文件下面的 image tag 即可，将默认的 stable 更改为 1.7.9，为了测试方便，我们把 Service 的类型也改成 NodePort</p> 
  <pre><code>...
image:
  repository: nginx
  tag: 1.7.9
  pullPolicy: IfNotPresent

nameOverride: ""
fullnameOverride: ""

service:
  type: NodePort
  port: 80
...
</code></pre> 
  <p>现在我们来尝试安装下这个 Chart :</p> 
  <pre><code>$ helm install ./hello-helm
NAME:   iced-ferret
LAST DEPLOYED: Thu Aug 30 23:39:45 2018
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&gt; v1/Service
NAME                    TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)  AGE
iced-ferret-hello-helm  ClusterIP  10.100.118.77  &lt;none&gt;       80/TCP   0s

==&gt; v1beta2/Deployment
NAME                    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
iced-ferret-hello-helm  1        0        0           0          0s

==&gt; v1/Pod(related)
NAME                                     READY  STATUS   RESTARTS  AGE
iced-ferret-hello-helm-58cb69d5bb-s9f2m  0/1    Pending  0         0s


NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=hello-helm,release=iced-ferret" -o jsonpath="{.items[0].metadata.name}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl port-forward $POD_NAME 8080:80

$ kubectl get pods -l app=hello-helm(我用grep helm 查找的）
NAME                                      READY     STATUS    RESTARTS   AGE
iced-ferret-hello-helm-58cb69d5bb-s9f2m   1/1       Running   0          2m
$ kubectl get svc -l app=hello-helm
NAME                       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
iced-ferret-hello-helm   NodePort   10.104.127.141   &lt;none&gt;        80:31236/TCP   3m
</code></pre> 
  <p>等到 Pod 创建完成后，我们可以根据创建的 Service 的 NodePort 来访问该服务了，然后在浏览器中打开http://k8s.haimaxy.com:31236就可以正常的访问我们刚刚部署的 nginx 应用了。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190725102441578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JpZ0RhdGFfTWluaW5n,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">查看release：</p> 
  <pre><code>$ helm list
NAME             REVISION    UPDATED                     STATUS      CHART               APP VERSION    NAMESPACE
winning-zebra    1           Thu Aug 30 23:50:29 2018    DEPLOYED    hello-helm-0.1.0    1.0            default
</code></pre> 
  <p>打包chart：</p> 
  <pre><code>$ helm package hello-helm
Successfully packaged chart and saved it to: /root/course/kubeadm/helm/hello-helm-0.1.0.tgz
</code></pre> 
  <p>然后我们就可以将打包的tgz文件分发到任意的服务器上，通过helm fetch就可以获取到该 Chart 了。</p> 
  <p>删除release：</p> 
  <pre><code>$ helm delete winning-zebra
release "winning-zebra" deleted
</code></pre> 
  <p>然后我们看到kubernetes集群上的该 nginx 服务也已经被删除了。</p> 
  <pre><code>$ kubectl get pods -l app=hello-helm
No resources found.
</code></pre> 
  <p>还有更多关于Helm的使用命令，我们可以前往官方文档查看。下节课我们再来和大家详细讲解。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
