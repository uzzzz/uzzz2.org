<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>2、hadoop的运行模式（centos+docker+hadoop） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="2、hadoop的运行模式（centos+docker+hadoop）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 伪分布式模式 一、启动HFDS运行MapReduce程序 二、启动Yarn运行MapReduce程序 三、配置历史服务器 四、配置日志的聚集功能 完全分布式运行模式 一、集群部署介绍 二、集群配置 三、镜像制作与容器启动 四、启动集群 本案例基于centos + docker + hadoop进行测试。 上一节介绍了hadoop的环境搭建，以及hadoop的wordcount示例的本地运行模式，本章节介绍hadoop的其它两种运行模式，依然以wordcount为例。 hadoop包括3中运行模式：本地运行模式、伪分布式模式、完全分布式模式 List item：本地运行模式：没有启动hadoop的守护进程，所有的程序都运行在本地的JVM中，适合开发学习和调试mapreduce程序。 伪分布式模式：启动hadoop的守护进程，模拟一个简单的集群，伪分布式集群可以配置只有一台机器，适用于学习和调试。 伪分布式模式 以wordcount为例，下面配置伪分布式运行集群模式。 一、启动HFDS运行MapReduce程序 1、配置集群的JAVA_HOME环境 在hadoop的安装目录下的etc目录下有一个hadoop-env.sh文件（/opt/module/hadoop-2.7.2/etc/hadoop/hadoop-env.sh），添加JAVA_HOME环境路径，如下： export JAVA_HOME=/opt/module/jdk1.8.0_144 2、配置core-site.xml 在/opt/module/hadoop-2.7.2/etc/hadoop/hadoop-env.sh文件中添加如下配置： &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://lzj01:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; 其中lzj01为本案例ip地址 3、配置hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 4、启动集群 格式化NameNode（第一次启动时格式化，以后就不要总格式化） 启动命令：hdfs namenode -format 启动NameNode 启动命令：hadoop-daemon.sh start namenode 启动DataNode 启动命令：hadoop-daemon.sh start datanode 5、检验集群是否启动成功 执行jps命令，结果如下： 6、操作集群 在HDFS文件系统上创建一个input文件夹 执行命令：hdfs dfs -mkdir -p /user/lzj/input 将上一节中的wc.input测试文件上传到文件系统上 执行命令：hdfs dfs -put wcinput/wc.input /user/lzj/input/ 查看上传到集群上的文件 执行命令：hdfs dfs -ls /user/lzj/input/ 或hdfs dfs -cat /user/lzj/ input/wc.input 7、执行MapReduce程序进行计算 执行命令：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/lzj/input/ /user/lzj/output 8、查看运行结果 执行命令：hdfs dfs -cat /user/lzj/output/part-r-00000 查看结果如下 bu 1 chen 3 chuan 1 dao 1 de 1 deng 1 feng 1 hao 1 hong 1 hua 2 huang 2 jia 1 jie 4 ju 1 jun 1 lang 1 li 3 lin 1 liu 1 long 1 lun 1 mao 1 ning 1 qi 2 qian 1 rong 1 sheng 1 song 1 su 1 wang 3 xu 1 xue 3 xun 2 ya 1 yi 2 you 1 yu 1 yuan 1 zhang 2 zheng 1 zhi 2 zhong 1 zhou 1 zhuang 1 9、可以将分析结果文件从文件系统上下载到本地 执行命令：hdfs dfs -get /user/lzj/output/part-r-00000 ./wcoutput/ 执行命令后，可以在当前目录下wcoutput目录下查看到从文件系统上下载的文件。 10、可以删除文件系统上的文件或文件夹 执行命令：hdfs dfs -rm -r /user/lzj/output 删除文件系统上的output目录以及其下的文件。 11、可以从网页端查看文件系统中的文件 本案例访问地址：http://lzj01:50070 其中lzj01为运行hadoop的ip地址，50070为hdfs的端口，访问结果如下： 二、启动Yarn运行MapReduce程序 下面配置伪分布式集群，启动yarn运行MapReduce程序，在yarn上执行wordcount案例。 1、配置集群 （a）配置yarn-env.sh 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到yarn-env.sh文件，并在其中加入JAVA_HOME环境： export JAVA_HOME=/opt/module/jdk1.8.0_144 (b) 配置yarn-site.xml 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到yarn-site.xml文件，并在其中添加如下配置： &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;lzj01&lt;/value&gt; &lt;/property&gt; 其中lzj01为本宿主机ip地址。 （c) 配置mapred-env.sh 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到mapred-env.sh文件，并在其中添加java环境： export JAVA_HOME=/opt/module/jdk1.8.0_144 （d) 配置mapred-site.xml 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到mapred-site.xml.template文件并重命名为mapred-site.xml文件，在mapred-site.xml文件中配置MapReduce的运行方式： &lt;!-- 指定MR运行在YARN上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 2、启动集群 （a）在启动集群前必须保证NameNode和DataNode已经启动，通过jps观察如下： （b) 启动ResourceManager （c）启动NodeManager 3、集群操作 （a）访问yarn页面 在浏览器中访问http://lzj01:8088/cluster, 8088为访问yarn端口。页面如下： 其中显示了集群的配置资源和job的执行状态。 （b) 删除文件系统上的output下的文件 在执行案例前，要把之前案例生成的output文件删除掉，否则案例会报错，提示文件已经存在。执行删除命令： hdfs dfs -rm -R /user/lzj/output © 执行案例wordcount 的MapReduce程序 wordcount案例的执行命令如下：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/lzj/input /user/lzj/output 执行结束后，访问yarn的网页界面如下： 当网页中红色标记的progress进度为100%时，表示执行完毕，可以从网页中查看任务执行的状态、节点等信息。 三、配置历史服务器 为了查看MapReduce的历史运行情况，需要配置历史服务器，配置步骤如下： 1、配置mapred-site.xml文件 在mapred-site文件中添加如下配置： &lt;!-- 历史服务器端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;lzj01:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;lzj01:19888&lt;/value&gt; &lt;/property&gt; 其中lzj01为运行程序的ip地址 2、启动历史服务器 启动历史服务器，执行命令： mr-jobhistory-daemon.sh start historyserver 3、查看历史服务器是否启动 执行jps命令查看历史服务器运行状态 4、访问历史服务器JobHistory 访问http://lzj01:19888/jobhistory，其中lzj01为配置历史服务器的域名。 由于只运行了一个实例，所以只有一个唯一的job id的任务。 四、配置日志的聚集功能 日志聚集：每次运行完程序后，都会在本地生成log文件，也只能在一台宿主机上查看日志，日志聚集功能就是把应用完成后生成的日志信息上传到HFDS分布式文件系统上，这样集群上的所有宿主机都可以查看日志信息。 配置日志聚集功能步骤如下： 1、配置文件yarn-site.xml 在yarn-site.xml文件中添加如下内容： &lt;!-- 日志聚集功能使能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 日志保留时间设置2天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;172800&lt;/value&gt; &lt;/property&gt; 2、重启NodeManager 、ResourceManager和HistoryManager 首先执行关闭命令，分别执行如下命令： yarn-daemon.sh start resourcemanager yarn-daemon.sh start nodemanager mr-jobhistory-daemon.sh start historyserver 3、执行wordcount之前，先删除hdfs分布式文件系统上的输出文件夹，因为执行案例后会重新生成 执行命令： hdfs dfs -rm -R /user/lzj/output 4、查看日志 访问：http://lzj01:19888/jobhistory 图中红框选中的job id为本案例执行的job，点击，可查看job的详细信息 点击图中橙色标记的logs按钮，就可以查看案例执行的详细日志信息 完全分布式运行模式 一、集群部署介绍 本节介绍完全分布式，也是真正实战时用到的。本例中搭建一个包括三台容器的集群，分别为192.168.85.137、172.17.0.2、172.17.0.3。集群按一下方式进行配置 192.168.85.137 172.17.0.2 172.17.0.3 HDFS NameNodeDataNode DataNode SecondaryNameNodeDataNode YARN ResourceManagerNodeManager NodeManager NodeManager 二、集群配置 下面操作均在192.168.85.137上进行操作 1、核心配置文件core-site.xml 用vi打开core-site.xml文件，添加如下内容： &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.85.137:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; NameNode部署在192.168.85.137容器中。 2、配置HDFS 用vi打开hadoop-env.sh文件，添加如下内容： export JAVA_HOME=/opt/module/jdk1.8.0_144 用vi打开hdfs-site.xml文件，添加内容如下： &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;172.17.0.3:50090&lt;/value&gt; &lt;/property&gt; 把hdfs的辅助接点部署在172.17.0.3容器中。 3、配置YARN 用vi打开yarn-env.sh文件，添加如下内容： export JAVA_HOME=/opt/module/jdk1.8.0_144 用vi打开yarn-site.xml文件，添加如下内容： &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;192.168.85.137&lt;/value&gt; &lt;/property&gt; 将yarn的资源管理节点ResourceManager部署在192.168.85.137中。 4、配置MapReduce 配置mapred-env.sh，用vi 打开mapred-env.sh文件，添加如下内容： export JAVA_HOME=/opt/module/jdk1.8.0_144 配置mapred-site.xml，hadoop中默认文件是mapred-site.xml.template，复制成需要的名字，执行cp mapred-site.xml.template mapred-site.xml 用vi打开mapred-site.xml文件，添加如下内容： &lt;!-- 指定MR运行在Yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 三、镜像制作与容器启动 上一步全部配置均在192.168.85.137机器上进行的配置，下面把上述所有配置做成镜像，然后通过镜像启动容器，避免重复繁琐的配置hadoop，关于本例hadoop镜像制作请参考https://blog.csdn.net/u010502101/article/details/97697397，dockerfile内容如下： #version 0.1 FROM centos WORKDIR /opt RUN mkdir module ADD . /opt/module/ #set java enviroment RUN echo &quot;#JAVA_HOME&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export JAVA_HOME=/opt/module/jdk1.8.0_144&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export PATH=\$PATH:\$JAVA_HOME/bin&quot; &gt;&gt; /root/.bashrc #set hadoop enviroment RUN echo &quot;#HADOOP_HOME&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export HADOOP_HOME=/opt/module/hadoop-2.7.2&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export PATH=\$PATH:\$HADOOP_HOME/bin&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export PATH=\$PATH:\$HADOOP_HOME/sbin&quot; &gt;&gt; /root/.bashrc #install net tools, for using ping RUN yum -y install net-tools #set host RUN echo &quot;172.17.0.3 hadoop003&quot; &gt;&gt; /etc/hosts RUN echo &quot;172.17.0.2 hadoop002&quot; &gt;&gt; /etc/hosts RUN echo &quot;192.168.85.137 fanhao-test&quot; &gt;&gt; /etc/hosts #install which tool, hadoop use it RUN yum -y install which #install ssh RUN yum -y install openssh* #RUN systemctl start sshd #install service #RUN yum -y install initscripts #解决Docker中CentOS镜像无法使用systemd的问题 #ENV container docker RUN (cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == systemd-tmpfiles-setup.service ] || rm -f $i; done); \ rm -f /lib/systemd/system/multi-user.target.wants/*;\ rm -f /etc/systemd/system/*.wants/*;\ rm -f /lib/systemd/system/local-fs.target.wants/*; \ rm -f /lib/systemd/system/sockets.target.wants/*udev*; \ rm -f /lib/systemd/system/sockets.target.wants/*initctl*; \ rm -f /lib/systemd/system/basic.target.wants/*;\ rm -f /lib/systemd/system/anaconda.target.wants/*; VOLUME [ &quot;/sys/fs/cgroup&quot; ] CMD [&quot;/usr/sbin/init&quot;] dockerfile文件与hadoop安装处于同一层目录。 1、执行build命令，制作镜像 docker build -t=&quot;softwarebird/hadoop:0.4&quot; . 2、启动容器 用制作的softwarebird/hadoop:0.4镜像分别启动名为hadoop1和hadoop2的容器 启动hadoop1 docker run -itd --privileged=true --name hadoop1 softwarebird/hadoop:0.4 docker exec -it 7e4906d8751f /bin/bash 另外打开一个session，启动hadoop2 docker run -itd --privileged=true --name hadoop2 softwarebird/hadoop:0.4 docker exec -it efea0d9e0841 /bin/bash 3、配置ssh免密登录 切换到192.168.85.137宿主机上执行systemctl start sshd，重启ssh服务。 生成公钥和私钥，执行命令 ssh-keygen -t rsa 将192.168.85.137上的公钥copy到其它机器中，执行命令： ssh-copy-id 192.168.85.137 ssh-copy-id 172.17.0.2 ssh-copy-id 172.17.0.3 然后分别切换到172.17.0.2和172.17.0.3容器中，分别执行systemctl start sshd，然后分别执行下面命令： ssh-keygen -t rsa ssh-copy-id 192.168.85.137 ssh-copy-id 172.17.0.2 ssh-copy-id 172.17.0.3 四、启动集群 1、配置slaves 分别打开192.168.85.137、172.17.0.2、172.17.0.3集群中hadoop中配置文件slaves，并添加如下内容： 192.168.85.137 172.17.0.2 172.17.0.3 表示集群由这三个机器组成。 2、启动hdfs集群 切换到192.168.85.137宿主机，首次启动集群前，需要格式化namenode，执行命令hdfs namenode -format 启动hdfs，执行命令： start-dfs.sh hdfs启动后，执行jps命令，显示如下： 可知192.168.85.137中部署了NameNode和DataNode。 切换到172.17.0.2中，执行jps命令，显示如下： 可知172.17.0.2中只部署了DataNode。 切换到172.17.03中，执行jps命令，显示如下： 可知172.17.0.3中，除启动了DataNode外，还启动了备用节点SecondaryNameNode节点。 3、启动yarn 切换到192.168.85.137中，执行start-yarn.sh启动yarn，然后用jps命令查看进程，显示如下： 可知，192.168.85.137除启动hdfs进程外，又启动了ResourceManager和NodeManager进程。 切换到172.17.0.2容器中，执行jps命令查看，显示如下： 可见172.17.0.2中除启动hdfs进程外，又启动了NodeManager进程。 切换到172.17.0.3容器中，执行jps命令，显示如下： 可见172.17.0.3容器中除启动hdfs进程外，也只是启动了NodeManager进程。 4、验证集群启动是否成功 在浏览器中输入http://192.168.85.137:50070/，显示界面如下： 集群启动成功。" />
<meta property="og:description" content="文章目录 伪分布式模式 一、启动HFDS运行MapReduce程序 二、启动Yarn运行MapReduce程序 三、配置历史服务器 四、配置日志的聚集功能 完全分布式运行模式 一、集群部署介绍 二、集群配置 三、镜像制作与容器启动 四、启动集群 本案例基于centos + docker + hadoop进行测试。 上一节介绍了hadoop的环境搭建，以及hadoop的wordcount示例的本地运行模式，本章节介绍hadoop的其它两种运行模式，依然以wordcount为例。 hadoop包括3中运行模式：本地运行模式、伪分布式模式、完全分布式模式 List item：本地运行模式：没有启动hadoop的守护进程，所有的程序都运行在本地的JVM中，适合开发学习和调试mapreduce程序。 伪分布式模式：启动hadoop的守护进程，模拟一个简单的集群，伪分布式集群可以配置只有一台机器，适用于学习和调试。 伪分布式模式 以wordcount为例，下面配置伪分布式运行集群模式。 一、启动HFDS运行MapReduce程序 1、配置集群的JAVA_HOME环境 在hadoop的安装目录下的etc目录下有一个hadoop-env.sh文件（/opt/module/hadoop-2.7.2/etc/hadoop/hadoop-env.sh），添加JAVA_HOME环境路径，如下： export JAVA_HOME=/opt/module/jdk1.8.0_144 2、配置core-site.xml 在/opt/module/hadoop-2.7.2/etc/hadoop/hadoop-env.sh文件中添加如下配置： &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://lzj01:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; 其中lzj01为本案例ip地址 3、配置hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 4、启动集群 格式化NameNode（第一次启动时格式化，以后就不要总格式化） 启动命令：hdfs namenode -format 启动NameNode 启动命令：hadoop-daemon.sh start namenode 启动DataNode 启动命令：hadoop-daemon.sh start datanode 5、检验集群是否启动成功 执行jps命令，结果如下： 6、操作集群 在HDFS文件系统上创建一个input文件夹 执行命令：hdfs dfs -mkdir -p /user/lzj/input 将上一节中的wc.input测试文件上传到文件系统上 执行命令：hdfs dfs -put wcinput/wc.input /user/lzj/input/ 查看上传到集群上的文件 执行命令：hdfs dfs -ls /user/lzj/input/ 或hdfs dfs -cat /user/lzj/ input/wc.input 7、执行MapReduce程序进行计算 执行命令：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/lzj/input/ /user/lzj/output 8、查看运行结果 执行命令：hdfs dfs -cat /user/lzj/output/part-r-00000 查看结果如下 bu 1 chen 3 chuan 1 dao 1 de 1 deng 1 feng 1 hao 1 hong 1 hua 2 huang 2 jia 1 jie 4 ju 1 jun 1 lang 1 li 3 lin 1 liu 1 long 1 lun 1 mao 1 ning 1 qi 2 qian 1 rong 1 sheng 1 song 1 su 1 wang 3 xu 1 xue 3 xun 2 ya 1 yi 2 you 1 yu 1 yuan 1 zhang 2 zheng 1 zhi 2 zhong 1 zhou 1 zhuang 1 9、可以将分析结果文件从文件系统上下载到本地 执行命令：hdfs dfs -get /user/lzj/output/part-r-00000 ./wcoutput/ 执行命令后，可以在当前目录下wcoutput目录下查看到从文件系统上下载的文件。 10、可以删除文件系统上的文件或文件夹 执行命令：hdfs dfs -rm -r /user/lzj/output 删除文件系统上的output目录以及其下的文件。 11、可以从网页端查看文件系统中的文件 本案例访问地址：http://lzj01:50070 其中lzj01为运行hadoop的ip地址，50070为hdfs的端口，访问结果如下： 二、启动Yarn运行MapReduce程序 下面配置伪分布式集群，启动yarn运行MapReduce程序，在yarn上执行wordcount案例。 1、配置集群 （a）配置yarn-env.sh 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到yarn-env.sh文件，并在其中加入JAVA_HOME环境： export JAVA_HOME=/opt/module/jdk1.8.0_144 (b) 配置yarn-site.xml 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到yarn-site.xml文件，并在其中添加如下配置： &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;lzj01&lt;/value&gt; &lt;/property&gt; 其中lzj01为本宿主机ip地址。 （c) 配置mapred-env.sh 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到mapred-env.sh文件，并在其中添加java环境： export JAVA_HOME=/opt/module/jdk1.8.0_144 （d) 配置mapred-site.xml 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到mapred-site.xml.template文件并重命名为mapred-site.xml文件，在mapred-site.xml文件中配置MapReduce的运行方式： &lt;!-- 指定MR运行在YARN上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 2、启动集群 （a）在启动集群前必须保证NameNode和DataNode已经启动，通过jps观察如下： （b) 启动ResourceManager （c）启动NodeManager 3、集群操作 （a）访问yarn页面 在浏览器中访问http://lzj01:8088/cluster, 8088为访问yarn端口。页面如下： 其中显示了集群的配置资源和job的执行状态。 （b) 删除文件系统上的output下的文件 在执行案例前，要把之前案例生成的output文件删除掉，否则案例会报错，提示文件已经存在。执行删除命令： hdfs dfs -rm -R /user/lzj/output © 执行案例wordcount 的MapReduce程序 wordcount案例的执行命令如下：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/lzj/input /user/lzj/output 执行结束后，访问yarn的网页界面如下： 当网页中红色标记的progress进度为100%时，表示执行完毕，可以从网页中查看任务执行的状态、节点等信息。 三、配置历史服务器 为了查看MapReduce的历史运行情况，需要配置历史服务器，配置步骤如下： 1、配置mapred-site.xml文件 在mapred-site文件中添加如下配置： &lt;!-- 历史服务器端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;lzj01:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;lzj01:19888&lt;/value&gt; &lt;/property&gt; 其中lzj01为运行程序的ip地址 2、启动历史服务器 启动历史服务器，执行命令： mr-jobhistory-daemon.sh start historyserver 3、查看历史服务器是否启动 执行jps命令查看历史服务器运行状态 4、访问历史服务器JobHistory 访问http://lzj01:19888/jobhistory，其中lzj01为配置历史服务器的域名。 由于只运行了一个实例，所以只有一个唯一的job id的任务。 四、配置日志的聚集功能 日志聚集：每次运行完程序后，都会在本地生成log文件，也只能在一台宿主机上查看日志，日志聚集功能就是把应用完成后生成的日志信息上传到HFDS分布式文件系统上，这样集群上的所有宿主机都可以查看日志信息。 配置日志聚集功能步骤如下： 1、配置文件yarn-site.xml 在yarn-site.xml文件中添加如下内容： &lt;!-- 日志聚集功能使能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 日志保留时间设置2天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;172800&lt;/value&gt; &lt;/property&gt; 2、重启NodeManager 、ResourceManager和HistoryManager 首先执行关闭命令，分别执行如下命令： yarn-daemon.sh start resourcemanager yarn-daemon.sh start nodemanager mr-jobhistory-daemon.sh start historyserver 3、执行wordcount之前，先删除hdfs分布式文件系统上的输出文件夹，因为执行案例后会重新生成 执行命令： hdfs dfs -rm -R /user/lzj/output 4、查看日志 访问：http://lzj01:19888/jobhistory 图中红框选中的job id为本案例执行的job，点击，可查看job的详细信息 点击图中橙色标记的logs按钮，就可以查看案例执行的详细日志信息 完全分布式运行模式 一、集群部署介绍 本节介绍完全分布式，也是真正实战时用到的。本例中搭建一个包括三台容器的集群，分别为192.168.85.137、172.17.0.2、172.17.0.3。集群按一下方式进行配置 192.168.85.137 172.17.0.2 172.17.0.3 HDFS NameNodeDataNode DataNode SecondaryNameNodeDataNode YARN ResourceManagerNodeManager NodeManager NodeManager 二、集群配置 下面操作均在192.168.85.137上进行操作 1、核心配置文件core-site.xml 用vi打开core-site.xml文件，添加如下内容： &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.85.137:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; NameNode部署在192.168.85.137容器中。 2、配置HDFS 用vi打开hadoop-env.sh文件，添加如下内容： export JAVA_HOME=/opt/module/jdk1.8.0_144 用vi打开hdfs-site.xml文件，添加内容如下： &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;172.17.0.3:50090&lt;/value&gt; &lt;/property&gt; 把hdfs的辅助接点部署在172.17.0.3容器中。 3、配置YARN 用vi打开yarn-env.sh文件，添加如下内容： export JAVA_HOME=/opt/module/jdk1.8.0_144 用vi打开yarn-site.xml文件，添加如下内容： &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;192.168.85.137&lt;/value&gt; &lt;/property&gt; 将yarn的资源管理节点ResourceManager部署在192.168.85.137中。 4、配置MapReduce 配置mapred-env.sh，用vi 打开mapred-env.sh文件，添加如下内容： export JAVA_HOME=/opt/module/jdk1.8.0_144 配置mapred-site.xml，hadoop中默认文件是mapred-site.xml.template，复制成需要的名字，执行cp mapred-site.xml.template mapred-site.xml 用vi打开mapred-site.xml文件，添加如下内容： &lt;!-- 指定MR运行在Yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 三、镜像制作与容器启动 上一步全部配置均在192.168.85.137机器上进行的配置，下面把上述所有配置做成镜像，然后通过镜像启动容器，避免重复繁琐的配置hadoop，关于本例hadoop镜像制作请参考https://blog.csdn.net/u010502101/article/details/97697397，dockerfile内容如下： #version 0.1 FROM centos WORKDIR /opt RUN mkdir module ADD . /opt/module/ #set java enviroment RUN echo &quot;#JAVA_HOME&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export JAVA_HOME=/opt/module/jdk1.8.0_144&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export PATH=\$PATH:\$JAVA_HOME/bin&quot; &gt;&gt; /root/.bashrc #set hadoop enviroment RUN echo &quot;#HADOOP_HOME&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export HADOOP_HOME=/opt/module/hadoop-2.7.2&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export PATH=\$PATH:\$HADOOP_HOME/bin&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export PATH=\$PATH:\$HADOOP_HOME/sbin&quot; &gt;&gt; /root/.bashrc #install net tools, for using ping RUN yum -y install net-tools #set host RUN echo &quot;172.17.0.3 hadoop003&quot; &gt;&gt; /etc/hosts RUN echo &quot;172.17.0.2 hadoop002&quot; &gt;&gt; /etc/hosts RUN echo &quot;192.168.85.137 fanhao-test&quot; &gt;&gt; /etc/hosts #install which tool, hadoop use it RUN yum -y install which #install ssh RUN yum -y install openssh* #RUN systemctl start sshd #install service #RUN yum -y install initscripts #解决Docker中CentOS镜像无法使用systemd的问题 #ENV container docker RUN (cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == systemd-tmpfiles-setup.service ] || rm -f $i; done); \ rm -f /lib/systemd/system/multi-user.target.wants/*;\ rm -f /etc/systemd/system/*.wants/*;\ rm -f /lib/systemd/system/local-fs.target.wants/*; \ rm -f /lib/systemd/system/sockets.target.wants/*udev*; \ rm -f /lib/systemd/system/sockets.target.wants/*initctl*; \ rm -f /lib/systemd/system/basic.target.wants/*;\ rm -f /lib/systemd/system/anaconda.target.wants/*; VOLUME [ &quot;/sys/fs/cgroup&quot; ] CMD [&quot;/usr/sbin/init&quot;] dockerfile文件与hadoop安装处于同一层目录。 1、执行build命令，制作镜像 docker build -t=&quot;softwarebird/hadoop:0.4&quot; . 2、启动容器 用制作的softwarebird/hadoop:0.4镜像分别启动名为hadoop1和hadoop2的容器 启动hadoop1 docker run -itd --privileged=true --name hadoop1 softwarebird/hadoop:0.4 docker exec -it 7e4906d8751f /bin/bash 另外打开一个session，启动hadoop2 docker run -itd --privileged=true --name hadoop2 softwarebird/hadoop:0.4 docker exec -it efea0d9e0841 /bin/bash 3、配置ssh免密登录 切换到192.168.85.137宿主机上执行systemctl start sshd，重启ssh服务。 生成公钥和私钥，执行命令 ssh-keygen -t rsa 将192.168.85.137上的公钥copy到其它机器中，执行命令： ssh-copy-id 192.168.85.137 ssh-copy-id 172.17.0.2 ssh-copy-id 172.17.0.3 然后分别切换到172.17.0.2和172.17.0.3容器中，分别执行systemctl start sshd，然后分别执行下面命令： ssh-keygen -t rsa ssh-copy-id 192.168.85.137 ssh-copy-id 172.17.0.2 ssh-copy-id 172.17.0.3 四、启动集群 1、配置slaves 分别打开192.168.85.137、172.17.0.2、172.17.0.3集群中hadoop中配置文件slaves，并添加如下内容： 192.168.85.137 172.17.0.2 172.17.0.3 表示集群由这三个机器组成。 2、启动hdfs集群 切换到192.168.85.137宿主机，首次启动集群前，需要格式化namenode，执行命令hdfs namenode -format 启动hdfs，执行命令： start-dfs.sh hdfs启动后，执行jps命令，显示如下： 可知192.168.85.137中部署了NameNode和DataNode。 切换到172.17.0.2中，执行jps命令，显示如下： 可知172.17.0.2中只部署了DataNode。 切换到172.17.03中，执行jps命令，显示如下： 可知172.17.0.3中，除启动了DataNode外，还启动了备用节点SecondaryNameNode节点。 3、启动yarn 切换到192.168.85.137中，执行start-yarn.sh启动yarn，然后用jps命令查看进程，显示如下： 可知，192.168.85.137除启动hdfs进程外，又启动了ResourceManager和NodeManager进程。 切换到172.17.0.2容器中，执行jps命令查看，显示如下： 可见172.17.0.2中除启动hdfs进程外，又启动了NodeManager进程。 切换到172.17.0.3容器中，执行jps命令，显示如下： 可见172.17.0.3容器中除启动hdfs进程外，也只是启动了NodeManager进程。 4、验证集群启动是否成功 在浏览器中输入http://192.168.85.137:50070/，显示界面如下： 集群启动成功。" />
<link rel="canonical" href="https://uzzz.org/2019/08/20/795071.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/20/795071.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-20T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 伪分布式模式 一、启动HFDS运行MapReduce程序 二、启动Yarn运行MapReduce程序 三、配置历史服务器 四、配置日志的聚集功能 完全分布式运行模式 一、集群部署介绍 二、集群配置 三、镜像制作与容器启动 四、启动集群 本案例基于centos + docker + hadoop进行测试。 上一节介绍了hadoop的环境搭建，以及hadoop的wordcount示例的本地运行模式，本章节介绍hadoop的其它两种运行模式，依然以wordcount为例。 hadoop包括3中运行模式：本地运行模式、伪分布式模式、完全分布式模式 List item：本地运行模式：没有启动hadoop的守护进程，所有的程序都运行在本地的JVM中，适合开发学习和调试mapreduce程序。 伪分布式模式：启动hadoop的守护进程，模拟一个简单的集群，伪分布式集群可以配置只有一台机器，适用于学习和调试。 伪分布式模式 以wordcount为例，下面配置伪分布式运行集群模式。 一、启动HFDS运行MapReduce程序 1、配置集群的JAVA_HOME环境 在hadoop的安装目录下的etc目录下有一个hadoop-env.sh文件（/opt/module/hadoop-2.7.2/etc/hadoop/hadoop-env.sh），添加JAVA_HOME环境路径，如下： export JAVA_HOME=/opt/module/jdk1.8.0_144 2、配置core-site.xml 在/opt/module/hadoop-2.7.2/etc/hadoop/hadoop-env.sh文件中添加如下配置： &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://lzj01:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; 其中lzj01为本案例ip地址 3、配置hdfs-site.xml &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 4、启动集群 格式化NameNode（第一次启动时格式化，以后就不要总格式化） 启动命令：hdfs namenode -format 启动NameNode 启动命令：hadoop-daemon.sh start namenode 启动DataNode 启动命令：hadoop-daemon.sh start datanode 5、检验集群是否启动成功 执行jps命令，结果如下： 6、操作集群 在HDFS文件系统上创建一个input文件夹 执行命令：hdfs dfs -mkdir -p /user/lzj/input 将上一节中的wc.input测试文件上传到文件系统上 执行命令：hdfs dfs -put wcinput/wc.input /user/lzj/input/ 查看上传到集群上的文件 执行命令：hdfs dfs -ls /user/lzj/input/ 或hdfs dfs -cat /user/lzj/ input/wc.input 7、执行MapReduce程序进行计算 执行命令：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/lzj/input/ /user/lzj/output 8、查看运行结果 执行命令：hdfs dfs -cat /user/lzj/output/part-r-00000 查看结果如下 bu 1 chen 3 chuan 1 dao 1 de 1 deng 1 feng 1 hao 1 hong 1 hua 2 huang 2 jia 1 jie 4 ju 1 jun 1 lang 1 li 3 lin 1 liu 1 long 1 lun 1 mao 1 ning 1 qi 2 qian 1 rong 1 sheng 1 song 1 su 1 wang 3 xu 1 xue 3 xun 2 ya 1 yi 2 you 1 yu 1 yuan 1 zhang 2 zheng 1 zhi 2 zhong 1 zhou 1 zhuang 1 9、可以将分析结果文件从文件系统上下载到本地 执行命令：hdfs dfs -get /user/lzj/output/part-r-00000 ./wcoutput/ 执行命令后，可以在当前目录下wcoutput目录下查看到从文件系统上下载的文件。 10、可以删除文件系统上的文件或文件夹 执行命令：hdfs dfs -rm -r /user/lzj/output 删除文件系统上的output目录以及其下的文件。 11、可以从网页端查看文件系统中的文件 本案例访问地址：http://lzj01:50070 其中lzj01为运行hadoop的ip地址，50070为hdfs的端口，访问结果如下： 二、启动Yarn运行MapReduce程序 下面配置伪分布式集群，启动yarn运行MapReduce程序，在yarn上执行wordcount案例。 1、配置集群 （a）配置yarn-env.sh 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到yarn-env.sh文件，并在其中加入JAVA_HOME环境： export JAVA_HOME=/opt/module/jdk1.8.0_144 (b) 配置yarn-site.xml 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到yarn-site.xml文件，并在其中添加如下配置： &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;lzj01&lt;/value&gt; &lt;/property&gt; 其中lzj01为本宿主机ip地址。 （c) 配置mapred-env.sh 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到mapred-env.sh文件，并在其中添加java环境： export JAVA_HOME=/opt/module/jdk1.8.0_144 （d) 配置mapred-site.xml 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到mapred-site.xml.template文件并重命名为mapred-site.xml文件，在mapred-site.xml文件中配置MapReduce的运行方式： &lt;!-- 指定MR运行在YARN上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 2、启动集群 （a）在启动集群前必须保证NameNode和DataNode已经启动，通过jps观察如下： （b) 启动ResourceManager （c）启动NodeManager 3、集群操作 （a）访问yarn页面 在浏览器中访问http://lzj01:8088/cluster, 8088为访问yarn端口。页面如下： 其中显示了集群的配置资源和job的执行状态。 （b) 删除文件系统上的output下的文件 在执行案例前，要把之前案例生成的output文件删除掉，否则案例会报错，提示文件已经存在。执行删除命令： hdfs dfs -rm -R /user/lzj/output © 执行案例wordcount 的MapReduce程序 wordcount案例的执行命令如下：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/lzj/input /user/lzj/output 执行结束后，访问yarn的网页界面如下： 当网页中红色标记的progress进度为100%时，表示执行完毕，可以从网页中查看任务执行的状态、节点等信息。 三、配置历史服务器 为了查看MapReduce的历史运行情况，需要配置历史服务器，配置步骤如下： 1、配置mapred-site.xml文件 在mapred-site文件中添加如下配置： &lt;!-- 历史服务器端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;lzj01:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 历史服务器web端地址 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;lzj01:19888&lt;/value&gt; &lt;/property&gt; 其中lzj01为运行程序的ip地址 2、启动历史服务器 启动历史服务器，执行命令： mr-jobhistory-daemon.sh start historyserver 3、查看历史服务器是否启动 执行jps命令查看历史服务器运行状态 4、访问历史服务器JobHistory 访问http://lzj01:19888/jobhistory，其中lzj01为配置历史服务器的域名。 由于只运行了一个实例，所以只有一个唯一的job id的任务。 四、配置日志的聚集功能 日志聚集：每次运行完程序后，都会在本地生成log文件，也只能在一台宿主机上查看日志，日志聚集功能就是把应用完成后生成的日志信息上传到HFDS分布式文件系统上，这样集群上的所有宿主机都可以查看日志信息。 配置日志聚集功能步骤如下： 1、配置文件yarn-site.xml 在yarn-site.xml文件中添加如下内容： &lt;!-- 日志聚集功能使能 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 日志保留时间设置2天 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;172800&lt;/value&gt; &lt;/property&gt; 2、重启NodeManager 、ResourceManager和HistoryManager 首先执行关闭命令，分别执行如下命令： yarn-daemon.sh start resourcemanager yarn-daemon.sh start nodemanager mr-jobhistory-daemon.sh start historyserver 3、执行wordcount之前，先删除hdfs分布式文件系统上的输出文件夹，因为执行案例后会重新生成 执行命令： hdfs dfs -rm -R /user/lzj/output 4、查看日志 访问：http://lzj01:19888/jobhistory 图中红框选中的job id为本案例执行的job，点击，可查看job的详细信息 点击图中橙色标记的logs按钮，就可以查看案例执行的详细日志信息 完全分布式运行模式 一、集群部署介绍 本节介绍完全分布式，也是真正实战时用到的。本例中搭建一个包括三台容器的集群，分别为192.168.85.137、172.17.0.2、172.17.0.3。集群按一下方式进行配置 192.168.85.137 172.17.0.2 172.17.0.3 HDFS NameNodeDataNode DataNode SecondaryNameNodeDataNode YARN ResourceManagerNodeManager NodeManager NodeManager 二、集群配置 下面操作均在192.168.85.137上进行操作 1、核心配置文件core-site.xml 用vi打开core-site.xml文件，添加如下内容： &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.85.137:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt; NameNode部署在192.168.85.137容器中。 2、配置HDFS 用vi打开hadoop-env.sh文件，添加如下内容： export JAVA_HOME=/opt/module/jdk1.8.0_144 用vi打开hdfs-site.xml文件，添加内容如下： &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;172.17.0.3:50090&lt;/value&gt; &lt;/property&gt; 把hdfs的辅助接点部署在172.17.0.3容器中。 3、配置YARN 用vi打开yarn-env.sh文件，添加如下内容： export JAVA_HOME=/opt/module/jdk1.8.0_144 用vi打开yarn-site.xml文件，添加如下内容： &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;192.168.85.137&lt;/value&gt; &lt;/property&gt; 将yarn的资源管理节点ResourceManager部署在192.168.85.137中。 4、配置MapReduce 配置mapred-env.sh，用vi 打开mapred-env.sh文件，添加如下内容： export JAVA_HOME=/opt/module/jdk1.8.0_144 配置mapred-site.xml，hadoop中默认文件是mapred-site.xml.template，复制成需要的名字，执行cp mapred-site.xml.template mapred-site.xml 用vi打开mapred-site.xml文件，添加如下内容： &lt;!-- 指定MR运行在Yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 三、镜像制作与容器启动 上一步全部配置均在192.168.85.137机器上进行的配置，下面把上述所有配置做成镜像，然后通过镜像启动容器，避免重复繁琐的配置hadoop，关于本例hadoop镜像制作请参考https://blog.csdn.net/u010502101/article/details/97697397，dockerfile内容如下： #version 0.1 FROM centos WORKDIR /opt RUN mkdir module ADD . /opt/module/ #set java enviroment RUN echo &quot;#JAVA_HOME&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export JAVA_HOME=/opt/module/jdk1.8.0_144&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export PATH=\\$PATH:\\$JAVA_HOME/bin&quot; &gt;&gt; /root/.bashrc #set hadoop enviroment RUN echo &quot;#HADOOP_HOME&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export HADOOP_HOME=/opt/module/hadoop-2.7.2&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export PATH=\\$PATH:\\$HADOOP_HOME/bin&quot; &gt;&gt; /root/.bashrc RUN echo &quot;export PATH=\\$PATH:\\$HADOOP_HOME/sbin&quot; &gt;&gt; /root/.bashrc #install net tools, for using ping RUN yum -y install net-tools #set host RUN echo &quot;172.17.0.3 hadoop003&quot; &gt;&gt; /etc/hosts RUN echo &quot;172.17.0.2 hadoop002&quot; &gt;&gt; /etc/hosts RUN echo &quot;192.168.85.137 fanhao-test&quot; &gt;&gt; /etc/hosts #install which tool, hadoop use it RUN yum -y install which #install ssh RUN yum -y install openssh* #RUN systemctl start sshd #install service #RUN yum -y install initscripts #解决Docker中CentOS镜像无法使用systemd的问题 #ENV container docker RUN (cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == systemd-tmpfiles-setup.service ] || rm -f $i; done); \\ rm -f /lib/systemd/system/multi-user.target.wants/*;\\ rm -f /etc/systemd/system/*.wants/*;\\ rm -f /lib/systemd/system/local-fs.target.wants/*; \\ rm -f /lib/systemd/system/sockets.target.wants/*udev*; \\ rm -f /lib/systemd/system/sockets.target.wants/*initctl*; \\ rm -f /lib/systemd/system/basic.target.wants/*;\\ rm -f /lib/systemd/system/anaconda.target.wants/*; VOLUME [ &quot;/sys/fs/cgroup&quot; ] CMD [&quot;/usr/sbin/init&quot;] dockerfile文件与hadoop安装处于同一层目录。 1、执行build命令，制作镜像 docker build -t=&quot;softwarebird/hadoop:0.4&quot; . 2、启动容器 用制作的softwarebird/hadoop:0.4镜像分别启动名为hadoop1和hadoop2的容器 启动hadoop1 docker run -itd --privileged=true --name hadoop1 softwarebird/hadoop:0.4 docker exec -it 7e4906d8751f /bin/bash 另外打开一个session，启动hadoop2 docker run -itd --privileged=true --name hadoop2 softwarebird/hadoop:0.4 docker exec -it efea0d9e0841 /bin/bash 3、配置ssh免密登录 切换到192.168.85.137宿主机上执行systemctl start sshd，重启ssh服务。 生成公钥和私钥，执行命令 ssh-keygen -t rsa 将192.168.85.137上的公钥copy到其它机器中，执行命令： ssh-copy-id 192.168.85.137 ssh-copy-id 172.17.0.2 ssh-copy-id 172.17.0.3 然后分别切换到172.17.0.2和172.17.0.3容器中，分别执行systemctl start sshd，然后分别执行下面命令： ssh-keygen -t rsa ssh-copy-id 192.168.85.137 ssh-copy-id 172.17.0.2 ssh-copy-id 172.17.0.3 四、启动集群 1、配置slaves 分别打开192.168.85.137、172.17.0.2、172.17.0.3集群中hadoop中配置文件slaves，并添加如下内容： 192.168.85.137 172.17.0.2 172.17.0.3 表示集群由这三个机器组成。 2、启动hdfs集群 切换到192.168.85.137宿主机，首次启动集群前，需要格式化namenode，执行命令hdfs namenode -format 启动hdfs，执行命令： start-dfs.sh hdfs启动后，执行jps命令，显示如下： 可知192.168.85.137中部署了NameNode和DataNode。 切换到172.17.0.2中，执行jps命令，显示如下： 可知172.17.0.2中只部署了DataNode。 切换到172.17.03中，执行jps命令，显示如下： 可知172.17.0.3中，除启动了DataNode外，还启动了备用节点SecondaryNameNode节点。 3、启动yarn 切换到192.168.85.137中，执行start-yarn.sh启动yarn，然后用jps命令查看进程，显示如下： 可知，192.168.85.137除启动hdfs进程外，又启动了ResourceManager和NodeManager进程。 切换到172.17.0.2容器中，执行jps命令查看，显示如下： 可见172.17.0.2中除启动hdfs进程外，又启动了NodeManager进程。 切换到172.17.0.3容器中，执行jps命令，显示如下： 可见172.17.0.3容器中除启动hdfs进程外，也只是启动了NodeManager进程。 4、验证集群启动是否成功 在浏览器中输入http://192.168.85.137:50070/，显示界面如下： 集群启动成功。","@type":"BlogPosting","url":"https://uzzz.org/2019/08/20/795071.html","headline":"2、hadoop的运行模式（centos+docker+hadoop）","dateModified":"2019-08-20T00:00:00+08:00","datePublished":"2019-08-20T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/20/795071.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>2、hadoop的运行模式（centos+docker+hadoop）</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <li><a href="#_9" rel="nofollow" data-token="c372211aca1d55a1b88f833fc4869680">伪分布式模式</a></li>
    <ul>
     <li><a href="#HFDSMapReduce_12" rel="nofollow" data-token="af1f8b725b058d6f9d8428c073568213">一、启动HFDS运行MapReduce程序</a></li>
     <li><a href="#YarnMapReduce_122" rel="nofollow" data-token="16212777b4f2eadc46facfd948113f21">二、启动Yarn运行MapReduce程序</a></li>
     <li><a href="#_180" rel="nofollow" data-token="3aa969fe44198f992b854db268f7fee4">三、配置历史服务器</a></li>
     <li><a href="#_208" rel="nofollow" data-token="624e3ced135f349ff71844de2cf02a17">四、配置日志的聚集功能</a></li>
    </ul>
    <li><a href="#_243" rel="nofollow" data-token="d4619cf9adb791921a05972133b8d216">完全分布式运行模式</a></li>
    <ul>
     <li><a href="#_244" rel="nofollow" data-token="99be59c3fa38f339d9cff5e8e0a94590">一、集群部署介绍</a></li>
     <li><a href="#_271" rel="nofollow" data-token="12d92f068f9d4e58d147996f0e67826f">二、集群配置</a></li>
     <li><a href="#_340" rel="nofollow" data-token="01e728014c43c4e396902cd1e36a1505">三、镜像制作与容器启动</a></li>
     <li><a href="#_419" rel="nofollow" data-token="ce61ab2575cc59d696a2902fdb9cf406">四、启动集群</a></li>
    </ul>
   </ul>
  </div>
  <br> 
  <strong>本案例基于centos + docker + hadoop进行测试。</strong>
  <p></p> 
  <p>上一节介绍了hadoop的环境搭建，以及hadoop的wordcount示例的<a href="https://blog.csdn.net/u010502101/article/details/91171553" rel="nofollow" data-token="08a3c6a969a3c61b00917cba7f5de424">本地运行模式</a>，本章节介绍hadoop的其它两种运行模式，依然以wordcount为例。<br> hadoop包括3中运行模式：本地运行模式、伪分布式模式、完全分布式模式</p> 
  <ul> 
   <li>List item：本地运行模式：没有启动hadoop的守护进程，所有的程序都运行在本地的JVM中，适合开发学习和调试mapreduce程序。</li> 
   <li>伪分布式模式：启动hadoop的守护进程，模拟一个简单的集群，伪分布式集群可以配置只有一台机器，适用于学习和调试。</li> 
  </ul> 
  <h1><a id="_9"></a>伪分布式模式</h1> 
  <p>以wordcount为例，下面配置伪分布式运行集群模式。</p> 
  <h2><a id="HFDSMapReduce_12"></a>一、启动HFDS运行MapReduce程序</h2> 
  <p><strong>1、配置集群的JAVA_HOME环境</strong><br> 在hadoop的安装目录下的etc目录下有一个hadoop-env.sh文件（/opt/module/hadoop-2.7.2/etc/hadoop/hadoop-env.sh），添加JAVA_HOME环境路径，如下：</p> 
  <pre><code>export JAVA_HOME=/opt/module/jdk1.8.0_144
</code></pre> 
  <p><strong>2、配置core-site.xml</strong><br> 在/opt/module/hadoop-2.7.2/etc/hadoop/hadoop-env.sh文件中添加如下配置：</p> 
  <pre><code>&lt;!-- 指定HDFS中NameNode的地址 --&gt;
&lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://lzj01:9000&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;
&lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <p>其中lzj01为本案例ip地址</p> 
  <p><strong>3、配置hdfs-site.xml</strong></p> 
  <pre><code>&lt;!-- 指定HDFS副本的数量 --&gt;
&lt;property&gt;
	&lt;name&gt;dfs.replication&lt;/name&gt;
	&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <p><strong>4、启动集群</strong></p> 
  <ul> 
   <li>格式化NameNode（第一次启动时格式化，以后就不要总格式化）<br> 启动命令：hdfs namenode -format</li> 
   <li>启动NameNode<br> 启动命令：hadoop-daemon.sh start namenode</li> 
   <li>启动DataNode<br> 启动命令：hadoop-daemon.sh start datanode</li> 
  </ul> 
  <p><strong>5、检验集群是否启动成功</strong><br> 执行jps命令，结果如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190611000158629.PNG" alt="在这里插入图片描述"><br> <strong>6、操作集群</strong></p> 
  <ul> 
   <li>在HDFS文件系统上创建一个input文件夹<br> 执行命令：<code>hdfs dfs -mkdir -p /user/lzj/input</code></li> 
   <li>将上一节中的wc.input测试文件上传到文件系统上<br> 执行命令：<code>hdfs dfs -put wcinput/wc.input /user/lzj/input/</code></li> 
   <li>查看上传到集群上的文件<br> 执行命令：<code>hdfs dfs -ls /user/lzj/input/</code> 或<code>hdfs dfs -cat /user/lzj/ input/wc.input</code></li> 
  </ul> 
  <p><strong>7、执行MapReduce程序进行计算</strong><br> 执行命令：<code>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/lzj/input/ /user/lzj/output</code><br> <strong>8、查看运行结果</strong><br> 执行命令：<code>hdfs dfs -cat /user/lzj/output/part-r-00000</code><br> 查看结果如下</p> 
  <pre><code>bu      1
chen    3
chuan   1
dao     1
de      1
deng    1
feng    1
hao     1
hong    1
hua     2
huang   2
jia     1
jie     4
ju      1
jun     1
lang    1
li      3
lin     1
liu     1
long    1
lun     1
mao     1
ning    1
qi      2
qian    1
rong    1
sheng   1
song    1
su      1
wang    3
xu      1
xue     3
xun     2
ya      1
yi      2
you     1
yu      1
yuan    1
zhang   2
zheng   1
zhi     2
zhong   1
zhou    1
zhuang  1
</code></pre> 
  <p><strong>9、可以将分析结果文件从文件系统上下载到本地</strong><br> 执行命令：<code>hdfs dfs -get /user/lzj/output/part-r-00000 ./wcoutput/</code><br> 执行命令后，可以在当前目录下wcoutput目录下查看到从文件系统上下载的文件。<br> <strong>10、可以删除文件系统上的文件或文件夹</strong><br> 执行命令：<code>hdfs dfs -rm -r /user/lzj/output</code> 删除文件系统上的output目录以及其下的文件。<br> <strong>11、可以从网页端查看文件系统中的文件</strong><br> 本案例访问地址：http://lzj01:50070<br> 其中lzj01为运行hadoop的ip地址，50070为hdfs的端口，访问结果如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190611222957971.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MDIxMDE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="YarnMapReduce_122"></a>二、启动Yarn运行MapReduce程序</h2> 
  <p>下面配置伪分布式集群，启动yarn运行MapReduce程序，在yarn上执行wordcount案例。<br> <strong>1、配置集群</strong><br> （a）配置yarn-env.sh<br> 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到yarn-env.sh文件，并在其中加入JAVA_HOME环境：</p> 
  <pre><code>export JAVA_HOME=/opt/module/jdk1.8.0_144
</code></pre> 
  <p>(b) 配置yarn-site.xml<br> 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到yarn-site.xml文件，并在其中添加如下配置：</p> 
  <pre><code>   &lt;!-- Reducer获取数据的方式 --&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
            &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
    
    &lt;!-- 指定YARN的ResourceManager的地址 --&gt;
    &lt;property&gt;
            &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
            &lt;value&gt;lzj01&lt;/value&gt;
    &lt;/property&gt;
</code></pre> 
  <p>其中lzj01为本宿主机ip地址。<br> （c) 配置mapred-env.sh<br> 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到mapred-env.sh文件，并在其中添加java环境：<br> export JAVA_HOME=/opt/module/jdk1.8.0_144<br> （d) 配置mapred-site.xml<br> 在/opt/module/hadoop-2.7.2/etc/hadoop目录下找到mapred-site.xml.template文件并重命名为mapred-site.xml文件，在mapred-site.xml文件中配置MapReduce的运行方式：</p> 
  <pre><code>&lt;!-- 指定MR运行在YARN上 --&gt;
&lt;property&gt;
		&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
		&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <p><strong>2、启动集群</strong><br> （a）在启动集群前必须保证NameNode和DataNode已经启动，通过jps观察如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190614232655302.PNG" alt="在这里插入图片描述"><br> （b) 启动ResourceManager<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019061423300754.PNG" alt="在这里插入图片描述"><br> （c）启动NodeManager<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019061423315822.PNG" alt="在这里插入图片描述"><br> <strong>3、集群操作</strong><br> （a）访问yarn页面<br> 在浏览器中访问<code>http://lzj01:8088/cluster</code>, 8088为访问yarn端口。页面如下：</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190616105129225.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MDIxMDE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 其中显示了集群的配置资源和job的执行状态。<br> （b) 删除文件系统上的output下的文件<br> 在执行案例前，要把之前案例生成的output文件删除掉，否则案例会报错，提示文件已经存在。执行删除命令：</p> 
  <pre><code>hdfs dfs -rm -R /user/lzj/output
</code></pre> 
  <p>© 执行案例wordcount 的MapReduce程序<br> wordcount案例的执行命令如下：<code>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/lzj/input /user/lzj/output</code><br> 执行结束后，访问yarn的网页界面如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190616110800324.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MDIxMDE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 当网页中红色标记的progress进度为100%时，表示执行完毕，可以从网页中查看任务执行的状态、节点等信息。</p> 
  <h2><a id="_180"></a>三、配置历史服务器</h2> 
  <p>为了查看MapReduce的历史运行情况，需要配置历史服务器，配置步骤如下：<br> <strong>1、配置mapred-site.xml文件</strong><br> 在mapred-site文件中添加如下配置：</p> 
  <pre><code>&lt;!-- 历史服务器端地址 --&gt;
&lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
        &lt;value&gt;lzj01:10020&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 历史服务器web端地址 --&gt;
&lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
        &lt;value&gt;lzj01:19888&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <p>其中lzj01为运行程序的ip地址<br> <strong>2、启动历史服务器</strong><br> 启动历史服务器，执行命令：</p> 
  <pre><code>mr-jobhistory-daemon.sh start historyserver
</code></pre> 
  <p><strong>3、查看历史服务器是否启动</strong><br> 执行jps命令查看历史服务器运行状态<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190616131536797.PNG" alt="在这里插入图片描述"><br> 4、访问历史服务器JobHistory<br> 访问http://lzj01:19888/jobhistory，其中lzj01为配置历史服务器的域名。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190618214407648.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MDIxMDE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 由于只运行了一个实例，所以只有一个唯一的job id的任务。</p> 
  <h2><a id="_208"></a>四、配置日志的聚集功能</h2> 
  <p>日志聚集：每次运行完程序后，都会在本地生成log文件，也只能在一台宿主机上查看日志，日志聚集功能就是把应用完成后生成的日志信息上传到HFDS分布式文件系统上，这样集群上的所有宿主机都可以查看日志信息。<br> 配置日志聚集功能步骤如下：<br> <strong>1、配置文件yarn-site.xml</strong><br> 在yarn-site.xml文件中添加如下内容：</p> 
  <pre><code>&lt;!-- 日志聚集功能使能 --&gt;
&lt;property&gt;
        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 日志保留时间设置2天 --&gt;
&lt;property&gt;
        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
        &lt;value&gt;172800&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <p><strong>2、重启NodeManager 、ResourceManager和HistoryManager</strong><br> 首先执行关闭命令，分别执行如下命令：</p> 
  <pre><code>yarn-daemon.sh start resourcemanager
yarn-daemon.sh start nodemanager
mr-jobhistory-daemon.sh start historyserver
</code></pre> 
  <p><strong>3、执行wordcount之前，先删除hdfs分布式文件系统上的输出文件夹，因为执行案例后会重新生成</strong><br> 执行命令：</p> 
  <pre><code>hdfs dfs -rm -R /user/lzj/output
</code></pre> 
  <p><strong>4、查看日志</strong><br> 访问：http://lzj01:19888/jobhistory<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190620214913935.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MDIxMDE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 图中红框选中的job id为本案例执行的job，点击，可查看job的详细信息<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190620215219213.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MDIxMDE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 点击图中橙色标记的logs按钮，就可以查看案例执行的详细日志信息<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190620215416652.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MDIxMDE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h1><a id="_243"></a>完全分布式运行模式</h1> 
  <h2><a id="_244"></a>一、集群部署介绍</h2> 
  <p>本节介绍完全分布式，也是真正实战时用到的。本例中搭建一个包括三台容器的集群，分别为192.168.85.137、172.17.0.2、172.17.0.3。集群按一下方式进行配置</p> 
  <table> 
   <thead> 
    <tr> 
     <th></th> 
     <th>192.168.85.137</th> 
     <th>172.17.0.2</th> 
     <th>172.17.0.3</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td>HDFS</td> 
     <td>NameNode<br>DataNode</td> 
     <td>DataNode</td> 
     <td>SecondaryNameNode<br>DataNode</td> 
    </tr> 
    <tr> 
     <td>YARN</td> 
     <td>ResourceManager<br>NodeManager</td> 
     <td>NodeManager</td> 
     <td>NodeManager</td> 
    </tr> 
   </tbody> 
  </table> 
  <h2><a id="_271"></a>二、集群配置</h2> 
  <p>下面操作均在192.168.85.137上进行操作<br> <strong>1、核心配置文件core-site.xml</strong><br> 用vi打开core-site.xml文件，添加如下内容：</p> 
  <pre><code>   &lt;!-- 指定HDFS中NameNode的地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://192.168.85.137:9000&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;
    &lt;/property&gt;
</code></pre> 
  <p>NameNode部署在192.168.85.137容器中。<br> <strong>2、配置HDFS</strong><br> 用vi打开hadoop-env.sh文件，添加如下内容：</p> 
  <pre><code>export JAVA_HOME=/opt/module/jdk1.8.0_144
</code></pre> 
  <p>用vi打开hdfs-site.xml文件，添加内容如下：</p> 
  <pre><code>&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;3&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
    &lt;value&gt;172.17.0.3:50090&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <p>把hdfs的辅助接点部署在172.17.0.3容器中。<br> <strong>3、配置YARN</strong><br> 用vi打开yarn-env.sh文件，添加如下内容：</p> 
  <pre><code>export JAVA_HOME=/opt/module/jdk1.8.0_144
</code></pre> 
  <p>用vi打开yarn-site.xml文件，添加如下内容：</p> 
  <pre><code>&lt;!-- Reducer获取数据的方式 --&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 指定YARN的ResourceManager的地址 --&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
    &lt;value&gt;192.168.85.137&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <p>将yarn的资源管理节点ResourceManager部署在192.168.85.137中。<br> <strong>4、配置MapReduce</strong><br> 配置mapred-env.sh，用vi 打开mapred-env.sh文件，添加如下内容：</p> 
  <pre><code>export JAVA_HOME=/opt/module/jdk1.8.0_144
</code></pre> 
  <p>配置mapred-site.xml，hadoop中默认文件是mapred-site.xml.template，复制成需要的名字，执行<code>cp mapred-site.xml.template mapred-site.xml</code><br> 用vi打开mapred-site.xml文件，添加如下内容：</p> 
  <pre><code>&lt;!-- 指定MR运行在Yarn上 --&gt;
&lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <h2><a id="_340"></a>三、镜像制作与容器启动</h2> 
  <p>上一步全部配置均在192.168.85.137机器上进行的配置，下面把上述所有配置做成镜像，然后通过镜像启动容器，避免重复繁琐的配置hadoop，关于本例<a href="https://blog.csdn.net/u010502101/article/details/97697397" rel="nofollow" data-token="3773d51fb33280f614f272caea287b8a">hadoop镜像制作</a>请参考https://blog.csdn.net/u010502101/article/details/97697397，dockerfile内容如下：</p> 
  <pre><code>#version 0.1
FROM centos
WORKDIR /opt
RUN mkdir  module
ADD . /opt/module/
#set java enviroment
RUN echo "#JAVA_HOME" &gt;&gt; /root/.bashrc
RUN echo "export JAVA_HOME=/opt/module/jdk1.8.0_144" &gt;&gt; /root/.bashrc
RUN echo "export PATH=\$PATH:\$JAVA_HOME/bin" &gt;&gt; /root/.bashrc
#set hadoop enviroment
RUN echo "#HADOOP_HOME" &gt;&gt; /root/.bashrc
RUN echo "export HADOOP_HOME=/opt/module/hadoop-2.7.2" &gt;&gt; /root/.bashrc
RUN echo "export PATH=\$PATH:\$HADOOP_HOME/bin" &gt;&gt; /root/.bashrc
RUN echo "export PATH=\$PATH:\$HADOOP_HOME/sbin" &gt;&gt; /root/.bashrc
#install net tools, for using ping
RUN yum -y install net-tools
#set host
RUN echo "172.17.0.3      hadoop003" &gt;&gt; /etc/hosts
RUN echo "172.17.0.2      hadoop002" &gt;&gt; /etc/hosts
RUN echo "192.168.85.137      fanhao-test" &gt;&gt; /etc/hosts
#install which tool, hadoop use it
RUN yum -y install which

#install ssh
RUN yum -y install openssh*
#RUN systemctl start sshd

#install service
#RUN yum -y install initscripts

#解决Docker中CentOS镜像无法使用systemd的问题
#ENV container docker
RUN (cd /lib/systemd/system/sysinit.target.wants/; for i in *; do [ $i == systemd-tmpfiles-setup.service ] || rm -f $i; done); \
rm -f /lib/systemd/system/multi-user.target.wants/*;\
rm -f /etc/systemd/system/*.wants/*;\
rm -f /lib/systemd/system/local-fs.target.wants/*; \
rm -f /lib/systemd/system/sockets.target.wants/*udev*; \
rm -f /lib/systemd/system/sockets.target.wants/*initctl*; \
rm -f /lib/systemd/system/basic.target.wants/*;\
rm -f /lib/systemd/system/anaconda.target.wants/*;
VOLUME [ "/sys/fs/cgroup" ]
CMD ["/usr/sbin/init"]
</code></pre> 
  <p>dockerfile文件与hadoop安装处于同一层目录。<br> <strong>1、执行build命令，制作镜像</strong></p> 
  <pre><code>docker build -t="softwarebird/hadoop:0.4" .
</code></pre> 
  <p><strong>2、启动容器</strong><br> 用制作的softwarebird/hadoop:0.4镜像分别启动名为hadoop1和hadoop2的容器<br> 启动hadoop1</p> 
  <pre><code>docker run -itd --privileged=true --name hadoop1 softwarebird/hadoop:0.4
docker exec -it 7e4906d8751f /bin/bash
</code></pre> 
  <p>另外打开一个session，启动hadoop2</p> 
  <pre><code>docker run -itd --privileged=true --name hadoop2 softwarebird/hadoop:0.4
docker exec -it efea0d9e0841 /bin/bash
</code></pre> 
  <p><strong>3、配置ssh免密登录</strong><br> 切换到192.168.85.137宿主机上执行<code>systemctl start sshd</code>，重启ssh服务。<br> 生成公钥和私钥，执行命令</p> 
  <pre><code>		    ssh-keygen -t rsa
</code></pre> 
  <p>将192.168.85.137上的公钥copy到其它机器中，执行命令：</p> 
  <pre><code>		    ssh-copy-id 192.168.85.137
		    ssh-copy-id 172.17.0.2
		    ssh-copy-id 172.17.0.3
</code></pre> 
  <p>然后分别切换到172.17.0.2和172.17.0.3容器中，分别执行<code>systemctl start sshd</code>，然后分别执行下面命令：</p> 
  <pre><code>				ssh-keygen -t rsa
			    ssh-copy-id 192.168.85.137
			    ssh-copy-id 172.17.0.2
			    ssh-copy-id 172.17.0.3
</code></pre> 
  <h2><a id="_419"></a>四、启动集群</h2> 
  <p><strong>1、配置slaves</strong><br> 分别打开192.168.85.137、172.17.0.2、172.17.0.3集群中hadoop中配置文件slaves，并添加如下内容：</p> 
  <pre><code>	    	192.168.85.137
	    	172.17.0.2
	    	172.17.0.3
</code></pre> 
  <p>表示集群由这三个机器组成。<br> <strong>2、启动hdfs集群</strong><br> 切换到192.168.85.137宿主机，首次启动集群前，需要格式化namenode，执行命令<code>hdfs namenode -format</code><br> 启动hdfs，执行命令：</p> 
  <pre><code>			    start-dfs.sh
</code></pre> 
  <p>hdfs启动后，执行jps命令，显示如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731000558596.PNG" alt="在这里插入图片描述"><br> 可知192.168.85.137中部署了NameNode和DataNode。</p> 
  <p>切换到172.17.0.2中，执行jps命令，显示如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731000823680.PNG" alt="在这里插入图片描述"><br> 可知172.17.0.2中只部署了DataNode。</p> 
  <p>切换到172.17.03中，执行jps命令，显示如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731000951310.PNG" alt="在这里插入图片描述"><br> 可知172.17.0.3中，除启动了DataNode外，还启动了备用节点SecondaryNameNode节点。</p> 
  <p><strong>3、启动yarn</strong><br> 切换到192.168.85.137中，执行start-yarn.sh启动yarn，然后用jps命令查看进程，显示如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731001309614.PNG" alt="在这里插入图片描述"><br> 可知，192.168.85.137除启动hdfs进程外，又启动了ResourceManager和NodeManager进程。</p> 
  <p>切换到172.17.0.2容器中，执行jps命令查看，显示如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731001520889.PNG" alt="在这里插入图片描述"><br> 可见172.17.0.2中除启动hdfs进程外，又启动了NodeManager进程。</p> 
  <p>切换到172.17.0.3容器中，执行jps命令，显示如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731001701588.PNG" alt="在这里插入图片描述"><br> 可见172.17.0.3容器中除启动hdfs进程外，也只是启动了NodeManager进程。</p> 
  <p><strong>4、验证集群启动是否成功</strong><br> 在浏览器中输入http://192.168.85.137:50070/，显示界面如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190820230317674.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MDIxMDE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 集群启动成功。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
