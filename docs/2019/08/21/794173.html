<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>python数据分析与挖掘实战学习笔记（四）–聚类算法 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="python数据分析与挖掘实战学习笔记（四）–聚类算法" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="第五章 数据建模 （一）聚类分析 1、主要方法 2、距离分析 度量样本之间的相似性，采用距离算法：&nbsp; &nbsp; 文档相似性度量 &nbsp;3、K-means分类 #-*- coding: utf-8 -*- # 使用K-Means算法聚类消费行为特征数据 import pandas as pd # 参数初始化 inputfile = &#39;../data/consumption_data.xls&#39; &nbsp;# 销量及其他属性数据 outputfile = &#39;../tmp/data_type.xls&#39; &nbsp;# 保存结果的文件名 k = 3 &nbsp;# 聚类的类别 iteration = 500 &nbsp;# 聚类最大循环次数 data = pd.read_excel(inputfile, index_col=&#39;Id&#39;) &nbsp;# 读取数据 data_zs = 1.0 * (data - data.mean()) / data.std() &nbsp;# 数据标准化 from sklearn.cluster import KMeans model = KMeans(n_clusters=k, n_jobs=1, max_iter=iteration) &nbsp;# 分为k类，并发数4 model.fit(data_zs) &nbsp;# 开始聚类 # 简单打印结果 r1 = pd.Series(model.labels_).value_counts() &nbsp;# 统计各个类别的数目 r2 = pd.DataFrame(model.cluster_centers_) &nbsp;# 找出聚类中心 r = pd.concat([r2, r1], axis=1) &nbsp;# 横向连接（0是纵向），得到聚类中心对应的类别下的数目 r.columns = list(data.columns) + [u&#39;类别数目&#39;] &nbsp;# 重命名表头 print(r) # 详细输出原始数据及其类别 r = pd.concat([data, pd.Series(model.labels_, index=data.index)], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; axis=1) &nbsp;# 详细输出每个样本对应的类别 r.columns = list(data.columns) + [u&#39;聚类类别&#39;] &nbsp;# 重命名表头 r.to_excel(outputfile) &nbsp;# 保存结果 def density_plot(data): &nbsp;# 自定义作图函数 &nbsp; &nbsp; import matplotlib.pyplot as plt &nbsp; &nbsp; plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] &nbsp;# 用来正常显示中文标签 &nbsp; &nbsp; plt.rcParams[&#39;axes.unicode_minus&#39;] = False &nbsp;# 用来正常显示负号 &nbsp; &nbsp; p = data.plot(kind=&#39;kde&#39;, linewidth=2, subplots=True, sharex=False) &nbsp; &nbsp; [p[i].set_ylabel(u&#39;密度&#39;) for i in range(k)] &nbsp; &nbsp; plt.legend() &nbsp; &nbsp; return plt pic_output = &#39;../tmp/pd_&#39; &nbsp;# 概率密度图文件名前缀 for i in range(k): &nbsp; &nbsp; density_plot(data[r[u&#39;聚类类别&#39;] == i]).savefig(u&#39;%s%s.png&#39; % (pic_output, i)) #利用TSNE绘图 #-*- coding: utf-8 -*- # 接k_means.py from sklearn.manifold import TSNE tsne = TSNE() tsne.fit_transform(data_zs) &nbsp;# 进行数据降维 tsne = pd.DataFrame(tsne.embedding_, index=data_zs.index) &nbsp;# 转换数据格式 import matplotlib.pyplot as plt plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] &nbsp;# 用来正常显示中文标签 plt.rcParams[&#39;axes.unicode_minus&#39;] = False &nbsp;# 用来正常显示负号 # 不同类别用不同颜色和样式绘图 d = tsne[r[u&#39;聚类类别&#39;] == 0] plt.plot(d[0], d[1], &#39;r.&#39;) d = tsne[r[u&#39;聚类类别&#39;] == 1] plt.plot(d[0], d[1], &#39;go&#39;) d = tsne[r[u&#39;聚类类别&#39;] == 2] plt.plot(d[0], d[1], &#39;b*&#39;) plt.show() &nbsp;聚类分析算法评价 P111 （1）purity评价法 （2）RI评价法 （3）F值评价法 4、Meanshift 与kmeans算法不同，mean shift 算法可自动决定类别的数目。与kmeans算法一样的是，两者都是用集合内数据点的均值进行中心点的移动。 算法核心：算法的关键操作是通过感兴趣区域内的数据密度变化计算中心点的漂移向量，从而移动中心点进行下一次迭代，直到到达密度最大处（中心点不变）。从每个数据点出发都可以进行该操作，在这个过程，统计出现在感兴趣区域内的数据的次数。该参数将在最后作为分类的依据。 mean shift 算法中，bandwidth（带宽）是重要参数。 案例来源：AI with python（mean_shift.py） import numpy as np import matplotlib.pyplot as plt from sklearn.cluster import MeanShift, estimate_bandwidth from itertools import cycle # Load data from input file X = np.loadtxt(&#39;../code/data_clustering.txt&#39;, delimiter=&#39;,&#39;) # Estimate the bandwidth of X bandwidth_X = estimate_bandwidth(X, quantile=0.1, n_samples=len(X)) # Cluster data with MeanShift meanshift_model = MeanShift(bandwidth=bandwidth_X, bin_seeding=True) meanshift_model.fit(X) # Extract the centers of clusters cluster_centers = meanshift_model.cluster_centers_ print(&#39;\nCenters of clusters:\n&#39;, cluster_centers) # Estimate the number of clusters labels = meanshift_model.labels_ num_clusters = len(np.unique(labels)) print(&quot;\nNumber of clusters in input data =&quot;, num_clusters) # Plot the points and cluster centers plt.figure() markers = &#39;o*xvs&#39; for i, marker in zip(range(num_clusters), markers): &nbsp; &nbsp; # Plot points that belong to the current cluster &nbsp; &nbsp; plt.scatter(X[labels==i, 0], X[labels==i, 1], marker=marker, color=&#39;black&#39;) &nbsp; &nbsp; # Plot the cluster center &nbsp; &nbsp; cluster_center = cluster_centers[i] &nbsp; &nbsp; plt.plot(cluster_center[0], cluster_center[1], marker=&#39;o&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; markerfacecolor=&#39;black&#39;, markeredgecolor=&#39;black&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; markersize=15) plt.title(&#39;Clusters&#39;) plt.show() &nbsp; 5、GMM算法 GMM算法主要利用EM算法来估计高斯混合模型中的参数，然后根据计算得到的&nbsp;概率进行聚类。&nbsp; GMM分布 高斯混合分布是假设总体的分布有多个不同的高斯分布混合而成，其中每一个高斯分布所占的权重不相同。&nbsp; GMM和K-means直观对比 最后我们比较GMM和K-means两个算法的步骤。 GMM： 先计算所有数据对每个分模型的响应度 根据响应度计算每个分模型的参数 迭代 K-means： 先计算所有数据对于K个点的距离，取距离最近的点作为自己所属于的类 根据上一步的类别划分更新点的位置（点的位置就可以看做是模型参数） 迭代 可以看出GMM和K-means还是有很大的相同点的。GMM中数据对高斯分量的响应度就相当于K-means中的距离计算，GMM中的根据响应度计算高斯分量参数就相当于K-means中计算分类点的位置。然后它们都通过不断迭代达到最优。不同的是：GMM模型给出的是每一个观测点由哪个高斯分量生成的概率，而K-means直接给出一个观测点属于哪一类。 案例来源AI with python&nbsp; （gmm_classifier.py） import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import patches &nbsp; # from sklearn import datasets from sklearn.mixture import GaussianMixture &nbsp;#GMM更换为GaussianMixture from sklearn.model_selection import StratifiedKFold # Load the iris dataset iris = datasets.load_iris() #print(iris) &nbsp;#数据分为data和target两组 # Split dataset into training and testing (80/20 split) skf = StratifiedKFold(n_splits=5) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #将数据分为5组。 indices=skf.split(iris.data,iris.target) &nbsp; &nbsp; &nbsp; #将数据分为4组train，1组test # Take the first foldtrain_index, test_index = next(iter(indices))&nbsp; &nbsp; # Extract training data and labels X_train = iris.data[train_index] y_train = iris.target[train_index] # Extract testing data and labels X_test = iris.data[test_index] y_test = iris.target[test_index] # Extract the number of classes num_classes = len(np.unique(y_train)) # Build GMM classifier = GaussianMixture(n_components=num_classes, covariance_type=&#39;full&#39;, &nbsp; &nbsp; #n_components指的是下层分布由几个构成，本项目中指的是num_classes. covariance_type指一致性算法的类别 &nbsp; &nbsp; &nbsp; &nbsp; init_params=&#39;kmeans&#39;, max_iter=20) &nbsp;#init_params中w代表weights，c代表covariance在迭代中进行更新；n_iter迭代次数 # Initialize the GMM means&nbsp; classifier.means_ = np.array([X_train[y_train == i].mean(axis=0) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for i in range(num_classes)]) # Train the GMM classifier&nbsp; classifier.fit(X_train) # Draw boundaries plt.figure() colors = &#39;bgr&#39; for i, color in enumerate(colors): &nbsp; &nbsp; # Extract eigenvalues and eigenvectors &nbsp; &nbsp; eigenvalues, eigenvectors = np.linalg.eigh(classifier.covariances_[i][:2, :2]) &nbsp; &nbsp;#参照GaussianMixture的属性修改为covariances_。在covariances_()时报错，希望通过dataframe的类对象的方法得到#numpy数组。不应带括号，他是属性，不是方法。 &nbsp; &nbsp; # Normalize the first eigenvector &nbsp; &nbsp; norm_vec = eigenvectors[0] / np.linalg.norm(eigenvectors[0]) &nbsp; &nbsp; # Extract the angle of tilt &nbsp; &nbsp; angle = np.arctan2(norm_vec[1], norm_vec[0]) &nbsp; &nbsp; angle = 180 * angle / np.pi&nbsp; &nbsp; &nbsp; # Scaling factor to magnify the ellipses &nbsp; &nbsp; # (random value chosen to suit our needs) &nbsp; &nbsp; scaling_factor = 8 &nbsp; &nbsp; eigenvalues *= scaling_factor&nbsp; &nbsp; &nbsp; # Draw the ellipse &nbsp; &nbsp; ellipse = patches.Ellipse(classifier.means_[i, :2],&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; eigenvalues[0], eigenvalues[1], 180 + angle,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; color=color) &nbsp; &nbsp; axis_handle = plt.subplot(1, 1, 1) &nbsp; &nbsp; ellipse.set_clip_box(axis_handle.bbox) &nbsp; &nbsp; ellipse.set_alpha(0.6) &nbsp; &nbsp; axis_handle.add_artist(ellipse) # Plot the data&nbsp; colors = &#39;bgr&#39; for i, color in enumerate(colors): &nbsp; &nbsp; cur_data = iris.data[iris.target == i] &nbsp; &nbsp; plt.scatter(cur_data[:,0], cur_data[:,1], marker=&#39;o&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;, s=40,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label=iris.target_names[i]) &nbsp; &nbsp; test_data = X_test[y_test == i] &nbsp; &nbsp; plt.scatter(test_data[:,0], test_data[:,1], marker=&#39;s&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; facecolors=&#39;black&#39;, edgecolors=&#39;black&#39;, s=40,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label=iris.target_names[i]) # Compute predictions for training and testing data y_train_pred = classifier.predict(X_train) accuracy_training = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100 print(&#39;Accuracy on training data =&#39;, accuracy_training) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; y_test_pred = classifier.predict(X_test) accuracy_testing = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100 print(&#39;Accuracy on testing data =&#39;, accuracy_testing) plt.title(&#39;GMM classifier&#39;) plt.xticks(()) plt.yticks(()) plt.show() &nbsp; 存在问题&nbsp; ：生成图形有差别，预测精度也有问题。特征值、特征向量的用法&nbsp; 6、近邻传播算法 Affinity Propagation 聚类算法的通俗解释。下面案例是网上找的AP算法的一个案例。亲测可用。 https://blog.csdn.net/notHeadache/article/details/89003044 print(__doc__) from sklearn.cluster import AffinityPropagation from sklearn import metrics from sklearn.datasets.samples_generator import make_blobs # ############################################################################# # Generate sample data centers = [[1, 1], [-1, -1], [1, -1]] X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; random_state=0) # ############################################################################# # Compute Affinity Propagation af = AffinityPropagation(preference=-50).fit(X) cluster_centers_indices = af.cluster_centers_indices_ labels = af.labels_ n_clusters_ = len(cluster_centers_indices) &nbsp; &nbsp;# print(&#39;Estimated number of clusters: %d&#39; % n_clusters_) print(&quot;Homogeneity: %0.3f&quot; % metrics.homogeneity_score(labels_true, labels)) print(&quot;Completeness: %0.3f&quot; % metrics.completeness_score(labels_true, labels)) print(&quot;V-measure: %0.3f&quot; % metrics.v_measure_score(labels_true, labels)) print(&quot;Adjusted Rand Index: %0.3f&quot; &nbsp; &nbsp; &nbsp; % metrics.adjusted_rand_score(labels_true, labels)) print(&quot;Adjusted Mutual Information: %0.3f&quot; &nbsp; &nbsp; &nbsp; % metrics.adjusted_mutual_info_score(labels_true, labels)) print(&quot;Silhouette Coefficient: %0.3f&quot; &nbsp; &nbsp; &nbsp; % metrics.silhouette_score(X, labels, metric=&#39;sqeuclidean&#39;)) # ############################################################################# # Plot result import matplotlib.pyplot as plt from itertools import cycle plt.close(&#39;all&#39;) plt.figure(1) plt.clf() colors = cycle(&#39;bgrcmykbgrcmykbgrcmykbgrcmyk&#39;) for k, col in zip(range(n_clusters_), colors): &nbsp; &nbsp; class_members = labels == k &nbsp; &nbsp; cluster_center = X[cluster_centers_indices[k]] &nbsp; &nbsp; plt.plot(X[class_members, 0], X[class_members, 1], col + &#39;.&#39;) &nbsp; &nbsp; plt.plot(cluster_center[0], cluster_center[1], &#39;o&#39;, markerfacecolor=col, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;markeredgecolor=&#39;k&#39;, markersize=14) &nbsp; &nbsp; for x in X[class_members]: &nbsp; &nbsp; &nbsp; &nbsp; plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col) plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_) plt.show() &nbsp; &nbsp; （二）问题要点 1、数据连接 r = pd.concat([r2, r1], axis=1) &nbsp;# 横向连接（0是纵向），得到聚类中心对应的类别下的数目 r.columns = list(data.columns) + [u&#39;类别数目&#39;] &nbsp;# 重命名表头 2、分别绘图 pic_output = &#39;../tmp/pd_&#39; &nbsp;# 概率密度图文件名前缀 for i in range(k): &nbsp; &nbsp; density_plot(data[r[u&#39;聚类类别&#39;] == i]).savefig(u&#39;%s%s.png&#39; % (pic_output, i))&nbsp; &nbsp;3、数据标准化 data_zs = 1.0 * (data - data.mean()) / data.std() &nbsp;# 数据标准化 4、kmeans++算法&nbsp; kmeans = KMeans(init=&#39;k-means++&#39;, n_clusters=num_clusters, n_init=10) &nbsp; #k-means++采用智能方法找出初始聚类中心，n_init为迭代次数 5、最优分类次数计算（采用轮廓系数判定） &nbsp; import numpy as np import matplotlib.pyplot as plt from sklearn import metrics from sklearn.cluster import KMeans # Load data from input file X = np.loadtxt(&#39;../code/data_quality.txt&#39;, delimiter=&#39;,&#39;) # Plot input data plt.figure() plt.scatter(X[:,0], X[:,1], color=&#39;black&#39;, s=80, marker=&#39;o&#39;, facecolors=&#39;none&#39;) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 plt.title(&#39;Input data&#39;) plt.xlim(x_min, x_max) plt.ylim(y_min, y_max) plt.xticks(()) plt.yticks(()) # Initialize variables scores = [] values = np.arange(2, 10) # Iterate through the defined range for num_clusters in values: &nbsp; &nbsp; #从2到10里面选择 &nbsp; &nbsp; # Train the KMeans clustering model &nbsp; &nbsp; kmeans = KMeans(init=&#39;k-means++&#39;, n_clusters=num_clusters, n_init=10) &nbsp; &nbsp; kmeans.fit(X) &nbsp; &nbsp; score = metrics.silhouette_score(X, kmeans.labels_,metric=&#39;euclidean&#39;, sample_size=len(X)) &nbsp; #轮廓系数 &nbsp; &nbsp; print(&quot;\nNumber of clusters =&quot;, num_clusters) &nbsp; &nbsp; print(&quot;Silhouette score =&quot;, score) &nbsp; &nbsp; scores.append(score) # Plot silhouette scores plt.figure() plt.bar(values, scores, width=0.7, color=&#39;black&#39;, align=&#39;center&#39;) plt.title(&#39;Silhouette score vs number of clusters&#39;) # Extract best score and optimal number of clusters num_clusters = np.argmax(scores) + values[0] print(&#39;\nOptimal number of clusters =&#39;, num_clusters) plt.show()" />
<meta property="og:description" content="第五章 数据建模 （一）聚类分析 1、主要方法 2、距离分析 度量样本之间的相似性，采用距离算法：&nbsp; &nbsp; 文档相似性度量 &nbsp;3、K-means分类 #-*- coding: utf-8 -*- # 使用K-Means算法聚类消费行为特征数据 import pandas as pd # 参数初始化 inputfile = &#39;../data/consumption_data.xls&#39; &nbsp;# 销量及其他属性数据 outputfile = &#39;../tmp/data_type.xls&#39; &nbsp;# 保存结果的文件名 k = 3 &nbsp;# 聚类的类别 iteration = 500 &nbsp;# 聚类最大循环次数 data = pd.read_excel(inputfile, index_col=&#39;Id&#39;) &nbsp;# 读取数据 data_zs = 1.0 * (data - data.mean()) / data.std() &nbsp;# 数据标准化 from sklearn.cluster import KMeans model = KMeans(n_clusters=k, n_jobs=1, max_iter=iteration) &nbsp;# 分为k类，并发数4 model.fit(data_zs) &nbsp;# 开始聚类 # 简单打印结果 r1 = pd.Series(model.labels_).value_counts() &nbsp;# 统计各个类别的数目 r2 = pd.DataFrame(model.cluster_centers_) &nbsp;# 找出聚类中心 r = pd.concat([r2, r1], axis=1) &nbsp;# 横向连接（0是纵向），得到聚类中心对应的类别下的数目 r.columns = list(data.columns) + [u&#39;类别数目&#39;] &nbsp;# 重命名表头 print(r) # 详细输出原始数据及其类别 r = pd.concat([data, pd.Series(model.labels_, index=data.index)], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; axis=1) &nbsp;# 详细输出每个样本对应的类别 r.columns = list(data.columns) + [u&#39;聚类类别&#39;] &nbsp;# 重命名表头 r.to_excel(outputfile) &nbsp;# 保存结果 def density_plot(data): &nbsp;# 自定义作图函数 &nbsp; &nbsp; import matplotlib.pyplot as plt &nbsp; &nbsp; plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] &nbsp;# 用来正常显示中文标签 &nbsp; &nbsp; plt.rcParams[&#39;axes.unicode_minus&#39;] = False &nbsp;# 用来正常显示负号 &nbsp; &nbsp; p = data.plot(kind=&#39;kde&#39;, linewidth=2, subplots=True, sharex=False) &nbsp; &nbsp; [p[i].set_ylabel(u&#39;密度&#39;) for i in range(k)] &nbsp; &nbsp; plt.legend() &nbsp; &nbsp; return plt pic_output = &#39;../tmp/pd_&#39; &nbsp;# 概率密度图文件名前缀 for i in range(k): &nbsp; &nbsp; density_plot(data[r[u&#39;聚类类别&#39;] == i]).savefig(u&#39;%s%s.png&#39; % (pic_output, i)) #利用TSNE绘图 #-*- coding: utf-8 -*- # 接k_means.py from sklearn.manifold import TSNE tsne = TSNE() tsne.fit_transform(data_zs) &nbsp;# 进行数据降维 tsne = pd.DataFrame(tsne.embedding_, index=data_zs.index) &nbsp;# 转换数据格式 import matplotlib.pyplot as plt plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] &nbsp;# 用来正常显示中文标签 plt.rcParams[&#39;axes.unicode_minus&#39;] = False &nbsp;# 用来正常显示负号 # 不同类别用不同颜色和样式绘图 d = tsne[r[u&#39;聚类类别&#39;] == 0] plt.plot(d[0], d[1], &#39;r.&#39;) d = tsne[r[u&#39;聚类类别&#39;] == 1] plt.plot(d[0], d[1], &#39;go&#39;) d = tsne[r[u&#39;聚类类别&#39;] == 2] plt.plot(d[0], d[1], &#39;b*&#39;) plt.show() &nbsp;聚类分析算法评价 P111 （1）purity评价法 （2）RI评价法 （3）F值评价法 4、Meanshift 与kmeans算法不同，mean shift 算法可自动决定类别的数目。与kmeans算法一样的是，两者都是用集合内数据点的均值进行中心点的移动。 算法核心：算法的关键操作是通过感兴趣区域内的数据密度变化计算中心点的漂移向量，从而移动中心点进行下一次迭代，直到到达密度最大处（中心点不变）。从每个数据点出发都可以进行该操作，在这个过程，统计出现在感兴趣区域内的数据的次数。该参数将在最后作为分类的依据。 mean shift 算法中，bandwidth（带宽）是重要参数。 案例来源：AI with python（mean_shift.py） import numpy as np import matplotlib.pyplot as plt from sklearn.cluster import MeanShift, estimate_bandwidth from itertools import cycle # Load data from input file X = np.loadtxt(&#39;../code/data_clustering.txt&#39;, delimiter=&#39;,&#39;) # Estimate the bandwidth of X bandwidth_X = estimate_bandwidth(X, quantile=0.1, n_samples=len(X)) # Cluster data with MeanShift meanshift_model = MeanShift(bandwidth=bandwidth_X, bin_seeding=True) meanshift_model.fit(X) # Extract the centers of clusters cluster_centers = meanshift_model.cluster_centers_ print(&#39;\nCenters of clusters:\n&#39;, cluster_centers) # Estimate the number of clusters labels = meanshift_model.labels_ num_clusters = len(np.unique(labels)) print(&quot;\nNumber of clusters in input data =&quot;, num_clusters) # Plot the points and cluster centers plt.figure() markers = &#39;o*xvs&#39; for i, marker in zip(range(num_clusters), markers): &nbsp; &nbsp; # Plot points that belong to the current cluster &nbsp; &nbsp; plt.scatter(X[labels==i, 0], X[labels==i, 1], marker=marker, color=&#39;black&#39;) &nbsp; &nbsp; # Plot the cluster center &nbsp; &nbsp; cluster_center = cluster_centers[i] &nbsp; &nbsp; plt.plot(cluster_center[0], cluster_center[1], marker=&#39;o&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; markerfacecolor=&#39;black&#39;, markeredgecolor=&#39;black&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; markersize=15) plt.title(&#39;Clusters&#39;) plt.show() &nbsp; 5、GMM算法 GMM算法主要利用EM算法来估计高斯混合模型中的参数，然后根据计算得到的&nbsp;概率进行聚类。&nbsp; GMM分布 高斯混合分布是假设总体的分布有多个不同的高斯分布混合而成，其中每一个高斯分布所占的权重不相同。&nbsp; GMM和K-means直观对比 最后我们比较GMM和K-means两个算法的步骤。 GMM： 先计算所有数据对每个分模型的响应度 根据响应度计算每个分模型的参数 迭代 K-means： 先计算所有数据对于K个点的距离，取距离最近的点作为自己所属于的类 根据上一步的类别划分更新点的位置（点的位置就可以看做是模型参数） 迭代 可以看出GMM和K-means还是有很大的相同点的。GMM中数据对高斯分量的响应度就相当于K-means中的距离计算，GMM中的根据响应度计算高斯分量参数就相当于K-means中计算分类点的位置。然后它们都通过不断迭代达到最优。不同的是：GMM模型给出的是每一个观测点由哪个高斯分量生成的概率，而K-means直接给出一个观测点属于哪一类。 案例来源AI with python&nbsp; （gmm_classifier.py） import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import patches &nbsp; # from sklearn import datasets from sklearn.mixture import GaussianMixture &nbsp;#GMM更换为GaussianMixture from sklearn.model_selection import StratifiedKFold # Load the iris dataset iris = datasets.load_iris() #print(iris) &nbsp;#数据分为data和target两组 # Split dataset into training and testing (80/20 split) skf = StratifiedKFold(n_splits=5) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #将数据分为5组。 indices=skf.split(iris.data,iris.target) &nbsp; &nbsp; &nbsp; #将数据分为4组train，1组test # Take the first foldtrain_index, test_index = next(iter(indices))&nbsp; &nbsp; # Extract training data and labels X_train = iris.data[train_index] y_train = iris.target[train_index] # Extract testing data and labels X_test = iris.data[test_index] y_test = iris.target[test_index] # Extract the number of classes num_classes = len(np.unique(y_train)) # Build GMM classifier = GaussianMixture(n_components=num_classes, covariance_type=&#39;full&#39;, &nbsp; &nbsp; #n_components指的是下层分布由几个构成，本项目中指的是num_classes. covariance_type指一致性算法的类别 &nbsp; &nbsp; &nbsp; &nbsp; init_params=&#39;kmeans&#39;, max_iter=20) &nbsp;#init_params中w代表weights，c代表covariance在迭代中进行更新；n_iter迭代次数 # Initialize the GMM means&nbsp; classifier.means_ = np.array([X_train[y_train == i].mean(axis=0) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for i in range(num_classes)]) # Train the GMM classifier&nbsp; classifier.fit(X_train) # Draw boundaries plt.figure() colors = &#39;bgr&#39; for i, color in enumerate(colors): &nbsp; &nbsp; # Extract eigenvalues and eigenvectors &nbsp; &nbsp; eigenvalues, eigenvectors = np.linalg.eigh(classifier.covariances_[i][:2, :2]) &nbsp; &nbsp;#参照GaussianMixture的属性修改为covariances_。在covariances_()时报错，希望通过dataframe的类对象的方法得到#numpy数组。不应带括号，他是属性，不是方法。 &nbsp; &nbsp; # Normalize the first eigenvector &nbsp; &nbsp; norm_vec = eigenvectors[0] / np.linalg.norm(eigenvectors[0]) &nbsp; &nbsp; # Extract the angle of tilt &nbsp; &nbsp; angle = np.arctan2(norm_vec[1], norm_vec[0]) &nbsp; &nbsp; angle = 180 * angle / np.pi&nbsp; &nbsp; &nbsp; # Scaling factor to magnify the ellipses &nbsp; &nbsp; # (random value chosen to suit our needs) &nbsp; &nbsp; scaling_factor = 8 &nbsp; &nbsp; eigenvalues *= scaling_factor&nbsp; &nbsp; &nbsp; # Draw the ellipse &nbsp; &nbsp; ellipse = patches.Ellipse(classifier.means_[i, :2],&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; eigenvalues[0], eigenvalues[1], 180 + angle,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; color=color) &nbsp; &nbsp; axis_handle = plt.subplot(1, 1, 1) &nbsp; &nbsp; ellipse.set_clip_box(axis_handle.bbox) &nbsp; &nbsp; ellipse.set_alpha(0.6) &nbsp; &nbsp; axis_handle.add_artist(ellipse) # Plot the data&nbsp; colors = &#39;bgr&#39; for i, color in enumerate(colors): &nbsp; &nbsp; cur_data = iris.data[iris.target == i] &nbsp; &nbsp; plt.scatter(cur_data[:,0], cur_data[:,1], marker=&#39;o&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;, s=40,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label=iris.target_names[i]) &nbsp; &nbsp; test_data = X_test[y_test == i] &nbsp; &nbsp; plt.scatter(test_data[:,0], test_data[:,1], marker=&#39;s&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; facecolors=&#39;black&#39;, edgecolors=&#39;black&#39;, s=40,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label=iris.target_names[i]) # Compute predictions for training and testing data y_train_pred = classifier.predict(X_train) accuracy_training = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100 print(&#39;Accuracy on training data =&#39;, accuracy_training) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; y_test_pred = classifier.predict(X_test) accuracy_testing = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100 print(&#39;Accuracy on testing data =&#39;, accuracy_testing) plt.title(&#39;GMM classifier&#39;) plt.xticks(()) plt.yticks(()) plt.show() &nbsp; 存在问题&nbsp; ：生成图形有差别，预测精度也有问题。特征值、特征向量的用法&nbsp; 6、近邻传播算法 Affinity Propagation 聚类算法的通俗解释。下面案例是网上找的AP算法的一个案例。亲测可用。 https://blog.csdn.net/notHeadache/article/details/89003044 print(__doc__) from sklearn.cluster import AffinityPropagation from sklearn import metrics from sklearn.datasets.samples_generator import make_blobs # ############################################################################# # Generate sample data centers = [[1, 1], [-1, -1], [1, -1]] X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; random_state=0) # ############################################################################# # Compute Affinity Propagation af = AffinityPropagation(preference=-50).fit(X) cluster_centers_indices = af.cluster_centers_indices_ labels = af.labels_ n_clusters_ = len(cluster_centers_indices) &nbsp; &nbsp;# print(&#39;Estimated number of clusters: %d&#39; % n_clusters_) print(&quot;Homogeneity: %0.3f&quot; % metrics.homogeneity_score(labels_true, labels)) print(&quot;Completeness: %0.3f&quot; % metrics.completeness_score(labels_true, labels)) print(&quot;V-measure: %0.3f&quot; % metrics.v_measure_score(labels_true, labels)) print(&quot;Adjusted Rand Index: %0.3f&quot; &nbsp; &nbsp; &nbsp; % metrics.adjusted_rand_score(labels_true, labels)) print(&quot;Adjusted Mutual Information: %0.3f&quot; &nbsp; &nbsp; &nbsp; % metrics.adjusted_mutual_info_score(labels_true, labels)) print(&quot;Silhouette Coefficient: %0.3f&quot; &nbsp; &nbsp; &nbsp; % metrics.silhouette_score(X, labels, metric=&#39;sqeuclidean&#39;)) # ############################################################################# # Plot result import matplotlib.pyplot as plt from itertools import cycle plt.close(&#39;all&#39;) plt.figure(1) plt.clf() colors = cycle(&#39;bgrcmykbgrcmykbgrcmykbgrcmyk&#39;) for k, col in zip(range(n_clusters_), colors): &nbsp; &nbsp; class_members = labels == k &nbsp; &nbsp; cluster_center = X[cluster_centers_indices[k]] &nbsp; &nbsp; plt.plot(X[class_members, 0], X[class_members, 1], col + &#39;.&#39;) &nbsp; &nbsp; plt.plot(cluster_center[0], cluster_center[1], &#39;o&#39;, markerfacecolor=col, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;markeredgecolor=&#39;k&#39;, markersize=14) &nbsp; &nbsp; for x in X[class_members]: &nbsp; &nbsp; &nbsp; &nbsp; plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col) plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_) plt.show() &nbsp; &nbsp; （二）问题要点 1、数据连接 r = pd.concat([r2, r1], axis=1) &nbsp;# 横向连接（0是纵向），得到聚类中心对应的类别下的数目 r.columns = list(data.columns) + [u&#39;类别数目&#39;] &nbsp;# 重命名表头 2、分别绘图 pic_output = &#39;../tmp/pd_&#39; &nbsp;# 概率密度图文件名前缀 for i in range(k): &nbsp; &nbsp; density_plot(data[r[u&#39;聚类类别&#39;] == i]).savefig(u&#39;%s%s.png&#39; % (pic_output, i))&nbsp; &nbsp;3、数据标准化 data_zs = 1.0 * (data - data.mean()) / data.std() &nbsp;# 数据标准化 4、kmeans++算法&nbsp; kmeans = KMeans(init=&#39;k-means++&#39;, n_clusters=num_clusters, n_init=10) &nbsp; #k-means++采用智能方法找出初始聚类中心，n_init为迭代次数 5、最优分类次数计算（采用轮廓系数判定） &nbsp; import numpy as np import matplotlib.pyplot as plt from sklearn import metrics from sklearn.cluster import KMeans # Load data from input file X = np.loadtxt(&#39;../code/data_quality.txt&#39;, delimiter=&#39;,&#39;) # Plot input data plt.figure() plt.scatter(X[:,0], X[:,1], color=&#39;black&#39;, s=80, marker=&#39;o&#39;, facecolors=&#39;none&#39;) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 plt.title(&#39;Input data&#39;) plt.xlim(x_min, x_max) plt.ylim(y_min, y_max) plt.xticks(()) plt.yticks(()) # Initialize variables scores = [] values = np.arange(2, 10) # Iterate through the defined range for num_clusters in values: &nbsp; &nbsp; #从2到10里面选择 &nbsp; &nbsp; # Train the KMeans clustering model &nbsp; &nbsp; kmeans = KMeans(init=&#39;k-means++&#39;, n_clusters=num_clusters, n_init=10) &nbsp; &nbsp; kmeans.fit(X) &nbsp; &nbsp; score = metrics.silhouette_score(X, kmeans.labels_,metric=&#39;euclidean&#39;, sample_size=len(X)) &nbsp; #轮廓系数 &nbsp; &nbsp; print(&quot;\nNumber of clusters =&quot;, num_clusters) &nbsp; &nbsp; print(&quot;Silhouette score =&quot;, score) &nbsp; &nbsp; scores.append(score) # Plot silhouette scores plt.figure() plt.bar(values, scores, width=0.7, color=&#39;black&#39;, align=&#39;center&#39;) plt.title(&#39;Silhouette score vs number of clusters&#39;) # Extract best score and optimal number of clusters num_clusters = np.argmax(scores) + values[0] print(&#39;\nOptimal number of clusters =&#39;, num_clusters) plt.show()" />
<link rel="canonical" href="https://uzzz.org/2019/08/21/794173.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/21/794173.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-21T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"第五章 数据建模 （一）聚类分析 1、主要方法 2、距离分析 度量样本之间的相似性，采用距离算法：&nbsp; &nbsp; 文档相似性度量 &nbsp;3、K-means分类 #-*- coding: utf-8 -*- # 使用K-Means算法聚类消费行为特征数据 import pandas as pd # 参数初始化 inputfile = &#39;../data/consumption_data.xls&#39; &nbsp;# 销量及其他属性数据 outputfile = &#39;../tmp/data_type.xls&#39; &nbsp;# 保存结果的文件名 k = 3 &nbsp;# 聚类的类别 iteration = 500 &nbsp;# 聚类最大循环次数 data = pd.read_excel(inputfile, index_col=&#39;Id&#39;) &nbsp;# 读取数据 data_zs = 1.0 * (data - data.mean()) / data.std() &nbsp;# 数据标准化 from sklearn.cluster import KMeans model = KMeans(n_clusters=k, n_jobs=1, max_iter=iteration) &nbsp;# 分为k类，并发数4 model.fit(data_zs) &nbsp;# 开始聚类 # 简单打印结果 r1 = pd.Series(model.labels_).value_counts() &nbsp;# 统计各个类别的数目 r2 = pd.DataFrame(model.cluster_centers_) &nbsp;# 找出聚类中心 r = pd.concat([r2, r1], axis=1) &nbsp;# 横向连接（0是纵向），得到聚类中心对应的类别下的数目 r.columns = list(data.columns) + [u&#39;类别数目&#39;] &nbsp;# 重命名表头 print(r) # 详细输出原始数据及其类别 r = pd.concat([data, pd.Series(model.labels_, index=data.index)], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; axis=1) &nbsp;# 详细输出每个样本对应的类别 r.columns = list(data.columns) + [u&#39;聚类类别&#39;] &nbsp;# 重命名表头 r.to_excel(outputfile) &nbsp;# 保存结果 def density_plot(data): &nbsp;# 自定义作图函数 &nbsp; &nbsp; import matplotlib.pyplot as plt &nbsp; &nbsp; plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] &nbsp;# 用来正常显示中文标签 &nbsp; &nbsp; plt.rcParams[&#39;axes.unicode_minus&#39;] = False &nbsp;# 用来正常显示负号 &nbsp; &nbsp; p = data.plot(kind=&#39;kde&#39;, linewidth=2, subplots=True, sharex=False) &nbsp; &nbsp; [p[i].set_ylabel(u&#39;密度&#39;) for i in range(k)] &nbsp; &nbsp; plt.legend() &nbsp; &nbsp; return plt pic_output = &#39;../tmp/pd_&#39; &nbsp;# 概率密度图文件名前缀 for i in range(k): &nbsp; &nbsp; density_plot(data[r[u&#39;聚类类别&#39;] == i]).savefig(u&#39;%s%s.png&#39; % (pic_output, i)) #利用TSNE绘图 #-*- coding: utf-8 -*- # 接k_means.py from sklearn.manifold import TSNE tsne = TSNE() tsne.fit_transform(data_zs) &nbsp;# 进行数据降维 tsne = pd.DataFrame(tsne.embedding_, index=data_zs.index) &nbsp;# 转换数据格式 import matplotlib.pyplot as plt plt.rcParams[&#39;font.sans-serif&#39;] = [&#39;SimHei&#39;] &nbsp;# 用来正常显示中文标签 plt.rcParams[&#39;axes.unicode_minus&#39;] = False &nbsp;# 用来正常显示负号 # 不同类别用不同颜色和样式绘图 d = tsne[r[u&#39;聚类类别&#39;] == 0] plt.plot(d[0], d[1], &#39;r.&#39;) d = tsne[r[u&#39;聚类类别&#39;] == 1] plt.plot(d[0], d[1], &#39;go&#39;) d = tsne[r[u&#39;聚类类别&#39;] == 2] plt.plot(d[0], d[1], &#39;b*&#39;) plt.show() &nbsp;聚类分析算法评价 P111 （1）purity评价法 （2）RI评价法 （3）F值评价法 4、Meanshift 与kmeans算法不同，mean shift 算法可自动决定类别的数目。与kmeans算法一样的是，两者都是用集合内数据点的均值进行中心点的移动。 算法核心：算法的关键操作是通过感兴趣区域内的数据密度变化计算中心点的漂移向量，从而移动中心点进行下一次迭代，直到到达密度最大处（中心点不变）。从每个数据点出发都可以进行该操作，在这个过程，统计出现在感兴趣区域内的数据的次数。该参数将在最后作为分类的依据。 mean shift 算法中，bandwidth（带宽）是重要参数。 案例来源：AI with python（mean_shift.py） import numpy as np import matplotlib.pyplot as plt from sklearn.cluster import MeanShift, estimate_bandwidth from itertools import cycle # Load data from input file X = np.loadtxt(&#39;../code/data_clustering.txt&#39;, delimiter=&#39;,&#39;) # Estimate the bandwidth of X bandwidth_X = estimate_bandwidth(X, quantile=0.1, n_samples=len(X)) # Cluster data with MeanShift meanshift_model = MeanShift(bandwidth=bandwidth_X, bin_seeding=True) meanshift_model.fit(X) # Extract the centers of clusters cluster_centers = meanshift_model.cluster_centers_ print(&#39;\\nCenters of clusters:\\n&#39;, cluster_centers) # Estimate the number of clusters labels = meanshift_model.labels_ num_clusters = len(np.unique(labels)) print(&quot;\\nNumber of clusters in input data =&quot;, num_clusters) # Plot the points and cluster centers plt.figure() markers = &#39;o*xvs&#39; for i, marker in zip(range(num_clusters), markers): &nbsp; &nbsp; # Plot points that belong to the current cluster &nbsp; &nbsp; plt.scatter(X[labels==i, 0], X[labels==i, 1], marker=marker, color=&#39;black&#39;) &nbsp; &nbsp; # Plot the cluster center &nbsp; &nbsp; cluster_center = cluster_centers[i] &nbsp; &nbsp; plt.plot(cluster_center[0], cluster_center[1], marker=&#39;o&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; markerfacecolor=&#39;black&#39;, markeredgecolor=&#39;black&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; markersize=15) plt.title(&#39;Clusters&#39;) plt.show() &nbsp; 5、GMM算法 GMM算法主要利用EM算法来估计高斯混合模型中的参数，然后根据计算得到的&nbsp;概率进行聚类。&nbsp; GMM分布 高斯混合分布是假设总体的分布有多个不同的高斯分布混合而成，其中每一个高斯分布所占的权重不相同。&nbsp; GMM和K-means直观对比 最后我们比较GMM和K-means两个算法的步骤。 GMM： 先计算所有数据对每个分模型的响应度 根据响应度计算每个分模型的参数 迭代 K-means： 先计算所有数据对于K个点的距离，取距离最近的点作为自己所属于的类 根据上一步的类别划分更新点的位置（点的位置就可以看做是模型参数） 迭代 可以看出GMM和K-means还是有很大的相同点的。GMM中数据对高斯分量的响应度就相当于K-means中的距离计算，GMM中的根据响应度计算高斯分量参数就相当于K-means中计算分类点的位置。然后它们都通过不断迭代达到最优。不同的是：GMM模型给出的是每一个观测点由哪个高斯分量生成的概率，而K-means直接给出一个观测点属于哪一类。 案例来源AI with python&nbsp; （gmm_classifier.py） import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import patches &nbsp; # from sklearn import datasets from sklearn.mixture import GaussianMixture &nbsp;#GMM更换为GaussianMixture from sklearn.model_selection import StratifiedKFold # Load the iris dataset iris = datasets.load_iris() #print(iris) &nbsp;#数据分为data和target两组 # Split dataset into training and testing (80/20 split) skf = StratifiedKFold(n_splits=5) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #将数据分为5组。 indices=skf.split(iris.data,iris.target) &nbsp; &nbsp; &nbsp; #将数据分为4组train，1组test # Take the first foldtrain_index, test_index = next(iter(indices))&nbsp; &nbsp; # Extract training data and labels X_train = iris.data[train_index] y_train = iris.target[train_index] # Extract testing data and labels X_test = iris.data[test_index] y_test = iris.target[test_index] # Extract the number of classes num_classes = len(np.unique(y_train)) # Build GMM classifier = GaussianMixture(n_components=num_classes, covariance_type=&#39;full&#39;, &nbsp; &nbsp; #n_components指的是下层分布由几个构成，本项目中指的是num_classes. covariance_type指一致性算法的类别 &nbsp; &nbsp; &nbsp; &nbsp; init_params=&#39;kmeans&#39;, max_iter=20) &nbsp;#init_params中w代表weights，c代表covariance在迭代中进行更新；n_iter迭代次数 # Initialize the GMM means&nbsp; classifier.means_ = np.array([X_train[y_train == i].mean(axis=0) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for i in range(num_classes)]) # Train the GMM classifier&nbsp; classifier.fit(X_train) # Draw boundaries plt.figure() colors = &#39;bgr&#39; for i, color in enumerate(colors): &nbsp; &nbsp; # Extract eigenvalues and eigenvectors &nbsp; &nbsp; eigenvalues, eigenvectors = np.linalg.eigh(classifier.covariances_[i][:2, :2]) &nbsp; &nbsp;#参照GaussianMixture的属性修改为covariances_。在covariances_()时报错，希望通过dataframe的类对象的方法得到#numpy数组。不应带括号，他是属性，不是方法。 &nbsp; &nbsp; # Normalize the first eigenvector &nbsp; &nbsp; norm_vec = eigenvectors[0] / np.linalg.norm(eigenvectors[0]) &nbsp; &nbsp; # Extract the angle of tilt &nbsp; &nbsp; angle = np.arctan2(norm_vec[1], norm_vec[0]) &nbsp; &nbsp; angle = 180 * angle / np.pi&nbsp; &nbsp; &nbsp; # Scaling factor to magnify the ellipses &nbsp; &nbsp; # (random value chosen to suit our needs) &nbsp; &nbsp; scaling_factor = 8 &nbsp; &nbsp; eigenvalues *= scaling_factor&nbsp; &nbsp; &nbsp; # Draw the ellipse &nbsp; &nbsp; ellipse = patches.Ellipse(classifier.means_[i, :2],&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; eigenvalues[0], eigenvalues[1], 180 + angle,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; color=color) &nbsp; &nbsp; axis_handle = plt.subplot(1, 1, 1) &nbsp; &nbsp; ellipse.set_clip_box(axis_handle.bbox) &nbsp; &nbsp; ellipse.set_alpha(0.6) &nbsp; &nbsp; axis_handle.add_artist(ellipse) # Plot the data&nbsp; colors = &#39;bgr&#39; for i, color in enumerate(colors): &nbsp; &nbsp; cur_data = iris.data[iris.target == i] &nbsp; &nbsp; plt.scatter(cur_data[:,0], cur_data[:,1], marker=&#39;o&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; facecolors=&#39;none&#39;, edgecolors=&#39;black&#39;, s=40,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label=iris.target_names[i]) &nbsp; &nbsp; test_data = X_test[y_test == i] &nbsp; &nbsp; plt.scatter(test_data[:,0], test_data[:,1], marker=&#39;s&#39;,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; facecolors=&#39;black&#39;, edgecolors=&#39;black&#39;, s=40,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label=iris.target_names[i]) # Compute predictions for training and testing data y_train_pred = classifier.predict(X_train) accuracy_training = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100 print(&#39;Accuracy on training data =&#39;, accuracy_training) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; y_test_pred = classifier.predict(X_test) accuracy_testing = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100 print(&#39;Accuracy on testing data =&#39;, accuracy_testing) plt.title(&#39;GMM classifier&#39;) plt.xticks(()) plt.yticks(()) plt.show() &nbsp; 存在问题&nbsp; ：生成图形有差别，预测精度也有问题。特征值、特征向量的用法&nbsp; 6、近邻传播算法 Affinity Propagation 聚类算法的通俗解释。下面案例是网上找的AP算法的一个案例。亲测可用。 https://blog.csdn.net/notHeadache/article/details/89003044 print(__doc__) from sklearn.cluster import AffinityPropagation from sklearn import metrics from sklearn.datasets.samples_generator import make_blobs # ############################################################################# # Generate sample data centers = [[1, 1], [-1, -1], [1, -1]] X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; random_state=0) # ############################################################################# # Compute Affinity Propagation af = AffinityPropagation(preference=-50).fit(X) cluster_centers_indices = af.cluster_centers_indices_ labels = af.labels_ n_clusters_ = len(cluster_centers_indices) &nbsp; &nbsp;# print(&#39;Estimated number of clusters: %d&#39; % n_clusters_) print(&quot;Homogeneity: %0.3f&quot; % metrics.homogeneity_score(labels_true, labels)) print(&quot;Completeness: %0.3f&quot; % metrics.completeness_score(labels_true, labels)) print(&quot;V-measure: %0.3f&quot; % metrics.v_measure_score(labels_true, labels)) print(&quot;Adjusted Rand Index: %0.3f&quot; &nbsp; &nbsp; &nbsp; % metrics.adjusted_rand_score(labels_true, labels)) print(&quot;Adjusted Mutual Information: %0.3f&quot; &nbsp; &nbsp; &nbsp; % metrics.adjusted_mutual_info_score(labels_true, labels)) print(&quot;Silhouette Coefficient: %0.3f&quot; &nbsp; &nbsp; &nbsp; % metrics.silhouette_score(X, labels, metric=&#39;sqeuclidean&#39;)) # ############################################################################# # Plot result import matplotlib.pyplot as plt from itertools import cycle plt.close(&#39;all&#39;) plt.figure(1) plt.clf() colors = cycle(&#39;bgrcmykbgrcmykbgrcmykbgrcmyk&#39;) for k, col in zip(range(n_clusters_), colors): &nbsp; &nbsp; class_members = labels == k &nbsp; &nbsp; cluster_center = X[cluster_centers_indices[k]] &nbsp; &nbsp; plt.plot(X[class_members, 0], X[class_members, 1], col + &#39;.&#39;) &nbsp; &nbsp; plt.plot(cluster_center[0], cluster_center[1], &#39;o&#39;, markerfacecolor=col, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;markeredgecolor=&#39;k&#39;, markersize=14) &nbsp; &nbsp; for x in X[class_members]: &nbsp; &nbsp; &nbsp; &nbsp; plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col) plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_) plt.show() &nbsp; &nbsp; （二）问题要点 1、数据连接 r = pd.concat([r2, r1], axis=1) &nbsp;# 横向连接（0是纵向），得到聚类中心对应的类别下的数目 r.columns = list(data.columns) + [u&#39;类别数目&#39;] &nbsp;# 重命名表头 2、分别绘图 pic_output = &#39;../tmp/pd_&#39; &nbsp;# 概率密度图文件名前缀 for i in range(k): &nbsp; &nbsp; density_plot(data[r[u&#39;聚类类别&#39;] == i]).savefig(u&#39;%s%s.png&#39; % (pic_output, i))&nbsp; &nbsp;3、数据标准化 data_zs = 1.0 * (data - data.mean()) / data.std() &nbsp;# 数据标准化 4、kmeans++算法&nbsp; kmeans = KMeans(init=&#39;k-means++&#39;, n_clusters=num_clusters, n_init=10) &nbsp; #k-means++采用智能方法找出初始聚类中心，n_init为迭代次数 5、最优分类次数计算（采用轮廓系数判定） &nbsp; import numpy as np import matplotlib.pyplot as plt from sklearn import metrics from sklearn.cluster import KMeans # Load data from input file X = np.loadtxt(&#39;../code/data_quality.txt&#39;, delimiter=&#39;,&#39;) # Plot input data plt.figure() plt.scatter(X[:,0], X[:,1], color=&#39;black&#39;, s=80, marker=&#39;o&#39;, facecolors=&#39;none&#39;) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 plt.title(&#39;Input data&#39;) plt.xlim(x_min, x_max) plt.ylim(y_min, y_max) plt.xticks(()) plt.yticks(()) # Initialize variables scores = [] values = np.arange(2, 10) # Iterate through the defined range for num_clusters in values: &nbsp; &nbsp; #从2到10里面选择 &nbsp; &nbsp; # Train the KMeans clustering model &nbsp; &nbsp; kmeans = KMeans(init=&#39;k-means++&#39;, n_clusters=num_clusters, n_init=10) &nbsp; &nbsp; kmeans.fit(X) &nbsp; &nbsp; score = metrics.silhouette_score(X, kmeans.labels_,metric=&#39;euclidean&#39;, sample_size=len(X)) &nbsp; #轮廓系数 &nbsp; &nbsp; print(&quot;\\nNumber of clusters =&quot;, num_clusters) &nbsp; &nbsp; print(&quot;Silhouette score =&quot;, score) &nbsp; &nbsp; scores.append(score) # Plot silhouette scores plt.figure() plt.bar(values, scores, width=0.7, color=&#39;black&#39;, align=&#39;center&#39;) plt.title(&#39;Silhouette score vs number of clusters&#39;) # Extract best score and optimal number of clusters num_clusters = np.argmax(scores) + values[0] print(&#39;\\nOptimal number of clusters =&#39;, num_clusters) plt.show()","@type":"BlogPosting","url":"https://uzzz.org/2019/08/21/794173.html","headline":"python数据分析与挖掘实战学习笔记（四）–聚类算法","dateModified":"2019-08-21T00:00:00+08:00","datePublished":"2019-08-21T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/21/794173.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>python数据分析与挖掘实战学习笔记（四）--聚类算法</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1>第五章 数据建模</h1> 
  <h2>（一）聚类分析</h2> 
  <h3>1、主要方法</h3> 
  <p><img alt="" class="has" height="562" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722172713329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lpbGlfc2hhMTE=,size_16,color_FFFFFF,t_70" width="1200"></p> 
  <p><img alt="" class="has" height="395" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722172757171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lpbGlfc2hhMTE=,size_16,color_FFFFFF,t_70" width="1190"></p> 
  <h3>2、距离分析</h3> 
  <p>度量样本之间的相似性，采用距离算法：&nbsp;</p> 
  <p><img alt="" class="has" height="539" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722172908696.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lpbGlfc2hhMTE=,size_16,color_FFFFFF,t_70" width="901"></p> 
  <p>&nbsp;</p> 
  <p>文档相似性度量</p> 
  <h3><img alt="" class="has" height="529" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722173040288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lpbGlfc2hhMTE=,size_16,color_FFFFFF,t_70" width="1190">&nbsp;3、K-means分类</h3> 
  <blockquote> 
   <p>#-*- coding: utf-8 -*-<br> # 使用K-Means算法聚类消费行为特征数据</p> 
   <p>import pandas as pd</p> 
   <p># 参数初始化<br> inputfile = '../data/consumption_data.xls' &nbsp;# 销量及其他属性数据<br> outputfile = '../tmp/data_type.xls' &nbsp;# 保存结果的文件名<br> k = 3 &nbsp;# 聚类的类别<br> iteration = 500 &nbsp;# 聚类最大循环次数<br> data = pd.read_excel(inputfile, index_col='Id') &nbsp;# 读取数据<br> data_zs = 1.0 * (data - data.mean()) / data.std() &nbsp;# 数据标准化</p> 
   <p>from sklearn.cluster import KMeans<br> model = KMeans(n_clusters=k, n_jobs=1, max_iter=iteration) &nbsp;# 分为k类，并发数4<br> model.fit(data_zs) &nbsp;# 开始聚类</p> 
   <p># 简单打印结果<br> r1 = pd.Series(model.labels_).value_counts() &nbsp;# 统计各个类别的数目<br> r2 = pd.DataFrame(model.cluster_centers_) &nbsp;# 找出聚类中心<br> r = pd.concat([r2, r1], axis=1) &nbsp;# 横向连接（0是纵向），得到聚类中心对应的类别下的数目<br> r.columns = list(data.columns) + [u'类别数目'] &nbsp;# 重命名表头<br> print(r)</p> 
   <p># 详细输出原始数据及其类别<br> r = pd.concat([data, pd.Series(model.labels_, index=data.index)],<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; axis=1) &nbsp;# 详细输出每个样本对应的类别<br> r.columns = list(data.columns) + [u'聚类类别'] &nbsp;# 重命名表头<br> r.to_excel(outputfile) &nbsp;# 保存结果</p> 
   <p><br> def density_plot(data): &nbsp;# 自定义作图函数<br> &nbsp; &nbsp; import matplotlib.pyplot as plt<br> &nbsp; &nbsp; plt.rcParams['font.sans-serif'] = ['SimHei'] &nbsp;# 用来正常显示中文标签<br> &nbsp; &nbsp; plt.rcParams['axes.unicode_minus'] = False &nbsp;# 用来正常显示负号<br> &nbsp; &nbsp; p = data.plot(kind='kde', linewidth=2, subplots=True, sharex=False)<br> &nbsp; &nbsp; [p[i].set_ylabel(u'密度') for i in range(k)]<br> &nbsp; &nbsp; plt.legend()<br> &nbsp; &nbsp; return plt</p> 
   <p>pic_output = '../tmp/pd_' &nbsp;# 概率密度图文件名前缀<br> for i in range(k):<br> &nbsp; &nbsp; density_plot(data[r[u'聚类类别'] == i]).savefig(u'%s%s.png' % (pic_output, i))</p> 
   <p>#利用TSNE绘图</p> 
   <p>#-*- coding: utf-8 -*-<br> # 接k_means.py<br> from sklearn.manifold import TSNE</p> 
   <p><span style="color:#f33b45;">tsne = TSNE()<br> tsne.fit_transform(data_zs) &nbsp;# 进行数据降维<br> tsne = pd.DataFrame(tsne.embedding_, index=data_zs.index) &nbsp;# 转换数据格式</span></p> 
   <p>import matplotlib.pyplot as plt<br> plt.rcParams['font.sans-serif'] = ['SimHei'] &nbsp;# 用来正常显示中文标签<br> plt.rcParams['axes.unicode_minus'] = False &nbsp;# 用来正常显示负号</p> 
   <p># 不同类别用不同颜色和样式绘图<br> d = tsne[r[u'聚类类别'] == 0]<br> plt.plot(d[0], d[1], 'r.')<br> d = tsne[r[u'聚类类别'] == 1]<br> plt.plot(d[0], d[1], 'go')<br> d = tsne[r[u'聚类类别'] == 2]<br> plt.plot(d[0], d[1], 'b*')<br> plt.show()</p> 
  </blockquote> 
  <p>&nbsp;<strong>聚类分析算法评价 P111</strong></p> 
  <p>（1）purity评价法</p> 
  <p>（2）RI评价法</p> 
  <p>（3）F值评价法</p> 
  <p><img alt="" class="has" height="485" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722205538194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lpbGlfc2hhMTE=,size_16,color_FFFFFF,t_70" width="1188"></p> 
  <h3>4、Meanshift</h3> 
  <p>与kmeans算法不同，mean shift 算法可自动决定类别的数目。与kmeans算法一样的是，两者都是用集合内数据点的均值进行中心点的移动。<br> 算法核心：算法的关键操作是通过感兴趣区域内的数据密度变化计算中心点的漂移向量，从而移动中心点进行下一次迭代，直到到达密度最大处（中心点不变）。从每个数据点出发都可以进行该操作，在这个过程，统计出现在感兴趣区域内的数据的次数。该参数将在最后作为分类的依据。<br> mean shift 算法中，bandwidth（带宽）是重要参数。</p> 
  <p>案例来源：AI with python（mean_shift.py）</p> 
  <blockquote> 
   <p>import numpy as np<br> import matplotlib.pyplot as plt<br> from sklearn.cluster import MeanShift, estimate_bandwidth<br> from itertools import cycle</p> 
   <p># Load data from input file<br> X = np.loadtxt('../code/data_clustering.txt', delimiter=',')</p> 
   <p><span style="color:#f33b45;"># Estimate the bandwidth of X<br> bandwidth_X = estimate_bandwidth(X, quantile=0.1, n_samples=len(X))</span></p> 
   <p># Cluster data with MeanShift<br> meanshift_model = MeanShift(bandwidth=bandwidth_X, bin_seeding=True)<br> meanshift_model.fit(X)</p> 
   <p># Extract the centers of clusters<br> cluster_centers = meanshift_model.cluster_centers_<br> print('\nCenters of clusters:\n', cluster_centers)</p> 
   <p># Estimate the number of clusters<br> labels = meanshift_model.labels_<br> num_clusters = len(np.unique(labels))<br> print("\nNumber of clusters in input data =", num_clusters)</p> 
   <p># Plot the points and cluster centers<br> plt.figure()<br> markers = 'o*xvs'<br> for i, marker in zip(range(num_clusters), markers):<br> &nbsp; &nbsp; # Plot points that belong to the current cluster<br> &nbsp; &nbsp; plt.scatter(X[labels==i, 0], X[labels==i, 1], marker=marker, color='black')</p> 
   <p>&nbsp; &nbsp; # Plot the cluster center<br> &nbsp; &nbsp; cluster_center = cluster_centers[i]<br> &nbsp; &nbsp; plt.plot(cluster_center[0], cluster_center[1], marker='o',&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; markerfacecolor='black', markeredgecolor='black',&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; markersize=15)</p> 
   <p>plt.title('Clusters')<br> plt.show()<br> &nbsp;</p> 
  </blockquote> 
  <h3>5、GMM算法</h3> 
  <p>GMM算法主要利用EM算法来估计高斯混合模型中的参数，然后根据计算得到的&nbsp;概率进行聚类。&nbsp;</p> 
  <p>GMM分布<br> 高斯混合分布是假设总体的分布有多个不同的高斯分布混合而成，其中每一个高斯分布所占的权重不相同。&nbsp;</p> 
  <p><img alt="" class="has" height="358" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190722214603941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lpbGlfc2hhMTE=,size_16,color_FFFFFF,t_70" width="1074"></p> 
  <h2 id="gmm和k-means直观对比"><strong>GMM和K-means直观对比</strong></h2> 
  <p>最后我们比较GMM和K-means两个算法的步骤。</p> 
  <p><strong>GMM：</strong></p> 
  <ul>
   <li>先计算所有数据对每个分模型的响应度</li> 
   <li>根据响应度计算每个分模型的参数</li> 
   <li>迭代</li> 
  </ul>
  <p><strong>K-means：</strong></p> 
  <ul>
   <li>先计算所有数据对于K个点的距离，取距离最近的点作为自己所属于的类</li> 
   <li>根据上一步的类别划分更新点的位置（点的位置就可以看做是模型参数）</li> 
   <li>迭代</li> 
  </ul>
  <p>可以看出GMM和K-means还是有很大的相同点的。GMM中数据对高斯分量的响应度就相当于K-means中的距离计算，GMM中的根据响应度计算高斯分量参数就相当于K-means中计算分类点的位置。然后它们都通过不断迭代达到最优。不同的是：GMM模型给出的是每一个观测点由哪个高斯分量生成的概率，而K-means直接给出一个观测点属于哪一类。</p> 
  <p>案例来源AI with python&nbsp; （gmm_classifier.py）</p> 
  <blockquote> 
   <p>import numpy as np<br> import pandas as pd<br> import matplotlib.pyplot as plt<br> from matplotlib import patches &nbsp; #</p> 
   <p>from sklearn import datasets<br> from sklearn.mixture import GaussianMixture &nbsp;#GMM更换为GaussianMixture<br> from sklearn.model_selection import StratifiedKFold</p> 
   <p># Load the iris dataset<br> iris = datasets.load_iris()<br> #print(iris) &nbsp;#数据分为data和target两组</p> 
   <p># Split dataset into training and testing (80/20 split)<br> skf = StratifiedKFold(n_splits=5) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #将数据分为5组。<br> indices=skf.split(iris.data,iris.target) &nbsp; &nbsp; &nbsp; #将数据分为4组train，1组test</p> 
   <p># Take the first fold<br><span style="color:#3399ea;">train_index, test_index = next(iter(indices))&nbsp; &nbsp;</span></p> 
   <p># Extract training data and labels<br> X_train = iris.data[train_index]<br> y_train = iris.target[train_index]</p> 
   <p># Extract testing data and labels<br> X_test = iris.data[test_index]<br> y_test = iris.target[test_index]</p> 
   <p># Extract the number of classes<br> num_classes = len(np.unique(y_train))</p> 
   <p># Build GMM<br> classifier = GaussianMixture(n_components=num_classes, covariance_type='full', &nbsp; &nbsp; #n_components指的是下层分布由几个构成，本项目中指的是num_classes. covariance_type指一致性算法的类别<br> &nbsp; &nbsp; &nbsp; &nbsp; init_params='kmeans', max_iter=20) &nbsp;#init_params中w代表weights，c代表covariance在迭代中进行更新；n_iter迭代次数</p> 
   <p># Initialize the GMM means&nbsp;<br> classifier.means_ = np.array([X_train[y_train == i].mean(axis=0)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for i in range(num_classes)])</p> 
   <p># Train the GMM classifier&nbsp;<br> classifier.fit(X_train)</p> 
   <p># Draw boundaries<br> plt.figure()<br> colors = 'bgr'<br> for i, color in enumerate(colors):<br> &nbsp; &nbsp; # Extract eigenvalues and eigenvectors<br> &nbsp; &nbsp; eigenvalues, eigenvectors = np.linalg.eigh<span style="color:#f33b45;">(classifier.covariances_[i][:2, :2]) </span>&nbsp;</p> 
   <p>&nbsp;#参照GaussianMixture的属性修改为covariances_。在covariances_()时报错，希望通过dataframe的类对象的方法得到#numpy数组。不应带括号，他是属性，不是方法。</p> 
   <p>&nbsp; &nbsp; # Normalize the first eigenvector<br> &nbsp; &nbsp; norm_vec = eigenvectors[0] / np.linalg.norm(eigenvectors[0])</p> 
   <p>&nbsp; &nbsp; # Extract the angle of tilt<br> &nbsp; &nbsp; angle = np.arctan2(norm_vec[1], norm_vec[0])<br> &nbsp; &nbsp; angle = 180 * angle / np.pi&nbsp;</p> 
   <p>&nbsp; &nbsp; # Scaling factor to magnify the ellipses<br> &nbsp; &nbsp; # (random value chosen to suit our needs)<br> &nbsp; &nbsp; scaling_factor = 8<br> &nbsp; &nbsp; eigenvalues *= scaling_factor&nbsp;</p> 
   <p>&nbsp; &nbsp; # Draw the ellipse<br> &nbsp; &nbsp; ellipse = patches.Ellipse(classifier.means_[i, :2],&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; eigenvalues[0], eigenvalues[1], 180 + angle,&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; color=color)<br> &nbsp; &nbsp; axis_handle = plt.subplot(1, 1, 1)<br> &nbsp; &nbsp; ellipse.set_clip_box(axis_handle.bbox)<br> &nbsp; &nbsp; ellipse.set_alpha(0.6)<br> &nbsp; &nbsp; axis_handle.add_artist(ellipse)</p> 
   <p># Plot the data&nbsp;<br> colors = 'bgr'<br> for i, color in enumerate(colors):<br> &nbsp; &nbsp; cur_data = iris.data[iris.target == i]<br> &nbsp; &nbsp; plt.scatter(cur_data[:,0], cur_data[:,1], marker='o',&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; facecolors='none', edgecolors='black', s=40,&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label=iris.target_names[i])</p> 
   <p>&nbsp; &nbsp; test_data = X_test[y_test == i]<br> &nbsp; &nbsp; plt.scatter(test_data[:,0], test_data[:,1], marker='s',&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; facecolors='black', edgecolors='black', s=40,&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label=iris.target_names[i])</p> 
   <p># Compute predictions for training and testing data<br> y_train_pred = classifier.predict(X_train)<br> accuracy_training = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100<br> print('Accuracy on training data =', accuracy_training)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br> y_test_pred = classifier.predict(X_test)<br> accuracy_testing = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100<br> print('Accuracy on testing data =', accuracy_testing)</p> 
   <p>plt.title('GMM classifier')<br> plt.xticks(())<br> plt.yticks(())</p> 
   <p>plt.show()<br> &nbsp;</p> 
  </blockquote> 
  <p><strong><span style="color:#f33b45;">存在问题&nbsp; ：生成图形有差别，预测精度也有问题。特征值、特征向量的用法&nbsp;</span></strong></p> 
  <p><strong>6、近邻传播算法</strong></p> 
  <p>Affinity Propagation 聚类算法的通俗解释。下面案例是网上找的AP算法的一个案例。亲测可用。</p> 
  <p><a href="https://blog.csdn.net/notHeadache/article/details/89003044" rel="nofollow" data-token="ae5d705d8211e269f2060522fdbc4817">https://blog.csdn.net/notHeadache/article/details/89003044</a></p> 
  <blockquote> 
   <p><strong>print(__doc__)</strong></p> 
   <p><strong>from sklearn.cluster import AffinityPropagation<br> from sklearn import metrics<br> from sklearn.datasets.samples_generator import make_blobs</strong></p> 
   <p><strong># #############################################################################<br> # Generate sample data<br> centers = [[1, 1], [-1, -1], [1, -1]]<br> X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; random_state=0)</strong></p> 
   <p><strong># #############################################################################<br> # Compute Affinity Propagation<br> af = AffinityPropagation(preference=-50).fit(X)<br> cluster_centers_indices = af.cluster_centers_indices_<br> labels = af.labels_</strong></p> 
   <p><strong>n_clusters_ = len(cluster_centers_indices) &nbsp; &nbsp;#</strong></p> 
   <p><strong>print('Estimated number of clusters: %d' % n_clusters_)<br> print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))<br> print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))<br> print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))<br> print("Adjusted Rand Index: %0.3f"<br> &nbsp; &nbsp; &nbsp; % metrics.adjusted_rand_score(labels_true, labels))<br> print("Adjusted Mutual Information: %0.3f"<br> &nbsp; &nbsp; &nbsp; % metrics.adjusted_mutual_info_score(labels_true, labels))<br> print("Silhouette Coefficient: %0.3f"<br> &nbsp; &nbsp; &nbsp; % metrics.silhouette_score(X, labels, metric='sqeuclidean'))</strong></p> 
   <p><strong># #############################################################################<br> # Plot result<br> import matplotlib.pyplot as plt<br> from itertools import cycle</strong></p> 
   <p><strong>plt.close('all')<br> plt.figure(1)<br> plt.clf()</strong></p> 
   <p><strong>colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')<br> for k, col in zip(range(n_clusters_), colors):<br> &nbsp; &nbsp; class_members = labels == k<br> &nbsp; &nbsp; cluster_center = X[cluster_centers_indices[k]]<br> &nbsp; &nbsp; plt.plot(X[class_members, 0], X[class_members, 1], col + '.')<br> &nbsp; &nbsp; plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;markeredgecolor='k', markersize=14)<br> &nbsp; &nbsp; for x in X[class_members]:<br> &nbsp; &nbsp; &nbsp; &nbsp; plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)</strong></p> 
   <p><strong>plt.title('Estimated number of clusters: %d' % n_clusters_)<br> plt.show()</strong><br> &nbsp;</p> 
  </blockquote> 
  <h3><img alt="" class="has" height="551" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190821145830361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lpbGlfc2hhMTE=,size_16,color_FFFFFF,t_70" width="693"></h3> 
  <h3>&nbsp;</h3> 
  <h3>（二）问题要点</h3> 
  <p><strong>1、数据连接</strong></p> 
  <blockquote> 
   <p>r = pd.concat([r2, r1], axis=1) &nbsp;# 横向连接（0是纵向），得到聚类中心对应的类别下的数目<br> r.columns = list(data.columns) + [u'类别数目'] &nbsp;# 重命名表头</p> 
  </blockquote> 
  <p><strong>2、分别绘图</strong></p> 
  <blockquote> 
   <p>pic_output = '../tmp/pd_' &nbsp;# 概率密度图文件名前缀<br> for i in range(k):<br> &nbsp; &nbsp; density_plot(data[r[u'聚类类别'] == i]).savefig(u'%s%s.png' % (pic_output, i))&nbsp;</p> 
  </blockquote> 
  <p>&nbsp;3、数据标准化</p> 
  <blockquote> 
   <p>data_zs = 1.0 * (data - data.mean()) / data.std() &nbsp;# 数据标准化</p> 
  </blockquote> 
  <p>4、kmeans++算法&nbsp;</p> 
  <blockquote> 
   <p>kmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10) &nbsp; #k-means++采用智能方法找出初始聚类中心，n_init为迭代次数</p> 
  </blockquote> 
  <p>5、最优分类次数计算（采用轮廓系数判定）</p> 
  <blockquote> 
   <p>&nbsp;</p> 
   <p>import numpy as np<br> import matplotlib.pyplot as plt<br> from sklearn import metrics<br> from sklearn.cluster import KMeans</p> 
   <p># Load data from input file<br> X = np.loadtxt('../code/data_quality.txt', delimiter=',')</p> 
   <p># Plot input data<br> plt.figure()<br> plt.scatter(X[:,0], X[:,1], color='black', s=80, marker='o', facecolors='none')<br> x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1<br> y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1<br> plt.title('Input data')<br> plt.xlim(x_min, x_max)<br> plt.ylim(y_min, y_max)<br> plt.xticks(())<br> plt.yticks(())</p> 
   <p># Initialize variables<br> scores = []<br> values = np.arange(2, 10)</p> 
   <p># Iterate through the defined range<br> for num_clusters in values: &nbsp; &nbsp; #从2到10里面选择<br> &nbsp; &nbsp; # Train the KMeans clustering model<br> &nbsp; &nbsp; kmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)<br> &nbsp; &nbsp; kmeans.fit(X)<br> &nbsp; &nbsp; score = metrics.silhouette_score(X, kmeans.labels_,metric='euclidean', sample_size=len(X)) &nbsp; #轮廓系数<br> &nbsp; &nbsp; print("\nNumber of clusters =", num_clusters)<br> &nbsp; &nbsp; print("Silhouette score =", score)<br> &nbsp; &nbsp; scores.append(score)</p> 
   <p># Plot silhouette scores<br> plt.figure()<br> plt.bar(values, scores, width=0.7, color='black', align='center')<br> plt.title('Silhouette score vs number of clusters')</p> 
   <p># Extract best score and optimal number of clusters<br> num_clusters = np.argmax(scores) + values[0]<br> print('\nOptimal number of clusters =', num_clusters)</p> 
   <p>plt.show()</p> 
  </blockquote> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
