<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>【Semantic Segmentation】语义分割综述 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="【Semantic Segmentation】语义分割综述" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="【Semantic Segmentation】语义分割综述 Encoder And Decoder [FCN] Fully Convolutional Networks for Semantic Segmentation 2016-05 FCN-32 FCN-16 FCN-8 code by pytorch [U-Net] Convolutional Networks for Biomedical Image Segmentation 2015-05 [SegNet] A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation 2015-11 Pyramid &amp; Multi-path [RefineNet] Multi-Path Refinement Networks for High-Resolution Semantic Segmentation 2016-11 [PSPNet] Pyramid Scene Parsing Network 2016-12 [PPM] Pyramid Pooling Module Atrous Convolution [DeepLabV2] Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs [ASPP] atrous spatial pyramid pooling [DeepLabV3] Rethinking Atrous Convolution for Semantic Image Segmentation 2017-06 [DeepLabV3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2018-02 [DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes 2019-01 Attention [Attention U-Net] Learning Where to Look for the Pancreas（ MIDL 2018） [PAN] Pyramid Attention Network for Semantic Segmentation (BMVC 2018) [PSANet] Point-wise Spatial Attention Network for Scene Parsing (ECCV 2018) [EncNet] Context Encoding for Semantic Segmentation (CVPR 2018) [DANet] Dual Attention Network for Scene Segmentation 2018-09 （CVPR 2019） Faster [ESPNet] Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation 2018-03 [Fast-SCNN] Fast Semantic Segmentation Network 2019-02 [BiSeNet] Bilateral Segmentation Network for Real-time Semantic Segmentation 2018-08 [ShelfNet] ShelfNet for Real-time Semantic Segmentation 2018-11 Semi-supervised Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features CVPR18 Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing CVPR18 NAS [Auto-DeepLab] Hierarchical Neural Architecture Search for Semantic Image Segmentation (CVPR2019) 3D 语义分割是指对图像中的每一个像素分配一个类别。 单独一个像素并没有特殊的含义，必须将其放置在上下文中才能识别像素的类别。 我们要增大感受野必然要将feature map 缩小，最后必然要将feature map还原到原图大小。 而语言分割核心的问题就是在于 大的特征图和大的感受野之间的冲突。 同时，由于要分割的目标具有多尺度的特性，大的物体往往可以很好的分割，但是小的物体不明显，不具有分割的尺度。 多尺度是目标检测和分割都存在的问题。 Encoder And Decoder encoder-decoder是语义分割最基础的网络结构。主要论文如下： [FCN] Fully Convolutional Networks for Semantic Segmentation 2016-05 在前面的backbone做传统的卷积操作，不断增大感受野的同时，feature map size 不断缩小。为了解决feature map size 变小不能给每个像素分类的问题，进行上采样将feature map放大到原图。 为了提高对细节像素的分类准确度，将encoder不同pooling大小的feature map 按照pixel-wise 与decoder层的pool相加，再做上采样。 FCN-32 FCN-16 FCN-8 code by pytorch 使用resnet 作为backbone 实现了fcn-4s如下，参考了torchvision的版本。 torchvision实现的是fcn-32s. 下面的变量pool与图中的pool并不是对应的。 使用双线性插值初始化转置卷积参数。 class FCNHead(nn.Sequential): &#39;&#39;&#39; To merge the feature mapping with different scale in the middle with the feature mapping by upsampling need to change channel dimensionality to the same. &#39;&#39;&#39; def __init__(self, in_channels, channels): inter_channels = in_channels // 4 layers = [ nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False), nn.BatchNorm2d(inter_channels), nn.LeakyReLU(), nn.Dropout(0.1), nn.Conv2d(inter_channels, channels, 1) ] super(FCNHead, self).__init__(*layers) class FCNUpsampling(nn.Sequential): &#39;&#39;&#39; &#39;&#39;&#39; def __init__(self, num_classes, kernel_size, stride=1, padding=0): layers = [ nn.ConvTranspose2d(num_classes, num_classes, kernel_size, stride=stride, padding=padding, bias=False) ] super(FCNUpsampling, self).__init__(*layers) class FCN(nn.Module): def __init__(self, backbone, num_classes, aux_classifier=None): super(FCN, self).__init__() # Using the modified resNet to get 4 different scales of the tensor, # in fact, the last three used in the paper, # first reserved for experiment self.backbone = getBackBone(backbone) self.pool1_FCNHead = FCNHead(256, num_classes) self.pool2_FCNHead = FCNHead(512, num_classes) self.pool3_FCNHead = FCNHead(1024, num_classes) self.pool4_FCNHead = FCNHead(2048, num_classes) # upsampling using transposeConvolution # out = s(in-1)+d(k-1)+1-2p # while s = s , d =1, k=2s, p = s/2, we will get out = s*in # we need to zoom in 32 times by 2 x 2 x 2 x 4 self.up_score2 = FCNUpsampling(num_classes, 4, stride=2, padding=1) self.up_score4 = FCNUpsampling(num_classes, 8, stride=4, padding=2) self.up_score8 = FCNUpsampling(num_classes, 16, stride=8, padding=4) self.up_score32 = FCNUpsampling(num_classes, 64, stride=32, padding=16) self.aux_classifier = aux_classifier self.initial_weight() def forward(self, x): result = OrderedDict() input_shape = x.shape[-2:] # pool1 scaling = 1/4 channel = 256 # pool2 scaling = 1/8 channel = 512 # pool3 scaling = 1/16 channel = 1024 # pool4 scaling = 1/32 channel = 2048 pool1, pool2, pool3, pool4 = self.backbone(x) # pool1_same_channel scaling = 1/4 channel = num_classes # pool2_same_channel scaling = 1/8 channel = num_classes # pool3_same_channel scaling = 1/16 channel = num_classes # pool4_same_channel scaling = 1/32 channel = num_classes pool1_same_channel = self.pool1_FCNHead(pool1) pool2_same_channel = self.pool2_FCNHead(pool2) pool3_same_channel = self.pool3_FCNHead(pool3) pool4_same_channel = self.pool4_FCNHead(pool4) if self.aux_classifier is not None: result[&quot;aux&quot;] = self.up_score32(pool4_same_channel) # merge x and pool3 scaling = 1/16 x = self.up_score2(pool4_same_channel) + pool3_same_channel # merge x and pool2 scaling = 1/8 x = self.up_score2(x) + pool2_same_channel # merge x and pool2 scaling = 1/4 x = self.up_score2(x) + pool1_same_channel # scaling = 1 result[&quot;out&quot;] = self.up_score4(x) return result def initial_weight(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;leaky_relu&#39;) elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m,nn.ConvTranspose2d): m.weight = torch.nn.Parameter(self.bilinear_kernel(m.in_channels,m.out_channels,m.kernel_size[0])) def bilinear_kernel(self, in_channels, out_channels, kernel_size): factor = (kernel_size + 1) // 2 if kernel_size % 2 == 1: center = factor - 1 else: center = factor - 0.5 og = np.ogrid[:kernel_size, :kernel_size] filt = (1 - abs(og[0] - center) / factor) * \ (1 - abs(og[1] - center) / factor) weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=&#39;float32&#39;) weight[range(in_channels), range(out_channels), :, :] = filt return torch.from_numpy(weight) [完整代码] https://github.com/ArronHZG/FCN/ 上采样 最近插值 双线性插值 &gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) &gt;&gt;&gt; input tensor([[[[ 1., 2.], [ 3., 4.]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;) &gt;&gt;&gt; m(input) tensor([[[[ 1., 1., 2., 2.], [ 1., 1., 2., 2.], [ 3., 3., 4., 4.], [ 3., 3., 4., 4.]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;) # align_corners=False &gt;&gt;&gt; m(input) tensor([[[[ 1.0000, 1.2500, 1.7500, 2.0000], [ 1.5000, 1.7500, 2.2500, 2.5000], [ 2.5000, 2.7500, 3.2500, 3.5000], [ 3.0000, 3.2500, 3.7500, 4.0000]]]]) &gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;, align_corners=True) &gt;&gt; m(input) tensor([[[[ 1.0000, 1.3333, 1.6667, 2.0000], [ 1.6667, 2.0000, 2.3333, 2.6667], [ 2.3333, 2.6667, 3.0000, 3.3333], [ 3.0000, 3.3333, 3.6667, 4.0000]]]]) &gt;&gt;&gt; # Try scaling the same data in a larger tensor &gt;&gt;&gt; &gt;&gt;&gt; input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3) &gt;&gt;&gt; input_3x3[:, :, :2, :2].copy_(input) tensor([[[[ 1., 2.], [ 3., 4.]]]]) &gt;&gt;&gt; input_3x3 tensor([[[[ 1., 2., 0.], [ 3., 4., 0.], [ 0., 0., 0.]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;) # align_corners=False &gt;&gt;&gt; # Notice that values in top left corner are the same with the small input (except at boundary) &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000, 1.2500, 1.7500, 1.5000, 0.5000, 0.0000], [ 1.5000, 1.7500, 2.2500, 1.8750, 0.6250, 0.0000], [ 2.5000, 2.7500, 3.2500, 2.6250, 0.8750, 0.0000], [ 2.2500, 2.4375, 2.8125, 2.2500, 0.7500, 0.0000], [ 0.7500, 0.8125, 0.9375, 0.7500, 0.2500, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;, align_corners=True) &gt;&gt;&gt; # Notice that values in top left corner are now changed &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000, 1.4000, 1.8000, 1.6000, 0.8000, 0.0000], [ 1.8000, 2.2000, 2.6000, 2.2400, 1.1200, 0.0000], [ 2.6000, 3.0000, 3.4000, 2.8800, 1.4400, 0.0000], [ 2.4000, 2.7200, 3.0400, 2.5600, 1.2800, 0.0000], [ 1.2000, 1.3600, 1.5200, 1.2800, 0.6400, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) 参考 【翻译】https://www.cnblogs.com/xuanxufeng/p/6249834.html [U-Net] Convolutional Networks for Biomedical Image Segmentation 2015-05 U-Net结构与FCN类似。 区别主要在于： U-Net 使用 转置卷积 进行上采样；FCN 使用双线性插值进行上采样。 FCN根据放缩的程度有8，16，32的版本，U-Net 每一次缩小feature map都有对应的上采样操作恢复到原来feature map size. 转置卷积 看动图 https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md [SegNet] A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation 2015-11 SegNe类似于U-Net,但是为了减少冗余，通过记录pooling indices 将 encoder的信息输入到decoder之中。 1）提升边缘刻画度； 2）减少训练的参数； 3）这种上采样模式可以包含到任何编码-解码网络中。 MaxUnpool1d &gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) &gt;&gt;&gt; # Example showcasing the use of output_size &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices, output_size=input.size()) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8., 0.]]]) &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) MaxUnpool2d &gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[[ 1., 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [13, 14, 15, 16]]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[[ 0., 0., 0., 0.], [ 0., 6., 0., 8.], [ 0., 0., 0., 0.], [ 0., 14., 0., 16.]]]]) &gt;&gt;&gt; # specify a different output size than input size &gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) tensor([[[[ 0., 0., 0., 0., 0.], [ 6., 0., 8., 0., 0.], [ 0., 0., 0., 14., 0.], [ 16., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.]]]]) Pyramid &amp; Multi-path 使用特征金字塔的方式增强利用空间上下文的能力。 [RefineNet] Multi-Path Refinement Networks for High-Resolution Semantic Segmentation 2016-11 注意ResNet去除了BN. [PSPNet] Pyramid Scene Parsing Network 2016-12 [PPM] Pyramid Pooling Module Atrous Convolution 为了解决encoder+decoder 损失图片细节精度的问题，使用空洞卷积（Atrous Convolution）解决问题。 以前的CNN主要问题总结：    （1）Up-sampling / pooling layer    （2）内部数据结构丢失；空间层级化信息丢失。    （3）小物体信息无法重建 (假设有四个pooling layer 则 任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。) [DeepLabV2] Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs [ASPP] atrous spatial pyramid pooling [DeepLabV3] Rethinking Atrous Convolution for Semantic Image Segmentation 2017-06 [DeepLabV3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2018-02 参考 [细致讲解] https://blog.csdn.net/u011974639/article/details/79518175 [code] [DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes 2019-01 DenseASPP 结合了ASPP和denseNet 的优点。 ASPP 的结构是一种平行的结构，通过不同的空洞卷积获得不同的感受野，兼顾了宏观和微观的语义信息。 而另一种结构例如Encode 和 Decode 是一种级联结构（cascading) ,天然地可以获得更大的感受野，但是对于物体边缘信息的判别就不是很准。 DenseASPP 综合了上述两种结构的优势。在Cityscapes数据集上在cat 取得了90.7的mIOU。 Attention 语义分割的核心是通过像素周围语义信息，去判定当前像素的类别。比如说在某一个区域，检测到是指猫，但是增大感受野之后，发现这只猫是一个人T-shirt上的图案，那么这只猫的所有像素就应该分类为人。 上述的方法都是在研究感受野如何增加，可以更好的获取某一个区域的语义。但是可想而知，在深度神经网络，前面感受野较小，后面感受野较大，在信息流通的过程中，浅层的神经网络并不确切第知道某一个区域应该是什么，例如是猫还是人，需要信息逐渐传到后面，才能得以确认。 受限于原始卷积操作的结构，感受野必然受限于局部的信息，对于远端的信息融合能力不够，例如一个车和船比较像，在湖中，我们就直接推断这是一个船，在路上我们就推断这是一个车。 模型就算足够深，获得了更大的感受野，但是任然不能解决，长距离依赖的问题，而且要考虑到之后上采样，或者转置卷积为原始的图片大小，空间信息不能丢失太多，一般下采样的倍数控制到16或者32倍。 注意力机制可以在encoder 的过程中就考虑到长距离的信息，进行信息融合。可以改变上述局部感受野的问题。 前置阅读 核心的Attention机制 例如 channel-wise 的SeNet 以及 pixel-wise 的 Non-local Neural Network 【Attention】注意力机制在图像上的应用 [Attention U-Net] Learning Where to Look for the Pancreas（ MIDL 2018） [PAN] Pyramid Attention Network for Semantic Segmentation (BMVC 2018) [PSANet] Point-wise Spatial Attention Network for Scene Parsing (ECCV 2018) [EncNet] Context Encoding for Semantic Segmentation (CVPR 2018) [DANet] Dual Attention Network for Scene Segmentation 2018-09 （CVPR 2019） Faster 为了提高模型速度，使用 1 × 1 1\times1 1×1卷积进行channel reduce。提高模型速度。 [ESPNet] Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation 2018-03 [Fast-SCNN] Fast Semantic Segmentation Network 2019-02 [BiSeNet] Bilateral Segmentation Network for Real-time Semantic Segmentation 2018-08 [ShelfNet] ShelfNet for Real-time Semantic Segmentation 2018-11 Semi-supervised Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features CVPR18 Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing CVPR18 NAS [Auto-DeepLab] Hierarchical Neural Architecture Search for Semantic Image Segmentation (CVPR2019) 3D 未完待续。。。" />
<meta property="og:description" content="【Semantic Segmentation】语义分割综述 Encoder And Decoder [FCN] Fully Convolutional Networks for Semantic Segmentation 2016-05 FCN-32 FCN-16 FCN-8 code by pytorch [U-Net] Convolutional Networks for Biomedical Image Segmentation 2015-05 [SegNet] A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation 2015-11 Pyramid &amp; Multi-path [RefineNet] Multi-Path Refinement Networks for High-Resolution Semantic Segmentation 2016-11 [PSPNet] Pyramid Scene Parsing Network 2016-12 [PPM] Pyramid Pooling Module Atrous Convolution [DeepLabV2] Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs [ASPP] atrous spatial pyramid pooling [DeepLabV3] Rethinking Atrous Convolution for Semantic Image Segmentation 2017-06 [DeepLabV3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2018-02 [DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes 2019-01 Attention [Attention U-Net] Learning Where to Look for the Pancreas（ MIDL 2018） [PAN] Pyramid Attention Network for Semantic Segmentation (BMVC 2018) [PSANet] Point-wise Spatial Attention Network for Scene Parsing (ECCV 2018) [EncNet] Context Encoding for Semantic Segmentation (CVPR 2018) [DANet] Dual Attention Network for Scene Segmentation 2018-09 （CVPR 2019） Faster [ESPNet] Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation 2018-03 [Fast-SCNN] Fast Semantic Segmentation Network 2019-02 [BiSeNet] Bilateral Segmentation Network for Real-time Semantic Segmentation 2018-08 [ShelfNet] ShelfNet for Real-time Semantic Segmentation 2018-11 Semi-supervised Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features CVPR18 Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing CVPR18 NAS [Auto-DeepLab] Hierarchical Neural Architecture Search for Semantic Image Segmentation (CVPR2019) 3D 语义分割是指对图像中的每一个像素分配一个类别。 单独一个像素并没有特殊的含义，必须将其放置在上下文中才能识别像素的类别。 我们要增大感受野必然要将feature map 缩小，最后必然要将feature map还原到原图大小。 而语言分割核心的问题就是在于 大的特征图和大的感受野之间的冲突。 同时，由于要分割的目标具有多尺度的特性，大的物体往往可以很好的分割，但是小的物体不明显，不具有分割的尺度。 多尺度是目标检测和分割都存在的问题。 Encoder And Decoder encoder-decoder是语义分割最基础的网络结构。主要论文如下： [FCN] Fully Convolutional Networks for Semantic Segmentation 2016-05 在前面的backbone做传统的卷积操作，不断增大感受野的同时，feature map size 不断缩小。为了解决feature map size 变小不能给每个像素分类的问题，进行上采样将feature map放大到原图。 为了提高对细节像素的分类准确度，将encoder不同pooling大小的feature map 按照pixel-wise 与decoder层的pool相加，再做上采样。 FCN-32 FCN-16 FCN-8 code by pytorch 使用resnet 作为backbone 实现了fcn-4s如下，参考了torchvision的版本。 torchvision实现的是fcn-32s. 下面的变量pool与图中的pool并不是对应的。 使用双线性插值初始化转置卷积参数。 class FCNHead(nn.Sequential): &#39;&#39;&#39; To merge the feature mapping with different scale in the middle with the feature mapping by upsampling need to change channel dimensionality to the same. &#39;&#39;&#39; def __init__(self, in_channels, channels): inter_channels = in_channels // 4 layers = [ nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False), nn.BatchNorm2d(inter_channels), nn.LeakyReLU(), nn.Dropout(0.1), nn.Conv2d(inter_channels, channels, 1) ] super(FCNHead, self).__init__(*layers) class FCNUpsampling(nn.Sequential): &#39;&#39;&#39; &#39;&#39;&#39; def __init__(self, num_classes, kernel_size, stride=1, padding=0): layers = [ nn.ConvTranspose2d(num_classes, num_classes, kernel_size, stride=stride, padding=padding, bias=False) ] super(FCNUpsampling, self).__init__(*layers) class FCN(nn.Module): def __init__(self, backbone, num_classes, aux_classifier=None): super(FCN, self).__init__() # Using the modified resNet to get 4 different scales of the tensor, # in fact, the last three used in the paper, # first reserved for experiment self.backbone = getBackBone(backbone) self.pool1_FCNHead = FCNHead(256, num_classes) self.pool2_FCNHead = FCNHead(512, num_classes) self.pool3_FCNHead = FCNHead(1024, num_classes) self.pool4_FCNHead = FCNHead(2048, num_classes) # upsampling using transposeConvolution # out = s(in-1)+d(k-1)+1-2p # while s = s , d =1, k=2s, p = s/2, we will get out = s*in # we need to zoom in 32 times by 2 x 2 x 2 x 4 self.up_score2 = FCNUpsampling(num_classes, 4, stride=2, padding=1) self.up_score4 = FCNUpsampling(num_classes, 8, stride=4, padding=2) self.up_score8 = FCNUpsampling(num_classes, 16, stride=8, padding=4) self.up_score32 = FCNUpsampling(num_classes, 64, stride=32, padding=16) self.aux_classifier = aux_classifier self.initial_weight() def forward(self, x): result = OrderedDict() input_shape = x.shape[-2:] # pool1 scaling = 1/4 channel = 256 # pool2 scaling = 1/8 channel = 512 # pool3 scaling = 1/16 channel = 1024 # pool4 scaling = 1/32 channel = 2048 pool1, pool2, pool3, pool4 = self.backbone(x) # pool1_same_channel scaling = 1/4 channel = num_classes # pool2_same_channel scaling = 1/8 channel = num_classes # pool3_same_channel scaling = 1/16 channel = num_classes # pool4_same_channel scaling = 1/32 channel = num_classes pool1_same_channel = self.pool1_FCNHead(pool1) pool2_same_channel = self.pool2_FCNHead(pool2) pool3_same_channel = self.pool3_FCNHead(pool3) pool4_same_channel = self.pool4_FCNHead(pool4) if self.aux_classifier is not None: result[&quot;aux&quot;] = self.up_score32(pool4_same_channel) # merge x and pool3 scaling = 1/16 x = self.up_score2(pool4_same_channel) + pool3_same_channel # merge x and pool2 scaling = 1/8 x = self.up_score2(x) + pool2_same_channel # merge x and pool2 scaling = 1/4 x = self.up_score2(x) + pool1_same_channel # scaling = 1 result[&quot;out&quot;] = self.up_score4(x) return result def initial_weight(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;leaky_relu&#39;) elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m,nn.ConvTranspose2d): m.weight = torch.nn.Parameter(self.bilinear_kernel(m.in_channels,m.out_channels,m.kernel_size[0])) def bilinear_kernel(self, in_channels, out_channels, kernel_size): factor = (kernel_size + 1) // 2 if kernel_size % 2 == 1: center = factor - 1 else: center = factor - 0.5 og = np.ogrid[:kernel_size, :kernel_size] filt = (1 - abs(og[0] - center) / factor) * \ (1 - abs(og[1] - center) / factor) weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=&#39;float32&#39;) weight[range(in_channels), range(out_channels), :, :] = filt return torch.from_numpy(weight) [完整代码] https://github.com/ArronHZG/FCN/ 上采样 最近插值 双线性插值 &gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) &gt;&gt;&gt; input tensor([[[[ 1., 2.], [ 3., 4.]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;) &gt;&gt;&gt; m(input) tensor([[[[ 1., 1., 2., 2.], [ 1., 1., 2., 2.], [ 3., 3., 4., 4.], [ 3., 3., 4., 4.]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;) # align_corners=False &gt;&gt;&gt; m(input) tensor([[[[ 1.0000, 1.2500, 1.7500, 2.0000], [ 1.5000, 1.7500, 2.2500, 2.5000], [ 2.5000, 2.7500, 3.2500, 3.5000], [ 3.0000, 3.2500, 3.7500, 4.0000]]]]) &gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;, align_corners=True) &gt;&gt; m(input) tensor([[[[ 1.0000, 1.3333, 1.6667, 2.0000], [ 1.6667, 2.0000, 2.3333, 2.6667], [ 2.3333, 2.6667, 3.0000, 3.3333], [ 3.0000, 3.3333, 3.6667, 4.0000]]]]) &gt;&gt;&gt; # Try scaling the same data in a larger tensor &gt;&gt;&gt; &gt;&gt;&gt; input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3) &gt;&gt;&gt; input_3x3[:, :, :2, :2].copy_(input) tensor([[[[ 1., 2.], [ 3., 4.]]]]) &gt;&gt;&gt; input_3x3 tensor([[[[ 1., 2., 0.], [ 3., 4., 0.], [ 0., 0., 0.]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;) # align_corners=False &gt;&gt;&gt; # Notice that values in top left corner are the same with the small input (except at boundary) &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000, 1.2500, 1.7500, 1.5000, 0.5000, 0.0000], [ 1.5000, 1.7500, 2.2500, 1.8750, 0.6250, 0.0000], [ 2.5000, 2.7500, 3.2500, 2.6250, 0.8750, 0.0000], [ 2.2500, 2.4375, 2.8125, 2.2500, 0.7500, 0.0000], [ 0.7500, 0.8125, 0.9375, 0.7500, 0.2500, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;, align_corners=True) &gt;&gt;&gt; # Notice that values in top left corner are now changed &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000, 1.4000, 1.8000, 1.6000, 0.8000, 0.0000], [ 1.8000, 2.2000, 2.6000, 2.2400, 1.1200, 0.0000], [ 2.6000, 3.0000, 3.4000, 2.8800, 1.4400, 0.0000], [ 2.4000, 2.7200, 3.0400, 2.5600, 1.2800, 0.0000], [ 1.2000, 1.3600, 1.5200, 1.2800, 0.6400, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) 参考 【翻译】https://www.cnblogs.com/xuanxufeng/p/6249834.html [U-Net] Convolutional Networks for Biomedical Image Segmentation 2015-05 U-Net结构与FCN类似。 区别主要在于： U-Net 使用 转置卷积 进行上采样；FCN 使用双线性插值进行上采样。 FCN根据放缩的程度有8，16，32的版本，U-Net 每一次缩小feature map都有对应的上采样操作恢复到原来feature map size. 转置卷积 看动图 https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md [SegNet] A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation 2015-11 SegNe类似于U-Net,但是为了减少冗余，通过记录pooling indices 将 encoder的信息输入到decoder之中。 1）提升边缘刻画度； 2）减少训练的参数； 3）这种上采样模式可以包含到任何编码-解码网络中。 MaxUnpool1d &gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) &gt;&gt;&gt; # Example showcasing the use of output_size &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices, output_size=input.size()) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8., 0.]]]) &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) MaxUnpool2d &gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[[ 1., 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [13, 14, 15, 16]]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[[ 0., 0., 0., 0.], [ 0., 6., 0., 8.], [ 0., 0., 0., 0.], [ 0., 14., 0., 16.]]]]) &gt;&gt;&gt; # specify a different output size than input size &gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) tensor([[[[ 0., 0., 0., 0., 0.], [ 6., 0., 8., 0., 0.], [ 0., 0., 0., 14., 0.], [ 16., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.]]]]) Pyramid &amp; Multi-path 使用特征金字塔的方式增强利用空间上下文的能力。 [RefineNet] Multi-Path Refinement Networks for High-Resolution Semantic Segmentation 2016-11 注意ResNet去除了BN. [PSPNet] Pyramid Scene Parsing Network 2016-12 [PPM] Pyramid Pooling Module Atrous Convolution 为了解决encoder+decoder 损失图片细节精度的问题，使用空洞卷积（Atrous Convolution）解决问题。 以前的CNN主要问题总结：    （1）Up-sampling / pooling layer    （2）内部数据结构丢失；空间层级化信息丢失。    （3）小物体信息无法重建 (假设有四个pooling layer 则 任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。) [DeepLabV2] Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs [ASPP] atrous spatial pyramid pooling [DeepLabV3] Rethinking Atrous Convolution for Semantic Image Segmentation 2017-06 [DeepLabV3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2018-02 参考 [细致讲解] https://blog.csdn.net/u011974639/article/details/79518175 [code] [DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes 2019-01 DenseASPP 结合了ASPP和denseNet 的优点。 ASPP 的结构是一种平行的结构，通过不同的空洞卷积获得不同的感受野，兼顾了宏观和微观的语义信息。 而另一种结构例如Encode 和 Decode 是一种级联结构（cascading) ,天然地可以获得更大的感受野，但是对于物体边缘信息的判别就不是很准。 DenseASPP 综合了上述两种结构的优势。在Cityscapes数据集上在cat 取得了90.7的mIOU。 Attention 语义分割的核心是通过像素周围语义信息，去判定当前像素的类别。比如说在某一个区域，检测到是指猫，但是增大感受野之后，发现这只猫是一个人T-shirt上的图案，那么这只猫的所有像素就应该分类为人。 上述的方法都是在研究感受野如何增加，可以更好的获取某一个区域的语义。但是可想而知，在深度神经网络，前面感受野较小，后面感受野较大，在信息流通的过程中，浅层的神经网络并不确切第知道某一个区域应该是什么，例如是猫还是人，需要信息逐渐传到后面，才能得以确认。 受限于原始卷积操作的结构，感受野必然受限于局部的信息，对于远端的信息融合能力不够，例如一个车和船比较像，在湖中，我们就直接推断这是一个船，在路上我们就推断这是一个车。 模型就算足够深，获得了更大的感受野，但是任然不能解决，长距离依赖的问题，而且要考虑到之后上采样，或者转置卷积为原始的图片大小，空间信息不能丢失太多，一般下采样的倍数控制到16或者32倍。 注意力机制可以在encoder 的过程中就考虑到长距离的信息，进行信息融合。可以改变上述局部感受野的问题。 前置阅读 核心的Attention机制 例如 channel-wise 的SeNet 以及 pixel-wise 的 Non-local Neural Network 【Attention】注意力机制在图像上的应用 [Attention U-Net] Learning Where to Look for the Pancreas（ MIDL 2018） [PAN] Pyramid Attention Network for Semantic Segmentation (BMVC 2018) [PSANet] Point-wise Spatial Attention Network for Scene Parsing (ECCV 2018) [EncNet] Context Encoding for Semantic Segmentation (CVPR 2018) [DANet] Dual Attention Network for Scene Segmentation 2018-09 （CVPR 2019） Faster 为了提高模型速度，使用 1 × 1 1\times1 1×1卷积进行channel reduce。提高模型速度。 [ESPNet] Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation 2018-03 [Fast-SCNN] Fast Semantic Segmentation Network 2019-02 [BiSeNet] Bilateral Segmentation Network for Real-time Semantic Segmentation 2018-08 [ShelfNet] ShelfNet for Real-time Semantic Segmentation 2018-11 Semi-supervised Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features CVPR18 Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing CVPR18 NAS [Auto-DeepLab] Hierarchical Neural Architecture Search for Semantic Image Segmentation (CVPR2019) 3D 未完待续。。。" />
<link rel="canonical" href="https://uzzz.org/2019/08/22/794199.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/22/794199.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-22T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"【Semantic Segmentation】语义分割综述 Encoder And Decoder [FCN] Fully Convolutional Networks for Semantic Segmentation 2016-05 FCN-32 FCN-16 FCN-8 code by pytorch [U-Net] Convolutional Networks for Biomedical Image Segmentation 2015-05 [SegNet] A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation 2015-11 Pyramid &amp; Multi-path [RefineNet] Multi-Path Refinement Networks for High-Resolution Semantic Segmentation 2016-11 [PSPNet] Pyramid Scene Parsing Network 2016-12 [PPM] Pyramid Pooling Module Atrous Convolution [DeepLabV2] Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs [ASPP] atrous spatial pyramid pooling [DeepLabV3] Rethinking Atrous Convolution for Semantic Image Segmentation 2017-06 [DeepLabV3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2018-02 [DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes 2019-01 Attention [Attention U-Net] Learning Where to Look for the Pancreas（ MIDL 2018） [PAN] Pyramid Attention Network for Semantic Segmentation (BMVC 2018) [PSANet] Point-wise Spatial Attention Network for Scene Parsing (ECCV 2018) [EncNet] Context Encoding for Semantic Segmentation (CVPR 2018) [DANet] Dual Attention Network for Scene Segmentation 2018-09 （CVPR 2019） Faster [ESPNet] Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation 2018-03 [Fast-SCNN] Fast Semantic Segmentation Network 2019-02 [BiSeNet] Bilateral Segmentation Network for Real-time Semantic Segmentation 2018-08 [ShelfNet] ShelfNet for Real-time Semantic Segmentation 2018-11 Semi-supervised Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features CVPR18 Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing CVPR18 NAS [Auto-DeepLab] Hierarchical Neural Architecture Search for Semantic Image Segmentation (CVPR2019) 3D 语义分割是指对图像中的每一个像素分配一个类别。 单独一个像素并没有特殊的含义，必须将其放置在上下文中才能识别像素的类别。 我们要增大感受野必然要将feature map 缩小，最后必然要将feature map还原到原图大小。 而语言分割核心的问题就是在于 大的特征图和大的感受野之间的冲突。 同时，由于要分割的目标具有多尺度的特性，大的物体往往可以很好的分割，但是小的物体不明显，不具有分割的尺度。 多尺度是目标检测和分割都存在的问题。 Encoder And Decoder encoder-decoder是语义分割最基础的网络结构。主要论文如下： [FCN] Fully Convolutional Networks for Semantic Segmentation 2016-05 在前面的backbone做传统的卷积操作，不断增大感受野的同时，feature map size 不断缩小。为了解决feature map size 变小不能给每个像素分类的问题，进行上采样将feature map放大到原图。 为了提高对细节像素的分类准确度，将encoder不同pooling大小的feature map 按照pixel-wise 与decoder层的pool相加，再做上采样。 FCN-32 FCN-16 FCN-8 code by pytorch 使用resnet 作为backbone 实现了fcn-4s如下，参考了torchvision的版本。 torchvision实现的是fcn-32s. 下面的变量pool与图中的pool并不是对应的。 使用双线性插值初始化转置卷积参数。 class FCNHead(nn.Sequential): &#39;&#39;&#39; To merge the feature mapping with different scale in the middle with the feature mapping by upsampling need to change channel dimensionality to the same. &#39;&#39;&#39; def __init__(self, in_channels, channels): inter_channels = in_channels // 4 layers = [ nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False), nn.BatchNorm2d(inter_channels), nn.LeakyReLU(), nn.Dropout(0.1), nn.Conv2d(inter_channels, channels, 1) ] super(FCNHead, self).__init__(*layers) class FCNUpsampling(nn.Sequential): &#39;&#39;&#39; &#39;&#39;&#39; def __init__(self, num_classes, kernel_size, stride=1, padding=0): layers = [ nn.ConvTranspose2d(num_classes, num_classes, kernel_size, stride=stride, padding=padding, bias=False) ] super(FCNUpsampling, self).__init__(*layers) class FCN(nn.Module): def __init__(self, backbone, num_classes, aux_classifier=None): super(FCN, self).__init__() # Using the modified resNet to get 4 different scales of the tensor, # in fact, the last three used in the paper, # first reserved for experiment self.backbone = getBackBone(backbone) self.pool1_FCNHead = FCNHead(256, num_classes) self.pool2_FCNHead = FCNHead(512, num_classes) self.pool3_FCNHead = FCNHead(1024, num_classes) self.pool4_FCNHead = FCNHead(2048, num_classes) # upsampling using transposeConvolution # out = s(in-1)+d(k-1)+1-2p # while s = s , d =1, k=2s, p = s/2, we will get out = s*in # we need to zoom in 32 times by 2 x 2 x 2 x 4 self.up_score2 = FCNUpsampling(num_classes, 4, stride=2, padding=1) self.up_score4 = FCNUpsampling(num_classes, 8, stride=4, padding=2) self.up_score8 = FCNUpsampling(num_classes, 16, stride=8, padding=4) self.up_score32 = FCNUpsampling(num_classes, 64, stride=32, padding=16) self.aux_classifier = aux_classifier self.initial_weight() def forward(self, x): result = OrderedDict() input_shape = x.shape[-2:] # pool1 scaling = 1/4 channel = 256 # pool2 scaling = 1/8 channel = 512 # pool3 scaling = 1/16 channel = 1024 # pool4 scaling = 1/32 channel = 2048 pool1, pool2, pool3, pool4 = self.backbone(x) # pool1_same_channel scaling = 1/4 channel = num_classes # pool2_same_channel scaling = 1/8 channel = num_classes # pool3_same_channel scaling = 1/16 channel = num_classes # pool4_same_channel scaling = 1/32 channel = num_classes pool1_same_channel = self.pool1_FCNHead(pool1) pool2_same_channel = self.pool2_FCNHead(pool2) pool3_same_channel = self.pool3_FCNHead(pool3) pool4_same_channel = self.pool4_FCNHead(pool4) if self.aux_classifier is not None: result[&quot;aux&quot;] = self.up_score32(pool4_same_channel) # merge x and pool3 scaling = 1/16 x = self.up_score2(pool4_same_channel) + pool3_same_channel # merge x and pool2 scaling = 1/8 x = self.up_score2(x) + pool2_same_channel # merge x and pool2 scaling = 1/4 x = self.up_score2(x) + pool1_same_channel # scaling = 1 result[&quot;out&quot;] = self.up_score4(x) return result def initial_weight(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=&#39;fan_out&#39;, nonlinearity=&#39;leaky_relu&#39;) elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) elif isinstance(m,nn.ConvTranspose2d): m.weight = torch.nn.Parameter(self.bilinear_kernel(m.in_channels,m.out_channels,m.kernel_size[0])) def bilinear_kernel(self, in_channels, out_channels, kernel_size): factor = (kernel_size + 1) // 2 if kernel_size % 2 == 1: center = factor - 1 else: center = factor - 0.5 og = np.ogrid[:kernel_size, :kernel_size] filt = (1 - abs(og[0] - center) / factor) * \\ (1 - abs(og[1] - center) / factor) weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=&#39;float32&#39;) weight[range(in_channels), range(out_channels), :, :] = filt return torch.from_numpy(weight) [完整代码] https://github.com/ArronHZG/FCN/ 上采样 最近插值 双线性插值 &gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) &gt;&gt;&gt; input tensor([[[[ 1., 2.], [ 3., 4.]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;) &gt;&gt;&gt; m(input) tensor([[[[ 1., 1., 2., 2.], [ 1., 1., 2., 2.], [ 3., 3., 4., 4.], [ 3., 3., 4., 4.]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;) # align_corners=False &gt;&gt;&gt; m(input) tensor([[[[ 1.0000, 1.2500, 1.7500, 2.0000], [ 1.5000, 1.7500, 2.2500, 2.5000], [ 2.5000, 2.7500, 3.2500, 3.5000], [ 3.0000, 3.2500, 3.7500, 4.0000]]]]) &gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;, align_corners=True) &gt;&gt; m(input) tensor([[[[ 1.0000, 1.3333, 1.6667, 2.0000], [ 1.6667, 2.0000, 2.3333, 2.6667], [ 2.3333, 2.6667, 3.0000, 3.3333], [ 3.0000, 3.3333, 3.6667, 4.0000]]]]) &gt;&gt;&gt; # Try scaling the same data in a larger tensor &gt;&gt;&gt; &gt;&gt;&gt; input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3) &gt;&gt;&gt; input_3x3[:, :, :2, :2].copy_(input) tensor([[[[ 1., 2.], [ 3., 4.]]]]) &gt;&gt;&gt; input_3x3 tensor([[[[ 1., 2., 0.], [ 3., 4., 0.], [ 0., 0., 0.]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;) # align_corners=False &gt;&gt;&gt; # Notice that values in top left corner are the same with the small input (except at boundary) &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000, 1.2500, 1.7500, 1.5000, 0.5000, 0.0000], [ 1.5000, 1.7500, 2.2500, 1.8750, 0.6250, 0.0000], [ 2.5000, 2.7500, 3.2500, 2.6250, 0.8750, 0.0000], [ 2.2500, 2.4375, 2.8125, 2.2500, 0.7500, 0.0000], [ 0.7500, 0.8125, 0.9375, 0.7500, 0.2500, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;, align_corners=True) &gt;&gt;&gt; # Notice that values in top left corner are now changed &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000, 1.4000, 1.8000, 1.6000, 0.8000, 0.0000], [ 1.8000, 2.2000, 2.6000, 2.2400, 1.1200, 0.0000], [ 2.6000, 3.0000, 3.4000, 2.8800, 1.4400, 0.0000], [ 2.4000, 2.7200, 3.0400, 2.5600, 1.2800, 0.0000], [ 1.2000, 1.3600, 1.5200, 1.2800, 0.6400, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]) 参考 【翻译】https://www.cnblogs.com/xuanxufeng/p/6249834.html [U-Net] Convolutional Networks for Biomedical Image Segmentation 2015-05 U-Net结构与FCN类似。 区别主要在于： U-Net 使用 转置卷积 进行上采样；FCN 使用双线性插值进行上采样。 FCN根据放缩的程度有8，16，32的版本，U-Net 每一次缩小feature map都有对应的上采样操作恢复到原来feature map size. 转置卷积 看动图 https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md [SegNet] A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation 2015-11 SegNe类似于U-Net,但是为了减少冗余，通过记录pooling indices 将 encoder的信息输入到decoder之中。 1）提升边缘刻画度； 2）减少训练的参数； 3）这种上采样模式可以包含到任何编码-解码网络中。 MaxUnpool1d &gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) &gt;&gt;&gt; # Example showcasing the use of output_size &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices, output_size=input.size()) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8., 0.]]]) &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0., 2., 0., 4., 0., 6., 0., 8.]]]) MaxUnpool2d &gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[[ 1., 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [13, 14, 15, 16]]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[[ 0., 0., 0., 0.], [ 0., 6., 0., 8.], [ 0., 0., 0., 0.], [ 0., 14., 0., 16.]]]]) &gt;&gt;&gt; # specify a different output size than input size &gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) tensor([[[[ 0., 0., 0., 0., 0.], [ 6., 0., 8., 0., 0.], [ 0., 0., 0., 14., 0.], [ 16., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0.]]]]) Pyramid &amp; Multi-path 使用特征金字塔的方式增强利用空间上下文的能力。 [RefineNet] Multi-Path Refinement Networks for High-Resolution Semantic Segmentation 2016-11 注意ResNet去除了BN. [PSPNet] Pyramid Scene Parsing Network 2016-12 [PPM] Pyramid Pooling Module Atrous Convolution 为了解决encoder+decoder 损失图片细节精度的问题，使用空洞卷积（Atrous Convolution）解决问题。 以前的CNN主要问题总结：    （1）Up-sampling / pooling layer    （2）内部数据结构丢失；空间层级化信息丢失。    （3）小物体信息无法重建 (假设有四个pooling layer 则 任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。) [DeepLabV2] Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs [ASPP] atrous spatial pyramid pooling [DeepLabV3] Rethinking Atrous Convolution for Semantic Image Segmentation 2017-06 [DeepLabV3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2018-02 参考 [细致讲解] https://blog.csdn.net/u011974639/article/details/79518175 [code] [DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes 2019-01 DenseASPP 结合了ASPP和denseNet 的优点。 ASPP 的结构是一种平行的结构，通过不同的空洞卷积获得不同的感受野，兼顾了宏观和微观的语义信息。 而另一种结构例如Encode 和 Decode 是一种级联结构（cascading) ,天然地可以获得更大的感受野，但是对于物体边缘信息的判别就不是很准。 DenseASPP 综合了上述两种结构的优势。在Cityscapes数据集上在cat 取得了90.7的mIOU。 Attention 语义分割的核心是通过像素周围语义信息，去判定当前像素的类别。比如说在某一个区域，检测到是指猫，但是增大感受野之后，发现这只猫是一个人T-shirt上的图案，那么这只猫的所有像素就应该分类为人。 上述的方法都是在研究感受野如何增加，可以更好的获取某一个区域的语义。但是可想而知，在深度神经网络，前面感受野较小，后面感受野较大，在信息流通的过程中，浅层的神经网络并不确切第知道某一个区域应该是什么，例如是猫还是人，需要信息逐渐传到后面，才能得以确认。 受限于原始卷积操作的结构，感受野必然受限于局部的信息，对于远端的信息融合能力不够，例如一个车和船比较像，在湖中，我们就直接推断这是一个船，在路上我们就推断这是一个车。 模型就算足够深，获得了更大的感受野，但是任然不能解决，长距离依赖的问题，而且要考虑到之后上采样，或者转置卷积为原始的图片大小，空间信息不能丢失太多，一般下采样的倍数控制到16或者32倍。 注意力机制可以在encoder 的过程中就考虑到长距离的信息，进行信息融合。可以改变上述局部感受野的问题。 前置阅读 核心的Attention机制 例如 channel-wise 的SeNet 以及 pixel-wise 的 Non-local Neural Network 【Attention】注意力机制在图像上的应用 [Attention U-Net] Learning Where to Look for the Pancreas（ MIDL 2018） [PAN] Pyramid Attention Network for Semantic Segmentation (BMVC 2018) [PSANet] Point-wise Spatial Attention Network for Scene Parsing (ECCV 2018) [EncNet] Context Encoding for Semantic Segmentation (CVPR 2018) [DANet] Dual Attention Network for Scene Segmentation 2018-09 （CVPR 2019） Faster 为了提高模型速度，使用 1 × 1 1\\times1 1×1卷积进行channel reduce。提高模型速度。 [ESPNet] Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation 2018-03 [Fast-SCNN] Fast Semantic Segmentation Network 2019-02 [BiSeNet] Bilateral Segmentation Network for Real-time Semantic Segmentation 2018-08 [ShelfNet] ShelfNet for Real-time Semantic Segmentation 2018-11 Semi-supervised Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features CVPR18 Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing CVPR18 NAS [Auto-DeepLab] Hierarchical Neural Architecture Search for Semantic Image Segmentation (CVPR2019) 3D 未完待续。。。","@type":"BlogPosting","url":"https://uzzz.org/2019/08/22/794199.html","headline":"【Semantic Segmentation】语义分割综述","dateModified":"2019-08-22T00:00:00+08:00","datePublished":"2019-08-22T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/22/794199.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>【Semantic Segmentation】语义分割综述</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>【Semantic Segmentation】语义分割综述</h3>
   <ul>
    <li><a href="#Encoder_And_Decoder_7" rel="nofollow" data-token="1756cb31062014f4d07296d36a112053">Encoder And Decoder</a></li>
    <ul>
     <li><a href="#FCN_Fully_Convolutional_Networks_for_Semantic_Segmentation__201605_9" rel="nofollow" data-token="54fac0dcdea5b2aba6570775e4c4dda5">[FCN] Fully Convolutional Networks for Semantic Segmentation 2016-05</a></li>
     <ul>
      <li><a href="#FCN32_15" rel="nofollow" data-token="58999a7aff898c08320250730fa09489">FCN-32</a></li>
      <li><a href="#FCN16_17" rel="nofollow" data-token="3a7534aa451b3771b7fb8cd2813f4078">FCN-16</a></li>
      <li><a href="#FCN8_19" rel="nofollow" data-token="d5d62ed28611e176355126f01650c10c">FCN-8</a></li>
      <li><a href="#code_by_pytorch_21" rel="nofollow" data-token="e31fcb1cea018d5d5963f3c3132f0f23">code by pytorch</a></li>
     </ul>
     <li><a href="#UNet_Convolutional_Networks_for_Biomedical_Image_Segmentation_201505_204" rel="nofollow" data-token="2e08bcd1507f90aac9bbe83ecdc7056a">[U-Net] Convolutional Networks for Biomedical Image Segmentation 2015-05</a></li>
     <li><a href="#SegNet_A_Deep_Convolutional_EncoderDecoder_Architecture_for_Image_Segmentation_201511_217" rel="nofollow" data-token="6ce27f00af4ba79f617f9647053def9b">[SegNet] A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation 2015-11</a></li>
    </ul>
    <li><a href="#Pyramid__Multipath_266" rel="nofollow" data-token="ccb5502fed87fda6f3d4a4ad6c703df6">Pyramid &amp; Multi-path</a></li>
    <ul>
     <li><a href="#RefineNet_MultiPath_Refinement_Networks_for_HighResolution_Semantic_Segmentation_201611_268" rel="nofollow" data-token="f51913c261e1a65c55a1e6fd9e8bf1cd">[RefineNet] Multi-Path Refinement Networks for High-Resolution Semantic Segmentation 2016-11</a></li>
     <li><a href="#PSPNet_Pyramid_Scene_Parsing_Network_201612_272" rel="nofollow" data-token="36c76c4df308dcf434c78cb88fa42e48">[PSPNet] Pyramid Scene Parsing Network 2016-12</a></li>
     <ul>
      <li><a href="#PPM_Pyramid_Pooling_Module_274" rel="nofollow" data-token="18149585eff2ebb1b27ef9db1e484fd6">[PPM] Pyramid Pooling Module</a></li>
     </ul>
    </ul>
    <li><a href="#Atrous_Convolution_275" rel="nofollow" data-token="a2904ae0f8b2bd36afe57de439a2f81a">Atrous Convolution</a></li>
    <ul>
     <li><a href="#DeepLabV2_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolutionand_Fully_Connected_CRFs_281" rel="nofollow" data-token="9e17aca51ea4e70c3db16f75720ce9d7">[DeepLabV2] Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs</a></li>
     <ul>
      <li><a href="#ASPP_atrous_spatial_pyramid_pooling_285" rel="nofollow" data-token="8c16581f359f4651d5be23027610141a">[ASPP] atrous spatial pyramid pooling</a></li>
     </ul>
     <li><a href="#DeepLabV3_Rethinking_Atrous_Convolution_for_Semantic_Image_Segmentation_201706_288" rel="nofollow" data-token="8ddab972971b7c3669ef7a06982b5eb0">[DeepLabV3] Rethinking Atrous Convolution for Semantic Image Segmentation 2017-06</a></li>
     <li><a href="#DeepLabV3_EncoderDecoder_with_Atrous_Separable_Convolution_for_Semantic_Image_Segmentation_201802_291" rel="nofollow" data-token="f513c5be113b06f8f7d64924e2125de8">[DeepLabV3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2018-02</a></li>
     <li><a href="#DenseASPP_DenseASPP_for_Semantic_Segmentation_in_Street_Scenes_201901_297" rel="nofollow" data-token="225da07c0da2d70ce6017da77611bd67">[DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes 2019-01</a></li>
    </ul>
    <li><a href="#Attention_304" rel="nofollow" data-token="7ee4addc4c6badcbac755b712aa90360">Attention</a></li>
    <ul>
     <li><a href="#Attention_UNet_Learning_Where_to_Look_for_the_Pancreas_MIDL_2018_318" rel="nofollow" data-token="6b74659647aa0f18c392eee795c5aef0">[Attention U-Net] Learning Where to Look for the Pancreas（ MIDL 2018）</a></li>
     <li><a href="#PAN_Pyramid_Attention_Network_for_Semantic_Segmentation_BMVC_2018_322" rel="nofollow" data-token="56c10855a206647b46d9bfe443e734a8">[PAN] Pyramid Attention Network for Semantic Segmentation (BMVC 2018)</a></li>
     <li><a href="#PSANet_Pointwise_Spatial_Attention_Network_for_Scene_Parsing_ECCV_2018_329" rel="nofollow" data-token="d76504764762df11f11fa5c941f0d17a">[PSANet] Point-wise Spatial Attention Network for Scene Parsing (ECCV 2018)</a></li>
     <li><a href="#EncNet_Context_Encoding_for_Semantic_Segmentation_CVPR_2018_335" rel="nofollow" data-token="b79ef17e5b4ce6a74afa2231ac1b1e4d">[EncNet] Context Encoding for Semantic Segmentation (CVPR 2018)</a></li>
     <li><a href="#DANet_Dual_Attention_Network_for_Scene_Segmentation_201809_CVPR_2019_337" rel="nofollow" data-token="2b1463a9a915f1a89f9642ae6208f356">[DANet] Dual Attention Network for Scene Segmentation 2018-09 （CVPR 2019）</a></li>
    </ul>
    <li><a href="#Faster_340" rel="nofollow" data-token="fcc76d080aeb5247a1a3f0f6e52fe401">Faster</a></li>
    <ul>
     <li><a href="#ESPNet_Efficient_Spatial_Pyramid_of_Dilated_Convolutions_for_Semantic_Segmentation_201803_342" rel="nofollow" data-token="b3feea93cbf6b2a77a62d08ad662ee9c">[ESPNet] Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation 2018-03</a></li>
     <li><a href="#FastSCNN_Fast_Semantic_Segmentation_Network_201902_349" rel="nofollow" data-token="7f23c7386fdcc23922ffd98a8b22f89e">[Fast-SCNN] Fast Semantic Segmentation Network 2019-02</a></li>
     <li><a href="#BiSeNet_Bilateral_Segmentation_Network_for_Realtime_Semantic_Segmentation_201808_353" rel="nofollow" data-token="8d2c3d0e90c1e5f8565de0874238261f">[BiSeNet] Bilateral Segmentation Network for Real-time Semantic Segmentation 2018-08</a></li>
     <li><a href="#ShelfNet_ShelfNet_for_Realtime_Semantic_Segmentation_201811_355" rel="nofollow" data-token="c946f6514821312aaf4e7a7f5105ffe9">[ShelfNet] ShelfNet for Real-time Semantic Segmentation 2018-11</a></li>
    </ul>
    <li><a href="#Semisupervised_357" rel="nofollow" data-token="84888d9da1c9b6402a48ed77c6539c07">Semi-supervised</a></li>
    <ul>
     <li><a href="#WeaklySupervised_Semantic_Segmentation_by_Iteratively_Mining_Common_Object_Features_CVPR18_358" rel="nofollow" data-token="949f7a40139bb326dd922a95a161013c">Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features CVPR18</a></li>
     <li><a href="#WeaklySupervised_Semantic_Segmentation_Network_With_Deep_Seeded_Region_Growing_CVPR18_359" rel="nofollow" data-token="ca14d2cf7cab6e58ca95a46fcd0e959b">Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing CVPR18</a></li>
    </ul>
    <li><a href="#NAS_360" rel="nofollow" data-token="0c99be9220c635569a6e41bd56061031">NAS</a></li>
    <ul>
     <li><a href="#AutoDeepLab_Hierarchical_Neural_Architecture_Search_for_Semantic_Image_Segmentation_CVPR2019_361" rel="nofollow" data-token="4e3bdfe98cc634ccfe8575d156927f7f">[Auto-DeepLab] Hierarchical Neural Architecture Search for Semantic Image Segmentation (CVPR2019)</a></li>
    </ul>
    <li><a href="#3D_362" rel="nofollow" data-token="4ec2cff9d5703c653e050ed106e7f72f">3D</a></li>
   </ul>
  </div>
  <br> 语义分割是指对图像中的每一个像素分配一个类别。
  <br> 单独一个像素并没有特殊的含义，必须将其放置在上下文中才能识别像素的类别。
  <br> 我们要增大感受野必然要将feature map 缩小，最后必然要将feature map还原到原图大小。
  <br> 而语言分割核心的问题就是在于
  <strong>大的特征图和大的感受野之间的冲突</strong>。
  <br> 同时，由于要分割的目标具有多尺度的特性，大的物体往往可以很好的分割，但是小的物体不明显，不具有分割的尺度。
  <br> 多尺度是目标检测和分割都存在的问题。
  <p></p> 
  <h1><a id="Encoder_And_Decoder_7"></a>Encoder And Decoder</h1> 
  <p>encoder-decoder是语义分割最基础的网络结构。主要论文如下：</p> 
  <h2><a id="FCN_Fully_Convolutional_Networks_for_Semantic_Segmentation__201605_9"></a>[FCN] Fully Convolutional Networks for Semantic Segmentation 2016-05</h2> 
  <p>在前面的backbone做传统的卷积操作，不断增大感受野的同时，feature map size 不断缩小。为了解决feature map size 变小不能给每个像素分类的问题，进行上采样将feature map放大到原图。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629104446502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 为了提高对细节像素的分类准确度，将encoder不同pooling大小的feature map 按照pixel-wise 与decoder层的pool相加，再做上采样。<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019062911003752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190807014250444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="FCN32_15"></a>FCN-32</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629230806728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="FCN16_17"></a>FCN-16</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629230828389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="FCN8_19"></a>FCN-8</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629230910393.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="code_by_pytorch_21"></a>code by pytorch</h3> 
  <p>使用resnet 作为backbone 实现了fcn-4s如下，参考了torchvision的版本。<br> torchvision实现的是fcn-32s.<br> 下面的变量pool与图中的pool并不是对应的。<br> 使用双线性插值初始化转置卷积参数。</p> 
  <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">FCNHead</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">''' To merge the feature mapping with different scale in the middle with the feature mapping by upsampling need to change channel dimensionality to the same. '''</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        inter_channels <span class="token operator">=</span> in_channels <span class="token operator">//</span> <span class="token number">4</span>
        layers <span class="token operator">=</span> <span class="token punctuation">[</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> inter_channels<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>inter_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>inter_channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span>

        <span class="token builtin">super</span><span class="token punctuation">(</span>FCNHead<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">FCNUpsampling</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">''' '''</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        layers <span class="token operator">=</span> <span class="token punctuation">[</span>
            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>
                               stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span>padding<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>FCNUpsampling<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>
<span class="token keyword">class</span> <span class="token class-name">FCN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> backbone<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> aux_classifier<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>FCN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># Using the modified resNet to get 4 different scales of the tensor,</span>
        <span class="token comment"># in fact, the last three used in the paper,</span>
        <span class="token comment"># first reserved for experiment</span>
        self<span class="token punctuation">.</span>backbone <span class="token operator">=</span> getBackBone<span class="token punctuation">(</span>backbone<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool1_FCNHead <span class="token operator">=</span> FCNHead<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool2_FCNHead <span class="token operator">=</span> FCNHead<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool3_FCNHead <span class="token operator">=</span> FCNHead<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool4_FCNHead <span class="token operator">=</span> FCNHead<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>

        <span class="token comment"># upsampling using transposeConvolution</span>
        <span class="token comment"># out = s(in-1)+d(k-1)+1-2p</span>
        <span class="token comment"># while s = s , d =1, k=2s, p = s/2, we will get out = s*in</span>
        <span class="token comment"># we need to zoom in 32 times by 2 x 2 x 2 x 4</span>
        self<span class="token punctuation">.</span>up_score2 <span class="token operator">=</span> FCNUpsampling<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up_score4 <span class="token operator">=</span> FCNUpsampling<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up_score8 <span class="token operator">=</span> FCNUpsampling<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up_score32 <span class="token operator">=</span> FCNUpsampling<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>aux_classifier <span class="token operator">=</span> aux_classifier

        self<span class="token punctuation">.</span>initial_weight<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        result <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>

        input_shape <span class="token operator">=</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token comment"># pool1 scaling = 1/4 channel = 256</span>
        <span class="token comment"># pool2 scaling = 1/8 channel = 512</span>
        <span class="token comment"># pool3 scaling = 1/16 channel = 1024</span>
        <span class="token comment"># pool4 scaling = 1/32 channel = 2048</span>
        pool1<span class="token punctuation">,</span> pool2<span class="token punctuation">,</span> pool3<span class="token punctuation">,</span> pool4 <span class="token operator">=</span> self<span class="token punctuation">.</span>backbone<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># pool1_same_channel scaling = 1/4 channel = num_classes</span>
        <span class="token comment"># pool2_same_channel scaling = 1/8 channel = num_classes</span>
        <span class="token comment"># pool3_same_channel scaling = 1/16 channel = num_classes</span>
        <span class="token comment"># pool4_same_channel scaling = 1/32 channel = num_classes</span>
        pool1_same_channel <span class="token operator">=</span> self<span class="token punctuation">.</span>pool1_FCNHead<span class="token punctuation">(</span>pool1<span class="token punctuation">)</span>
        pool2_same_channel <span class="token operator">=</span> self<span class="token punctuation">.</span>pool2_FCNHead<span class="token punctuation">(</span>pool2<span class="token punctuation">)</span>
        pool3_same_channel <span class="token operator">=</span> self<span class="token punctuation">.</span>pool3_FCNHead<span class="token punctuation">(</span>pool3<span class="token punctuation">)</span>
        pool4_same_channel <span class="token operator">=</span> self<span class="token punctuation">.</span>pool4_FCNHead<span class="token punctuation">(</span>pool4<span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>aux_classifier <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            result<span class="token punctuation">[</span><span class="token string">"aux"</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>up_score32<span class="token punctuation">(</span>pool4_same_channel<span class="token punctuation">)</span>

        <span class="token comment"># merge x and pool3 scaling = 1/16</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up_score2<span class="token punctuation">(</span>pool4_same_channel<span class="token punctuation">)</span> <span class="token operator">+</span> pool3_same_channel

        <span class="token comment"># merge x and pool2 scaling = 1/8</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up_score2<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">+</span> pool2_same_channel

        <span class="token comment"># merge x and pool2 scaling = 1/4</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>up_score2<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">+</span> pool1_same_channel

        <span class="token comment"># scaling = 1</span>
        result<span class="token punctuation">[</span><span class="token string">"out"</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>up_score4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> result

    <span class="token keyword">def</span> <span class="token function">initial_weight</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_out'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'leaky_relu'</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token punctuation">(</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
                m<span class="token punctuation">.</span>weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bilinear_kernel<span class="token punctuation">(</span>m<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span>m<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span>m<span class="token punctuation">.</span>kernel_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">bilinear_kernel</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        factor <span class="token operator">=</span> <span class="token punctuation">(</span>kernel_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>
        <span class="token keyword">if</span> kernel_size <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            center <span class="token operator">=</span> factor <span class="token operator">-</span> <span class="token number">1</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            center <span class="token operator">=</span> factor <span class="token operator">-</span> <span class="token number">0.5</span>
        og <span class="token operator">=</span> np<span class="token punctuation">.</span>ogrid<span class="token punctuation">[</span><span class="token punctuation">:</span>kernel_size<span class="token punctuation">,</span> <span class="token punctuation">:</span>kernel_size<span class="token punctuation">]</span>
        filt <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>og<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> center<span class="token punctuation">)</span> <span class="token operator">/</span> factor<span class="token punctuation">)</span> <span class="token operator">*</span> \
               <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>og<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> center<span class="token punctuation">)</span> <span class="token operator">/</span> factor<span class="token punctuation">)</span>
        weight <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> kernel_size<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span>
        weight<span class="token punctuation">[</span><span class="token builtin">range</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">range</span><span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> filt
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>weight<span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <p>[完整代码] https://github.com/ArronHZG/FCN/</p> 
  </blockquote> 
  <blockquote> 
   <p><strong>上采样</strong> 最近插值 双线性插值</p> 
   <pre><code class="prism language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'nearest'</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>  <span class="token comment"># align_corners=False</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.2500</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">1.5000</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">2.2500</span><span class="token punctuation">,</span>  <span class="token number">2.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">2.5000</span><span class="token punctuation">,</span>  <span class="token number">2.7500</span><span class="token punctuation">,</span>  <span class="token number">3.2500</span><span class="token punctuation">,</span>  <span class="token number">3.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.2500</span><span class="token punctuation">,</span>  <span class="token number">3.7500</span><span class="token punctuation">,</span>  <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.3333</span><span class="token punctuation">,</span>  <span class="token number">1.6667</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">1.6667</span><span class="token punctuation">,</span>  <span class="token number">2.0000</span><span class="token punctuation">,</span>  <span class="token number">2.3333</span><span class="token punctuation">,</span>  <span class="token number">2.6667</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">2.3333</span><span class="token punctuation">,</span>  <span class="token number">2.6667</span><span class="token punctuation">,</span>  <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.3333</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.3333</span><span class="token punctuation">,</span>  <span class="token number">3.6667</span><span class="token punctuation">,</span>  <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># Try scaling the same data in a larger tensor</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> input_3x3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> input_3x3<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>copy_<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> input_3x3
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>  <span class="token comment"># align_corners=False</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># Notice that values in top left corner are the same with the small input (except at boundary)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span>input_3x3<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.2500</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">1.5000</span><span class="token punctuation">,</span>  <span class="token number">0.5000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">1.5000</span><span class="token punctuation">,</span>  <span class="token number">1.7500</span><span class="token punctuation">,</span>  <span class="token number">2.2500</span><span class="token punctuation">,</span>  <span class="token number">1.8750</span><span class="token punctuation">,</span>  <span class="token number">0.6250</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">2.5000</span><span class="token punctuation">,</span>  <span class="token number">2.7500</span><span class="token punctuation">,</span>  <span class="token number">3.2500</span><span class="token punctuation">,</span>  <span class="token number">2.6250</span><span class="token punctuation">,</span>  <span class="token number">0.8750</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">2.2500</span><span class="token punctuation">,</span>  <span class="token number">2.4375</span><span class="token punctuation">,</span>  <span class="token number">2.8125</span><span class="token punctuation">,</span>  <span class="token number">2.2500</span><span class="token punctuation">,</span>  <span class="token number">0.7500</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">0.7500</span><span class="token punctuation">,</span>  <span class="token number">0.8125</span><span class="token punctuation">,</span>  <span class="token number">0.9375</span><span class="token punctuation">,</span>  <span class="token number">0.7500</span><span class="token punctuation">,</span>  <span class="token number">0.2500</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># Notice that values in top left corner are now changed</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> m<span class="token punctuation">(</span>input_3x3<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">1.4000</span><span class="token punctuation">,</span>  <span class="token number">1.8000</span><span class="token punctuation">,</span>  <span class="token number">1.6000</span><span class="token punctuation">,</span>  <span class="token number">0.8000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">1.8000</span><span class="token punctuation">,</span>  <span class="token number">2.2000</span><span class="token punctuation">,</span>  <span class="token number">2.6000</span><span class="token punctuation">,</span>  <span class="token number">2.2400</span><span class="token punctuation">,</span>  <span class="token number">1.1200</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">2.6000</span><span class="token punctuation">,</span>  <span class="token number">3.0000</span><span class="token punctuation">,</span>  <span class="token number">3.4000</span><span class="token punctuation">,</span>  <span class="token number">2.8800</span><span class="token punctuation">,</span>  <span class="token number">1.4400</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">2.4000</span><span class="token punctuation">,</span>  <span class="token number">2.7200</span><span class="token punctuation">,</span>  <span class="token number">3.0400</span><span class="token punctuation">,</span>  <span class="token number">2.5600</span><span class="token punctuation">,</span>  <span class="token number">1.2800</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">1.2000</span><span class="token punctuation">,</span>  <span class="token number">1.3600</span><span class="token punctuation">,</span>  <span class="token number">1.5200</span><span class="token punctuation">,</span>  <span class="token number">1.2800</span><span class="token punctuation">,</span>  <span class="token number">0.6400</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
  </blockquote> 
  <blockquote> 
   <p><strong>参考</strong><br> 【翻译】https://www.cnblogs.com/xuanxufeng/p/6249834.html</p> 
  </blockquote> 
  <h2><a id="UNet_Convolutional_Networks_for_Biomedical_Image_Segmentation_201505_204"></a>[U-Net] Convolutional Networks for Biomedical Image Segmentation 2015-05</h2> 
  <p>U-Net结构与FCN类似。<br> 区别主要在于：</p> 
  <ol> 
   <li>U-Net 使用 <strong>转置卷积</strong> 进行上采样；FCN 使用双线性插值进行上采样。</li> 
   <li>FCN根据放缩的程度有8，16，32的版本，U-Net 每一次缩小feature map都有对应的上采样操作恢复到原来feature map size.<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629231900573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
  </ol> 
  <blockquote> 
   <p><strong>转置卷积</strong><br> 看动图 https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190807014626469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190807014807996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019080701482795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  </blockquote> 
  <h2><a id="SegNet_A_Deep_Convolutional_EncoderDecoder_Architecture_for_Image_Segmentation_201511_217"></a>[SegNet] A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation 2015-11</h2> 
  <p>SegNe类似于U-Net,但是为了减少冗余，通过记录pooling indices 将 encoder的信息输入到decoder之中。<br> 1）提升边缘刻画度；<br> 2）减少训练的参数；<br> 3）这种上采样模式可以包含到任何编码-解码网络中。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629232314191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630141116110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <blockquote> 
   <p><strong>MaxUnpool1d</strong></p> 
   <pre><code class="prism language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool1d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> return_indices<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> unpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxUnpool1d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output<span class="token punctuation">,</span> indices <span class="token operator">=</span> pool<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> unpool<span class="token punctuation">(</span>output<span class="token punctuation">,</span> indices<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># Example showcasing the use of output_size</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output<span class="token punctuation">,</span> indices <span class="token operator">=</span> pool<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> unpool<span class="token punctuation">(</span>output<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token builtin">input</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> unpool<span class="token punctuation">(</span>output<span class="token punctuation">,</span> indices<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
   <p><strong>MaxUnpool2d</strong></p> 
   <pre><code class="prism language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> return_indices<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> unpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxUnpool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                            <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                            <span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                            <span class="token punctuation">[</span><span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> output<span class="token punctuation">,</span> indices <span class="token operator">=</span> pool<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> unpool<span class="token punctuation">(</span>output<span class="token punctuation">,</span> indices<span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">14</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">16</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token comment"># specify a different output size than input size</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> unpool<span class="token punctuation">(</span>output<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> output_size<span class="token operator">=</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">14</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span> <span class="token number">16</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">[</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
  </blockquote> 
  <h1><a id="Pyramid__Multipath_266"></a>Pyramid &amp; Multi-path</h1> 
  <p>使用特征金字塔的方式增强利用空间上下文的能力。</p> 
  <h2><a id="RefineNet_MultiPath_Refinement_Networks_for_HighResolution_Semantic_Segmentation_201611_268"></a>[RefineNet] Multi-Path Refinement Networks for High-Resolution Semantic Segmentation 2016-11</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630141735473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 注意ResNet去除了BN.<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019063014175522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="PSPNet_Pyramid_Scene_Parsing_Network_201612_272"></a>[PSPNet] Pyramid Scene Parsing Network 2016-12</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019062923544590.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="PPM_Pyramid_Pooling_Module_274"></a>[PPM] Pyramid Pooling Module</h3> 
  <h1><a id="Atrous_Convolution_275"></a>Atrous Convolution</h1> 
  <p>为了解决encoder+decoder 损失图片细节精度的问题，使用空洞卷积（Atrous Convolution）解决问题。<br> 以前的CNN主要问题总结：<br>    （1）Up-sampling / pooling layer<br>    （2）内部数据结构丢失；空间层级化信息丢失。<br>    （3）小物体信息无法重建 (假设有四个pooling layer 则 任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。)</p> 
  <h2><a id="DeepLabV2_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolutionand_Fully_Connected_CRFs_281"></a>[DeepLabV2] Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629234448620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629234518975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" width="350" height="350"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629234543526.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" width="350" height="300"></p> 
  <h3><a id="ASPP_atrous_spatial_pyramid_pooling_285"></a>[ASPP] atrous spatial pyramid pooling</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629234601648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" width="400" height="300"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629235025189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" width="400" height="300"></p> 
  <h2><a id="DeepLabV3_Rethinking_Atrous_Convolution_for_Semantic_Image_Segmentation_201706_288"></a>[DeepLabV3] Rethinking Atrous Convolution for Semantic Image Segmentation 2017-06</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629104010980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190629235317772.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="DeepLabV3_EncoderDecoder_with_Atrous_Separable_Convolution_for_Semantic_Image_Segmentation_201802_291"></a>[DeepLabV3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation 2018-02</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630000931181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630000950795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <blockquote> 
   <p><strong>参考</strong><br> [细致讲解] https://blog.csdn.net/u011974639/article/details/79518175<br> [code]</p> 
  </blockquote> 
  <h2><a id="DenseASPP_DenseASPP_for_Semantic_Segmentation_in_Street_Scenes_201901_297"></a>[DenseASPP] DenseASPP for Semantic Segmentation in Street Scenes 2019-01</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630004612165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> DenseASPP 结合了ASPP和denseNet 的优点。<br> ASPP 的结构是一种平行的结构，通过不同的空洞卷积获得不同的感受野，兼顾了宏观和微观的语义信息。<br> 而另一种结构例如Encode 和 Decode 是一种级联结构（cascading) ,天然地可以获得更大的感受野，但是对于物体边缘信息的判别就不是很准。<br> DenseASPP 综合了上述两种结构的优势。在Cityscapes数据集上在cat 取得了90.7的mIOU。</p> 
  <h1><a id="Attention_304"></a>Attention</h1> 
  <p>语义分割的核心是通过像素周围语义信息，去判定当前像素的类别。比如说在某一个区域，检测到是指猫，但是增大感受野之后，发现这只猫是一个人T-shirt上的图案，那么这只猫的所有像素就应该分类为人。</p> 
  <p>上述的方法都是在研究感受野如何增加，可以更好的获取某一个区域的语义。但是可想而知，在深度神经网络，前面感受野较小，后面感受野较大，在信息流通的过程中，浅层的神经网络并不确切第知道某一个区域应该是什么，例如是猫还是人，需要信息逐渐传到后面，才能得以确认。</p> 
  <p>受限于原始卷积操作的结构，感受野必然受限于局部的信息，对于远端的信息融合能力不够，例如一个车和船比较像，在湖中，我们就直接推断这是一个船，在路上我们就推断这是一个车。</p> 
  <p>模型就算足够深，获得了更大的感受野，但是任然不能解决，长距离依赖的问题，而且要考虑到之后上采样，或者转置卷积为原始的图片大小，空间信息不能丢失太多，一般下采样的倍数控制到16或者32倍。</p> 
  <p>注意力机制可以在encoder 的过程中就考虑到长距离的信息，进行信息融合。可以改变上述局部感受野的问题。</p> 
  <blockquote> 
   <p>前置阅读<br> 核心的Attention机制 例如 channel-wise 的SeNet 以及 pixel-wise 的 Non-local Neural Network<br> <a href="https://blog.csdn.net/Arron_hou/article/details/95676716" rel="nofollow" data-token="91fa78fa32dd83a4dc23ca2b8dbbd300">【Attention】注意力机制在图像上的应用</a></p> 
  </blockquote> 
  <h2><a id="Attention_UNet_Learning_Where_to_Look_for_the_Pancreas_MIDL_2018_318"></a>[Attention U-Net] Learning Where to Look for the Pancreas（ MIDL 2018）</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630003450313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630003520779.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="PAN_Pyramid_Attention_Network_for_Semantic_Segmentation_BMVC_2018_322"></a>[PAN] Pyramid Attention Network for Semantic Segmentation (BMVC 2018)</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190712233218423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190712233243508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190712233258675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="PSANet_Pointwise_Spatial_Attention_Network_for_Scene_Parsing_ECCV_2018_329"></a>[PSANet] Point-wise Spatial Attention Network for Scene Parsing (ECCV 2018)</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190712233515511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190712233529553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190712233551941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="EncNet_Context_Encoding_for_Semantic_Segmentation_CVPR_2018_335"></a>[EncNet] Context Encoding for Semantic Segmentation (CVPR 2018)</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190712233707193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="DANet_Dual_Attention_Network_for_Scene_Segmentation_201809_CVPR_2019_337"></a>[DANet] Dual Attention Network for Scene Segmentation 2018-09 （CVPR 2019）</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630003725965.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630003743615.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" width="400" height="400"></p> 
  <h1><a id="Faster_340"></a>Faster</h1> 
  <p>为了提高模型速度，使用<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mn>
          1
         </mn>
         <mo>
          ×
         </mo>
         <mn>
          1
         </mn>
        </mrow>
        <annotation encoding="application/x-tex">
         1\times1
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>卷积进行channel reduce。提高模型速度。</p> 
  <h2><a id="ESPNet_Efficient_Spatial_Pyramid_of_Dilated_Convolutions_for_Semantic_Segmentation_201803_342"></a>[ESPNet] Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation 2018-03</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630002836254.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630002943321.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019063000300452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="FastSCNN_Fast_Semantic_Segmentation_Network_201902_349"></a>[Fast-SCNN] Fast Semantic Segmentation Network 2019-02</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190630003127558.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Fycm9uX2hvdQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="BiSeNet_Bilateral_Segmentation_Network_for_Realtime_Semantic_Segmentation_201808_353"></a>[BiSeNet] Bilateral Segmentation Network for Real-time Semantic Segmentation 2018-08</h2> 
  <h2><a id="ShelfNet_ShelfNet_for_Realtime_Semantic_Segmentation_201811_355"></a>[ShelfNet] ShelfNet for Real-time Semantic Segmentation 2018-11</h2> 
  <h1><a id="Semisupervised_357"></a>Semi-supervised</h1> 
  <h2><a id="WeaklySupervised_Semantic_Segmentation_by_Iteratively_Mining_Common_Object_Features_CVPR18_358"></a>Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features CVPR18</h2> 
  <h2><a id="WeaklySupervised_Semantic_Segmentation_Network_With_Deep_Seeded_Region_Growing_CVPR18_359"></a>Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing CVPR18</h2> 
  <h1><a id="NAS_360"></a>NAS</h1> 
  <h2><a id="AutoDeepLab_Hierarchical_Neural_Architecture_Search_for_Semantic_Image_Segmentation_CVPR2019_361"></a>[Auto-DeepLab] Hierarchical Neural Architecture Search for Semantic Image Segmentation (CVPR2019)</h2> 
  <h1><a id="3D_362"></a>3D</h1> 
  <p>未完待续。。。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
