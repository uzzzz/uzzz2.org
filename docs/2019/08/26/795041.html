<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>python机器学习-特征工程（三） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="python机器学习-特征工程（三）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="https://www.cnblogs.com/jasonfreak/p/5448385.html 三、特征提取 字典加载特征：DictVectorizer 文本特征提取：词频向量（CountVectorizer）TF-IDF向量（TfidfVectorizer，FfidfTransformer） 特征哈希向量（HashingVectorizer） 图像特征的提取：提取像素矩阵边缘和兴趣点 3.1、字典加载特征 用python中的字典存储特征是一种常用的做法，其优点是容易理解，但是sklearn的输入特征必须是numpy或Scipy的数组。可以用DictVectorizer从字典加载特征转化成numpy，并且对分类特征会采用独热编码。 me=[ {&#39;city&#39;:&#39;Dubai&#39;,&#39;temperature&#39;:33.}, {&#39;city&#39;:&#39;London&#39;,&#39;temperature&#39;:12.}, {&#39;city&#39;:&#39;San Francisco&#39;,&#39;temperature&#39;:18.} ] from sklearn.feature_extraction import DictVectorizer vec=DictVectorizer() print(vec.fit_transform(me).toarray()) vec.get_feature_names() [[ 1. 0. 0. 33.] [ 0. 1. 0. 12.] [ 0. 0. 1. 18.]] 3.2、字频向量 词库模型（Bag-of-words model）是文字模型化最常用的方法，它为每个单词设值一个特征值，依据是用类似单词的文章意思也差不多 CountVectorizer类会将文档全部转化成小写，然后把句子分割成块或有意义的字母序列，并统计他们出现的次数 可以使用stop_words选项排除一些常用的但没有意义的助词。 from sklearn.feature_extraction.text import CountVectorizer co=[ &#39;UNC played Duke in basketball&#39;, &#39;Duke lost the basketball game ,game over&#39;, &#39;I ate a sandwich&#39; ] vec=CountVectorizer(stop_words=&#39;english&#39;) print(vec.fit_transform(co).todense()) print(vec.vocabulary_) # 三行数据 [[0 1 1 0 0 1 0 1] [0 1 1 2 1 0 0 0] [1 0 0 0 0 0 1 0]] {&#39;unc&#39;: 7, &#39;played&#39;: 5, &#39;duke&#39;: 2, &#39;basketball&#39;: 1, &#39;lost&#39;: 4, &#39;game&#39;: 3, &#39;ate&#39;: 0, &#39;sandwich&#39;: 6} import jieba from sklearn.feature_extraction.text import CountVectorizer corpus=[ &#39;朋友，小红是我的&#39;, &#39;小明对小红说：“小红，我们还是不是朋友”&#39;, &#39;小明与小红是朋友&#39; ] cutcorpus=[&quot;/&quot;.join(jieba.cut(x)) for x in corpus] vec==CountVectorizer(stop_words=[&#39;好的&#39;,&#39;是的&#39;]) counts=vec.fit_transform(cutcorpus).todense() print(counts) # 查看映射字典 print(vec.vocabulary_) 可以用词频向量的欧式距离（L2范数）来衡量两个文档之间的距离（距离越小越相似) from sklearn.feature_extraction.text import CountVectorizer # 计算欧式距离 from sklearn.metrics.pairwise import euclidean_distances vectorizer=CountVectorizer() for x,y in [[0,1],[0,2],[1,2]]: dist=euclidean_distances(counts[x],counts[y]) print(&#39;文档{}与文档{}的距离{}&#39;.format(x,y,dist)) 3.3、Tf-idf权重向量 from sklearn.feature_extraction.text import TfidfTransformer transformer=TfidfTransformer(smooth_idf=False) counts=[[3,0,1], [2,0,0], [3,0,0], [4,0,0], [3,2,0], [3,0,2]] tfidf=transformer.fit_transform(counts) tfidf.toarray() array([[0.81940995, 0. , 0.57320793], [1. , 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ], [0.47330339, 0.88089948, 0. ], [0.58149261, 0. , 0.81355169]]) from sklearn.feature_extraction.text import TfidfVectorizer vectorizer=TfidfVectorizer() vectorizer.fit_transform(cutcorpus).toarray() vectorizer.vocabulary_ {&#39;小明&#39;: 0, &#39;小红&#39;: 1, &#39;我们&#39;: 2, &#39;是不是&#39;: 3, &#39;朋友&#39;: 4} 3.4、特征哈希值 词袋模型的方法很好用，也很直接，但在有些场景下很难使用，比如分词后的词汇字典表非常大， 达到100万+,此时如果直接使用词频向量或Tf-idf权重向量的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们该怎么办呢？ 我们可以应用哈希技巧进行降维。 Hash函数可以将一个任意长度的字符串映射到_个固定长度的散列数字中去。Hash函数是一种典 型的多对一映射。 正向快速：给定明文和hash算法，在有限时间和有限资源内能计算出hash值。 逆向困难：给定（若干）hash值，在有限时间内很难（基本不可能）逆推出明文。 输入敏感：原始输入信息修改一点信息，产生的hash值看起来应该都有很大不同。 碰撞避免：很难找到两段内容不同的明文，使得它们的hash值一致（发生碰撞）。即对于任意两个不同的数据块，其hash值相同的可能性极小；对于一个给定的数据块，找到和它hash值相同的数据块极为困难。 目前流行的Hash函数包括MD4，MD5，SHA等。 from sklearn.feature_extraction.text import HashingVectorizer corpus=[&#39;smart boy&#39;,&#39;ate&#39;,&#39;bacon&#39;,&#39;a cat&#39;] # HashingVectorizeras是无状态的，不需要fit vectorizer=HashingVectorizer(n_features=6,stop_words=&#39;english&#39;) print(vectorizer.transform(corpus).todense()) [[-0.70710678 -0.70710678 0. 0. 0. 0. ] [ 0. 0. 0. 1. 0. 0. ] [ 0. 0. 0. 0. -1. 0. ] [ 0. 1. 0. 0. 0. 0. ]] from sklearn.feature_extraction.text import HashingVectorizer corpus=[ &#39;UNC played Duke in basketball&#39;, &#39;Duke lost the basketball game ,game over&#39;, &#39;I ate a sandwich&#39; ] vectorizer=HashingVectorizer(n_features=6) counts=vectorizer.transform(corpus).todense() print(counts) counts.shape [[ 0. 0. -0.89442719 0. 0. -0.4472136 ] [-0.37796447 -0.75592895 -0.37796447 0. 0. -0.37796447] [ 0. 0. 0.70710678 0.70710678 0. 0. ]] Out[9]:(3, 6) 四、特征选择 https://www.cnblogs.com/jasonfreak/p/5448385.html 当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来 说，从两个方面考虑来选择特征： 特征是否发散：如果一个特征不发散，例如方差接近于0 ,也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： 1、Filter:过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 2、Wrapper:包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 3、Embedded :嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 4.1、Filter过滤法 4.1.1、方差选择法 使用方差选择法，先要计算各个特征的方差，然后根据阙值，选择方差大于阙值的特征。（用的不是很多） from sklearn.feature_selection import VarianceThreshold **方差选择法，返回值为特征选择后的数据** **参数thresshold为方差的阙值，方差大于3（threshold=3）** vardata=VarianceThreshold(threshold=3).fit_transform(iris.data) vardata.shape (150, 1) 4.1.2、相关系数法 使用相关系数，先要计算各个特征对目标值的相关系数。用feature_selection库的SelectKBest类结合相关系数来选择 from sklearn.feature_selection import SelectKBest from scipy.stats import pearsonr import numpy as np **选择K个最好的特征，返回选择特征后的数据** **第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量** **输出二元（评分，P值）的数组，数组第i项为第i个特征的评分和P值** f=lambda X ,Y:np.array(list(map(lambda x:pearsonr(x,Y)[0],X.T))).T SelectKBest(f,k=2).fit_transform(iris.data,iris.target) 4.1.3、卡方检验 from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 SelectKBest(chi2,k=2).fit_transform(iris.data,iris.target) 4.1.4、互信息法 经典的互信息法也是评价定性自变量对定性因变量的相关性的。相关系数，卡方检验，互信息选择原理相似，但相关系数通常只适用于连续特征选择 import numpy as np from sklearn.feature_selection import SelectKBest from sklearn import metrics mic=metrics.mutual_info_score g=lambda X ,Y:np.array(list(map(lambda x:mic(x,Y),X.T))).T SelectKBest(g,k=2).fit_transform(iris.data,iris.target) 4.2、Wrapper 递归特征消除法（RFE） 递归消除特征法使用一个基膜型进行多伦训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练 from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression **递归特征消除法，返回特征选择后的数据** **参数estimator为基膜型** **n_features_to_select=2特征个数** RFE(estimator=LogisticRegression(),n_features_to_select=2).fit_transform(iris.data,iris.target) 4.3、Embedded嵌入法 https://blog.csdn.net/jinping_shi/article/details/52433975 使用带惩罚项的基模型，除了筛选出特征外同时也进行了降维。使用feature_selection库的SelectFormModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下 from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression # L1正则化的回归模型 SelectFromModel(LogisticRegression(penalty=&#39;l1&#39;,C=0.1)).fit_transform(iris.data,iris.target) L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1 ,选择在L2 中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：" />
<meta property="og:description" content="https://www.cnblogs.com/jasonfreak/p/5448385.html 三、特征提取 字典加载特征：DictVectorizer 文本特征提取：词频向量（CountVectorizer）TF-IDF向量（TfidfVectorizer，FfidfTransformer） 特征哈希向量（HashingVectorizer） 图像特征的提取：提取像素矩阵边缘和兴趣点 3.1、字典加载特征 用python中的字典存储特征是一种常用的做法，其优点是容易理解，但是sklearn的输入特征必须是numpy或Scipy的数组。可以用DictVectorizer从字典加载特征转化成numpy，并且对分类特征会采用独热编码。 me=[ {&#39;city&#39;:&#39;Dubai&#39;,&#39;temperature&#39;:33.}, {&#39;city&#39;:&#39;London&#39;,&#39;temperature&#39;:12.}, {&#39;city&#39;:&#39;San Francisco&#39;,&#39;temperature&#39;:18.} ] from sklearn.feature_extraction import DictVectorizer vec=DictVectorizer() print(vec.fit_transform(me).toarray()) vec.get_feature_names() [[ 1. 0. 0. 33.] [ 0. 1. 0. 12.] [ 0. 0. 1. 18.]] 3.2、字频向量 词库模型（Bag-of-words model）是文字模型化最常用的方法，它为每个单词设值一个特征值，依据是用类似单词的文章意思也差不多 CountVectorizer类会将文档全部转化成小写，然后把句子分割成块或有意义的字母序列，并统计他们出现的次数 可以使用stop_words选项排除一些常用的但没有意义的助词。 from sklearn.feature_extraction.text import CountVectorizer co=[ &#39;UNC played Duke in basketball&#39;, &#39;Duke lost the basketball game ,game over&#39;, &#39;I ate a sandwich&#39; ] vec=CountVectorizer(stop_words=&#39;english&#39;) print(vec.fit_transform(co).todense()) print(vec.vocabulary_) # 三行数据 [[0 1 1 0 0 1 0 1] [0 1 1 2 1 0 0 0] [1 0 0 0 0 0 1 0]] {&#39;unc&#39;: 7, &#39;played&#39;: 5, &#39;duke&#39;: 2, &#39;basketball&#39;: 1, &#39;lost&#39;: 4, &#39;game&#39;: 3, &#39;ate&#39;: 0, &#39;sandwich&#39;: 6} import jieba from sklearn.feature_extraction.text import CountVectorizer corpus=[ &#39;朋友，小红是我的&#39;, &#39;小明对小红说：“小红，我们还是不是朋友”&#39;, &#39;小明与小红是朋友&#39; ] cutcorpus=[&quot;/&quot;.join(jieba.cut(x)) for x in corpus] vec==CountVectorizer(stop_words=[&#39;好的&#39;,&#39;是的&#39;]) counts=vec.fit_transform(cutcorpus).todense() print(counts) # 查看映射字典 print(vec.vocabulary_) 可以用词频向量的欧式距离（L2范数）来衡量两个文档之间的距离（距离越小越相似) from sklearn.feature_extraction.text import CountVectorizer # 计算欧式距离 from sklearn.metrics.pairwise import euclidean_distances vectorizer=CountVectorizer() for x,y in [[0,1],[0,2],[1,2]]: dist=euclidean_distances(counts[x],counts[y]) print(&#39;文档{}与文档{}的距离{}&#39;.format(x,y,dist)) 3.3、Tf-idf权重向量 from sklearn.feature_extraction.text import TfidfTransformer transformer=TfidfTransformer(smooth_idf=False) counts=[[3,0,1], [2,0,0], [3,0,0], [4,0,0], [3,2,0], [3,0,2]] tfidf=transformer.fit_transform(counts) tfidf.toarray() array([[0.81940995, 0. , 0.57320793], [1. , 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ], [0.47330339, 0.88089948, 0. ], [0.58149261, 0. , 0.81355169]]) from sklearn.feature_extraction.text import TfidfVectorizer vectorizer=TfidfVectorizer() vectorizer.fit_transform(cutcorpus).toarray() vectorizer.vocabulary_ {&#39;小明&#39;: 0, &#39;小红&#39;: 1, &#39;我们&#39;: 2, &#39;是不是&#39;: 3, &#39;朋友&#39;: 4} 3.4、特征哈希值 词袋模型的方法很好用，也很直接，但在有些场景下很难使用，比如分词后的词汇字典表非常大， 达到100万+,此时如果直接使用词频向量或Tf-idf权重向量的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们该怎么办呢？ 我们可以应用哈希技巧进行降维。 Hash函数可以将一个任意长度的字符串映射到_个固定长度的散列数字中去。Hash函数是一种典 型的多对一映射。 正向快速：给定明文和hash算法，在有限时间和有限资源内能计算出hash值。 逆向困难：给定（若干）hash值，在有限时间内很难（基本不可能）逆推出明文。 输入敏感：原始输入信息修改一点信息，产生的hash值看起来应该都有很大不同。 碰撞避免：很难找到两段内容不同的明文，使得它们的hash值一致（发生碰撞）。即对于任意两个不同的数据块，其hash值相同的可能性极小；对于一个给定的数据块，找到和它hash值相同的数据块极为困难。 目前流行的Hash函数包括MD4，MD5，SHA等。 from sklearn.feature_extraction.text import HashingVectorizer corpus=[&#39;smart boy&#39;,&#39;ate&#39;,&#39;bacon&#39;,&#39;a cat&#39;] # HashingVectorizeras是无状态的，不需要fit vectorizer=HashingVectorizer(n_features=6,stop_words=&#39;english&#39;) print(vectorizer.transform(corpus).todense()) [[-0.70710678 -0.70710678 0. 0. 0. 0. ] [ 0. 0. 0. 1. 0. 0. ] [ 0. 0. 0. 0. -1. 0. ] [ 0. 1. 0. 0. 0. 0. ]] from sklearn.feature_extraction.text import HashingVectorizer corpus=[ &#39;UNC played Duke in basketball&#39;, &#39;Duke lost the basketball game ,game over&#39;, &#39;I ate a sandwich&#39; ] vectorizer=HashingVectorizer(n_features=6) counts=vectorizer.transform(corpus).todense() print(counts) counts.shape [[ 0. 0. -0.89442719 0. 0. -0.4472136 ] [-0.37796447 -0.75592895 -0.37796447 0. 0. -0.37796447] [ 0. 0. 0.70710678 0.70710678 0. 0. ]] Out[9]:(3, 6) 四、特征选择 https://www.cnblogs.com/jasonfreak/p/5448385.html 当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来 说，从两个方面考虑来选择特征： 特征是否发散：如果一个特征不发散，例如方差接近于0 ,也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： 1、Filter:过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 2、Wrapper:包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 3、Embedded :嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 4.1、Filter过滤法 4.1.1、方差选择法 使用方差选择法，先要计算各个特征的方差，然后根据阙值，选择方差大于阙值的特征。（用的不是很多） from sklearn.feature_selection import VarianceThreshold **方差选择法，返回值为特征选择后的数据** **参数thresshold为方差的阙值，方差大于3（threshold=3）** vardata=VarianceThreshold(threshold=3).fit_transform(iris.data) vardata.shape (150, 1) 4.1.2、相关系数法 使用相关系数，先要计算各个特征对目标值的相关系数。用feature_selection库的SelectKBest类结合相关系数来选择 from sklearn.feature_selection import SelectKBest from scipy.stats import pearsonr import numpy as np **选择K个最好的特征，返回选择特征后的数据** **第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量** **输出二元（评分，P值）的数组，数组第i项为第i个特征的评分和P值** f=lambda X ,Y:np.array(list(map(lambda x:pearsonr(x,Y)[0],X.T))).T SelectKBest(f,k=2).fit_transform(iris.data,iris.target) 4.1.3、卡方检验 from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 SelectKBest(chi2,k=2).fit_transform(iris.data,iris.target) 4.1.4、互信息法 经典的互信息法也是评价定性自变量对定性因变量的相关性的。相关系数，卡方检验，互信息选择原理相似，但相关系数通常只适用于连续特征选择 import numpy as np from sklearn.feature_selection import SelectKBest from sklearn import metrics mic=metrics.mutual_info_score g=lambda X ,Y:np.array(list(map(lambda x:mic(x,Y),X.T))).T SelectKBest(g,k=2).fit_transform(iris.data,iris.target) 4.2、Wrapper 递归特征消除法（RFE） 递归消除特征法使用一个基膜型进行多伦训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练 from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression **递归特征消除法，返回特征选择后的数据** **参数estimator为基膜型** **n_features_to_select=2特征个数** RFE(estimator=LogisticRegression(),n_features_to_select=2).fit_transform(iris.data,iris.target) 4.3、Embedded嵌入法 https://blog.csdn.net/jinping_shi/article/details/52433975 使用带惩罚项的基模型，除了筛选出特征外同时也进行了降维。使用feature_selection库的SelectFormModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下 from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression # L1正则化的回归模型 SelectFromModel(LogisticRegression(penalty=&#39;l1&#39;,C=0.1)).fit_transform(iris.data,iris.target) L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1 ,选择在L2 中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：" />
<link rel="canonical" href="https://uzzz.org/2019/08/26/795041.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/26/795041.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-26T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"https://www.cnblogs.com/jasonfreak/p/5448385.html 三、特征提取 字典加载特征：DictVectorizer 文本特征提取：词频向量（CountVectorizer）TF-IDF向量（TfidfVectorizer，FfidfTransformer） 特征哈希向量（HashingVectorizer） 图像特征的提取：提取像素矩阵边缘和兴趣点 3.1、字典加载特征 用python中的字典存储特征是一种常用的做法，其优点是容易理解，但是sklearn的输入特征必须是numpy或Scipy的数组。可以用DictVectorizer从字典加载特征转化成numpy，并且对分类特征会采用独热编码。 me=[ {&#39;city&#39;:&#39;Dubai&#39;,&#39;temperature&#39;:33.}, {&#39;city&#39;:&#39;London&#39;,&#39;temperature&#39;:12.}, {&#39;city&#39;:&#39;San Francisco&#39;,&#39;temperature&#39;:18.} ] from sklearn.feature_extraction import DictVectorizer vec=DictVectorizer() print(vec.fit_transform(me).toarray()) vec.get_feature_names() [[ 1. 0. 0. 33.] [ 0. 1. 0. 12.] [ 0. 0. 1. 18.]] 3.2、字频向量 词库模型（Bag-of-words model）是文字模型化最常用的方法，它为每个单词设值一个特征值，依据是用类似单词的文章意思也差不多 CountVectorizer类会将文档全部转化成小写，然后把句子分割成块或有意义的字母序列，并统计他们出现的次数 可以使用stop_words选项排除一些常用的但没有意义的助词。 from sklearn.feature_extraction.text import CountVectorizer co=[ &#39;UNC played Duke in basketball&#39;, &#39;Duke lost the basketball game ,game over&#39;, &#39;I ate a sandwich&#39; ] vec=CountVectorizer(stop_words=&#39;english&#39;) print(vec.fit_transform(co).todense()) print(vec.vocabulary_) # 三行数据 [[0 1 1 0 0 1 0 1] [0 1 1 2 1 0 0 0] [1 0 0 0 0 0 1 0]] {&#39;unc&#39;: 7, &#39;played&#39;: 5, &#39;duke&#39;: 2, &#39;basketball&#39;: 1, &#39;lost&#39;: 4, &#39;game&#39;: 3, &#39;ate&#39;: 0, &#39;sandwich&#39;: 6} import jieba from sklearn.feature_extraction.text import CountVectorizer corpus=[ &#39;朋友，小红是我的&#39;, &#39;小明对小红说：“小红，我们还是不是朋友”&#39;, &#39;小明与小红是朋友&#39; ] cutcorpus=[&quot;/&quot;.join(jieba.cut(x)) for x in corpus] vec==CountVectorizer(stop_words=[&#39;好的&#39;,&#39;是的&#39;]) counts=vec.fit_transform(cutcorpus).todense() print(counts) # 查看映射字典 print(vec.vocabulary_) 可以用词频向量的欧式距离（L2范数）来衡量两个文档之间的距离（距离越小越相似) from sklearn.feature_extraction.text import CountVectorizer # 计算欧式距离 from sklearn.metrics.pairwise import euclidean_distances vectorizer=CountVectorizer() for x,y in [[0,1],[0,2],[1,2]]: dist=euclidean_distances(counts[x],counts[y]) print(&#39;文档{}与文档{}的距离{}&#39;.format(x,y,dist)) 3.3、Tf-idf权重向量 from sklearn.feature_extraction.text import TfidfTransformer transformer=TfidfTransformer(smooth_idf=False) counts=[[3,0,1], [2,0,0], [3,0,0], [4,0,0], [3,2,0], [3,0,2]] tfidf=transformer.fit_transform(counts) tfidf.toarray() array([[0.81940995, 0. , 0.57320793], [1. , 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ], [0.47330339, 0.88089948, 0. ], [0.58149261, 0. , 0.81355169]]) from sklearn.feature_extraction.text import TfidfVectorizer vectorizer=TfidfVectorizer() vectorizer.fit_transform(cutcorpus).toarray() vectorizer.vocabulary_ {&#39;小明&#39;: 0, &#39;小红&#39;: 1, &#39;我们&#39;: 2, &#39;是不是&#39;: 3, &#39;朋友&#39;: 4} 3.4、特征哈希值 词袋模型的方法很好用，也很直接，但在有些场景下很难使用，比如分词后的词汇字典表非常大， 达到100万+,此时如果直接使用词频向量或Tf-idf权重向量的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们该怎么办呢？ 我们可以应用哈希技巧进行降维。 Hash函数可以将一个任意长度的字符串映射到_个固定长度的散列数字中去。Hash函数是一种典 型的多对一映射。 正向快速：给定明文和hash算法，在有限时间和有限资源内能计算出hash值。 逆向困难：给定（若干）hash值，在有限时间内很难（基本不可能）逆推出明文。 输入敏感：原始输入信息修改一点信息，产生的hash值看起来应该都有很大不同。 碰撞避免：很难找到两段内容不同的明文，使得它们的hash值一致（发生碰撞）。即对于任意两个不同的数据块，其hash值相同的可能性极小；对于一个给定的数据块，找到和它hash值相同的数据块极为困难。 目前流行的Hash函数包括MD4，MD5，SHA等。 from sklearn.feature_extraction.text import HashingVectorizer corpus=[&#39;smart boy&#39;,&#39;ate&#39;,&#39;bacon&#39;,&#39;a cat&#39;] # HashingVectorizeras是无状态的，不需要fit vectorizer=HashingVectorizer(n_features=6,stop_words=&#39;english&#39;) print(vectorizer.transform(corpus).todense()) [[-0.70710678 -0.70710678 0. 0. 0. 0. ] [ 0. 0. 0. 1. 0. 0. ] [ 0. 0. 0. 0. -1. 0. ] [ 0. 1. 0. 0. 0. 0. ]] from sklearn.feature_extraction.text import HashingVectorizer corpus=[ &#39;UNC played Duke in basketball&#39;, &#39;Duke lost the basketball game ,game over&#39;, &#39;I ate a sandwich&#39; ] vectorizer=HashingVectorizer(n_features=6) counts=vectorizer.transform(corpus).todense() print(counts) counts.shape [[ 0. 0. -0.89442719 0. 0. -0.4472136 ] [-0.37796447 -0.75592895 -0.37796447 0. 0. -0.37796447] [ 0. 0. 0.70710678 0.70710678 0. 0. ]] Out[9]:(3, 6) 四、特征选择 https://www.cnblogs.com/jasonfreak/p/5448385.html 当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来 说，从两个方面考虑来选择特征： 特征是否发散：如果一个特征不发散，例如方差接近于0 ,也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： 1、Filter:过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 2、Wrapper:包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 3、Embedded :嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 4.1、Filter过滤法 4.1.1、方差选择法 使用方差选择法，先要计算各个特征的方差，然后根据阙值，选择方差大于阙值的特征。（用的不是很多） from sklearn.feature_selection import VarianceThreshold **方差选择法，返回值为特征选择后的数据** **参数thresshold为方差的阙值，方差大于3（threshold=3）** vardata=VarianceThreshold(threshold=3).fit_transform(iris.data) vardata.shape (150, 1) 4.1.2、相关系数法 使用相关系数，先要计算各个特征对目标值的相关系数。用feature_selection库的SelectKBest类结合相关系数来选择 from sklearn.feature_selection import SelectKBest from scipy.stats import pearsonr import numpy as np **选择K个最好的特征，返回选择特征后的数据** **第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量** **输出二元（评分，P值）的数组，数组第i项为第i个特征的评分和P值** f=lambda X ,Y:np.array(list(map(lambda x:pearsonr(x,Y)[0],X.T))).T SelectKBest(f,k=2).fit_transform(iris.data,iris.target) 4.1.3、卡方检验 from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 SelectKBest(chi2,k=2).fit_transform(iris.data,iris.target) 4.1.4、互信息法 经典的互信息法也是评价定性自变量对定性因变量的相关性的。相关系数，卡方检验，互信息选择原理相似，但相关系数通常只适用于连续特征选择 import numpy as np from sklearn.feature_selection import SelectKBest from sklearn import metrics mic=metrics.mutual_info_score g=lambda X ,Y:np.array(list(map(lambda x:mic(x,Y),X.T))).T SelectKBest(g,k=2).fit_transform(iris.data,iris.target) 4.2、Wrapper 递归特征消除法（RFE） 递归消除特征法使用一个基膜型进行多伦训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练 from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression **递归特征消除法，返回特征选择后的数据** **参数estimator为基膜型** **n_features_to_select=2特征个数** RFE(estimator=LogisticRegression(),n_features_to_select=2).fit_transform(iris.data,iris.target) 4.3、Embedded嵌入法 https://blog.csdn.net/jinping_shi/article/details/52433975 使用带惩罚项的基模型，除了筛选出特征外同时也进行了降维。使用feature_selection库的SelectFormModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下 from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression # L1正则化的回归模型 SelectFromModel(LogisticRegression(penalty=&#39;l1&#39;,C=0.1)).fit_transform(iris.data,iris.target) L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1 ,选择在L2 中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：","@type":"BlogPosting","url":"https://uzzz.org/2019/08/26/795041.html","headline":"python机器学习-特征工程（三）","dateModified":"2019-08-26T00:00:00+08:00","datePublished":"2019-08-26T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/26/795041.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>python机器学习-特征工程（三）</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>https://www.cnblogs.com/jasonfreak/p/5448385.html</p> 
  <h1><a id="_1"></a>三、特征提取</h1> 
  <ul> 
   <li>字典加载特征：DictVectorizer</li> 
   <li>文本特征提取：词频向量（CountVectorizer）TF-IDF向量（TfidfVectorizer，FfidfTransformer） 特征哈希向量（HashingVectorizer）</li> 
   <li>图像特征的提取：提取像素矩阵边缘和兴趣点</li> 
  </ul> 
  <h2><a id="31_7"></a>3.1、字典加载特征</h2> 
  <p>用python中的字典存储特征是一种常用的做法，其优点是容易理解，但是sklearn的输入特征必须是numpy或Scipy的数组。可以用DictVectorizer从字典加载特征转化成numpy，并且对分类特征会采用独热编码。</p> 
  <pre><code class="prism language-python">me<span class="token operator">=</span><span class="token punctuation">[</span>
    <span class="token punctuation">{</span><span class="token string">'city'</span><span class="token punctuation">:</span><span class="token string">'Dubai'</span><span class="token punctuation">,</span><span class="token string">'temperature'</span><span class="token punctuation">:</span><span class="token number">33</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">'city'</span><span class="token punctuation">:</span><span class="token string">'London'</span><span class="token punctuation">,</span><span class="token string">'temperature'</span><span class="token punctuation">:</span><span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">'city'</span><span class="token punctuation">:</span><span class="token string">'San Francisco'</span><span class="token punctuation">,</span><span class="token string">'temperature'</span><span class="token punctuation">:</span><span class="token number">18</span><span class="token punctuation">.</span><span class="token punctuation">}</span>
<span class="token punctuation">]</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction <span class="token keyword">import</span> DictVectorizer
vec<span class="token operator">=</span>DictVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vec<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>me<span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
vec<span class="token punctuation">.</span>get_feature_names<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span>  <span class="token number">0</span><span class="token punctuation">.</span>  <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">33</span><span class="token punctuation">.</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>  <span class="token number">1</span><span class="token punctuation">.</span>  <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>  <span class="token number">0</span><span class="token punctuation">.</span>  <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">18</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

</code></pre> 
  <h2><a id="32_25"></a>3.2、字频向量</h2> 
  <p>词库模型（Bag-of-words model）是文字模型化最常用的方法，它为每个单词设值一个特征值，依据是用类似单词的文章意思也差不多</p> 
  <p>CountVectorizer类会将文档全部转化成小写，然后把句子分割成块或有意义的字母序列，并统计他们出现的次数<br> 可以使用stop_words选项排除一些常用的但没有意义的助词。</p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer
co<span class="token operator">=</span><span class="token punctuation">[</span>
    <span class="token string">'UNC played Duke in basketball'</span><span class="token punctuation">,</span>
    <span class="token string">'Duke lost the basketball game ,game over'</span><span class="token punctuation">,</span>
    <span class="token string">'I ate a sandwich'</span>
<span class="token punctuation">]</span>
vec<span class="token operator">=</span>CountVectorizer<span class="token punctuation">(</span>stop_words<span class="token operator">=</span><span class="token string">'english'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vec<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>co<span class="token punctuation">)</span><span class="token punctuation">.</span>todense<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vec<span class="token punctuation">.</span>vocabulary_<span class="token punctuation">)</span>

<span class="token comment"># 三行数据</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">1</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">0</span> <span class="token number">1</span> <span class="token number">1</span> <span class="token number">2</span> <span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">0</span> <span class="token number">1</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">{</span><span class="token string">'unc'</span><span class="token punctuation">:</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token string">'played'</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token string">'duke'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'basketball'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'lost'</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token string">'game'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'ate'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'sandwich'</span><span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">}</span>

</code></pre> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> jieba
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer
corpus<span class="token operator">=</span><span class="token punctuation">[</span>
    <span class="token string">'朋友，小红是我的'</span><span class="token punctuation">,</span>
    <span class="token string">'小明对小红说：“小红，我们还是不是朋友”'</span><span class="token punctuation">,</span>
    <span class="token string">'小明与小红是朋友'</span>
<span class="token punctuation">]</span>
cutcorpus<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"/"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> corpus<span class="token punctuation">]</span>
vec<span class="token operator">==</span>CountVectorizer<span class="token punctuation">(</span>stop_words<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'好的'</span><span class="token punctuation">,</span><span class="token string">'是的'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
counts<span class="token operator">=</span>vec<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>cutcorpus<span class="token punctuation">)</span><span class="token punctuation">.</span>todense<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>counts<span class="token punctuation">)</span>
<span class="token comment"># 查看映射字典</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vec<span class="token punctuation">.</span>vocabulary_<span class="token punctuation">)</span>
</code></pre> 
  <p><strong>可以用词频向量的欧式距离（L2范数）来衡量两个文档之间的距离（距离越小越相似)</strong><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731160129231.png" alt="在这里插入图片描述"></p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer
<span class="token comment"># 计算欧式距离</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>pairwise <span class="token keyword">import</span> euclidean_distances
vectorizer<span class="token operator">=</span>CountVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> x<span class="token punctuation">,</span>y <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    dist<span class="token operator">=</span>euclidean_distances<span class="token punctuation">(</span>counts<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span>counts<span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'文档{}与文档{}的距离{}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>dist<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
  <h2><a id="33Tfidf_77"></a>3.3、Tf-idf权重向量</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731160213495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjc0OTczNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfTransformer
transformer<span class="token operator">=</span>TfidfTransformer<span class="token punctuation">(</span>smooth_idf<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
counts<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
tfidf<span class="token operator">=</span>transformer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>counts<span class="token punctuation">)</span>
tfidf<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>


array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.81940995</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token number">0.57320793</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">0.47330339</span><span class="token punctuation">,</span> <span class="token number">0.88089948</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">]</span><span class="token punctuation">,</span>
       <span class="token punctuation">[</span><span class="token number">0.58149261</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">,</span> <span class="token number">0.81355169</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfVectorizer
vectorizer<span class="token operator">=</span>TfidfVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
vectorizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>cutcorpus<span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>
vectorizer<span class="token punctuation">.</span>vocabulary_ 


<span class="token punctuation">{</span><span class="token string">'小明'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'小红'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'我们'</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'是不是'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'朋友'</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">}</span>

</code></pre> 
  <h2><a id="34_109"></a>3.4、特征哈希值</h2> 
  <p>词袋模型的方法很好用，也很直接，但在有些场景下很难使用，比如分词后的词汇字典表非常大， 达到100万+,此时如果直接使用词频向量或Tf-idf权重向量的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们该怎么办呢？</p> 
  <p>我们可以应用哈希技巧进行降维。</p> 
  <p>Hash函数可以将一个任意长度的字符串映射到_个固定长度的散列数字中去。Hash函数是一种典 型的多对一映射。</p> 
  <ul> 
   <li>正向快速：给定明文和hash算法，在有限时间和有限资源内能计算出hash值。</li> 
   <li>逆向困难：给定（若干）hash值，在有限时间内很难（基本不可能）逆推出明文。</li> 
   <li>输入敏感：原始输入信息修改一点信息，产生的hash值看起来应该都有很大不同。</li> 
   <li>碰撞避免：很难找到两段内容不同的明文，使得它们的hash值一致（发生碰撞）。即对于任意两个不同的数据块，其hash值相同的可能性极小；对于一个给定的数据块，找到和它hash值相同的数据块极为困难。<br> 目前流行的Hash函数包括MD4，MD5，SHA等。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731161026985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjc0OTczNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190731161047503.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjc0OTczNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
  </ul> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> HashingVectorizer
corpus<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'smart boy'</span><span class="token punctuation">,</span><span class="token string">'ate'</span><span class="token punctuation">,</span><span class="token string">'bacon'</span><span class="token punctuation">,</span><span class="token string">'a cat'</span><span class="token punctuation">]</span>

<span class="token comment"># HashingVectorizeras是无状态的，不需要fit</span>

vectorizer<span class="token operator">=</span>HashingVectorizer<span class="token punctuation">(</span>n_features<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>stop_words<span class="token operator">=</span><span class="token string">'english'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vectorizer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token punctuation">.</span>todense<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.70710678</span> <span class="token operator">-</span><span class="token number">0.70710678</span>  <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">]</span>
 <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">1</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">]</span>
 <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>         <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">]</span>
 <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">1</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> HashingVectorizer

corpus<span class="token operator">=</span><span class="token punctuation">[</span>
    <span class="token string">'UNC played Duke in basketball'</span><span class="token punctuation">,</span>
    <span class="token string">'Duke lost the basketball game ,game over'</span><span class="token punctuation">,</span>
    <span class="token string">'I ate a sandwich'</span>
    <span class="token punctuation">]</span>
vectorizer<span class="token operator">=</span>HashingVectorizer<span class="token punctuation">(</span>n_features<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span>
counts<span class="token operator">=</span>vectorizer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token punctuation">.</span>todense<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>counts<span class="token punctuation">)</span>
counts<span class="token punctuation">.</span>shape

<span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>         <span class="token operator">-</span><span class="token number">0.89442719</span>  <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>         <span class="token operator">-</span><span class="token number">0.4472136</span> <span class="token punctuation">]</span>
 <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.37796447</span> <span class="token operator">-</span><span class="token number">0.75592895</span> <span class="token operator">-</span><span class="token number">0.37796447</span>  <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>         <span class="token operator">-</span><span class="token number">0.37796447</span><span class="token punctuation">]</span>
 <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0.70710678</span>  <span class="token number">0.70710678</span>  <span class="token number">0</span><span class="token punctuation">.</span>          <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token punctuation">]</span><span class="token punctuation">]</span>

Out<span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span>
</code></pre> 
  <h1><a id="_158"></a>四、特征选择</h1> 
  <p>https://www.cnblogs.com/jasonfreak/p/5448385.html<br> 当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来 说，从两个方面考虑来选择特征：</p> 
  <ul> 
   <li>特征是否发散：如果一个特征不发散，例如方差接近于0 ,也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。</li> 
   <li>特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。</li> 
  </ul> 
  <p>根据特征选择的形式又可以将特征选择方法分为3种：</p> 
  <p>1、Filter:过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。<br> 2、Wrapper:包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。<br> 3、Embedded :嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</p> 
  <h2><a id="41Filter_174"></a>4.1、Filter过滤法</h2> 
  <h3><a id="411_175"></a>4.1.1、方差选择法</h3> 
  <p>使用方差选择法，先要计算各个特征的方差，然后根据阙值，选择方差大于阙值的特征。（用的不是很多）</p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> VarianceThreshold

<span class="token operator">**</span>方差选择法，返回值为特征选择后的数据<span class="token operator">**</span>
<span class="token operator">**</span>参数thresshold为方差的阙值，方差大于<span class="token number">3</span>（threshold<span class="token operator">=</span><span class="token number">3</span>）<span class="token operator">**</span>

vardata<span class="token operator">=</span>VarianceThreshold<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
vardata<span class="token punctuation">.</span>shape

<span class="token punctuation">(</span><span class="token number">150</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

</code></pre> 
  <h2><a id="412_189"></a>4.1.2、相关系数法</h2> 
  <p>使用相关系数，先要计算各个特征对目标值的相关系数。用feature_selection库的SelectKBest类结合相关系数来选择</p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectKBest
<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> pearsonr
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token operator">**</span>选择K个最好的特征，返回选择特征后的数据<span class="token operator">**</span>
<span class="token operator">**</span>第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量<span class="token operator">**</span>
<span class="token operator">**</span>输出二元（评分，P值）的数组，数组第i项为第i个特征的评分和P值<span class="token operator">**</span>

f<span class="token operator">=</span><span class="token keyword">lambda</span> X <span class="token punctuation">,</span>Y<span class="token punctuation">:</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>pearsonr<span class="token punctuation">(</span>x<span class="token punctuation">,</span>Y<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T
SelectKBest<span class="token punctuation">(</span>f<span class="token punctuation">,</span>k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span>
</code></pre> 
  <h3><a id="413_205"></a>4.1.3、卡方检验</h3> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190826144345495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjc0OTczNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectKBest
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> chi2

SelectKBest<span class="token punctuation">(</span>chi2<span class="token punctuation">,</span>k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span>

</code></pre> 
  <h3><a id="414_215"></a>4.1.4、互信息法</h3> 
  <p>经典的互信息法也是评价定性自变量对定性因变量的相关性的。相关系数，卡方检验，互信息选择原理相似，但相关系数通常只适用于连续特征选择</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectKBest
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

mic<span class="token operator">=</span>metrics<span class="token punctuation">.</span>mutual_info_score

g<span class="token operator">=</span><span class="token keyword">lambda</span> X <span class="token punctuation">,</span>Y<span class="token punctuation">:</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>mic<span class="token punctuation">(</span>x<span class="token punctuation">,</span>Y<span class="token punctuation">)</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T

SelectKBest<span class="token punctuation">(</span>g<span class="token punctuation">,</span>k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span>
</code></pre> 
  <h2><a id="42Wrapper_229"></a>4.2、Wrapper</h2> 
  <p>递归特征消除法（RFE）<br> 递归消除特征法使用一个基膜型进行多伦训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练</p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> RFE
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression

<span class="token operator">**</span>递归特征消除法，返回特征选择后的数据<span class="token operator">**</span>
<span class="token operator">**</span>参数estimator为基膜型<span class="token operator">**</span>
<span class="token operator">**</span>n_features_to_select<span class="token operator">=</span><span class="token number">2</span>特征个数<span class="token operator">**</span>

RFE<span class="token punctuation">(</span>estimator<span class="token operator">=</span>LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_features_to_select<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span>

</code></pre> 
  <h2><a id="43Embedded_243"></a>4.3、Embedded嵌入法</h2> 
  <p>https://blog.csdn.net/jinping_shi/article/details/52433975</p> 
  <p>使用带惩罚项的基模型，除了筛选出特征外同时也进行了降维。使用feature_selection库的SelectFormModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下</p> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectFromModel
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression

<span class="token comment"># L1正则化的回归模型</span>

SelectFromModel<span class="token punctuation">(</span>LogisticRegression<span class="token punctuation">(</span>penalty<span class="token operator">=</span><span class="token string">'l1'</span><span class="token punctuation">,</span>C<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span>

</code></pre> 
  <p>L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1 ,选择在L2 中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
