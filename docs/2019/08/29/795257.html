<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Machine Learning Note Phase 1ï¼ˆ Doneï¼ï¼‰ | æœ‰ç»„ç»‡åœ¨ï¼</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Machine Learning Note Phase 1ï¼ˆ Doneï¼ï¼‰" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Machine Learning è¿™æ˜¯ç¬¬ä¸€ä»½æœºå™¨å­¦ä¹ ç¬”è®°ï¼Œåˆ›å»ºäº2019å¹´7æœˆ26æ—¥ï¼Œå®Œæˆäº2019å¹´8æœˆ2æ—¥ã€‚ è¯¥ç¬”è®°åŒ…æ‹¬å¦‚ä¸‹éƒ¨åˆ†: å¼•è¨€(Introduction) å•å˜é‡çº¿æ€§å›å½’(Linear Regression with One Variable) çº¿æ€§ä»£æ•°(Linear Algebra) å¤šå˜é‡çº¿æ€§å›å½’(Linear Regression with Multiple Variables) Octave é€»è¾‘å›å½’(Logistic Regression) æ­£åˆ™åŒ–(Regularization) Part 1 ï¼šIntroduction Machine Learningï¼š Idea: ç»™äºˆæœºå™¨è‡ªå·±å­¦ä¹ è§£å†³é—®é¢˜çš„èƒ½åŠ› å…ˆå¾—è‡ªå·±ææ˜ç™½äº†æ‰æœ‰å¯èƒ½æ•™ä¼šæœºå™¨è‡ªå·±å­¦ä¹ å‘ã€‚ã€‚ Machine learning algorithms: Supervised learning Unsupervised learning Othersï¼š Recommender systems(æ¨èç³»ç»Ÿ) ; Reinforcement learning(å¼ºåŒ–å­¦ä¹ ) Supervised learning(ç›‘ç£å­¦ä¹ ) Use to do : Make predictions Featureï¼š Right answers set is GIVEN Including: Regression problems(å›å½’é—®é¢˜) æˆ‘ä»¬ç»™å‡ºæ­£ç¡®çš„æ•°æ®é›†ï¼ŒæœŸæœ›ç®—æ³•èƒ½å¤Ÿé¢„æµ‹å‡ºcontinuous values(è¿ç»­å€¼)ã€‚ Classification problems(åˆ†ç±»é—®é¢˜) æˆ‘ä»¬ç»™å‡ºæ­£ç¡®çš„æ•°æ®é›†ï¼ŒæœŸæœ›ç®—æ³•èƒ½å¤Ÿé¢„æµ‹å‡ºdiscrete valued(ç¦»æ•£å€¼) ã€‚ ï¼ˆthe features can be more than 2 , Even infinite.ï¼‰ Unsupervised learningï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ Featureï¼š We just give the data and hope it can automatically find structure in the data for us. Includingï¼š Clustering algorithmsï¼ˆèšç±»ç®—æ³•ï¼‰ Problemsï¼š In fact I find the unsupervised learning looks like the classification problems . The only difference is whether or not the data set is GIVENGive the correct data ahead of time . Summaryï¼š We just give the Disorderly data sets , and hope the algorithm can automatically find out the structure of the data , and divide it into different clusters wisely. Octave Usageï¼š ç”¨octaveå»ºç«‹ç®—æ³•çš„åŸå‹ï¼Œå¦‚æœç®—æ³•æˆç«‹ï¼Œå†è¿ç§»åˆ°å…¶ä»–çš„ç¼–ç¨‹ç¯å¢ƒä¸­æ¥ï¼Œå¦‚æ­¤æ‰èƒ½ä½¿æ•ˆç‡æ›´é«˜ã€‚ Part 2 ï¼šå•å˜é‡çº¿æ€§å›å½’(Linear Regression with One Variable) ç®—æ³•å®ç°è¿‡ç¨‹ï¼š Methodï¼š Training set ïƒ  Learning algorithm ïƒ  hypothesis function(å‡è®¾å‡½æ•°) ïƒ  Find the appropriate form to represent the hypothesis function - ç®€å•æ¥è¯´å°±æ˜¯æˆ‘ä»¬ç»™å‡ºTraining Set ï¼Œ ç”¨å®ƒæ¥è®­ç»ƒæˆ‘ä»¬çš„Learning Algorithm ï¼Œæœ€ç»ˆæœŸæœ›èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªå‡½æ•° hï¼ˆthetaï¼‰, è¿™ä¹Ÿå°±æ˜¯é¢„æµ‹çš„æ¨¡å‹ï¼Œé€šè¿‡è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®ç°è¾“å…¥å˜é‡é›†ï¼Œç„¶åå°±èƒ½å¤Ÿè¿”å›å¯¹åº”çš„é¢„æµ‹å€¼ã€‚ Cost function(ä»£ä»·å‡½æ•°) å…³äºhï¼ˆxï¼‰å‡½æ•° å¯¹äºæˆ¿ä»·é—®é¢˜ï¼Œæˆ‘ä»¬å†³å®šä½¿ç”¨Linear function æ¥æ‹Ÿåˆæ›²çº¿ ï¼Œè®¾ hï¼ˆxï¼‰= thetaï¼ˆ0ï¼‰+thetaï¼ˆ1ï¼‰* x å› ä¸ºæˆ‘ä»¬æœ€åæƒ³è¦å¾—åˆ°çš„å°±æ˜¯ hï¼ˆxï¼‰ï¼Œå¯¹äº x æ¥è¯´ï¼Œå®ƒæ˜¯å§‹ç»ˆå˜åŒ–çš„ï¼Œæ¯ä¸€æ¡æ•°æ®å¯¹åº”ç€ä¸åŒçš„å˜é‡é›†ï¼Œæ— æ³•å›ºå®šï¼›æ‰€ä»¥algorithmæƒ³è¦ç¡®å®šçš„å°±æ˜¯ä¸å˜çš„é‡ thetaé›† äº†ã€‚åœ¨ä»¥åçš„æ“ä½œä¸­ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è¦æ±‚å¾—çš„æ˜¯ thetaé›†ï¼Œæ‰€ä»¥æ›´å¤šçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šæŠŠ x çœ‹ä½œæ˜¯å¸¸é‡ï¼Œå´æŠŠ theta çœ‹ä½œæ˜¯å˜é‡ï¼Œæ‰€ä»¥ä¸€å®šä¸è¦ç³Šæ¶‚äº†ã€‚ å»ºæ¨¡è¯¯å·®ï¼ˆmodeling errorï¼‰ æˆ‘ä»¬é€‰æ‹©çš„å‚æ•°å†³å®šäº†æˆ‘ä»¬å¾—åˆ°çš„ç›´çº¿ç›¸å¯¹äºæˆ‘ä»¬çš„è®­ç»ƒé›†çš„å‡†ç¡®ç¨‹åº¦ï¼Œæ¨¡å‹æ‰€é¢„æµ‹çš„å€¼ä¸è®­ç»ƒé›†ä¸­å®é™…å€¼ä¹‹é—´çš„å·®è·å°±æ˜¯å»ºæ¨¡è¯¯å·®ï¼ˆmodeling errorï¼‰ã€‚ Descriptionï¼š Cost function sometimes is also called squared error function(å¹³æ–¹è¯¯å·®å‡½æ•°) Cost function ç”¨äºæè¿°å»ºæ¨¡è¯¯å·®çš„å¤§å°ã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œè¯¯å·®è¶Šå°ï¼ŒCost function å°±èƒ½å¤Ÿæ›´å¥½çš„ä¸æ•°æ®é›†ç›¸æ‹Ÿåˆï¼Œä½†è¿™å¹¶ä¸æ˜¯ç»å¯¹çš„ï¼Œå› ä¸ºè¿™æœ‰å¯èƒ½ä¼šäº§ç”Ÿè¿‡æ‹Ÿåˆé—®é¢˜ã€‚ æŠŠå®ƒè¿›ä¸€æ­¥çš„æ‹†å¼€æ¥çœ‹å°±åº”è¯¥æ˜¯è¿™æ ·å­çš„ï¼šJï¼ˆ theta(0),theta(1) ) = 1/2m * Sigma [1-m] {thetaï¼ˆ0ï¼‰+thetaï¼ˆ1ï¼‰* x(i) - y(i)}(å¯èƒ½å°±æˆ‘ä¸€äººèƒ½çœ‹æ‡‚ã€‚ã€‚) å¹³æ–¹è¯¯å·®å‡½æ•°åªæ˜¯ Cost function ä¸­çš„ä¸€ç§å½¢å¼ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ä¹‹æ‰€ä»¥é€‰æ‹©squared error function ï¼Œ æ˜¯å› ä¸ºå¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å›å½’é—®é¢˜ï¼Œå®ƒæ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚è¿˜æœ‰å…¶ä»–çš„ä»£ä»·å‡½æ•°ä¹Ÿèƒ½å¾ˆå¥½åœ°å‘æŒ¥ä½œç”¨ï¼Œä½†æ˜¯å¹³æ–¹è¯¯å·®ä»£ä»·å‡½æ•°å¯èƒ½æ˜¯è§£å†³å›å½’é—®é¢˜æœ€å¸¸ç”¨çš„æ‰‹æ®µäº†ã€‚ Gradient descent(æ¢¯åº¦ä¸‹é™) algorithm Ideaï¼š æˆ‘ä»¬å·²ç»çŸ¥é“äº†ï¼Œ**hï¼ˆthetaï¼‰**æ˜¯ç”¨æ¥æ‹Ÿåˆæ•°æ®é›†çš„å‡½æ•°ï¼Œå¦‚æœæ‹Ÿåˆçš„å¥½ï¼Œé‚£ä¹ˆé¢„æµ‹çš„ç»“æœä¸ä¼šå·®ã€‚Jï¼ˆthetaé›†ï¼‰è¢«ç§°ä¸ºä»£ä»·å‡½æ•°ï¼Œå¦‚æœä»£ä»·å‡½æ•°è¾ƒå°ï¼Œåˆ™è¯´æ˜æ‹Ÿåˆçš„å¥½ã€‚è€Œæ­¤æ—¶çš„thetaé›†å°±æ˜¯æˆ‘ä»¬å°±æ˜¯é—®é¢˜çš„è§£äº†ã€‚ æˆ‘ä»¬å·²ç»çŸ¥é“äº†æˆ‘ä»¬çš„ç›®æ ‡thetaé›†ï¼Œé‚£ä¹ˆå¦‚ä½•æ±‚å¾—thetaé›†å‘¢ï¼Ÿ Gradient descentï¼ Usage: Start with some para0,para1â€¦ Keep changing paras to reduce J (para0,para1) to find some local minimum Start with different parameters , we may get different result ï¼ˆæ‰€ä»¥ï¼Œæˆ‘ä»¬ä¿è¯J(thetaé›†)ä¸å„ä¸ªthetaæ„æˆçš„å‡½æ•°æ˜¯ä¸€ä¸ªå‡¸å‡½æ•°ï¼Œæ¢å¥è¯è¯´ä¿è¯å­˜åœ¨å”¯ä¸€çš„æå°å€¼=æå°å€¼ï¼‰ Until we hopefully end up at a minimum Formulaï¼š æ‰¹é‡æ¢¯åº¦ä¸‹é™å…¬å¼ &lt;a&gt; is learning rate which control how big a step we use to upgrade the paras. So if the &lt;a&gt; is too small , the algorithm will run slowly ; but if &lt;a&gt; is too big , it can even make it away from the minimum point . å¦å¤–ï¼Œæ¯æ¬¡thetaä¸‹é™çš„å¹…åº¦éƒ½æ˜¯ä¸ä¸€æ ·çš„ï¼Œè¿™æ˜¯å› ä¸ºå®é™…ä¸Šthetaæ¯æ¬¡ä¸‹é™å¤šå°‘æ˜¯ç”± ã€&lt;a&gt;*Derivative Termã€‘è¿™ä¸ªæ•´ä½“å†³å®šçš„ï¼Œ&lt;a&gt;æ˜¯ä¸‹é™å¹…åº¦çš„é‡è¦å†³å®šå› å­ï¼Œä½†å¹¶ä¸æ˜¯å…¨éƒ¨ã€‚ åœ¨æ¯æ¬¡ä¸‹é™ä»¥åï¼ŒDerivative Terméƒ½ä¼šå˜å°ï¼Œä¸‹ä¸€æ¬¡ä¸‹é™çš„å¹…åº¦ä¹Ÿä¼šå˜å°ï¼Œå¦‚æ­¤ï¼Œæ§åˆ¶æ¢¯åº¦æ…¢æ…¢çš„ä¸‹é™ï¼Œç›´åˆ°Derivative Term=0ã€‚ the rest of the formula is the Derivative Term(å¯¼æ•°é¡¹) of theta In this image,we can find that no matter the Derivative Term of theta is positive or negative , the Gradient descent algorithm works well the Details of the formulaï¼š Attention: With the changing of the paras,the derivative term is changing too. So: Make sure updating the paras at the same time.ï¼ˆsimultaneous updateåŒæ­¥æ›´æ–°ï¼‰ For whatï¼Ÿ Because the derivative should use the former para rather than the new. How: åœ¨å…¨éƒ¨thetaæ²¡æœ‰æ›´æ–°å®Œæˆä¹‹å‰ï¼Œæˆ‘ä»¬ç”¨tempé›†æ¥æš‚æ—¶å­˜å‚¨å·²ç»æ›´æ–°å®Œæˆçš„thetaé›†ï¼Œç›´åˆ°æœ€åä¸€ä¸ªthetaä¹Ÿæ›´æ–°å®Œæˆä¹‹åï¼Œæˆ‘ä»¬æ‰å†ç”¨å­˜å‚¨åœ¨tempé›†é‡Œçš„æ–°thetaé›†æ›´æ–°åŸthetaé›†ã€‚ Gradient descent for linear regression â€œBatchâ€ Gradient descent ï¼š Each step of gradient descent uses all the training examples . åœ¨æ¢¯åº¦ä¸‹é™å…¬å¼ä¸­å­˜åœ¨æŸä¸€é¡¹éœ€è¦å¯¹æ‰€æœ‰mä¸ªè®­ç»ƒæ ·æœ¬æ±‚å’Œã€‚ Ideaï¼š å°†æ¢¯åº¦ä¸‹é™ä¸ä»£ä»·å‡½æ•°ç›¸ç»“åˆ The result: This is the result after derivation. And we can find that comparing with the first formula , the second has a x term at the end . It does tell us something that the variable is para0 and para1 , not the x or y . Although they looks more like variables. After derivation , for the first formula we have nothing left , for the second , we just has a constant term(å¸¸æ•°é¡¹) x left . Finallyï¼š We have other ways to computing Jï¼ˆthetaï¼‰ï¼š æ­£è§„æ–¹ç¨‹(normal equations)ï¼Œå®ƒå¯ä»¥åœ¨ä¸éœ€è¦å¤šæ­¥æ¢¯åº¦ä¸‹é™çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½è§£å‡ºä»£ä»·å‡½æ•°çš„æœ€å°å€¼ã€‚ å®é™…ä¸Šåœ¨æ•°æ®é‡è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œæ¢¯åº¦ä¸‹é™æ³•æ¯”æ­£è§„æ–¹ç¨‹è¦æ›´é€‚ç”¨ä¸€äº›ã€‚ Part 3 ï¼šçº¿æ€§ä»£æ•°(Linear Algebra) çŸ©é˜µå’Œå‘é‡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼æ¥ç»„ç»‡å¤§é‡çš„æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å½“æˆ‘ä»¬å¤„ç†å·¨å¤§çš„è®­ç»ƒé›†æ—¶ï¼Œå®ƒçš„ä¼˜åŠ¿å°±æ›´åŠ çš„æ˜æ˜¾ã€‚ Matrices and vectors (çŸ©é˜µå’Œå‘é‡) Matrix: Rectangular array of numbers. äºŒç»´æ•°ç»„ The dimension(ç»´åº¦) of the matrix : number of row * number of columns Matrix Elements : How to refer to the element Vector: An n*1 matrix . ( n dimensional vector. nç»´å‘é‡) How to refer to the element in it ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190726084319572.png) We use 1-indexed vector Cvention: We use upper case to refer to the matrix and lower case refer to the vector. Addition and scalar multiplication (åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•) è¡Œåˆ—æ•°ç›¸ç­‰æ‰èƒ½è¿›è¡ŒåŠ æ³•è¿ç®—ï¼š æ ‡é‡ä¸çŸ©é˜µçš„ä¹˜æ³•ï¼š Matrix-vector multiplication(çŸ©é˜µ-å‘é‡ä¹˜æ³•) Detailsï¼š Aâ€™s row number must be equal with Bâ€™s column number . Matrix-matrix multiplication(çŸ©é˜µ-çŸ©é˜µä¹˜æ³•) Detailsï¼š Just as matrix-vector , the most important thing is to make sure Aâ€™s columns number is equal with Bâ€™s rows number . Câ€™s columns number is equal with Bâ€™s . n fact , before multiply between Matrixs , we can divide B into several vectors by columns, and then we can make the Matrix- Vector multiplication. A column of B corresponding a hypothesis , and a column of C corresponding a set of prediction . Matrix multiplication properties(çŸ©é˜µä¹˜æ³•ç‰¹å¾) Properties : Do not enjoy commutative property(ä¸éµå¾ªäº¤æ¢å¾‹) Enjoy the associative property(éµå¾ªç»“åˆå¾‹) Identity Matrix(å•ä½çŸ©é˜µ): ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190726084424308.png) Featureï¼š å®ƒæ˜¯ä¸ªæ–¹é˜µï¼Œä»å·¦ä¸Šè§’åˆ°å³ä¸‹è§’çš„å¯¹è§’çº¿ï¼ˆç§°ä¸ºä¸»å¯¹è§’çº¿ï¼‰ä¸Šçš„å…ƒç´ å‡ä¸º1ã€‚é™¤æ­¤ä»¥å¤–å…¨éƒ½ä¸º0ã€‚å®ƒçš„ä½œç”¨å°±åƒæ˜¯ä¹˜æ³•ä¸­çš„1ã€‚ Usageï¼š is the identity matrix . A(m , n) * I(n , n) = I(m , m) * A(m , n) M1â€™s column number should be equal with M2â€™s row number . Matrix inverse operation(çŸ©é˜µçš„é€†è¿ç®—) Attentionï¼š Only square matrix have inverse . Not all number have inverse , such as 0 . Matrix transpose operation(çŸ©é˜µçš„è½¬ç½®è¿ç®—) Definition: Official definition: å³å°†Aä¸Bçš„x , yåæ ‡äº’æ¢ çŸ©é˜µè½¬ç½®çš„åŸºæœ¬æ€§è´¨ï¼š Part 4ï¼šå¤šå˜é‡çº¿æ€§å›å½’(Linear Regression with Multiple Variables) åœ¨ä¹‹å‰çš„éƒ¨åˆ†ä¸­ï¼Œå­¦ä¹ äº†å•å˜é‡/ç‰¹å¾çš„å›å½’æ¨¡å‹ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†è¿›è¡Œæ·±å…¥ï¼Œå°†å˜é‡çš„æ•°é‡å¢åŠ ï¼Œè®¨è®ºå¤šå˜é‡çš„çº¿æ€§å›å½’é—®é¢˜ã€‚ Multiple feature(å¤šåŠŸèƒ½) Exampleï¼š å¯¹äºæ–°é—®é¢˜çš„æ³¨é‡Šï¼š So the final prediction result is a vector too . å¯ä»¥å‘ç°ï¼Œå¤šå˜é‡çº¿æ€§å›å½’å’Œå•å˜é‡çº¿æ€§å›å½’åŸºæœ¬æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„å˜åŒ–å°±æ˜¯thetaå˜æˆäº†thetaé›†ï¼›å˜é‡xå˜æˆäº†xé›†ã€‚ Gradient descent for multiple variables(å¤šå…ƒæ¢¯åº¦ä¸‹é™æ³•) Ideaï¼š ä¸å•å˜é‡çº¿æ€§å›å½’ç±»ä¼¼ï¼Œåœ¨å¤šå˜é‡çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ„å»ºä¸€ä¸ªä»£ä»·å‡½æ•°ï¼Œåˆ™è¿™ä¸ªä»£ä»·å‡½æ•°æ˜¯æ‰€æœ‰å»ºæ¨¡è¯¯å·®çš„å¹³æ–¹å’Œ ä»£ä»·å‡½æ•°ï¼š æ¢¯åº¦ä¸‹é™æ³•åº”ç”¨åˆ°ä»£ä»·å‡½æ•°å¹¶æ±‚å¯¼ä¹‹åå¾—åˆ°ï¼š Feature scaling(ç‰¹å¾ç¼©æ”¾): Purpose: Make the gradient descent runs faster . å°¤å…¶æ˜¯å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚ Idea: Make sure number are in similar scale . Usageï¼š The range limit ï¼š Get every feature range from -3 to 3 or -1/3 to 1/3 . Thatâ€™s fine . In fact there is no problems , because the feature is a real number ! Thatâ€™s means it will never change . x æ˜¯å˜é‡ï¼Œå¹¶ä¸”å®ƒæ˜¯å·²çŸ¥çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä»data set ä¸­ç›´æ¥è·å¾—ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å®ƒè¿›è¡Œç‰¹å¾ç¼©æ”¾ï¼›thetaæ˜¯æœªçŸ¥çš„ï¼Œæ˜¯æˆ‘ä»¬è¦æ±‚çš„ç›®æ ‡ã€‚ Summaryï¼š ç‰¹å¾ç¼©æ”¾ç¼©æ”¾çš„æ˜¯ å·²çŸ¥çš„ å˜é‡x ï¼Œè€Œä¸æ˜¯ æœªçŸ¥çš„ å¸¸é‡theta ã€‚ è¿›è¡Œç‰¹å¾ç¼©æ”¾æ˜¯ä¸ºäº†ä¸ºäº†æé«˜æ¢¯åº¦ä¸‹é™çš„é€Ÿåº¦ã€‚ Learning rate (å­¦ä¹ ç‡) Ideaï¼š If the gradient descent is working properly , then J should decrease after every iteration. æ¢¯åº¦ä¸‹é™ç®—æ³•æ”¶æ•›æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°æ ¹æ®æ¨¡å‹çš„ä¸åŒè€Œä¸åŒï¼Œæˆ‘ä»¬ä¸èƒ½æå‰é¢„çŸ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è¿­ä»£æ¬¡æ•°å’Œä»£ä»·å‡½æ•°çš„å›¾è¡¨æ¥è§‚æµ‹ç®—æ³•åœ¨ä½•æ—¶è¶‹äºæ”¶æ•›ã€‚ Conventionï¼š é€šå¸¸å¯ä»¥è€ƒè™‘çš„å­¦ä¹ ç‡&lt;a&gt;çš„æ•°å€¼ï¼š 0.01ï¼›0.03ï¼›0.1ï¼›0.3ï¼›1ï¼›3ï¼›10ï¼› Summary: If &lt;a&gt; is too small ïƒ  slow move If &lt;a&gt; is too big ïƒ  J may not decrease on every iteration . Features and Polynomial regression(å¤šå˜é‡å’Œå¤šé¡¹å¼å›å½’) Ideaï¼š Linear regression å¹¶ä¸é€‚åº”æ‰€æœ‰çš„æ•°æ®æ¨¡å‹ï¼Œæœ‰æ—¶æˆ‘ä»¬éœ€è¦æ›²çº¿æ¥é€‚åº”æˆ‘ä»¬çš„æ•°æ®ï¼Œæ¯”å¦‚ä¸€ä¸ªäºŒæ¬¡æ–¹æ¨¡å‹ï¼š é€šå¸¸æˆ‘ä»¬éœ€è¦å…ˆè§‚å¯Ÿæ•°æ®å†ç¡®å®šæœ€ç»ˆçš„æ¨¡å‹ Attentionï¼š å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚ Normal equation(å¸¸è§„æ–¹ç¨‹) Ideaï¼š ä»æ•°å­¦ä¸Šæ¥çœ‹ï¼Œæƒ³è¦æ±‚å¾—Jï¼ˆtheta) çš„æœ€å°å€¼ï¼Œæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯å¯¹å®ƒè¿›è¡Œæ±‚å¯¼ï¼Œä»¤å®ƒçš„å¯¼æ•°=0ï¼Œå¾—å‡ºæ­¤æ—¶çš„thetaå°±æ˜¯æœ€ç»ˆçš„ç»“æœ è€Œæ­£è§„æ–¹ç¨‹æ–¹æ³•çš„ç¡®å°±æ˜¯è¿™ä¹ˆå¹²çš„ Attentionï¼š å¯¹äºé‚£äº›ä¸å¯é€†çš„çŸ©é˜µï¼ˆé€šå¸¸æ˜¯å› ä¸ºç‰¹å¾ä¹‹é—´ä¸ç‹¬ç«‹ï¼Œå¦‚åŒæ—¶åŒ…å«è‹±å°ºä¸ºå•ä½çš„å°ºå¯¸å’Œç±³ä¸ºå•ä½çš„å°ºå¯¸ä¸¤ä¸ªç‰¹å¾ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ç‰¹å¾æ•°é‡å¤§äºè®­ç»ƒé›†çš„æ•°é‡ï¼‰ï¼Œæ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯ä¸èƒ½ç”¨çš„ã€‚ è¿™å°±æ˜¯æ­£è§„æ–¹ç¨‹çš„å±€é™æ€§ Usage: Exampleï¼š The Comparison of Gradient descent and Normal equation Gradient descent : Need to choose Need many iterations Normal equation : Need to compute [xâ€™*x]^(-1) , So if n is large it will run slowly . Do not fit every situations (åªé€‚ç”¨äºçº¿æ€§æ¨¡å‹ï¼Œä¸é€‚åˆé€»è¾‘å›å½’æ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹). Summary: åªè¦ç‰¹å¾å˜é‡çš„æ•°ç›®å¹¶ä¸å¤§ï¼Œæ ‡å‡†æ–¹ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è®¡ç®—å‚æ•°çš„æ›¿ä»£æ–¹æ³•ã€‚å…·ä½“åœ°è¯´ï¼Œåªè¦ç‰¹å¾å˜é‡æ•°é‡å°äºä¸€ä¸‡ï¼Œæˆ‘é€šå¸¸ä½¿ç”¨æ ‡å‡†æ–¹ç¨‹æ³•ï¼Œè€Œä¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€‚ Normal equation ä¸å¯é€†æƒ…å†µ Ideaï¼š ç¬¬ä¸€è¦é™ä½featuresä¹‹é—´çš„ç›¸å…³åº¦ ç¬¬äºŒè¦é€‚å½“çš„å‡å°‘features çš„æ•°é‡ Ideaï¼š Part 5ï¼šOctaveæ•™ç¨‹(Octave Tutorial) è¿™é‡Œæ˜¯ä¸€ä¸ªğŸ”—â€“&gt; OctaveåŸºæœ¬è¯­æ³• Part 6ï¼šé€»è¾‘å›å½’(Logistic Regression) Vectorizationï¼ˆçŸ¢é‡åŒ–ï¼‰ Exampleï¼š ä½¿ç”¨å‘é‡çš„è½¬ç½®æ¥æ±‚ä¸¤ä¸ªå‘é‡çš„å†…ç§¯ï¼Œå¯ä»¥éå¸¸æ–¹ä¾¿çš„å¤„ç†å¤§é‡çš„æ•°æ®ã€‚ è¿™å°±æ˜¯æ‰€è°“çš„çŸ¢é‡åŒ–çš„çº¿æ€§å›å½’ã€‚ Classificationï¼ˆåˆ†ç±»é—®é¢˜ï¼‰ Convention in Classification problems: 0 equal to empty ; 1 means something is there. Attention: In classification problems , we want to predict &lt;ç¦»æ•£å€¼&gt; Hypothesis Representationï¼ˆå‡è¯´è¡¨ç¤ºï¼‰ Questionï¼š What kind of function we need to use to represent our hypothesis in the classification problems? Linear function image may out of the range[-1,1] Idea: Make sure Import : Logistic regression(é€»è¾‘å›å½’) Hjypothesisï¼š Statusï¼š The most popular machine learning algorithm. ä¸€ç§éå¸¸å¼ºå¤§ï¼Œç”šè‡³å¯èƒ½ä¸–ç•Œä¸Šä½¿ç”¨æœ€å¹¿æ³›çš„ä¸€ç§åˆ†ç±»ç®—æ³•ã€‚ Attentionï¼š Although we called it â€˜regressionâ€™ , but in fact it is used in classification Problems. Logistic function: Decision boundary å†³ç­–è¾¹ç•Œä¸æ˜¯è®­ç»ƒé›†çš„å±æ€§ï¼Œè€Œæ˜¯å‡è®¾æœ¬èº«ä»¥åŠå…¶å‚æ•°çš„å±æ€§ï¼›æ‰€ä»¥è¯´ åªè¦ç»™å®šäº†thetaé‚£ä¹ˆå†³ç­–è¾¹ç•Œå°±å·²ç»ç¡®å®šäº†ã€‚ï¼ˆthetaé›†ç¡®å®šäº†ï¼Œé‚£ä¹ˆå‡½æ•°å°±ç¡®å®šäº†ï¼Œå†³ç­–è¾¹ç•Œè‡ªç„¶ä¹Ÿå°±è¢«ç¡®å®šäº†ï¼‰ è®­ç»ƒé›†â€“&gt;ç”Ÿæˆthetaâ€“&gt;ç¡®å®šå†³ç­–è¾¹ç•Œ Logistic Regression Cost functionï¼ˆé€»è¾‘å›å½’çš„ä»£ä»·å‡½æ•°ï¼‰ Question: How to choose parameter thetaï¼Ÿ å› ä¸ºæˆ‘ä»¬æ”¹å˜äº†ä»£ä»·å‡½æ•°çš„æ¨¡å‹h(theta) ,è¿™æ ·J(theta)-theta çš„å‡½æ•°å›¾åƒå°±ä¼šç”±è®¸å¤šä¸ªæå°å€¼ï¼Œè¿™å¯¹äºæˆ‘ä»¬æ±‚å¾—thetaæ˜¯ä¸åˆ©çš„ Idea: ä¸ºäº†ä¿è¯Cost function æ˜¯å‡¸å‡½æ•°ï¼Œæˆ‘ä»¬é€‰æ‹©æ›´æ¢ä»£ä»·å‡½æ•°ã€‚ åœ¨å½“æ—¶å­¦ä»£ä»·å‡½æ•°çš„æ—¶å€™å°±ç›´åˆ°ï¼Œä»£ä»·å‡½æ•°å¹¶ä¸åªæ˜¯æœ‰ä¸€ç§å½¢å¼ã€‚å®é™…ä¸Šä»£ä»·å‡½æ•°åªæ˜¯ä¸ºäº†è¡¨å¾è¯¯å·®çš„çš„å¤§å°ï¼Œæ‰€ä»¥åªè¦æ˜¯èƒ½å¤Ÿåˆç†çš„è¡¨ç¤ºè¯¯å·®çš„å‡½æ•°éƒ½å¯ä»¥å½“ä½œæ˜¯è¯¯å·®å‡½æ•°ã€‚ Simplified cost function: åŸºäºåªæœ‰y=1 || y=0 ä¸¤ç§æƒ…å†µ æ¨ªåæ ‡è¡¨ç¤ºçš„æ˜¯hï¼ˆx) ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„æ¨¡å‹å¾—å‡ºæ¥çš„ y å€¼ ï¼Œ çºµåæ ‡æ˜¯Cost functionï¼Œå¯¹äº y=1 å’Œ y=0 æœ‰ä¸åŒçš„å›¾åƒçš„ï¼Œè¿™ä¸¤ä¸ªå›¾åƒéƒ½æ˜¯åœ¨ä¸€ç«¯ç­‰äº0 ï¼Œ åœ¨å¦ä¸€ç«¯è¶‹äºæ— ç©·å¤§ã€‚ Logistic Regression Gradient Descentï¼ˆé€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™ï¼‰ Ideaï¼š æˆ‘ä»¬å·²ç»æ‰¾åˆ°äº†åˆé€‚çš„Cost function äº† ï¼Œç°åœ¨å°±å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¢¯åº¦ä¸‹é™äº† Details: å¯¹äºæ¢¯åº¦ä¸‹é™ï¼Œæ— è®ºCost function æ˜¯ä»€ä¹ˆ ï¼Œå®ƒçš„ algorithmæ˜¯å§‹ç»ˆä¸å˜çš„ æˆ‘ä»¬å°†Cost function å¸¦å…¥, ç»“æœå¦‚ä¸‹ï¼š Attention: è™½ç„¶å¾—åˆ°çš„æ¢¯åº¦ä¸‹é™ç®—æ³•è¡¨é¢ä¸Šçœ‹ä¸Šå»ä¸çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸€æ ·ï¼Œä½†æ˜¯è¿™é‡Œçš„ hï¼ˆxï¼‰ä¸çº¿æ€§å›å½’ä¸­ä¸åŒï¼ˆæ­¤å¤„çš„æ˜¯é€»è¾‘å›å½’å‡½æ•°è€Œä¹‹å‰çš„æ˜¯çº¿æ€§å‡½æ•°ï¼‰æ‰€ä»¥å®é™…ä¸Šé€»è¾‘å‡½æ•°çš„æ¢¯åº¦ä¸‹é™ï¼Œè·Ÿçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™æ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„ä¸œè¥¿ã€‚ å¦å¤–ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•ä¹‹å‰ï¼Œè¿›è¡Œç‰¹å¾ç¼©æ”¾ä¾æ—§æ˜¯éå¸¸å¿…è¦çš„ã€‚ ä¸€äº›æ¢¯åº¦ä¸‹é™ç®—æ³•ä¹‹å¤–çš„é€‰æ‹©ï¼š é™¤äº†æ¢¯åº¦ä¸‹é™ç®—æ³•ä»¥å¤–ï¼Œè¿˜æœ‰ä¸€äº›å¸¸è¢«ç”¨æ¥ä»¤ä»£ä»·å‡½æ•°æœ€å°çš„ç®—æ³•ï¼Œè¿™äº›ç®—æ³•æ›´åŠ å¤æ‚å’Œä¼˜è¶Šï¼Œè€Œä¸”é€šå¸¸ä¸éœ€è¦äººå·¥é€‰æ‹©å­¦ä¹ ç‡ï¼Œé€šå¸¸æ¯”æ¢¯åº¦ä¸‹é™ç®—æ³•è¦æ›´åŠ å¿«é€Ÿã€‚è¿™äº›ç®—æ³•æœ‰ï¼šå…±è½­æ¢¯åº¦ï¼ˆConjugate Gradientï¼‰ï¼Œå±€éƒ¨ä¼˜åŒ–æ³•(Broyden fletcher goldfarb shann,BFGS)å’Œæœ‰é™å†…å­˜å±€éƒ¨ä¼˜åŒ–æ³•(LBFGS) ï¼Œfminuncæ˜¯ matlabå’Œoctave ä¸­éƒ½å¸¦çš„ä¸€ä¸ªæœ€å°å€¼ä¼˜åŒ–å‡½æ•°ï¼Œä½¿ç”¨æ—¶æˆ‘ä»¬éœ€è¦æä¾›ä»£ä»·å‡½æ•°å’Œæ¯ä¸ªå‚æ•°çš„æ±‚å¯¼ï¼Œä¸‹é¢æ˜¯ octave ä¸­ä½¿ç”¨ fminunc å‡½æ•°çš„ä»£ç ç¤ºä¾‹ï¼š function [jVal, gradient] = costFunction(theta) â€‹ jVal = [...code to compute J(theta)...]; gradient = [...code to compute derivative of J(theta)...]; end options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, &#39;100&#39;); initialTheta = zeros(2,1); [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); %å¾ˆé†‰äººï¼Œè¿™è°ä½›å¾—äº†å•Š Advanced Optimizationï¼ˆé«˜çº§ä¼˜åŒ–ï¼‰ é›¾é‡Œçœ‹èŠ±ã€‚ã€‚ã€‚ æ¢¯åº¦ä¸‹é™å¹¶ä¸æ˜¯æˆ‘ä»¬æ±‚thetaæœ€å¥½çš„æ–¹æ³•ï¼Œè¿˜æœ‰å…¶ä»–ä¸€äº›ç®—æ³•ï¼Œæ›´é«˜çº§ã€æ›´å¤æ‚ã€‚ å…±è½­æ¢¯åº¦æ³• BFGS (å˜å°ºåº¦æ³•) å’ŒL-BFGS (é™åˆ¶å˜å°ºåº¦æ³•) å°±æ˜¯å…¶ä¸­ä¸€äº›æ›´é«˜çº§çš„ä¼˜åŒ–ç®—æ³• ä¼˜ç‚¹ï¼š é€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨é€‰æ‹©å­¦ä¹ ç‡ ï¼› ç®—æ³•çš„è¿è¡Œé€Ÿåº¦é€šå¸¸è¿œè¿œè¶…è¿‡æ¢¯åº¦ä¸‹é™ã€‚ Octaveä»£ç å®ç°ï¼š %å®šä¹‰Cost function å‡½æ•° function [jVal, gradient]=costFunction(theta) ã€€ã€€jVal=(theta(1)-5)^2+(theta(2)-5)^2; ã€€ã€€gradient=zeros(2,1); ã€€ã€€gradient(1)=2*(theta(1)-5); ã€€ã€€gradient(2)=2*(theta(2)-5); end è°ƒç”¨è¯¥å‡½æ•°ï¼š ä½ è¦è®¾ç½®å‡ ä¸ªoptionsï¼Œè¿™ä¸ªoptionså˜é‡ä½œä¸ºä¸€ä¸ªæ•°æ®ç»“æ„å¯ä»¥å­˜å‚¨ä½ æƒ³è¦çš„optionsï¼Œ æ‰€ä»¥ GradObj å’ŒOnï¼Œè¿™é‡Œè®¾ç½®æ¢¯åº¦ç›®æ ‡å‚æ•°ä¸ºæ‰“å¼€(on)ï¼Œè¿™æ„å‘³ç€ä½ ç°åœ¨ç¡®å®è¦ç»™è¿™ ä¸ªç®—æ³•æä¾›ä¸€ä¸ªæ¢¯åº¦ï¼Œç„¶åè®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ¯”æ–¹è¯´100ï¼Œæˆ‘ä»¬ç»™å‡ºä¸€ä¸ª çš„çŒœæµ‹åˆå§‹ å€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ª2Ã—1çš„å‘é‡ï¼Œé‚£ä¹ˆè¿™ä¸ªå‘½ä»¤å°±è°ƒç”¨fminuncï¼Œè¿™ä¸ª@ç¬¦å·è¡¨ç¤ºæŒ‡å‘æˆ‘ä»¬åˆšåˆš å®šä¹‰çš„costFunction å‡½æ•°çš„æŒ‡é’ˆã€‚å¦‚æœä½ è°ƒç”¨å®ƒï¼Œå®ƒå°±ä¼šä½¿ç”¨ä¼—å¤šé«˜çº§ä¼˜åŒ–ç®—æ³•ä¸­çš„ä¸€ ä¸ªï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥æŠŠå®ƒå½“æˆæ¢¯åº¦ä¸‹é™ï¼Œåªä¸è¿‡å®ƒèƒ½è‡ªåŠ¨é€‰æ‹©å­¦ä¹ é€Ÿç‡ï¼Œä½ ä¸éœ€è¦è‡ªå·±æ¥ åšã€‚ç„¶åå®ƒä¼šå°è¯•ä½¿ç”¨è¿™äº›é«˜çº§çš„ä¼˜åŒ–ç®—æ³•ï¼Œå°±åƒåŠ å¼ºç‰ˆçš„æ¢¯åº¦ä¸‹é™æ³•ï¼Œä¸ºä½ æ‰¾åˆ°æœ€ä½³ çš„å€¼ã€‚ å­¦åˆ°çš„ä¸»è¦å†…å®¹æ˜¯ï¼šå†™ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒèƒ½è¿”å›ä»£ä»·å‡½æ•°å€¼ã€æ¢¯åº¦å€¼ï¼Œå› æ­¤è¦æŠŠè¿™ä¸ªåº”ç”¨åˆ°é€»è¾‘å›å½’ï¼Œæˆ–è€…ç”šè‡³çº¿æ€§å›å½’ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥æŠŠè¿™äº›ä¼˜åŒ–ç®—æ³•ç”¨äºçº¿æ€§å›å½’ï¼Œä½ éœ€è¦åšçš„å°±æ˜¯è¾“å…¥åˆé€‚çš„ä»£ç æ¥è®¡ç®—è¿™é‡Œçš„è¿™äº›ä¸œè¥¿ã€‚ Multiclass Classification_ One-vs-all ï¼ˆå¤šç±»åˆ«åˆ†ç±»ï¼šä¸€å¯¹å¤šï¼‰ Part 7ï¼šæ­£åˆ™åŒ–(Regularization) æœ‰æ—¶æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç®—æ³•åº”ç”¨åˆ°æŸäº›ç‰¹å®šçš„æœºå™¨å­¦ä¹ åº”ç”¨æ—¶ï¼Œé€šè¿‡å­¦ä¹ å¾—åˆ°çš„å‡è®¾å¯èƒ½èƒ½å¤Ÿéå¸¸å¥½åœ°é€‚åº”è®­ç»ƒé›†ï¼ˆä»£ä»·å‡½æ•°å¯èƒ½å‡ ä¹ä¸º0ï¼‰ï¼Œä½†æ˜¯å¯èƒ½ä¼šä¸èƒ½æ¨å¹¿åˆ°æ–°çš„æ•°æ®ã€‚è¿™å°±æ˜¯è¿‡æ‹Ÿåˆ(over-fitting)çš„é—®é¢˜ã€‚ æ­£åˆ™åŒ–(regularization)çš„æŠ€æœ¯ï¼Œå®ƒå¯ä»¥æ”¹å–„æˆ–è€…å‡å°‘è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚ å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜çš„æ€è·¯ ä¸¢å¼ƒä¸€äº›ä¸èƒ½å¸®åŠ©æˆ‘ä»¬æ­£ç¡®é¢„æµ‹çš„ç‰¹å¾ã€‚å¯ä»¥æ˜¯æ‰‹å·¥é€‰æ‹©ä¿ç•™å“ªäº›ç‰¹å¾ï¼Œæˆ–è€…ä½¿ç”¨ä¸€äº›æ¨¡å‹é€‰æ‹©çš„ç®—æ³•æ¥å¸®å¿™ï¼ˆä¾‹å¦‚PCAï¼‰ æ­£åˆ™åŒ–ã€‚ ä¿ç•™æ‰€æœ‰çš„ç‰¹å¾ï¼Œä½†æ˜¯å‡å°‘å‚æ•°çš„å¤§å°ï¼ˆmagnitudeï¼‰ã€‚ å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜çš„ä»£ä»·å‡½æ•° Ideaï¼š è¿‡æ‹Ÿåˆé—®é¢˜çš„äº§ç”Ÿå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºé«˜æ¬¡é¡¹ã€‚æ‰€ä»¥ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯å¯¹é«˜æ­¤é¡¹è¿›è¡Œé™åˆ¶ï¼Œå…·ä½“åšæ³•å°±æ˜¯åœ¨ä»£ä»·å‡½æ•°é‡Œå¯¹ä»–ä»¬æ–½åŠ æƒ©ç½šã€‚ åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°è¿™äº›å‚æ•°thetaçš„å€¼ï¼Œè¿™å°±æ˜¯æ­£åˆ™åŒ–çš„åŸºæœ¬æ–¹æ³•ã€‚ ä¾‹å¦‚ï¼š å‡å¦‚æˆ‘ä»¬æœ‰éå¸¸å¤šçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“å…¶ä¸­å“ªäº›ç‰¹å¾æˆ‘ä»¬è¦æƒ©ç½šï¼Œæˆ‘ä»¬å°†å¯¹æ‰€æœ‰çš„ç‰¹å¾è¿›è¡Œæƒ©ç½šï¼Œå¹¶ä¸”è®©ä»£ä»·å‡½æ•°æœ€ä¼˜åŒ–çš„è½¯ä»¶æ¥é€‰æ‹©è¿™äº›æƒ©ç½šçš„ç¨‹åº¦ã€‚è¿™æ ·çš„ç»“æœæ˜¯å¾—åˆ°äº†ä¸€ä¸ªè¾ƒä¸ºç®€å•çš„èƒ½é˜²æ­¢è¿‡æ‹Ÿåˆé—®é¢˜çš„å‡è®¾ï¼Œå°±åƒè¿™æ ·ï¼š å…¶ä¸­lambdaåˆç§°ä¸ºæ­£åˆ™åŒ–å‚æ•°ï¼ˆRguarization Parameterï¼‰ã€‚ æ³¨ï¼šæ ¹æ®æƒ¯ä¾‹ï¼Œæˆ‘ä»¬ä¸ theta0è¿›è¡Œæƒ©ç½šã€‚ **lamnda ** 38.Regularized Linear Regression(æ­£åˆ™åŒ–çº¿æ€§å›å½’) æ­£åˆ™åŒ–çº¿æ€§å›å½’çš„ä»£ä»·å‡½æ•°ä¸ºï¼š å¦‚æœæˆ‘ä»¬è¦ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ä»¤è¿™ä¸ªä»£ä»·å‡½æ•°æœ€å°åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¯¹theta0è¿›è¡Œæ­£åˆ™åŒ–ï¼Œæ‰€ä»¥æ¢¯åº¦ä¸‹é™ç®—æ³•å°†åˆ†ç±»è®¨è®ºï¼Œå¯¹theta0ç‰¹æ®Šå¤„ç†ã€‚ æˆ‘ä»¬å¯¹æ–¹ç¨‹è¿›è¡Œæ‹†è§£ï¼Œå¯ä»¥çœ‹å‡ºï¼Œå®é™…ä¸Šæ­£åˆ™åŒ–çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å˜åŒ–åœ¨äºï¼Œæ¯æ¬¡éƒ½åœ¨åŸæœ‰ç®—æ³•æ›´æ–°è§„åˆ™çš„åŸºç¡€ä¸Šä»¤å€¼å‡å°‘äº†ä¸€ä¸ªé¢å¤–çš„å¾ˆå°çš„å€¼ã€‚ æ­£è§„æ–¹ç¨‹æ¥æ±‚è§£æ­£åˆ™åŒ–çº¿æ€§å›å½’æ¨¡å‹ï¼š Regularized Logistic Regression ï¼ˆæ­£åˆ™åŒ–é€»è¾‘å›å½’æ¨¡å‹ï¼‰ ä»¿ç…§çº¿æ€§å›å½’æ­£åˆ™åŒ–çš„æ€è·¯ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿®æ”¹ä»£ä»·å‡½æ•°ï¼Œæ·»åŠ ä¸€ä¸ªæ­£åˆ™åŒ–çš„è¡¨è¾¾å¼å³å¯ï¼š" />
<meta property="og:description" content="Machine Learning è¿™æ˜¯ç¬¬ä¸€ä»½æœºå™¨å­¦ä¹ ç¬”è®°ï¼Œåˆ›å»ºäº2019å¹´7æœˆ26æ—¥ï¼Œå®Œæˆäº2019å¹´8æœˆ2æ—¥ã€‚ è¯¥ç¬”è®°åŒ…æ‹¬å¦‚ä¸‹éƒ¨åˆ†: å¼•è¨€(Introduction) å•å˜é‡çº¿æ€§å›å½’(Linear Regression with One Variable) çº¿æ€§ä»£æ•°(Linear Algebra) å¤šå˜é‡çº¿æ€§å›å½’(Linear Regression with Multiple Variables) Octave é€»è¾‘å›å½’(Logistic Regression) æ­£åˆ™åŒ–(Regularization) Part 1 ï¼šIntroduction Machine Learningï¼š Idea: ç»™äºˆæœºå™¨è‡ªå·±å­¦ä¹ è§£å†³é—®é¢˜çš„èƒ½åŠ› å…ˆå¾—è‡ªå·±ææ˜ç™½äº†æ‰æœ‰å¯èƒ½æ•™ä¼šæœºå™¨è‡ªå·±å­¦ä¹ å‘ã€‚ã€‚ Machine learning algorithms: Supervised learning Unsupervised learning Othersï¼š Recommender systems(æ¨èç³»ç»Ÿ) ; Reinforcement learning(å¼ºåŒ–å­¦ä¹ ) Supervised learning(ç›‘ç£å­¦ä¹ ) Use to do : Make predictions Featureï¼š Right answers set is GIVEN Including: Regression problems(å›å½’é—®é¢˜) æˆ‘ä»¬ç»™å‡ºæ­£ç¡®çš„æ•°æ®é›†ï¼ŒæœŸæœ›ç®—æ³•èƒ½å¤Ÿé¢„æµ‹å‡ºcontinuous values(è¿ç»­å€¼)ã€‚ Classification problems(åˆ†ç±»é—®é¢˜) æˆ‘ä»¬ç»™å‡ºæ­£ç¡®çš„æ•°æ®é›†ï¼ŒæœŸæœ›ç®—æ³•èƒ½å¤Ÿé¢„æµ‹å‡ºdiscrete valued(ç¦»æ•£å€¼) ã€‚ ï¼ˆthe features can be more than 2 , Even infinite.ï¼‰ Unsupervised learningï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ Featureï¼š We just give the data and hope it can automatically find structure in the data for us. Includingï¼š Clustering algorithmsï¼ˆèšç±»ç®—æ³•ï¼‰ Problemsï¼š In fact I find the unsupervised learning looks like the classification problems . The only difference is whether or not the data set is GIVENGive the correct data ahead of time . Summaryï¼š We just give the Disorderly data sets , and hope the algorithm can automatically find out the structure of the data , and divide it into different clusters wisely. Octave Usageï¼š ç”¨octaveå»ºç«‹ç®—æ³•çš„åŸå‹ï¼Œå¦‚æœç®—æ³•æˆç«‹ï¼Œå†è¿ç§»åˆ°å…¶ä»–çš„ç¼–ç¨‹ç¯å¢ƒä¸­æ¥ï¼Œå¦‚æ­¤æ‰èƒ½ä½¿æ•ˆç‡æ›´é«˜ã€‚ Part 2 ï¼šå•å˜é‡çº¿æ€§å›å½’(Linear Regression with One Variable) ç®—æ³•å®ç°è¿‡ç¨‹ï¼š Methodï¼š Training set ïƒ  Learning algorithm ïƒ  hypothesis function(å‡è®¾å‡½æ•°) ïƒ  Find the appropriate form to represent the hypothesis function - ç®€å•æ¥è¯´å°±æ˜¯æˆ‘ä»¬ç»™å‡ºTraining Set ï¼Œ ç”¨å®ƒæ¥è®­ç»ƒæˆ‘ä»¬çš„Learning Algorithm ï¼Œæœ€ç»ˆæœŸæœ›èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªå‡½æ•° hï¼ˆthetaï¼‰, è¿™ä¹Ÿå°±æ˜¯é¢„æµ‹çš„æ¨¡å‹ï¼Œé€šè¿‡è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®ç°è¾“å…¥å˜é‡é›†ï¼Œç„¶åå°±èƒ½å¤Ÿè¿”å›å¯¹åº”çš„é¢„æµ‹å€¼ã€‚ Cost function(ä»£ä»·å‡½æ•°) å…³äºhï¼ˆxï¼‰å‡½æ•° å¯¹äºæˆ¿ä»·é—®é¢˜ï¼Œæˆ‘ä»¬å†³å®šä½¿ç”¨Linear function æ¥æ‹Ÿåˆæ›²çº¿ ï¼Œè®¾ hï¼ˆxï¼‰= thetaï¼ˆ0ï¼‰+thetaï¼ˆ1ï¼‰* x å› ä¸ºæˆ‘ä»¬æœ€åæƒ³è¦å¾—åˆ°çš„å°±æ˜¯ hï¼ˆxï¼‰ï¼Œå¯¹äº x æ¥è¯´ï¼Œå®ƒæ˜¯å§‹ç»ˆå˜åŒ–çš„ï¼Œæ¯ä¸€æ¡æ•°æ®å¯¹åº”ç€ä¸åŒçš„å˜é‡é›†ï¼Œæ— æ³•å›ºå®šï¼›æ‰€ä»¥algorithmæƒ³è¦ç¡®å®šçš„å°±æ˜¯ä¸å˜çš„é‡ thetaé›† äº†ã€‚åœ¨ä»¥åçš„æ“ä½œä¸­ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è¦æ±‚å¾—çš„æ˜¯ thetaé›†ï¼Œæ‰€ä»¥æ›´å¤šçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šæŠŠ x çœ‹ä½œæ˜¯å¸¸é‡ï¼Œå´æŠŠ theta çœ‹ä½œæ˜¯å˜é‡ï¼Œæ‰€ä»¥ä¸€å®šä¸è¦ç³Šæ¶‚äº†ã€‚ å»ºæ¨¡è¯¯å·®ï¼ˆmodeling errorï¼‰ æˆ‘ä»¬é€‰æ‹©çš„å‚æ•°å†³å®šäº†æˆ‘ä»¬å¾—åˆ°çš„ç›´çº¿ç›¸å¯¹äºæˆ‘ä»¬çš„è®­ç»ƒé›†çš„å‡†ç¡®ç¨‹åº¦ï¼Œæ¨¡å‹æ‰€é¢„æµ‹çš„å€¼ä¸è®­ç»ƒé›†ä¸­å®é™…å€¼ä¹‹é—´çš„å·®è·å°±æ˜¯å»ºæ¨¡è¯¯å·®ï¼ˆmodeling errorï¼‰ã€‚ Descriptionï¼š Cost function sometimes is also called squared error function(å¹³æ–¹è¯¯å·®å‡½æ•°) Cost function ç”¨äºæè¿°å»ºæ¨¡è¯¯å·®çš„å¤§å°ã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œè¯¯å·®è¶Šå°ï¼ŒCost function å°±èƒ½å¤Ÿæ›´å¥½çš„ä¸æ•°æ®é›†ç›¸æ‹Ÿåˆï¼Œä½†è¿™å¹¶ä¸æ˜¯ç»å¯¹çš„ï¼Œå› ä¸ºè¿™æœ‰å¯èƒ½ä¼šäº§ç”Ÿè¿‡æ‹Ÿåˆé—®é¢˜ã€‚ æŠŠå®ƒè¿›ä¸€æ­¥çš„æ‹†å¼€æ¥çœ‹å°±åº”è¯¥æ˜¯è¿™æ ·å­çš„ï¼šJï¼ˆ theta(0),theta(1) ) = 1/2m * Sigma [1-m] {thetaï¼ˆ0ï¼‰+thetaï¼ˆ1ï¼‰* x(i) - y(i)}(å¯èƒ½å°±æˆ‘ä¸€äººèƒ½çœ‹æ‡‚ã€‚ã€‚) å¹³æ–¹è¯¯å·®å‡½æ•°åªæ˜¯ Cost function ä¸­çš„ä¸€ç§å½¢å¼ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ä¹‹æ‰€ä»¥é€‰æ‹©squared error function ï¼Œ æ˜¯å› ä¸ºå¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å›å½’é—®é¢˜ï¼Œå®ƒæ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚è¿˜æœ‰å…¶ä»–çš„ä»£ä»·å‡½æ•°ä¹Ÿèƒ½å¾ˆå¥½åœ°å‘æŒ¥ä½œç”¨ï¼Œä½†æ˜¯å¹³æ–¹è¯¯å·®ä»£ä»·å‡½æ•°å¯èƒ½æ˜¯è§£å†³å›å½’é—®é¢˜æœ€å¸¸ç”¨çš„æ‰‹æ®µäº†ã€‚ Gradient descent(æ¢¯åº¦ä¸‹é™) algorithm Ideaï¼š æˆ‘ä»¬å·²ç»çŸ¥é“äº†ï¼Œ**hï¼ˆthetaï¼‰**æ˜¯ç”¨æ¥æ‹Ÿåˆæ•°æ®é›†çš„å‡½æ•°ï¼Œå¦‚æœæ‹Ÿåˆçš„å¥½ï¼Œé‚£ä¹ˆé¢„æµ‹çš„ç»“æœä¸ä¼šå·®ã€‚Jï¼ˆthetaé›†ï¼‰è¢«ç§°ä¸ºä»£ä»·å‡½æ•°ï¼Œå¦‚æœä»£ä»·å‡½æ•°è¾ƒå°ï¼Œåˆ™è¯´æ˜æ‹Ÿåˆçš„å¥½ã€‚è€Œæ­¤æ—¶çš„thetaé›†å°±æ˜¯æˆ‘ä»¬å°±æ˜¯é—®é¢˜çš„è§£äº†ã€‚ æˆ‘ä»¬å·²ç»çŸ¥é“äº†æˆ‘ä»¬çš„ç›®æ ‡thetaé›†ï¼Œé‚£ä¹ˆå¦‚ä½•æ±‚å¾—thetaé›†å‘¢ï¼Ÿ Gradient descentï¼ Usage: Start with some para0,para1â€¦ Keep changing paras to reduce J (para0,para1) to find some local minimum Start with different parameters , we may get different result ï¼ˆæ‰€ä»¥ï¼Œæˆ‘ä»¬ä¿è¯J(thetaé›†)ä¸å„ä¸ªthetaæ„æˆçš„å‡½æ•°æ˜¯ä¸€ä¸ªå‡¸å‡½æ•°ï¼Œæ¢å¥è¯è¯´ä¿è¯å­˜åœ¨å”¯ä¸€çš„æå°å€¼=æå°å€¼ï¼‰ Until we hopefully end up at a minimum Formulaï¼š æ‰¹é‡æ¢¯åº¦ä¸‹é™å…¬å¼ &lt;a&gt; is learning rate which control how big a step we use to upgrade the paras. So if the &lt;a&gt; is too small , the algorithm will run slowly ; but if &lt;a&gt; is too big , it can even make it away from the minimum point . å¦å¤–ï¼Œæ¯æ¬¡thetaä¸‹é™çš„å¹…åº¦éƒ½æ˜¯ä¸ä¸€æ ·çš„ï¼Œè¿™æ˜¯å› ä¸ºå®é™…ä¸Šthetaæ¯æ¬¡ä¸‹é™å¤šå°‘æ˜¯ç”± ã€&lt;a&gt;*Derivative Termã€‘è¿™ä¸ªæ•´ä½“å†³å®šçš„ï¼Œ&lt;a&gt;æ˜¯ä¸‹é™å¹…åº¦çš„é‡è¦å†³å®šå› å­ï¼Œä½†å¹¶ä¸æ˜¯å…¨éƒ¨ã€‚ åœ¨æ¯æ¬¡ä¸‹é™ä»¥åï¼ŒDerivative Terméƒ½ä¼šå˜å°ï¼Œä¸‹ä¸€æ¬¡ä¸‹é™çš„å¹…åº¦ä¹Ÿä¼šå˜å°ï¼Œå¦‚æ­¤ï¼Œæ§åˆ¶æ¢¯åº¦æ…¢æ…¢çš„ä¸‹é™ï¼Œç›´åˆ°Derivative Term=0ã€‚ the rest of the formula is the Derivative Term(å¯¼æ•°é¡¹) of theta In this image,we can find that no matter the Derivative Term of theta is positive or negative , the Gradient descent algorithm works well the Details of the formulaï¼š Attention: With the changing of the paras,the derivative term is changing too. So: Make sure updating the paras at the same time.ï¼ˆsimultaneous updateåŒæ­¥æ›´æ–°ï¼‰ For whatï¼Ÿ Because the derivative should use the former para rather than the new. How: åœ¨å…¨éƒ¨thetaæ²¡æœ‰æ›´æ–°å®Œæˆä¹‹å‰ï¼Œæˆ‘ä»¬ç”¨tempé›†æ¥æš‚æ—¶å­˜å‚¨å·²ç»æ›´æ–°å®Œæˆçš„thetaé›†ï¼Œç›´åˆ°æœ€åä¸€ä¸ªthetaä¹Ÿæ›´æ–°å®Œæˆä¹‹åï¼Œæˆ‘ä»¬æ‰å†ç”¨å­˜å‚¨åœ¨tempé›†é‡Œçš„æ–°thetaé›†æ›´æ–°åŸthetaé›†ã€‚ Gradient descent for linear regression â€œBatchâ€ Gradient descent ï¼š Each step of gradient descent uses all the training examples . åœ¨æ¢¯åº¦ä¸‹é™å…¬å¼ä¸­å­˜åœ¨æŸä¸€é¡¹éœ€è¦å¯¹æ‰€æœ‰mä¸ªè®­ç»ƒæ ·æœ¬æ±‚å’Œã€‚ Ideaï¼š å°†æ¢¯åº¦ä¸‹é™ä¸ä»£ä»·å‡½æ•°ç›¸ç»“åˆ The result: This is the result after derivation. And we can find that comparing with the first formula , the second has a x term at the end . It does tell us something that the variable is para0 and para1 , not the x or y . Although they looks more like variables. After derivation , for the first formula we have nothing left , for the second , we just has a constant term(å¸¸æ•°é¡¹) x left . Finallyï¼š We have other ways to computing Jï¼ˆthetaï¼‰ï¼š æ­£è§„æ–¹ç¨‹(normal equations)ï¼Œå®ƒå¯ä»¥åœ¨ä¸éœ€è¦å¤šæ­¥æ¢¯åº¦ä¸‹é™çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½è§£å‡ºä»£ä»·å‡½æ•°çš„æœ€å°å€¼ã€‚ å®é™…ä¸Šåœ¨æ•°æ®é‡è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œæ¢¯åº¦ä¸‹é™æ³•æ¯”æ­£è§„æ–¹ç¨‹è¦æ›´é€‚ç”¨ä¸€äº›ã€‚ Part 3 ï¼šçº¿æ€§ä»£æ•°(Linear Algebra) çŸ©é˜µå’Œå‘é‡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼æ¥ç»„ç»‡å¤§é‡çš„æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å½“æˆ‘ä»¬å¤„ç†å·¨å¤§çš„è®­ç»ƒé›†æ—¶ï¼Œå®ƒçš„ä¼˜åŠ¿å°±æ›´åŠ çš„æ˜æ˜¾ã€‚ Matrices and vectors (çŸ©é˜µå’Œå‘é‡) Matrix: Rectangular array of numbers. äºŒç»´æ•°ç»„ The dimension(ç»´åº¦) of the matrix : number of row * number of columns Matrix Elements : How to refer to the element Vector: An n*1 matrix . ( n dimensional vector. nç»´å‘é‡) How to refer to the element in it ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190726084319572.png) We use 1-indexed vector Cvention: We use upper case to refer to the matrix and lower case refer to the vector. Addition and scalar multiplication (åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•) è¡Œåˆ—æ•°ç›¸ç­‰æ‰èƒ½è¿›è¡ŒåŠ æ³•è¿ç®—ï¼š æ ‡é‡ä¸çŸ©é˜µçš„ä¹˜æ³•ï¼š Matrix-vector multiplication(çŸ©é˜µ-å‘é‡ä¹˜æ³•) Detailsï¼š Aâ€™s row number must be equal with Bâ€™s column number . Matrix-matrix multiplication(çŸ©é˜µ-çŸ©é˜µä¹˜æ³•) Detailsï¼š Just as matrix-vector , the most important thing is to make sure Aâ€™s columns number is equal with Bâ€™s rows number . Câ€™s columns number is equal with Bâ€™s . n fact , before multiply between Matrixs , we can divide B into several vectors by columns, and then we can make the Matrix- Vector multiplication. A column of B corresponding a hypothesis , and a column of C corresponding a set of prediction . Matrix multiplication properties(çŸ©é˜µä¹˜æ³•ç‰¹å¾) Properties : Do not enjoy commutative property(ä¸éµå¾ªäº¤æ¢å¾‹) Enjoy the associative property(éµå¾ªç»“åˆå¾‹) Identity Matrix(å•ä½çŸ©é˜µ): ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190726084424308.png) Featureï¼š å®ƒæ˜¯ä¸ªæ–¹é˜µï¼Œä»å·¦ä¸Šè§’åˆ°å³ä¸‹è§’çš„å¯¹è§’çº¿ï¼ˆç§°ä¸ºä¸»å¯¹è§’çº¿ï¼‰ä¸Šçš„å…ƒç´ å‡ä¸º1ã€‚é™¤æ­¤ä»¥å¤–å…¨éƒ½ä¸º0ã€‚å®ƒçš„ä½œç”¨å°±åƒæ˜¯ä¹˜æ³•ä¸­çš„1ã€‚ Usageï¼š is the identity matrix . A(m , n) * I(n , n) = I(m , m) * A(m , n) M1â€™s column number should be equal with M2â€™s row number . Matrix inverse operation(çŸ©é˜µçš„é€†è¿ç®—) Attentionï¼š Only square matrix have inverse . Not all number have inverse , such as 0 . Matrix transpose operation(çŸ©é˜µçš„è½¬ç½®è¿ç®—) Definition: Official definition: å³å°†Aä¸Bçš„x , yåæ ‡äº’æ¢ çŸ©é˜µè½¬ç½®çš„åŸºæœ¬æ€§è´¨ï¼š Part 4ï¼šå¤šå˜é‡çº¿æ€§å›å½’(Linear Regression with Multiple Variables) åœ¨ä¹‹å‰çš„éƒ¨åˆ†ä¸­ï¼Œå­¦ä¹ äº†å•å˜é‡/ç‰¹å¾çš„å›å½’æ¨¡å‹ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†è¿›è¡Œæ·±å…¥ï¼Œå°†å˜é‡çš„æ•°é‡å¢åŠ ï¼Œè®¨è®ºå¤šå˜é‡çš„çº¿æ€§å›å½’é—®é¢˜ã€‚ Multiple feature(å¤šåŠŸèƒ½) Exampleï¼š å¯¹äºæ–°é—®é¢˜çš„æ³¨é‡Šï¼š So the final prediction result is a vector too . å¯ä»¥å‘ç°ï¼Œå¤šå˜é‡çº¿æ€§å›å½’å’Œå•å˜é‡çº¿æ€§å›å½’åŸºæœ¬æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„å˜åŒ–å°±æ˜¯thetaå˜æˆäº†thetaé›†ï¼›å˜é‡xå˜æˆäº†xé›†ã€‚ Gradient descent for multiple variables(å¤šå…ƒæ¢¯åº¦ä¸‹é™æ³•) Ideaï¼š ä¸å•å˜é‡çº¿æ€§å›å½’ç±»ä¼¼ï¼Œåœ¨å¤šå˜é‡çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ„å»ºä¸€ä¸ªä»£ä»·å‡½æ•°ï¼Œåˆ™è¿™ä¸ªä»£ä»·å‡½æ•°æ˜¯æ‰€æœ‰å»ºæ¨¡è¯¯å·®çš„å¹³æ–¹å’Œ ä»£ä»·å‡½æ•°ï¼š æ¢¯åº¦ä¸‹é™æ³•åº”ç”¨åˆ°ä»£ä»·å‡½æ•°å¹¶æ±‚å¯¼ä¹‹åå¾—åˆ°ï¼š Feature scaling(ç‰¹å¾ç¼©æ”¾): Purpose: Make the gradient descent runs faster . å°¤å…¶æ˜¯å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚ Idea: Make sure number are in similar scale . Usageï¼š The range limit ï¼š Get every feature range from -3 to 3 or -1/3 to 1/3 . Thatâ€™s fine . In fact there is no problems , because the feature is a real number ! Thatâ€™s means it will never change . x æ˜¯å˜é‡ï¼Œå¹¶ä¸”å®ƒæ˜¯å·²çŸ¥çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä»data set ä¸­ç›´æ¥è·å¾—ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å®ƒè¿›è¡Œç‰¹å¾ç¼©æ”¾ï¼›thetaæ˜¯æœªçŸ¥çš„ï¼Œæ˜¯æˆ‘ä»¬è¦æ±‚çš„ç›®æ ‡ã€‚ Summaryï¼š ç‰¹å¾ç¼©æ”¾ç¼©æ”¾çš„æ˜¯ å·²çŸ¥çš„ å˜é‡x ï¼Œè€Œä¸æ˜¯ æœªçŸ¥çš„ å¸¸é‡theta ã€‚ è¿›è¡Œç‰¹å¾ç¼©æ”¾æ˜¯ä¸ºäº†ä¸ºäº†æé«˜æ¢¯åº¦ä¸‹é™çš„é€Ÿåº¦ã€‚ Learning rate (å­¦ä¹ ç‡) Ideaï¼š If the gradient descent is working properly , then J should decrease after every iteration. æ¢¯åº¦ä¸‹é™ç®—æ³•æ”¶æ•›æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°æ ¹æ®æ¨¡å‹çš„ä¸åŒè€Œä¸åŒï¼Œæˆ‘ä»¬ä¸èƒ½æå‰é¢„çŸ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è¿­ä»£æ¬¡æ•°å’Œä»£ä»·å‡½æ•°çš„å›¾è¡¨æ¥è§‚æµ‹ç®—æ³•åœ¨ä½•æ—¶è¶‹äºæ”¶æ•›ã€‚ Conventionï¼š é€šå¸¸å¯ä»¥è€ƒè™‘çš„å­¦ä¹ ç‡&lt;a&gt;çš„æ•°å€¼ï¼š 0.01ï¼›0.03ï¼›0.1ï¼›0.3ï¼›1ï¼›3ï¼›10ï¼› Summary: If &lt;a&gt; is too small ïƒ  slow move If &lt;a&gt; is too big ïƒ  J may not decrease on every iteration . Features and Polynomial regression(å¤šå˜é‡å’Œå¤šé¡¹å¼å›å½’) Ideaï¼š Linear regression å¹¶ä¸é€‚åº”æ‰€æœ‰çš„æ•°æ®æ¨¡å‹ï¼Œæœ‰æ—¶æˆ‘ä»¬éœ€è¦æ›²çº¿æ¥é€‚åº”æˆ‘ä»¬çš„æ•°æ®ï¼Œæ¯”å¦‚ä¸€ä¸ªäºŒæ¬¡æ–¹æ¨¡å‹ï¼š é€šå¸¸æˆ‘ä»¬éœ€è¦å…ˆè§‚å¯Ÿæ•°æ®å†ç¡®å®šæœ€ç»ˆçš„æ¨¡å‹ Attentionï¼š å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚ Normal equation(å¸¸è§„æ–¹ç¨‹) Ideaï¼š ä»æ•°å­¦ä¸Šæ¥çœ‹ï¼Œæƒ³è¦æ±‚å¾—Jï¼ˆtheta) çš„æœ€å°å€¼ï¼Œæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯å¯¹å®ƒè¿›è¡Œæ±‚å¯¼ï¼Œä»¤å®ƒçš„å¯¼æ•°=0ï¼Œå¾—å‡ºæ­¤æ—¶çš„thetaå°±æ˜¯æœ€ç»ˆçš„ç»“æœ è€Œæ­£è§„æ–¹ç¨‹æ–¹æ³•çš„ç¡®å°±æ˜¯è¿™ä¹ˆå¹²çš„ Attentionï¼š å¯¹äºé‚£äº›ä¸å¯é€†çš„çŸ©é˜µï¼ˆé€šå¸¸æ˜¯å› ä¸ºç‰¹å¾ä¹‹é—´ä¸ç‹¬ç«‹ï¼Œå¦‚åŒæ—¶åŒ…å«è‹±å°ºä¸ºå•ä½çš„å°ºå¯¸å’Œç±³ä¸ºå•ä½çš„å°ºå¯¸ä¸¤ä¸ªç‰¹å¾ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ç‰¹å¾æ•°é‡å¤§äºè®­ç»ƒé›†çš„æ•°é‡ï¼‰ï¼Œæ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯ä¸èƒ½ç”¨çš„ã€‚ è¿™å°±æ˜¯æ­£è§„æ–¹ç¨‹çš„å±€é™æ€§ Usage: Exampleï¼š The Comparison of Gradient descent and Normal equation Gradient descent : Need to choose Need many iterations Normal equation : Need to compute [xâ€™*x]^(-1) , So if n is large it will run slowly . Do not fit every situations (åªé€‚ç”¨äºçº¿æ€§æ¨¡å‹ï¼Œä¸é€‚åˆé€»è¾‘å›å½’æ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹). Summary: åªè¦ç‰¹å¾å˜é‡çš„æ•°ç›®å¹¶ä¸å¤§ï¼Œæ ‡å‡†æ–¹ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è®¡ç®—å‚æ•°çš„æ›¿ä»£æ–¹æ³•ã€‚å…·ä½“åœ°è¯´ï¼Œåªè¦ç‰¹å¾å˜é‡æ•°é‡å°äºä¸€ä¸‡ï¼Œæˆ‘é€šå¸¸ä½¿ç”¨æ ‡å‡†æ–¹ç¨‹æ³•ï¼Œè€Œä¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€‚ Normal equation ä¸å¯é€†æƒ…å†µ Ideaï¼š ç¬¬ä¸€è¦é™ä½featuresä¹‹é—´çš„ç›¸å…³åº¦ ç¬¬äºŒè¦é€‚å½“çš„å‡å°‘features çš„æ•°é‡ Ideaï¼š Part 5ï¼šOctaveæ•™ç¨‹(Octave Tutorial) è¿™é‡Œæ˜¯ä¸€ä¸ªğŸ”—â€“&gt; OctaveåŸºæœ¬è¯­æ³• Part 6ï¼šé€»è¾‘å›å½’(Logistic Regression) Vectorizationï¼ˆçŸ¢é‡åŒ–ï¼‰ Exampleï¼š ä½¿ç”¨å‘é‡çš„è½¬ç½®æ¥æ±‚ä¸¤ä¸ªå‘é‡çš„å†…ç§¯ï¼Œå¯ä»¥éå¸¸æ–¹ä¾¿çš„å¤„ç†å¤§é‡çš„æ•°æ®ã€‚ è¿™å°±æ˜¯æ‰€è°“çš„çŸ¢é‡åŒ–çš„çº¿æ€§å›å½’ã€‚ Classificationï¼ˆåˆ†ç±»é—®é¢˜ï¼‰ Convention in Classification problems: 0 equal to empty ; 1 means something is there. Attention: In classification problems , we want to predict &lt;ç¦»æ•£å€¼&gt; Hypothesis Representationï¼ˆå‡è¯´è¡¨ç¤ºï¼‰ Questionï¼š What kind of function we need to use to represent our hypothesis in the classification problems? Linear function image may out of the range[-1,1] Idea: Make sure Import : Logistic regression(é€»è¾‘å›å½’) Hjypothesisï¼š Statusï¼š The most popular machine learning algorithm. ä¸€ç§éå¸¸å¼ºå¤§ï¼Œç”šè‡³å¯èƒ½ä¸–ç•Œä¸Šä½¿ç”¨æœ€å¹¿æ³›çš„ä¸€ç§åˆ†ç±»ç®—æ³•ã€‚ Attentionï¼š Although we called it â€˜regressionâ€™ , but in fact it is used in classification Problems. Logistic function: Decision boundary å†³ç­–è¾¹ç•Œä¸æ˜¯è®­ç»ƒé›†çš„å±æ€§ï¼Œè€Œæ˜¯å‡è®¾æœ¬èº«ä»¥åŠå…¶å‚æ•°çš„å±æ€§ï¼›æ‰€ä»¥è¯´ åªè¦ç»™å®šäº†thetaé‚£ä¹ˆå†³ç­–è¾¹ç•Œå°±å·²ç»ç¡®å®šäº†ã€‚ï¼ˆthetaé›†ç¡®å®šäº†ï¼Œé‚£ä¹ˆå‡½æ•°å°±ç¡®å®šäº†ï¼Œå†³ç­–è¾¹ç•Œè‡ªç„¶ä¹Ÿå°±è¢«ç¡®å®šäº†ï¼‰ è®­ç»ƒé›†â€“&gt;ç”Ÿæˆthetaâ€“&gt;ç¡®å®šå†³ç­–è¾¹ç•Œ Logistic Regression Cost functionï¼ˆé€»è¾‘å›å½’çš„ä»£ä»·å‡½æ•°ï¼‰ Question: How to choose parameter thetaï¼Ÿ å› ä¸ºæˆ‘ä»¬æ”¹å˜äº†ä»£ä»·å‡½æ•°çš„æ¨¡å‹h(theta) ,è¿™æ ·J(theta)-theta çš„å‡½æ•°å›¾åƒå°±ä¼šç”±è®¸å¤šä¸ªæå°å€¼ï¼Œè¿™å¯¹äºæˆ‘ä»¬æ±‚å¾—thetaæ˜¯ä¸åˆ©çš„ Idea: ä¸ºäº†ä¿è¯Cost function æ˜¯å‡¸å‡½æ•°ï¼Œæˆ‘ä»¬é€‰æ‹©æ›´æ¢ä»£ä»·å‡½æ•°ã€‚ åœ¨å½“æ—¶å­¦ä»£ä»·å‡½æ•°çš„æ—¶å€™å°±ç›´åˆ°ï¼Œä»£ä»·å‡½æ•°å¹¶ä¸åªæ˜¯æœ‰ä¸€ç§å½¢å¼ã€‚å®é™…ä¸Šä»£ä»·å‡½æ•°åªæ˜¯ä¸ºäº†è¡¨å¾è¯¯å·®çš„çš„å¤§å°ï¼Œæ‰€ä»¥åªè¦æ˜¯èƒ½å¤Ÿåˆç†çš„è¡¨ç¤ºè¯¯å·®çš„å‡½æ•°éƒ½å¯ä»¥å½“ä½œæ˜¯è¯¯å·®å‡½æ•°ã€‚ Simplified cost function: åŸºäºåªæœ‰y=1 || y=0 ä¸¤ç§æƒ…å†µ æ¨ªåæ ‡è¡¨ç¤ºçš„æ˜¯hï¼ˆx) ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„æ¨¡å‹å¾—å‡ºæ¥çš„ y å€¼ ï¼Œ çºµåæ ‡æ˜¯Cost functionï¼Œå¯¹äº y=1 å’Œ y=0 æœ‰ä¸åŒçš„å›¾åƒçš„ï¼Œè¿™ä¸¤ä¸ªå›¾åƒéƒ½æ˜¯åœ¨ä¸€ç«¯ç­‰äº0 ï¼Œ åœ¨å¦ä¸€ç«¯è¶‹äºæ— ç©·å¤§ã€‚ Logistic Regression Gradient Descentï¼ˆé€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™ï¼‰ Ideaï¼š æˆ‘ä»¬å·²ç»æ‰¾åˆ°äº†åˆé€‚çš„Cost function äº† ï¼Œç°åœ¨å°±å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¢¯åº¦ä¸‹é™äº† Details: å¯¹äºæ¢¯åº¦ä¸‹é™ï¼Œæ— è®ºCost function æ˜¯ä»€ä¹ˆ ï¼Œå®ƒçš„ algorithmæ˜¯å§‹ç»ˆä¸å˜çš„ æˆ‘ä»¬å°†Cost function å¸¦å…¥, ç»“æœå¦‚ä¸‹ï¼š Attention: è™½ç„¶å¾—åˆ°çš„æ¢¯åº¦ä¸‹é™ç®—æ³•è¡¨é¢ä¸Šçœ‹ä¸Šå»ä¸çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸€æ ·ï¼Œä½†æ˜¯è¿™é‡Œçš„ hï¼ˆxï¼‰ä¸çº¿æ€§å›å½’ä¸­ä¸åŒï¼ˆæ­¤å¤„çš„æ˜¯é€»è¾‘å›å½’å‡½æ•°è€Œä¹‹å‰çš„æ˜¯çº¿æ€§å‡½æ•°ï¼‰æ‰€ä»¥å®é™…ä¸Šé€»è¾‘å‡½æ•°çš„æ¢¯åº¦ä¸‹é™ï¼Œè·Ÿçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™æ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„ä¸œè¥¿ã€‚ å¦å¤–ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•ä¹‹å‰ï¼Œè¿›è¡Œç‰¹å¾ç¼©æ”¾ä¾æ—§æ˜¯éå¸¸å¿…è¦çš„ã€‚ ä¸€äº›æ¢¯åº¦ä¸‹é™ç®—æ³•ä¹‹å¤–çš„é€‰æ‹©ï¼š é™¤äº†æ¢¯åº¦ä¸‹é™ç®—æ³•ä»¥å¤–ï¼Œè¿˜æœ‰ä¸€äº›å¸¸è¢«ç”¨æ¥ä»¤ä»£ä»·å‡½æ•°æœ€å°çš„ç®—æ³•ï¼Œè¿™äº›ç®—æ³•æ›´åŠ å¤æ‚å’Œä¼˜è¶Šï¼Œè€Œä¸”é€šå¸¸ä¸éœ€è¦äººå·¥é€‰æ‹©å­¦ä¹ ç‡ï¼Œé€šå¸¸æ¯”æ¢¯åº¦ä¸‹é™ç®—æ³•è¦æ›´åŠ å¿«é€Ÿã€‚è¿™äº›ç®—æ³•æœ‰ï¼šå…±è½­æ¢¯åº¦ï¼ˆConjugate Gradientï¼‰ï¼Œå±€éƒ¨ä¼˜åŒ–æ³•(Broyden fletcher goldfarb shann,BFGS)å’Œæœ‰é™å†…å­˜å±€éƒ¨ä¼˜åŒ–æ³•(LBFGS) ï¼Œfminuncæ˜¯ matlabå’Œoctave ä¸­éƒ½å¸¦çš„ä¸€ä¸ªæœ€å°å€¼ä¼˜åŒ–å‡½æ•°ï¼Œä½¿ç”¨æ—¶æˆ‘ä»¬éœ€è¦æä¾›ä»£ä»·å‡½æ•°å’Œæ¯ä¸ªå‚æ•°çš„æ±‚å¯¼ï¼Œä¸‹é¢æ˜¯ octave ä¸­ä½¿ç”¨ fminunc å‡½æ•°çš„ä»£ç ç¤ºä¾‹ï¼š function [jVal, gradient] = costFunction(theta) â€‹ jVal = [...code to compute J(theta)...]; gradient = [...code to compute derivative of J(theta)...]; end options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, &#39;100&#39;); initialTheta = zeros(2,1); [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); %å¾ˆé†‰äººï¼Œè¿™è°ä½›å¾—äº†å•Š Advanced Optimizationï¼ˆé«˜çº§ä¼˜åŒ–ï¼‰ é›¾é‡Œçœ‹èŠ±ã€‚ã€‚ã€‚ æ¢¯åº¦ä¸‹é™å¹¶ä¸æ˜¯æˆ‘ä»¬æ±‚thetaæœ€å¥½çš„æ–¹æ³•ï¼Œè¿˜æœ‰å…¶ä»–ä¸€äº›ç®—æ³•ï¼Œæ›´é«˜çº§ã€æ›´å¤æ‚ã€‚ å…±è½­æ¢¯åº¦æ³• BFGS (å˜å°ºåº¦æ³•) å’ŒL-BFGS (é™åˆ¶å˜å°ºåº¦æ³•) å°±æ˜¯å…¶ä¸­ä¸€äº›æ›´é«˜çº§çš„ä¼˜åŒ–ç®—æ³• ä¼˜ç‚¹ï¼š é€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨é€‰æ‹©å­¦ä¹ ç‡ ï¼› ç®—æ³•çš„è¿è¡Œé€Ÿåº¦é€šå¸¸è¿œè¿œè¶…è¿‡æ¢¯åº¦ä¸‹é™ã€‚ Octaveä»£ç å®ç°ï¼š %å®šä¹‰Cost function å‡½æ•° function [jVal, gradient]=costFunction(theta) ã€€ã€€jVal=(theta(1)-5)^2+(theta(2)-5)^2; ã€€ã€€gradient=zeros(2,1); ã€€ã€€gradient(1)=2*(theta(1)-5); ã€€ã€€gradient(2)=2*(theta(2)-5); end è°ƒç”¨è¯¥å‡½æ•°ï¼š ä½ è¦è®¾ç½®å‡ ä¸ªoptionsï¼Œè¿™ä¸ªoptionså˜é‡ä½œä¸ºä¸€ä¸ªæ•°æ®ç»“æ„å¯ä»¥å­˜å‚¨ä½ æƒ³è¦çš„optionsï¼Œ æ‰€ä»¥ GradObj å’ŒOnï¼Œè¿™é‡Œè®¾ç½®æ¢¯åº¦ç›®æ ‡å‚æ•°ä¸ºæ‰“å¼€(on)ï¼Œè¿™æ„å‘³ç€ä½ ç°åœ¨ç¡®å®è¦ç»™è¿™ ä¸ªç®—æ³•æä¾›ä¸€ä¸ªæ¢¯åº¦ï¼Œç„¶åè®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ¯”æ–¹è¯´100ï¼Œæˆ‘ä»¬ç»™å‡ºä¸€ä¸ª çš„çŒœæµ‹åˆå§‹ å€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ª2Ã—1çš„å‘é‡ï¼Œé‚£ä¹ˆè¿™ä¸ªå‘½ä»¤å°±è°ƒç”¨fminuncï¼Œè¿™ä¸ª@ç¬¦å·è¡¨ç¤ºæŒ‡å‘æˆ‘ä»¬åˆšåˆš å®šä¹‰çš„costFunction å‡½æ•°çš„æŒ‡é’ˆã€‚å¦‚æœä½ è°ƒç”¨å®ƒï¼Œå®ƒå°±ä¼šä½¿ç”¨ä¼—å¤šé«˜çº§ä¼˜åŒ–ç®—æ³•ä¸­çš„ä¸€ ä¸ªï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥æŠŠå®ƒå½“æˆæ¢¯åº¦ä¸‹é™ï¼Œåªä¸è¿‡å®ƒèƒ½è‡ªåŠ¨é€‰æ‹©å­¦ä¹ é€Ÿç‡ï¼Œä½ ä¸éœ€è¦è‡ªå·±æ¥ åšã€‚ç„¶åå®ƒä¼šå°è¯•ä½¿ç”¨è¿™äº›é«˜çº§çš„ä¼˜åŒ–ç®—æ³•ï¼Œå°±åƒåŠ å¼ºç‰ˆçš„æ¢¯åº¦ä¸‹é™æ³•ï¼Œä¸ºä½ æ‰¾åˆ°æœ€ä½³ çš„å€¼ã€‚ å­¦åˆ°çš„ä¸»è¦å†…å®¹æ˜¯ï¼šå†™ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒèƒ½è¿”å›ä»£ä»·å‡½æ•°å€¼ã€æ¢¯åº¦å€¼ï¼Œå› æ­¤è¦æŠŠè¿™ä¸ªåº”ç”¨åˆ°é€»è¾‘å›å½’ï¼Œæˆ–è€…ç”šè‡³çº¿æ€§å›å½’ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥æŠŠè¿™äº›ä¼˜åŒ–ç®—æ³•ç”¨äºçº¿æ€§å›å½’ï¼Œä½ éœ€è¦åšçš„å°±æ˜¯è¾“å…¥åˆé€‚çš„ä»£ç æ¥è®¡ç®—è¿™é‡Œçš„è¿™äº›ä¸œè¥¿ã€‚ Multiclass Classification_ One-vs-all ï¼ˆå¤šç±»åˆ«åˆ†ç±»ï¼šä¸€å¯¹å¤šï¼‰ Part 7ï¼šæ­£åˆ™åŒ–(Regularization) æœ‰æ—¶æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç®—æ³•åº”ç”¨åˆ°æŸäº›ç‰¹å®šçš„æœºå™¨å­¦ä¹ åº”ç”¨æ—¶ï¼Œé€šè¿‡å­¦ä¹ å¾—åˆ°çš„å‡è®¾å¯èƒ½èƒ½å¤Ÿéå¸¸å¥½åœ°é€‚åº”è®­ç»ƒé›†ï¼ˆä»£ä»·å‡½æ•°å¯èƒ½å‡ ä¹ä¸º0ï¼‰ï¼Œä½†æ˜¯å¯èƒ½ä¼šä¸èƒ½æ¨å¹¿åˆ°æ–°çš„æ•°æ®ã€‚è¿™å°±æ˜¯è¿‡æ‹Ÿåˆ(over-fitting)çš„é—®é¢˜ã€‚ æ­£åˆ™åŒ–(regularization)çš„æŠ€æœ¯ï¼Œå®ƒå¯ä»¥æ”¹å–„æˆ–è€…å‡å°‘è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚ å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜çš„æ€è·¯ ä¸¢å¼ƒä¸€äº›ä¸èƒ½å¸®åŠ©æˆ‘ä»¬æ­£ç¡®é¢„æµ‹çš„ç‰¹å¾ã€‚å¯ä»¥æ˜¯æ‰‹å·¥é€‰æ‹©ä¿ç•™å“ªäº›ç‰¹å¾ï¼Œæˆ–è€…ä½¿ç”¨ä¸€äº›æ¨¡å‹é€‰æ‹©çš„ç®—æ³•æ¥å¸®å¿™ï¼ˆä¾‹å¦‚PCAï¼‰ æ­£åˆ™åŒ–ã€‚ ä¿ç•™æ‰€æœ‰çš„ç‰¹å¾ï¼Œä½†æ˜¯å‡å°‘å‚æ•°çš„å¤§å°ï¼ˆmagnitudeï¼‰ã€‚ å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜çš„ä»£ä»·å‡½æ•° Ideaï¼š è¿‡æ‹Ÿåˆé—®é¢˜çš„äº§ç”Ÿå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºé«˜æ¬¡é¡¹ã€‚æ‰€ä»¥ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯å¯¹é«˜æ­¤é¡¹è¿›è¡Œé™åˆ¶ï¼Œå…·ä½“åšæ³•å°±æ˜¯åœ¨ä»£ä»·å‡½æ•°é‡Œå¯¹ä»–ä»¬æ–½åŠ æƒ©ç½šã€‚ åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°è¿™äº›å‚æ•°thetaçš„å€¼ï¼Œè¿™å°±æ˜¯æ­£åˆ™åŒ–çš„åŸºæœ¬æ–¹æ³•ã€‚ ä¾‹å¦‚ï¼š å‡å¦‚æˆ‘ä»¬æœ‰éå¸¸å¤šçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“å…¶ä¸­å“ªäº›ç‰¹å¾æˆ‘ä»¬è¦æƒ©ç½šï¼Œæˆ‘ä»¬å°†å¯¹æ‰€æœ‰çš„ç‰¹å¾è¿›è¡Œæƒ©ç½šï¼Œå¹¶ä¸”è®©ä»£ä»·å‡½æ•°æœ€ä¼˜åŒ–çš„è½¯ä»¶æ¥é€‰æ‹©è¿™äº›æƒ©ç½šçš„ç¨‹åº¦ã€‚è¿™æ ·çš„ç»“æœæ˜¯å¾—åˆ°äº†ä¸€ä¸ªè¾ƒä¸ºç®€å•çš„èƒ½é˜²æ­¢è¿‡æ‹Ÿåˆé—®é¢˜çš„å‡è®¾ï¼Œå°±åƒè¿™æ ·ï¼š å…¶ä¸­lambdaåˆç§°ä¸ºæ­£åˆ™åŒ–å‚æ•°ï¼ˆRguarization Parameterï¼‰ã€‚ æ³¨ï¼šæ ¹æ®æƒ¯ä¾‹ï¼Œæˆ‘ä»¬ä¸ theta0è¿›è¡Œæƒ©ç½šã€‚ **lamnda ** 38.Regularized Linear Regression(æ­£åˆ™åŒ–çº¿æ€§å›å½’) æ­£åˆ™åŒ–çº¿æ€§å›å½’çš„ä»£ä»·å‡½æ•°ä¸ºï¼š å¦‚æœæˆ‘ä»¬è¦ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ä»¤è¿™ä¸ªä»£ä»·å‡½æ•°æœ€å°åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¯¹theta0è¿›è¡Œæ­£åˆ™åŒ–ï¼Œæ‰€ä»¥æ¢¯åº¦ä¸‹é™ç®—æ³•å°†åˆ†ç±»è®¨è®ºï¼Œå¯¹theta0ç‰¹æ®Šå¤„ç†ã€‚ æˆ‘ä»¬å¯¹æ–¹ç¨‹è¿›è¡Œæ‹†è§£ï¼Œå¯ä»¥çœ‹å‡ºï¼Œå®é™…ä¸Šæ­£åˆ™åŒ–çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å˜åŒ–åœ¨äºï¼Œæ¯æ¬¡éƒ½åœ¨åŸæœ‰ç®—æ³•æ›´æ–°è§„åˆ™çš„åŸºç¡€ä¸Šä»¤å€¼å‡å°‘äº†ä¸€ä¸ªé¢å¤–çš„å¾ˆå°çš„å€¼ã€‚ æ­£è§„æ–¹ç¨‹æ¥æ±‚è§£æ­£åˆ™åŒ–çº¿æ€§å›å½’æ¨¡å‹ï¼š Regularized Logistic Regression ï¼ˆæ­£åˆ™åŒ–é€»è¾‘å›å½’æ¨¡å‹ï¼‰ ä»¿ç…§çº¿æ€§å›å½’æ­£åˆ™åŒ–çš„æ€è·¯ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿®æ”¹ä»£ä»·å‡½æ•°ï¼Œæ·»åŠ ä¸€ä¸ªæ­£åˆ™åŒ–çš„è¡¨è¾¾å¼å³å¯ï¼š" />
<link rel="canonical" href="https://uzzz.org/2019/08/29/795257.html" />
<meta property="og:url" content="https://uzzz.org/2019/08/29/795257.html" />
<meta property="og:site_name" content="æœ‰ç»„ç»‡åœ¨ï¼" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-29T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Machine Learning è¿™æ˜¯ç¬¬ä¸€ä»½æœºå™¨å­¦ä¹ ç¬”è®°ï¼Œåˆ›å»ºäº2019å¹´7æœˆ26æ—¥ï¼Œå®Œæˆäº2019å¹´8æœˆ2æ—¥ã€‚ è¯¥ç¬”è®°åŒ…æ‹¬å¦‚ä¸‹éƒ¨åˆ†: å¼•è¨€(Introduction) å•å˜é‡çº¿æ€§å›å½’(Linear Regression with One Variable) çº¿æ€§ä»£æ•°(Linear Algebra) å¤šå˜é‡çº¿æ€§å›å½’(Linear Regression with Multiple Variables) Octave é€»è¾‘å›å½’(Logistic Regression) æ­£åˆ™åŒ–(Regularization) Part 1 ï¼šIntroduction Machine Learningï¼š Idea: ç»™äºˆæœºå™¨è‡ªå·±å­¦ä¹ è§£å†³é—®é¢˜çš„èƒ½åŠ› å…ˆå¾—è‡ªå·±ææ˜ç™½äº†æ‰æœ‰å¯èƒ½æ•™ä¼šæœºå™¨è‡ªå·±å­¦ä¹ å‘ã€‚ã€‚ Machine learning algorithms: Supervised learning Unsupervised learning Othersï¼š Recommender systems(æ¨èç³»ç»Ÿ) ; Reinforcement learning(å¼ºåŒ–å­¦ä¹ ) Supervised learning(ç›‘ç£å­¦ä¹ ) Use to do : Make predictions Featureï¼š Right answers set is GIVEN Including: Regression problems(å›å½’é—®é¢˜) æˆ‘ä»¬ç»™å‡ºæ­£ç¡®çš„æ•°æ®é›†ï¼ŒæœŸæœ›ç®—æ³•èƒ½å¤Ÿé¢„æµ‹å‡ºcontinuous values(è¿ç»­å€¼)ã€‚ Classification problems(åˆ†ç±»é—®é¢˜) æˆ‘ä»¬ç»™å‡ºæ­£ç¡®çš„æ•°æ®é›†ï¼ŒæœŸæœ›ç®—æ³•èƒ½å¤Ÿé¢„æµ‹å‡ºdiscrete valued(ç¦»æ•£å€¼) ã€‚ ï¼ˆthe features can be more than 2 , Even infinite.ï¼‰ Unsupervised learningï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ Featureï¼š We just give the data and hope it can automatically find structure in the data for us. Includingï¼š Clustering algorithmsï¼ˆèšç±»ç®—æ³•ï¼‰ Problemsï¼š In fact I find the unsupervised learning looks like the classification problems . The only difference is whether or not the data set is GIVENGive the correct data ahead of time . Summaryï¼š We just give the Disorderly data sets , and hope the algorithm can automatically find out the structure of the data , and divide it into different clusters wisely. Octave Usageï¼š ç”¨octaveå»ºç«‹ç®—æ³•çš„åŸå‹ï¼Œå¦‚æœç®—æ³•æˆç«‹ï¼Œå†è¿ç§»åˆ°å…¶ä»–çš„ç¼–ç¨‹ç¯å¢ƒä¸­æ¥ï¼Œå¦‚æ­¤æ‰èƒ½ä½¿æ•ˆç‡æ›´é«˜ã€‚ Part 2 ï¼šå•å˜é‡çº¿æ€§å›å½’(Linear Regression with One Variable) ç®—æ³•å®ç°è¿‡ç¨‹ï¼š Methodï¼š Training set ïƒ  Learning algorithm ïƒ  hypothesis function(å‡è®¾å‡½æ•°) ïƒ  Find the appropriate form to represent the hypothesis function - ç®€å•æ¥è¯´å°±æ˜¯æˆ‘ä»¬ç»™å‡ºTraining Set ï¼Œ ç”¨å®ƒæ¥è®­ç»ƒæˆ‘ä»¬çš„Learning Algorithm ï¼Œæœ€ç»ˆæœŸæœ›èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªå‡½æ•° hï¼ˆthetaï¼‰, è¿™ä¹Ÿå°±æ˜¯é¢„æµ‹çš„æ¨¡å‹ï¼Œé€šè¿‡è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®ç°è¾“å…¥å˜é‡é›†ï¼Œç„¶åå°±èƒ½å¤Ÿè¿”å›å¯¹åº”çš„é¢„æµ‹å€¼ã€‚ Cost function(ä»£ä»·å‡½æ•°) å…³äºhï¼ˆxï¼‰å‡½æ•° å¯¹äºæˆ¿ä»·é—®é¢˜ï¼Œæˆ‘ä»¬å†³å®šä½¿ç”¨Linear function æ¥æ‹Ÿåˆæ›²çº¿ ï¼Œè®¾ hï¼ˆxï¼‰= thetaï¼ˆ0ï¼‰+thetaï¼ˆ1ï¼‰* x å› ä¸ºæˆ‘ä»¬æœ€åæƒ³è¦å¾—åˆ°çš„å°±æ˜¯ hï¼ˆxï¼‰ï¼Œå¯¹äº x æ¥è¯´ï¼Œå®ƒæ˜¯å§‹ç»ˆå˜åŒ–çš„ï¼Œæ¯ä¸€æ¡æ•°æ®å¯¹åº”ç€ä¸åŒçš„å˜é‡é›†ï¼Œæ— æ³•å›ºå®šï¼›æ‰€ä»¥algorithmæƒ³è¦ç¡®å®šçš„å°±æ˜¯ä¸å˜çš„é‡ thetaé›† äº†ã€‚åœ¨ä»¥åçš„æ“ä½œä¸­ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è¦æ±‚å¾—çš„æ˜¯ thetaé›†ï¼Œæ‰€ä»¥æ›´å¤šçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šæŠŠ x çœ‹ä½œæ˜¯å¸¸é‡ï¼Œå´æŠŠ theta çœ‹ä½œæ˜¯å˜é‡ï¼Œæ‰€ä»¥ä¸€å®šä¸è¦ç³Šæ¶‚äº†ã€‚ å»ºæ¨¡è¯¯å·®ï¼ˆmodeling errorï¼‰ æˆ‘ä»¬é€‰æ‹©çš„å‚æ•°å†³å®šäº†æˆ‘ä»¬å¾—åˆ°çš„ç›´çº¿ç›¸å¯¹äºæˆ‘ä»¬çš„è®­ç»ƒé›†çš„å‡†ç¡®ç¨‹åº¦ï¼Œæ¨¡å‹æ‰€é¢„æµ‹çš„å€¼ä¸è®­ç»ƒé›†ä¸­å®é™…å€¼ä¹‹é—´çš„å·®è·å°±æ˜¯å»ºæ¨¡è¯¯å·®ï¼ˆmodeling errorï¼‰ã€‚ Descriptionï¼š Cost function sometimes is also called squared error function(å¹³æ–¹è¯¯å·®å‡½æ•°) Cost function ç”¨äºæè¿°å»ºæ¨¡è¯¯å·®çš„å¤§å°ã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œè¯¯å·®è¶Šå°ï¼ŒCost function å°±èƒ½å¤Ÿæ›´å¥½çš„ä¸æ•°æ®é›†ç›¸æ‹Ÿåˆï¼Œä½†è¿™å¹¶ä¸æ˜¯ç»å¯¹çš„ï¼Œå› ä¸ºè¿™æœ‰å¯èƒ½ä¼šäº§ç”Ÿè¿‡æ‹Ÿåˆé—®é¢˜ã€‚ æŠŠå®ƒè¿›ä¸€æ­¥çš„æ‹†å¼€æ¥çœ‹å°±åº”è¯¥æ˜¯è¿™æ ·å­çš„ï¼šJï¼ˆ theta(0),theta(1) ) = 1/2m * Sigma [1-m] {thetaï¼ˆ0ï¼‰+thetaï¼ˆ1ï¼‰* x(i) - y(i)}(å¯èƒ½å°±æˆ‘ä¸€äººèƒ½çœ‹æ‡‚ã€‚ã€‚) å¹³æ–¹è¯¯å·®å‡½æ•°åªæ˜¯ Cost function ä¸­çš„ä¸€ç§å½¢å¼ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ä¹‹æ‰€ä»¥é€‰æ‹©squared error function ï¼Œ æ˜¯å› ä¸ºå¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å›å½’é—®é¢˜ï¼Œå®ƒæ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚è¿˜æœ‰å…¶ä»–çš„ä»£ä»·å‡½æ•°ä¹Ÿèƒ½å¾ˆå¥½åœ°å‘æŒ¥ä½œç”¨ï¼Œä½†æ˜¯å¹³æ–¹è¯¯å·®ä»£ä»·å‡½æ•°å¯èƒ½æ˜¯è§£å†³å›å½’é—®é¢˜æœ€å¸¸ç”¨çš„æ‰‹æ®µäº†ã€‚ Gradient descent(æ¢¯åº¦ä¸‹é™) algorithm Ideaï¼š æˆ‘ä»¬å·²ç»çŸ¥é“äº†ï¼Œ**hï¼ˆthetaï¼‰**æ˜¯ç”¨æ¥æ‹Ÿåˆæ•°æ®é›†çš„å‡½æ•°ï¼Œå¦‚æœæ‹Ÿåˆçš„å¥½ï¼Œé‚£ä¹ˆé¢„æµ‹çš„ç»“æœä¸ä¼šå·®ã€‚Jï¼ˆthetaé›†ï¼‰è¢«ç§°ä¸ºä»£ä»·å‡½æ•°ï¼Œå¦‚æœä»£ä»·å‡½æ•°è¾ƒå°ï¼Œåˆ™è¯´æ˜æ‹Ÿåˆçš„å¥½ã€‚è€Œæ­¤æ—¶çš„thetaé›†å°±æ˜¯æˆ‘ä»¬å°±æ˜¯é—®é¢˜çš„è§£äº†ã€‚ æˆ‘ä»¬å·²ç»çŸ¥é“äº†æˆ‘ä»¬çš„ç›®æ ‡thetaé›†ï¼Œé‚£ä¹ˆå¦‚ä½•æ±‚å¾—thetaé›†å‘¢ï¼Ÿ Gradient descentï¼ Usage: Start with some para0,para1â€¦ Keep changing paras to reduce J (para0,para1) to find some local minimum Start with different parameters , we may get different result ï¼ˆæ‰€ä»¥ï¼Œæˆ‘ä»¬ä¿è¯J(thetaé›†)ä¸å„ä¸ªthetaæ„æˆçš„å‡½æ•°æ˜¯ä¸€ä¸ªå‡¸å‡½æ•°ï¼Œæ¢å¥è¯è¯´ä¿è¯å­˜åœ¨å”¯ä¸€çš„æå°å€¼=æå°å€¼ï¼‰ Until we hopefully end up at a minimum Formulaï¼š æ‰¹é‡æ¢¯åº¦ä¸‹é™å…¬å¼ &lt;a&gt; is learning rate which control how big a step we use to upgrade the paras. So if the &lt;a&gt; is too small , the algorithm will run slowly ; but if &lt;a&gt; is too big , it can even make it away from the minimum point . å¦å¤–ï¼Œæ¯æ¬¡thetaä¸‹é™çš„å¹…åº¦éƒ½æ˜¯ä¸ä¸€æ ·çš„ï¼Œè¿™æ˜¯å› ä¸ºå®é™…ä¸Šthetaæ¯æ¬¡ä¸‹é™å¤šå°‘æ˜¯ç”± ã€&lt;a&gt;*Derivative Termã€‘è¿™ä¸ªæ•´ä½“å†³å®šçš„ï¼Œ&lt;a&gt;æ˜¯ä¸‹é™å¹…åº¦çš„é‡è¦å†³å®šå› å­ï¼Œä½†å¹¶ä¸æ˜¯å…¨éƒ¨ã€‚ åœ¨æ¯æ¬¡ä¸‹é™ä»¥åï¼ŒDerivative Terméƒ½ä¼šå˜å°ï¼Œä¸‹ä¸€æ¬¡ä¸‹é™çš„å¹…åº¦ä¹Ÿä¼šå˜å°ï¼Œå¦‚æ­¤ï¼Œæ§åˆ¶æ¢¯åº¦æ…¢æ…¢çš„ä¸‹é™ï¼Œç›´åˆ°Derivative Term=0ã€‚ the rest of the formula is the Derivative Term(å¯¼æ•°é¡¹) of theta In this image,we can find that no matter the Derivative Term of theta is positive or negative , the Gradient descent algorithm works well the Details of the formulaï¼š Attention: With the changing of the paras,the derivative term is changing too. So: Make sure updating the paras at the same time.ï¼ˆsimultaneous updateåŒæ­¥æ›´æ–°ï¼‰ For whatï¼Ÿ Because the derivative should use the former para rather than the new. How: åœ¨å…¨éƒ¨thetaæ²¡æœ‰æ›´æ–°å®Œæˆä¹‹å‰ï¼Œæˆ‘ä»¬ç”¨tempé›†æ¥æš‚æ—¶å­˜å‚¨å·²ç»æ›´æ–°å®Œæˆçš„thetaé›†ï¼Œç›´åˆ°æœ€åä¸€ä¸ªthetaä¹Ÿæ›´æ–°å®Œæˆä¹‹åï¼Œæˆ‘ä»¬æ‰å†ç”¨å­˜å‚¨åœ¨tempé›†é‡Œçš„æ–°thetaé›†æ›´æ–°åŸthetaé›†ã€‚ Gradient descent for linear regression â€œBatchâ€ Gradient descent ï¼š Each step of gradient descent uses all the training examples . åœ¨æ¢¯åº¦ä¸‹é™å…¬å¼ä¸­å­˜åœ¨æŸä¸€é¡¹éœ€è¦å¯¹æ‰€æœ‰mä¸ªè®­ç»ƒæ ·æœ¬æ±‚å’Œã€‚ Ideaï¼š å°†æ¢¯åº¦ä¸‹é™ä¸ä»£ä»·å‡½æ•°ç›¸ç»“åˆ The result: This is the result after derivation. And we can find that comparing with the first formula , the second has a x term at the end . It does tell us something that the variable is para0 and para1 , not the x or y . Although they looks more like variables. After derivation , for the first formula we have nothing left , for the second , we just has a constant term(å¸¸æ•°é¡¹) x left . Finallyï¼š We have other ways to computing Jï¼ˆthetaï¼‰ï¼š æ­£è§„æ–¹ç¨‹(normal equations)ï¼Œå®ƒå¯ä»¥åœ¨ä¸éœ€è¦å¤šæ­¥æ¢¯åº¦ä¸‹é™çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½è§£å‡ºä»£ä»·å‡½æ•°çš„æœ€å°å€¼ã€‚ å®é™…ä¸Šåœ¨æ•°æ®é‡è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œæ¢¯åº¦ä¸‹é™æ³•æ¯”æ­£è§„æ–¹ç¨‹è¦æ›´é€‚ç”¨ä¸€äº›ã€‚ Part 3 ï¼šçº¿æ€§ä»£æ•°(Linear Algebra) çŸ©é˜µå’Œå‘é‡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼æ¥ç»„ç»‡å¤§é‡çš„æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å½“æˆ‘ä»¬å¤„ç†å·¨å¤§çš„è®­ç»ƒé›†æ—¶ï¼Œå®ƒçš„ä¼˜åŠ¿å°±æ›´åŠ çš„æ˜æ˜¾ã€‚ Matrices and vectors (çŸ©é˜µå’Œå‘é‡) Matrix: Rectangular array of numbers. äºŒç»´æ•°ç»„ The dimension(ç»´åº¦) of the matrix : number of row * number of columns Matrix Elements : How to refer to the element Vector: An n*1 matrix . ( n dimensional vector. nç»´å‘é‡) How to refer to the element in it ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190726084319572.png) We use 1-indexed vector Cvention: We use upper case to refer to the matrix and lower case refer to the vector. Addition and scalar multiplication (åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•) è¡Œåˆ—æ•°ç›¸ç­‰æ‰èƒ½è¿›è¡ŒåŠ æ³•è¿ç®—ï¼š æ ‡é‡ä¸çŸ©é˜µçš„ä¹˜æ³•ï¼š Matrix-vector multiplication(çŸ©é˜µ-å‘é‡ä¹˜æ³•) Detailsï¼š Aâ€™s row number must be equal with Bâ€™s column number . Matrix-matrix multiplication(çŸ©é˜µ-çŸ©é˜µä¹˜æ³•) Detailsï¼š Just as matrix-vector , the most important thing is to make sure Aâ€™s columns number is equal with Bâ€™s rows number . Câ€™s columns number is equal with Bâ€™s . n fact , before multiply between Matrixs , we can divide B into several vectors by columns, and then we can make the Matrix- Vector multiplication. A column of B corresponding a hypothesis , and a column of C corresponding a set of prediction . Matrix multiplication properties(çŸ©é˜µä¹˜æ³•ç‰¹å¾) Properties : Do not enjoy commutative property(ä¸éµå¾ªäº¤æ¢å¾‹) Enjoy the associative property(éµå¾ªç»“åˆå¾‹) Identity Matrix(å•ä½çŸ©é˜µ): ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190726084424308.png) Featureï¼š å®ƒæ˜¯ä¸ªæ–¹é˜µï¼Œä»å·¦ä¸Šè§’åˆ°å³ä¸‹è§’çš„å¯¹è§’çº¿ï¼ˆç§°ä¸ºä¸»å¯¹è§’çº¿ï¼‰ä¸Šçš„å…ƒç´ å‡ä¸º1ã€‚é™¤æ­¤ä»¥å¤–å…¨éƒ½ä¸º0ã€‚å®ƒçš„ä½œç”¨å°±åƒæ˜¯ä¹˜æ³•ä¸­çš„1ã€‚ Usageï¼š is the identity matrix . A(m , n) * I(n , n) = I(m , m) * A(m , n) M1â€™s column number should be equal with M2â€™s row number . Matrix inverse operation(çŸ©é˜µçš„é€†è¿ç®—) Attentionï¼š Only square matrix have inverse . Not all number have inverse , such as 0 . Matrix transpose operation(çŸ©é˜µçš„è½¬ç½®è¿ç®—) Definition: Official definition: å³å°†Aä¸Bçš„x , yåæ ‡äº’æ¢ çŸ©é˜µè½¬ç½®çš„åŸºæœ¬æ€§è´¨ï¼š Part 4ï¼šå¤šå˜é‡çº¿æ€§å›å½’(Linear Regression with Multiple Variables) åœ¨ä¹‹å‰çš„éƒ¨åˆ†ä¸­ï¼Œå­¦ä¹ äº†å•å˜é‡/ç‰¹å¾çš„å›å½’æ¨¡å‹ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†è¿›è¡Œæ·±å…¥ï¼Œå°†å˜é‡çš„æ•°é‡å¢åŠ ï¼Œè®¨è®ºå¤šå˜é‡çš„çº¿æ€§å›å½’é—®é¢˜ã€‚ Multiple feature(å¤šåŠŸèƒ½) Exampleï¼š å¯¹äºæ–°é—®é¢˜çš„æ³¨é‡Šï¼š So the final prediction result is a vector too . å¯ä»¥å‘ç°ï¼Œå¤šå˜é‡çº¿æ€§å›å½’å’Œå•å˜é‡çº¿æ€§å›å½’åŸºæœ¬æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„å˜åŒ–å°±æ˜¯thetaå˜æˆäº†thetaé›†ï¼›å˜é‡xå˜æˆäº†xé›†ã€‚ Gradient descent for multiple variables(å¤šå…ƒæ¢¯åº¦ä¸‹é™æ³•) Ideaï¼š ä¸å•å˜é‡çº¿æ€§å›å½’ç±»ä¼¼ï¼Œåœ¨å¤šå˜é‡çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ„å»ºä¸€ä¸ªä»£ä»·å‡½æ•°ï¼Œåˆ™è¿™ä¸ªä»£ä»·å‡½æ•°æ˜¯æ‰€æœ‰å»ºæ¨¡è¯¯å·®çš„å¹³æ–¹å’Œ ä»£ä»·å‡½æ•°ï¼š æ¢¯åº¦ä¸‹é™æ³•åº”ç”¨åˆ°ä»£ä»·å‡½æ•°å¹¶æ±‚å¯¼ä¹‹åå¾—åˆ°ï¼š Feature scaling(ç‰¹å¾ç¼©æ”¾): Purpose: Make the gradient descent runs faster . å°¤å…¶æ˜¯å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚ Idea: Make sure number are in similar scale . Usageï¼š The range limit ï¼š Get every feature range from -3 to 3 or -1/3 to 1/3 . Thatâ€™s fine . In fact there is no problems , because the feature is a real number ! Thatâ€™s means it will never change . x æ˜¯å˜é‡ï¼Œå¹¶ä¸”å®ƒæ˜¯å·²çŸ¥çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä»data set ä¸­ç›´æ¥è·å¾—ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å®ƒè¿›è¡Œç‰¹å¾ç¼©æ”¾ï¼›thetaæ˜¯æœªçŸ¥çš„ï¼Œæ˜¯æˆ‘ä»¬è¦æ±‚çš„ç›®æ ‡ã€‚ Summaryï¼š ç‰¹å¾ç¼©æ”¾ç¼©æ”¾çš„æ˜¯ å·²çŸ¥çš„ å˜é‡x ï¼Œè€Œä¸æ˜¯ æœªçŸ¥çš„ å¸¸é‡theta ã€‚ è¿›è¡Œç‰¹å¾ç¼©æ”¾æ˜¯ä¸ºäº†ä¸ºäº†æé«˜æ¢¯åº¦ä¸‹é™çš„é€Ÿåº¦ã€‚ Learning rate (å­¦ä¹ ç‡) Ideaï¼š If the gradient descent is working properly , then J should decrease after every iteration. æ¢¯åº¦ä¸‹é™ç®—æ³•æ”¶æ•›æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°æ ¹æ®æ¨¡å‹çš„ä¸åŒè€Œä¸åŒï¼Œæˆ‘ä»¬ä¸èƒ½æå‰é¢„çŸ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è¿­ä»£æ¬¡æ•°å’Œä»£ä»·å‡½æ•°çš„å›¾è¡¨æ¥è§‚æµ‹ç®—æ³•åœ¨ä½•æ—¶è¶‹äºæ”¶æ•›ã€‚ Conventionï¼š é€šå¸¸å¯ä»¥è€ƒè™‘çš„å­¦ä¹ ç‡&lt;a&gt;çš„æ•°å€¼ï¼š 0.01ï¼›0.03ï¼›0.1ï¼›0.3ï¼›1ï¼›3ï¼›10ï¼› Summary: If &lt;a&gt; is too small ïƒ  slow move If &lt;a&gt; is too big ïƒ  J may not decrease on every iteration . Features and Polynomial regression(å¤šå˜é‡å’Œå¤šé¡¹å¼å›å½’) Ideaï¼š Linear regression å¹¶ä¸é€‚åº”æ‰€æœ‰çš„æ•°æ®æ¨¡å‹ï¼Œæœ‰æ—¶æˆ‘ä»¬éœ€è¦æ›²çº¿æ¥é€‚åº”æˆ‘ä»¬çš„æ•°æ®ï¼Œæ¯”å¦‚ä¸€ä¸ªäºŒæ¬¡æ–¹æ¨¡å‹ï¼š é€šå¸¸æˆ‘ä»¬éœ€è¦å…ˆè§‚å¯Ÿæ•°æ®å†ç¡®å®šæœ€ç»ˆçš„æ¨¡å‹ Attentionï¼š å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚ Normal equation(å¸¸è§„æ–¹ç¨‹) Ideaï¼š ä»æ•°å­¦ä¸Šæ¥çœ‹ï¼Œæƒ³è¦æ±‚å¾—Jï¼ˆtheta) çš„æœ€å°å€¼ï¼Œæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯å¯¹å®ƒè¿›è¡Œæ±‚å¯¼ï¼Œä»¤å®ƒçš„å¯¼æ•°=0ï¼Œå¾—å‡ºæ­¤æ—¶çš„thetaå°±æ˜¯æœ€ç»ˆçš„ç»“æœ è€Œæ­£è§„æ–¹ç¨‹æ–¹æ³•çš„ç¡®å°±æ˜¯è¿™ä¹ˆå¹²çš„ Attentionï¼š å¯¹äºé‚£äº›ä¸å¯é€†çš„çŸ©é˜µï¼ˆé€šå¸¸æ˜¯å› ä¸ºç‰¹å¾ä¹‹é—´ä¸ç‹¬ç«‹ï¼Œå¦‚åŒæ—¶åŒ…å«è‹±å°ºä¸ºå•ä½çš„å°ºå¯¸å’Œç±³ä¸ºå•ä½çš„å°ºå¯¸ä¸¤ä¸ªç‰¹å¾ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ç‰¹å¾æ•°é‡å¤§äºè®­ç»ƒé›†çš„æ•°é‡ï¼‰ï¼Œæ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯ä¸èƒ½ç”¨çš„ã€‚ è¿™å°±æ˜¯æ­£è§„æ–¹ç¨‹çš„å±€é™æ€§ Usage: Exampleï¼š The Comparison of Gradient descent and Normal equation Gradient descent : Need to choose Need many iterations Normal equation : Need to compute [xâ€™*x]^(-1) , So if n is large it will run slowly . Do not fit every situations (åªé€‚ç”¨äºçº¿æ€§æ¨¡å‹ï¼Œä¸é€‚åˆé€»è¾‘å›å½’æ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹). Summary: åªè¦ç‰¹å¾å˜é‡çš„æ•°ç›®å¹¶ä¸å¤§ï¼Œæ ‡å‡†æ–¹ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è®¡ç®—å‚æ•°çš„æ›¿ä»£æ–¹æ³•ã€‚å…·ä½“åœ°è¯´ï¼Œåªè¦ç‰¹å¾å˜é‡æ•°é‡å°äºä¸€ä¸‡ï¼Œæˆ‘é€šå¸¸ä½¿ç”¨æ ‡å‡†æ–¹ç¨‹æ³•ï¼Œè€Œä¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€‚ Normal equation ä¸å¯é€†æƒ…å†µ Ideaï¼š ç¬¬ä¸€è¦é™ä½featuresä¹‹é—´çš„ç›¸å…³åº¦ ç¬¬äºŒè¦é€‚å½“çš„å‡å°‘features çš„æ•°é‡ Ideaï¼š Part 5ï¼šOctaveæ•™ç¨‹(Octave Tutorial) è¿™é‡Œæ˜¯ä¸€ä¸ªğŸ”—â€“&gt; OctaveåŸºæœ¬è¯­æ³• Part 6ï¼šé€»è¾‘å›å½’(Logistic Regression) Vectorizationï¼ˆçŸ¢é‡åŒ–ï¼‰ Exampleï¼š ä½¿ç”¨å‘é‡çš„è½¬ç½®æ¥æ±‚ä¸¤ä¸ªå‘é‡çš„å†…ç§¯ï¼Œå¯ä»¥éå¸¸æ–¹ä¾¿çš„å¤„ç†å¤§é‡çš„æ•°æ®ã€‚ è¿™å°±æ˜¯æ‰€è°“çš„çŸ¢é‡åŒ–çš„çº¿æ€§å›å½’ã€‚ Classificationï¼ˆåˆ†ç±»é—®é¢˜ï¼‰ Convention in Classification problems: 0 equal to empty ; 1 means something is there. Attention: In classification problems , we want to predict &lt;ç¦»æ•£å€¼&gt; Hypothesis Representationï¼ˆå‡è¯´è¡¨ç¤ºï¼‰ Questionï¼š What kind of function we need to use to represent our hypothesis in the classification problems? Linear function image may out of the range[-1,1] Idea: Make sure Import : Logistic regression(é€»è¾‘å›å½’) Hjypothesisï¼š Statusï¼š The most popular machine learning algorithm. ä¸€ç§éå¸¸å¼ºå¤§ï¼Œç”šè‡³å¯èƒ½ä¸–ç•Œä¸Šä½¿ç”¨æœ€å¹¿æ³›çš„ä¸€ç§åˆ†ç±»ç®—æ³•ã€‚ Attentionï¼š Although we called it â€˜regressionâ€™ , but in fact it is used in classification Problems. Logistic function: Decision boundary å†³ç­–è¾¹ç•Œä¸æ˜¯è®­ç»ƒé›†çš„å±æ€§ï¼Œè€Œæ˜¯å‡è®¾æœ¬èº«ä»¥åŠå…¶å‚æ•°çš„å±æ€§ï¼›æ‰€ä»¥è¯´ åªè¦ç»™å®šäº†thetaé‚£ä¹ˆå†³ç­–è¾¹ç•Œå°±å·²ç»ç¡®å®šäº†ã€‚ï¼ˆthetaé›†ç¡®å®šäº†ï¼Œé‚£ä¹ˆå‡½æ•°å°±ç¡®å®šäº†ï¼Œå†³ç­–è¾¹ç•Œè‡ªç„¶ä¹Ÿå°±è¢«ç¡®å®šäº†ï¼‰ è®­ç»ƒé›†â€“&gt;ç”Ÿæˆthetaâ€“&gt;ç¡®å®šå†³ç­–è¾¹ç•Œ Logistic Regression Cost functionï¼ˆé€»è¾‘å›å½’çš„ä»£ä»·å‡½æ•°ï¼‰ Question: How to choose parameter thetaï¼Ÿ å› ä¸ºæˆ‘ä»¬æ”¹å˜äº†ä»£ä»·å‡½æ•°çš„æ¨¡å‹h(theta) ,è¿™æ ·J(theta)-theta çš„å‡½æ•°å›¾åƒå°±ä¼šç”±è®¸å¤šä¸ªæå°å€¼ï¼Œè¿™å¯¹äºæˆ‘ä»¬æ±‚å¾—thetaæ˜¯ä¸åˆ©çš„ Idea: ä¸ºäº†ä¿è¯Cost function æ˜¯å‡¸å‡½æ•°ï¼Œæˆ‘ä»¬é€‰æ‹©æ›´æ¢ä»£ä»·å‡½æ•°ã€‚ åœ¨å½“æ—¶å­¦ä»£ä»·å‡½æ•°çš„æ—¶å€™å°±ç›´åˆ°ï¼Œä»£ä»·å‡½æ•°å¹¶ä¸åªæ˜¯æœ‰ä¸€ç§å½¢å¼ã€‚å®é™…ä¸Šä»£ä»·å‡½æ•°åªæ˜¯ä¸ºäº†è¡¨å¾è¯¯å·®çš„çš„å¤§å°ï¼Œæ‰€ä»¥åªè¦æ˜¯èƒ½å¤Ÿåˆç†çš„è¡¨ç¤ºè¯¯å·®çš„å‡½æ•°éƒ½å¯ä»¥å½“ä½œæ˜¯è¯¯å·®å‡½æ•°ã€‚ Simplified cost function: åŸºäºåªæœ‰y=1 || y=0 ä¸¤ç§æƒ…å†µ æ¨ªåæ ‡è¡¨ç¤ºçš„æ˜¯hï¼ˆx) ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„æ¨¡å‹å¾—å‡ºæ¥çš„ y å€¼ ï¼Œ çºµåæ ‡æ˜¯Cost functionï¼Œå¯¹äº y=1 å’Œ y=0 æœ‰ä¸åŒçš„å›¾åƒçš„ï¼Œè¿™ä¸¤ä¸ªå›¾åƒéƒ½æ˜¯åœ¨ä¸€ç«¯ç­‰äº0 ï¼Œ åœ¨å¦ä¸€ç«¯è¶‹äºæ— ç©·å¤§ã€‚ Logistic Regression Gradient Descentï¼ˆé€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™ï¼‰ Ideaï¼š æˆ‘ä»¬å·²ç»æ‰¾åˆ°äº†åˆé€‚çš„Cost function äº† ï¼Œç°åœ¨å°±å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¢¯åº¦ä¸‹é™äº† Details: å¯¹äºæ¢¯åº¦ä¸‹é™ï¼Œæ— è®ºCost function æ˜¯ä»€ä¹ˆ ï¼Œå®ƒçš„ algorithmæ˜¯å§‹ç»ˆä¸å˜çš„ æˆ‘ä»¬å°†Cost function å¸¦å…¥, ç»“æœå¦‚ä¸‹ï¼š Attention: è™½ç„¶å¾—åˆ°çš„æ¢¯åº¦ä¸‹é™ç®—æ³•è¡¨é¢ä¸Šçœ‹ä¸Šå»ä¸çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸€æ ·ï¼Œä½†æ˜¯è¿™é‡Œçš„ hï¼ˆxï¼‰ä¸çº¿æ€§å›å½’ä¸­ä¸åŒï¼ˆæ­¤å¤„çš„æ˜¯é€»è¾‘å›å½’å‡½æ•°è€Œä¹‹å‰çš„æ˜¯çº¿æ€§å‡½æ•°ï¼‰æ‰€ä»¥å®é™…ä¸Šé€»è¾‘å‡½æ•°çš„æ¢¯åº¦ä¸‹é™ï¼Œè·Ÿçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™æ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„ä¸œè¥¿ã€‚ å¦å¤–ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•ä¹‹å‰ï¼Œè¿›è¡Œç‰¹å¾ç¼©æ”¾ä¾æ—§æ˜¯éå¸¸å¿…è¦çš„ã€‚ ä¸€äº›æ¢¯åº¦ä¸‹é™ç®—æ³•ä¹‹å¤–çš„é€‰æ‹©ï¼š é™¤äº†æ¢¯åº¦ä¸‹é™ç®—æ³•ä»¥å¤–ï¼Œè¿˜æœ‰ä¸€äº›å¸¸è¢«ç”¨æ¥ä»¤ä»£ä»·å‡½æ•°æœ€å°çš„ç®—æ³•ï¼Œè¿™äº›ç®—æ³•æ›´åŠ å¤æ‚å’Œä¼˜è¶Šï¼Œè€Œä¸”é€šå¸¸ä¸éœ€è¦äººå·¥é€‰æ‹©å­¦ä¹ ç‡ï¼Œé€šå¸¸æ¯”æ¢¯åº¦ä¸‹é™ç®—æ³•è¦æ›´åŠ å¿«é€Ÿã€‚è¿™äº›ç®—æ³•æœ‰ï¼šå…±è½­æ¢¯åº¦ï¼ˆConjugate Gradientï¼‰ï¼Œå±€éƒ¨ä¼˜åŒ–æ³•(Broyden fletcher goldfarb shann,BFGS)å’Œæœ‰é™å†…å­˜å±€éƒ¨ä¼˜åŒ–æ³•(LBFGS) ï¼Œfminuncæ˜¯ matlabå’Œoctave ä¸­éƒ½å¸¦çš„ä¸€ä¸ªæœ€å°å€¼ä¼˜åŒ–å‡½æ•°ï¼Œä½¿ç”¨æ—¶æˆ‘ä»¬éœ€è¦æä¾›ä»£ä»·å‡½æ•°å’Œæ¯ä¸ªå‚æ•°çš„æ±‚å¯¼ï¼Œä¸‹é¢æ˜¯ octave ä¸­ä½¿ç”¨ fminunc å‡½æ•°çš„ä»£ç ç¤ºä¾‹ï¼š function [jVal, gradient] = costFunction(theta) â€‹ jVal = [...code to compute J(theta)...]; gradient = [...code to compute derivative of J(theta)...]; end options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, &#39;100&#39;); initialTheta = zeros(2,1); [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); %å¾ˆé†‰äººï¼Œè¿™è°ä½›å¾—äº†å•Š Advanced Optimizationï¼ˆé«˜çº§ä¼˜åŒ–ï¼‰ é›¾é‡Œçœ‹èŠ±ã€‚ã€‚ã€‚ æ¢¯åº¦ä¸‹é™å¹¶ä¸æ˜¯æˆ‘ä»¬æ±‚thetaæœ€å¥½çš„æ–¹æ³•ï¼Œè¿˜æœ‰å…¶ä»–ä¸€äº›ç®—æ³•ï¼Œæ›´é«˜çº§ã€æ›´å¤æ‚ã€‚ å…±è½­æ¢¯åº¦æ³• BFGS (å˜å°ºåº¦æ³•) å’ŒL-BFGS (é™åˆ¶å˜å°ºåº¦æ³•) å°±æ˜¯å…¶ä¸­ä¸€äº›æ›´é«˜çº§çš„ä¼˜åŒ–ç®—æ³• ä¼˜ç‚¹ï¼š é€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨é€‰æ‹©å­¦ä¹ ç‡ ï¼› ç®—æ³•çš„è¿è¡Œé€Ÿåº¦é€šå¸¸è¿œè¿œè¶…è¿‡æ¢¯åº¦ä¸‹é™ã€‚ Octaveä»£ç å®ç°ï¼š %å®šä¹‰Cost function å‡½æ•° function [jVal, gradient]=costFunction(theta) ã€€ã€€jVal=(theta(1)-5)^2+(theta(2)-5)^2; ã€€ã€€gradient=zeros(2,1); ã€€ã€€gradient(1)=2*(theta(1)-5); ã€€ã€€gradient(2)=2*(theta(2)-5); end è°ƒç”¨è¯¥å‡½æ•°ï¼š ä½ è¦è®¾ç½®å‡ ä¸ªoptionsï¼Œè¿™ä¸ªoptionså˜é‡ä½œä¸ºä¸€ä¸ªæ•°æ®ç»“æ„å¯ä»¥å­˜å‚¨ä½ æƒ³è¦çš„optionsï¼Œ æ‰€ä»¥ GradObj å’ŒOnï¼Œè¿™é‡Œè®¾ç½®æ¢¯åº¦ç›®æ ‡å‚æ•°ä¸ºæ‰“å¼€(on)ï¼Œè¿™æ„å‘³ç€ä½ ç°åœ¨ç¡®å®è¦ç»™è¿™ ä¸ªç®—æ³•æä¾›ä¸€ä¸ªæ¢¯åº¦ï¼Œç„¶åè®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ¯”æ–¹è¯´100ï¼Œæˆ‘ä»¬ç»™å‡ºä¸€ä¸ª çš„çŒœæµ‹åˆå§‹ å€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ª2Ã—1çš„å‘é‡ï¼Œé‚£ä¹ˆè¿™ä¸ªå‘½ä»¤å°±è°ƒç”¨fminuncï¼Œè¿™ä¸ª@ç¬¦å·è¡¨ç¤ºæŒ‡å‘æˆ‘ä»¬åˆšåˆš å®šä¹‰çš„costFunction å‡½æ•°çš„æŒ‡é’ˆã€‚å¦‚æœä½ è°ƒç”¨å®ƒï¼Œå®ƒå°±ä¼šä½¿ç”¨ä¼—å¤šé«˜çº§ä¼˜åŒ–ç®—æ³•ä¸­çš„ä¸€ ä¸ªï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥æŠŠå®ƒå½“æˆæ¢¯åº¦ä¸‹é™ï¼Œåªä¸è¿‡å®ƒèƒ½è‡ªåŠ¨é€‰æ‹©å­¦ä¹ é€Ÿç‡ï¼Œä½ ä¸éœ€è¦è‡ªå·±æ¥ åšã€‚ç„¶åå®ƒä¼šå°è¯•ä½¿ç”¨è¿™äº›é«˜çº§çš„ä¼˜åŒ–ç®—æ³•ï¼Œå°±åƒåŠ å¼ºç‰ˆçš„æ¢¯åº¦ä¸‹é™æ³•ï¼Œä¸ºä½ æ‰¾åˆ°æœ€ä½³ çš„å€¼ã€‚ å­¦åˆ°çš„ä¸»è¦å†…å®¹æ˜¯ï¼šå†™ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒèƒ½è¿”å›ä»£ä»·å‡½æ•°å€¼ã€æ¢¯åº¦å€¼ï¼Œå› æ­¤è¦æŠŠè¿™ä¸ªåº”ç”¨åˆ°é€»è¾‘å›å½’ï¼Œæˆ–è€…ç”šè‡³çº¿æ€§å›å½’ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥æŠŠè¿™äº›ä¼˜åŒ–ç®—æ³•ç”¨äºçº¿æ€§å›å½’ï¼Œä½ éœ€è¦åšçš„å°±æ˜¯è¾“å…¥åˆé€‚çš„ä»£ç æ¥è®¡ç®—è¿™é‡Œçš„è¿™äº›ä¸œè¥¿ã€‚ Multiclass Classification_ One-vs-all ï¼ˆå¤šç±»åˆ«åˆ†ç±»ï¼šä¸€å¯¹å¤šï¼‰ Part 7ï¼šæ­£åˆ™åŒ–(Regularization) æœ‰æ—¶æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç®—æ³•åº”ç”¨åˆ°æŸäº›ç‰¹å®šçš„æœºå™¨å­¦ä¹ åº”ç”¨æ—¶ï¼Œé€šè¿‡å­¦ä¹ å¾—åˆ°çš„å‡è®¾å¯èƒ½èƒ½å¤Ÿéå¸¸å¥½åœ°é€‚åº”è®­ç»ƒé›†ï¼ˆä»£ä»·å‡½æ•°å¯èƒ½å‡ ä¹ä¸º0ï¼‰ï¼Œä½†æ˜¯å¯èƒ½ä¼šä¸èƒ½æ¨å¹¿åˆ°æ–°çš„æ•°æ®ã€‚è¿™å°±æ˜¯è¿‡æ‹Ÿåˆ(over-fitting)çš„é—®é¢˜ã€‚ æ­£åˆ™åŒ–(regularization)çš„æŠ€æœ¯ï¼Œå®ƒå¯ä»¥æ”¹å–„æˆ–è€…å‡å°‘è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚ å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜çš„æ€è·¯ ä¸¢å¼ƒä¸€äº›ä¸èƒ½å¸®åŠ©æˆ‘ä»¬æ­£ç¡®é¢„æµ‹çš„ç‰¹å¾ã€‚å¯ä»¥æ˜¯æ‰‹å·¥é€‰æ‹©ä¿ç•™å“ªäº›ç‰¹å¾ï¼Œæˆ–è€…ä½¿ç”¨ä¸€äº›æ¨¡å‹é€‰æ‹©çš„ç®—æ³•æ¥å¸®å¿™ï¼ˆä¾‹å¦‚PCAï¼‰ æ­£åˆ™åŒ–ã€‚ ä¿ç•™æ‰€æœ‰çš„ç‰¹å¾ï¼Œä½†æ˜¯å‡å°‘å‚æ•°çš„å¤§å°ï¼ˆmagnitudeï¼‰ã€‚ å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜çš„ä»£ä»·å‡½æ•° Ideaï¼š è¿‡æ‹Ÿåˆé—®é¢˜çš„äº§ç”Ÿå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºé«˜æ¬¡é¡¹ã€‚æ‰€ä»¥ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯å¯¹é«˜æ­¤é¡¹è¿›è¡Œé™åˆ¶ï¼Œå…·ä½“åšæ³•å°±æ˜¯åœ¨ä»£ä»·å‡½æ•°é‡Œå¯¹ä»–ä»¬æ–½åŠ æƒ©ç½šã€‚ åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°è¿™äº›å‚æ•°thetaçš„å€¼ï¼Œè¿™å°±æ˜¯æ­£åˆ™åŒ–çš„åŸºæœ¬æ–¹æ³•ã€‚ ä¾‹å¦‚ï¼š å‡å¦‚æˆ‘ä»¬æœ‰éå¸¸å¤šçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“å…¶ä¸­å“ªäº›ç‰¹å¾æˆ‘ä»¬è¦æƒ©ç½šï¼Œæˆ‘ä»¬å°†å¯¹æ‰€æœ‰çš„ç‰¹å¾è¿›è¡Œæƒ©ç½šï¼Œå¹¶ä¸”è®©ä»£ä»·å‡½æ•°æœ€ä¼˜åŒ–çš„è½¯ä»¶æ¥é€‰æ‹©è¿™äº›æƒ©ç½šçš„ç¨‹åº¦ã€‚è¿™æ ·çš„ç»“æœæ˜¯å¾—åˆ°äº†ä¸€ä¸ªè¾ƒä¸ºç®€å•çš„èƒ½é˜²æ­¢è¿‡æ‹Ÿåˆé—®é¢˜çš„å‡è®¾ï¼Œå°±åƒè¿™æ ·ï¼š å…¶ä¸­lambdaåˆç§°ä¸ºæ­£åˆ™åŒ–å‚æ•°ï¼ˆRguarization Parameterï¼‰ã€‚ æ³¨ï¼šæ ¹æ®æƒ¯ä¾‹ï¼Œæˆ‘ä»¬ä¸ theta0è¿›è¡Œæƒ©ç½šã€‚ **lamnda ** 38.Regularized Linear Regression(æ­£åˆ™åŒ–çº¿æ€§å›å½’) æ­£åˆ™åŒ–çº¿æ€§å›å½’çš„ä»£ä»·å‡½æ•°ä¸ºï¼š å¦‚æœæˆ‘ä»¬è¦ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ä»¤è¿™ä¸ªä»£ä»·å‡½æ•°æœ€å°åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¯¹theta0è¿›è¡Œæ­£åˆ™åŒ–ï¼Œæ‰€ä»¥æ¢¯åº¦ä¸‹é™ç®—æ³•å°†åˆ†ç±»è®¨è®ºï¼Œå¯¹theta0ç‰¹æ®Šå¤„ç†ã€‚ æˆ‘ä»¬å¯¹æ–¹ç¨‹è¿›è¡Œæ‹†è§£ï¼Œå¯ä»¥çœ‹å‡ºï¼Œå®é™…ä¸Šæ­£åˆ™åŒ–çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å˜åŒ–åœ¨äºï¼Œæ¯æ¬¡éƒ½åœ¨åŸæœ‰ç®—æ³•æ›´æ–°è§„åˆ™çš„åŸºç¡€ä¸Šä»¤å€¼å‡å°‘äº†ä¸€ä¸ªé¢å¤–çš„å¾ˆå°çš„å€¼ã€‚ æ­£è§„æ–¹ç¨‹æ¥æ±‚è§£æ­£åˆ™åŒ–çº¿æ€§å›å½’æ¨¡å‹ï¼š Regularized Logistic Regression ï¼ˆæ­£åˆ™åŒ–é€»è¾‘å›å½’æ¨¡å‹ï¼‰ ä»¿ç…§çº¿æ€§å›å½’æ­£åˆ™åŒ–çš„æ€è·¯ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿®æ”¹ä»£ä»·å‡½æ•°ï¼Œæ·»åŠ ä¸€ä¸ªæ­£åˆ™åŒ–çš„è¡¨è¾¾å¼å³å¯ï¼š","@type":"BlogPosting","url":"https://uzzz.org/2019/08/29/795257.html","headline":"Machine Learning Note Phase 1ï¼ˆ Doneï¼ï¼‰","dateModified":"2019-08-29T00:00:00+08:00","datePublished":"2019-08-29T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://uzzz.org/2019/08/29/795257.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Machine Learning Note Phase 1ï¼ˆ Doneï¼ï¼‰</h1>
        
        
        <ul style="display: block;">
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>æŸšå­ç¤¾åŒº</strong></a></li>
 	    <li><a href="/donate/" style="line-height: unset;" target="_blank"><strong>Donate</strong></a></li>
        </ul>
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
<!-- match content ads -->
	        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
			<ins class="adsbygoogle"
			     style="display:block"
			     data-ad-format="autorelaxed"
			     data-ad-client="ca-pub-8889449066804352"
			     data-ad-slot="1928667997"></ins>
			<script>
			     (adsbygoogle = window.adsbygoogle || []).push({});
			</script>	



        <div id="article_content" class="article_content clearfix">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css"> 
 <div id="content_views" class="markdown_views prism-tomorrow-night-eighties"> 
  <!-- flowchart ç®­å¤´å›¾æ ‡ å‹¿åˆ  --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h1><a id="Machine_Learning_0"></a>Machine Learning</h1> 
  <blockquote> 
   <p>è¿™æ˜¯ç¬¬ä¸€ä»½æœºå™¨å­¦ä¹ ç¬”è®°ï¼Œåˆ›å»ºäº2019å¹´7æœˆ26æ—¥ï¼Œå®Œæˆäº2019å¹´8æœˆ2æ—¥ã€‚</p> 
  </blockquote> 
  <p>è¯¥ç¬”è®°åŒ…æ‹¬å¦‚ä¸‹éƒ¨åˆ†:</p> 
  <blockquote> 
   <ol> 
    <li>å¼•è¨€(Introduction)</li> 
    <li>å•å˜é‡çº¿æ€§å›å½’(Linear Regression with One Variable)</li> 
    <li>çº¿æ€§ä»£æ•°(Linear Algebra)</li> 
    <li>å¤šå˜é‡çº¿æ€§å›å½’(Linear Regression with Multiple Variables)</li> 
    <li>Octave</li> 
    <li>é€»è¾‘å›å½’(Logistic Regression)</li> 
    <li>æ­£åˆ™åŒ–(Regularization)</li> 
   </ol> 
  </blockquote> 
  <h2><a id="Part_1_Introduction_13"></a>Part 1 ï¼šIntroduction</h2> 
  <ol> 
   <li>Machine Learningï¼š</li> 
  </ol> 
  <ul> 
   <li>Idea: 
    <ul> 
     <li>ç»™äºˆæœºå™¨è‡ªå·±å­¦ä¹ è§£å†³é—®é¢˜çš„èƒ½åŠ›</li> 
     <li><strong>å…ˆå¾—è‡ªå·±ææ˜ç™½äº†æ‰æœ‰å¯èƒ½æ•™ä¼šæœºå™¨è‡ªå·±å­¦ä¹ å‘ã€‚ã€‚</strong></li> 
    </ul> </li> 
   <li>Machine learning algorithms: 
    <ul> 
     <li>Supervised learning</li> 
     <li>Unsupervised learning</li> 
     <li>Othersï¼š Recommender systems(æ¨èç³»ç»Ÿ) ; Reinforcement learning(å¼ºåŒ–å­¦ä¹ )</li> 
    </ul> </li> 
  </ul> 
  <ol start="2"> 
   <li>Supervised learning(ç›‘ç£å­¦ä¹ )</li> 
  </ol> 
  <ul> 
   <li> <p>Use to do :</p> 
    <ul> 
     <li>Make predictions</li> 
    </ul> </li> 
   <li> <p>Featureï¼š</p> 
    <ul> 
     <li>Right answers set is GIVEN</li> 
    </ul> </li> 
   <li> <p>Including:</p> 
    <ul> 
     <li>Regression problems(å›å½’é—®é¢˜)<br> æˆ‘ä»¬ç»™å‡ºæ­£ç¡®çš„æ•°æ®é›†ï¼ŒæœŸæœ›ç®—æ³•èƒ½å¤Ÿé¢„æµ‹å‡ºcontinuous values(è¿ç»­å€¼)ã€‚</li> 
     <li>Classification problems(åˆ†ç±»é—®é¢˜)<br> æˆ‘ä»¬ç»™å‡ºæ­£ç¡®çš„æ•°æ®é›†ï¼ŒæœŸæœ›ç®—æ³•èƒ½å¤Ÿé¢„æµ‹å‡ºdiscrete valued(ç¦»æ•£å€¼) ã€‚<br> ï¼ˆthe features can be more than 2 , Even infinite.ï¼‰</li> 
    </ul> </li> 
  </ul> 
  <ol start="3"> 
   <li>Unsupervised learningï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰</li> 
  </ol> 
  <ul> 
   <li> <p>Featureï¼š</p> 
    <ul> 
     <li>We just give the data and hope it can automatically find structure in the data for us.</li> 
    </ul> </li> 
   <li> <p>Includingï¼š</p> 
    <ul> 
     <li>Clustering algorithmsï¼ˆèšç±»ç®—æ³•ï¼‰</li> 
    </ul> </li> 
   <li> <p>Problemsï¼š</p> 
    <ul> 
     <li>In fact I find the unsupervised learning looks like the classification problems . The only difference is whether or not <s>the data set is GIVEN</s><strong>Give the correct data ahead of time</strong> .</li> 
    </ul> </li> 
   <li> <p>Summaryï¼š</p> 
    <ul> 
     <li>We just give the Disorderly data sets , and hope the algorithm can automatically find out the structure of the data , and divide it into different clusters wisely.</li> 
    </ul> </li> 
  </ul> 
  <ol start="4"> 
   <li>Octave</li> 
  </ol> 
  <p>Usageï¼š</p> 
  <ul> 
   <li>ç”¨octaveå»ºç«‹ç®—æ³•çš„åŸå‹ï¼Œå¦‚æœç®—æ³•æˆç«‹ï¼Œå†è¿ç§»åˆ°å…¶ä»–çš„ç¼–ç¨‹ç¯å¢ƒä¸­æ¥ï¼Œå¦‚æ­¤æ‰èƒ½ä½¿æ•ˆç‡æ›´é«˜ã€‚</li> 
  </ul> 
  <h2><a id="Part_2_Linear_Regression_with_One_Variable_55"></a>Part 2 ï¼šå•å˜é‡çº¿æ€§å›å½’(Linear Regression with One Variable)</h2> 
  <ol start="5"> 
   <li>ç®—æ³•å®ç°è¿‡ç¨‹ï¼š</li> 
  </ol> 
  <ul> 
   <li>Methodï¼š 
    <ul> 
     <li>Training set ïƒ  Learning algorithm ïƒ  hypothesis function(å‡è®¾å‡½æ•°) ïƒ  Find the appropriate form to represent the hypothesis function<br> -<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190729153513278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzUzNDcz,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
   <li>ç®€å•æ¥è¯´å°±æ˜¯æˆ‘ä»¬ç»™å‡ºTraining Set ï¼Œ ç”¨å®ƒæ¥è®­ç»ƒæˆ‘ä»¬çš„Learning Algorithm ï¼Œæœ€ç»ˆæœŸæœ›èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªå‡½æ•° hï¼ˆthetaï¼‰, è¿™ä¹Ÿå°±æ˜¯é¢„æµ‹çš„æ¨¡å‹ï¼Œé€šè¿‡è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®ç°è¾“å…¥å˜é‡é›†ï¼Œç„¶åå°±èƒ½å¤Ÿè¿”å›å¯¹åº”çš„é¢„æµ‹å€¼ã€‚</li> 
  </ul> 
  <ol start="6"> 
   <li>Cost function(ä»£ä»·å‡½æ•°)</li> 
  </ol> 
  <ul> 
   <li> <p>å…³äºhï¼ˆxï¼‰å‡½æ•°</p> 
    <ul> 
     <li>å¯¹äºæˆ¿ä»·é—®é¢˜ï¼Œæˆ‘ä»¬å†³å®šä½¿ç”¨Linear function æ¥æ‹Ÿåˆæ›²çº¿ ï¼Œè®¾ <strong>hï¼ˆxï¼‰= thetaï¼ˆ0ï¼‰+thetaï¼ˆ1ï¼‰* x</strong></li> 
     <li>å› ä¸ºæˆ‘ä»¬æœ€åæƒ³è¦å¾—åˆ°çš„å°±æ˜¯ <strong>hï¼ˆxï¼‰</strong>ï¼Œå¯¹äº <strong>x</strong> æ¥è¯´ï¼Œå®ƒæ˜¯å§‹ç»ˆå˜åŒ–çš„ï¼Œæ¯ä¸€æ¡æ•°æ®å¯¹åº”ç€ä¸åŒçš„å˜é‡é›†ï¼Œæ— æ³•å›ºå®šï¼›æ‰€ä»¥algorithmæƒ³è¦ç¡®å®šçš„å°±æ˜¯ä¸å˜çš„é‡ <strong>thetaé›†</strong> äº†ã€‚åœ¨ä»¥åçš„æ“ä½œä¸­ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è¦æ±‚å¾—çš„æ˜¯ <strong>thetaé›†</strong>ï¼Œæ‰€ä»¥æ›´å¤šçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šæŠŠ <strong>x</strong> çœ‹ä½œæ˜¯å¸¸é‡ï¼Œå´æŠŠ <strong>theta</strong> çœ‹ä½œæ˜¯å˜é‡ï¼Œæ‰€ä»¥ä¸€å®šä¸è¦ç³Šæ¶‚äº†ã€‚</li> 
    </ul> </li> 
   <li> <p>å»ºæ¨¡è¯¯å·®ï¼ˆmodeling errorï¼‰</p> 
    <ul> 
     <li>æˆ‘ä»¬é€‰æ‹©çš„å‚æ•°å†³å®šäº†æˆ‘ä»¬å¾—åˆ°çš„ç›´çº¿ç›¸å¯¹äºæˆ‘ä»¬çš„è®­ç»ƒé›†çš„å‡†ç¡®ç¨‹åº¦ï¼Œæ¨¡å‹æ‰€é¢„æµ‹çš„å€¼ä¸è®­ç»ƒé›†ä¸­å®é™…å€¼ä¹‹é—´çš„å·®è·å°±æ˜¯å»ºæ¨¡è¯¯å·®ï¼ˆmodeling errorï¼‰ã€‚</li> 
    </ul> </li> 
  </ul> 
  <p>Descriptionï¼š</p> 
  <ul> 
   <li>Cost function sometimes is also called squared error function(å¹³æ–¹è¯¯å·®å‡½æ•°)</li> 
   <li>Cost function ç”¨äºæè¿°å»ºæ¨¡è¯¯å·®çš„å¤§å°ã€‚<br> ä¸€èˆ¬æ¥è¯´ï¼Œè¯¯å·®è¶Šå°ï¼ŒCost function å°±èƒ½å¤Ÿæ›´å¥½çš„ä¸æ•°æ®é›†ç›¸æ‹Ÿåˆï¼Œä½†è¿™å¹¶ä¸æ˜¯ç»å¯¹çš„ï¼Œå› ä¸ºè¿™æœ‰å¯èƒ½ä¼šäº§ç”Ÿ<strong>è¿‡æ‹Ÿåˆé—®é¢˜</strong>ã€‚</li> 
   <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084206756.png" alt=""><br> æŠŠå®ƒè¿›ä¸€æ­¥çš„æ‹†å¼€æ¥çœ‹å°±åº”è¯¥æ˜¯è¿™æ ·å­çš„ï¼š<strong>Jï¼ˆ theta(0),theta(1) ) = 1/2m * Sigma [1-m] {thetaï¼ˆ0ï¼‰+thetaï¼ˆ1ï¼‰* x(i) - y(i)}</strong>(å¯èƒ½å°±æˆ‘ä¸€äººèƒ½çœ‹æ‡‚ã€‚ã€‚)</li> 
   <li>å¹³æ–¹è¯¯å·®å‡½æ•°åªæ˜¯ Cost function ä¸­çš„ä¸€ç§å½¢å¼ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ä¹‹æ‰€ä»¥é€‰æ‹©squared error function ï¼Œ æ˜¯å› ä¸ºå¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å›å½’é—®é¢˜ï¼Œå®ƒæ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚è¿˜æœ‰å…¶ä»–çš„ä»£ä»·å‡½æ•°ä¹Ÿèƒ½å¾ˆå¥½åœ°å‘æŒ¥ä½œç”¨ï¼Œä½†æ˜¯å¹³æ–¹è¯¯å·®ä»£ä»·å‡½æ•°å¯èƒ½æ˜¯è§£å†³å›å½’é—®é¢˜æœ€å¸¸ç”¨çš„æ‰‹æ®µäº†ã€‚</li> 
  </ul> 
  <ol start="7"> 
   <li>Gradient descent(æ¢¯åº¦ä¸‹é™) algorithm</li> 
  </ol> 
  <ul> 
   <li> <p>Ideaï¼š</p> 
    <ul> 
     <li>æˆ‘ä»¬å·²ç»çŸ¥é“äº†ï¼Œ**hï¼ˆthetaï¼‰**æ˜¯ç”¨æ¥æ‹Ÿåˆæ•°æ®é›†çš„å‡½æ•°ï¼Œå¦‚æœæ‹Ÿåˆçš„å¥½ï¼Œé‚£ä¹ˆé¢„æµ‹çš„ç»“æœä¸ä¼šå·®ã€‚<strong>Jï¼ˆthetaé›†ï¼‰<strong>è¢«ç§°ä¸ºä»£ä»·å‡½æ•°ï¼Œå¦‚æœä»£ä»·å‡½æ•°è¾ƒå°ï¼Œåˆ™è¯´æ˜æ‹Ÿåˆçš„å¥½ã€‚è€Œæ­¤æ—¶çš„</strong>thetaé›†</strong>å°±æ˜¯æˆ‘ä»¬å°±æ˜¯é—®é¢˜çš„è§£äº†ã€‚</li> 
     <li>æˆ‘ä»¬å·²ç»çŸ¥é“äº†æˆ‘ä»¬çš„ç›®æ ‡<strong>thetaé›†</strong>ï¼Œé‚£ä¹ˆå¦‚ä½•æ±‚å¾—<strong>thetaé›†</strong>å‘¢ï¼Ÿ</li> 
     <li><strong>Gradient descent</strong>ï¼</li> 
    </ul> </li> 
   <li> <p>Usage:</p> 
    <ul> 
     <li>Start with some para0,para1â€¦</li> 
     <li>Keep changing paras to reduce J (para0,para1) to find some <strong>local minimum</strong></li> 
     <li>Start with different parameters , we may get different <strong>result</strong></li> 
     <li>ï¼ˆæ‰€ä»¥ï¼Œæˆ‘ä»¬ä¿è¯<strong>J(thetaé›†)ä¸å„ä¸ªthetaæ„æˆçš„å‡½æ•°æ˜¯ä¸€ä¸ªå‡¸å‡½æ•°ï¼Œæ¢å¥è¯è¯´ä¿è¯å­˜åœ¨å”¯ä¸€çš„æå°å€¼=æå°å€¼</strong>ï¼‰</li> 
     <li>Until we hopefully end up at a minimum</li> 
    </ul> </li> 
   <li> <p>Formulaï¼š</p> 
    <ul> 
     <li> <p>æ‰¹é‡æ¢¯åº¦ä¸‹é™å…¬å¼<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084222408.png" alt=""></p> </li> 
     <li> <p>&lt;a&gt; is learning rate which control how big a step we use to upgrade the paras.<br> So if the &lt;a&gt; is too small , the algorithm will run slowly ; but if &lt;a&gt; is too big , it can even make it away from the minimum point .<br> å¦å¤–ï¼Œæ¯æ¬¡thetaä¸‹é™çš„å¹…åº¦éƒ½æ˜¯ä¸ä¸€æ ·çš„ï¼Œè¿™æ˜¯å› ä¸ºå®é™…ä¸Šthetaæ¯æ¬¡ä¸‹é™å¤šå°‘æ˜¯ç”± ã€&lt;a&gt;*Derivative Termã€‘è¿™ä¸ªæ•´ä½“å†³å®šçš„ï¼Œ&lt;a&gt;æ˜¯ä¸‹é™å¹…åº¦çš„é‡è¦å†³å®šå› å­ï¼Œä½†å¹¶ä¸æ˜¯å…¨éƒ¨ã€‚<br> åœ¨æ¯æ¬¡ä¸‹é™ä»¥åï¼ŒDerivative Terméƒ½ä¼šå˜å°ï¼Œä¸‹ä¸€æ¬¡ä¸‹é™çš„å¹…åº¦ä¹Ÿä¼šå˜å°ï¼Œå¦‚æ­¤ï¼Œæ§åˆ¶æ¢¯åº¦æ…¢æ…¢çš„ä¸‹é™ï¼Œç›´åˆ°Derivative Term=0ã€‚</p> </li> 
     <li> <p>the rest of the formula is the Derivative Term(å¯¼æ•°é¡¹) of theta</p> </li> 
     <li> <p>In this image,we can find that no matter the Derivative Term of theta is positive or negative , the Gradient descent algorithm works well<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084233952.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> </li> 
     <li> <p>the Details of the formulaï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084247350.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> <strong>Attention</strong>:<br> With the changing of the paras,the derivative term is changing too.<br> <strong>So</strong>: <br> Make sure updating the paras at the same time.ï¼ˆsimultaneous updateåŒæ­¥æ›´æ–°ï¼‰<br> For whatï¼Ÿ Because the derivative should use the former para rather than the new.<br> <strong>How</strong>:<br> åœ¨å…¨éƒ¨<strong>theta</strong>æ²¡æœ‰æ›´æ–°å®Œæˆä¹‹å‰ï¼Œæˆ‘ä»¬ç”¨<strong>tempé›†</strong>æ¥æš‚æ—¶å­˜å‚¨å·²ç»æ›´æ–°å®Œæˆçš„<strong>thetaé›†</strong>ï¼Œç›´åˆ°æœ€åä¸€ä¸ª<strong>theta</strong>ä¹Ÿæ›´æ–°å®Œæˆä¹‹åï¼Œæˆ‘ä»¬æ‰å†ç”¨å­˜å‚¨åœ¨<strong>tempé›†</strong>é‡Œçš„<strong>æ–°thetaé›†</strong>æ›´æ–°<strong>åŸthetaé›†</strong>ã€‚</p> </li> 
    </ul> </li> 
  </ul> 
  <ol start="8"> 
   <li>Gradient descent for linear regression</li> 
  </ol> 
  <ul> 
   <li> <p>â€œBatchâ€ Gradient descent ï¼š</p> 
    <ul> 
     <li>Each step of gradient descent uses all the training examples .</li> 
     <li>åœ¨æ¢¯åº¦ä¸‹é™å…¬å¼ä¸­å­˜åœ¨æŸä¸€é¡¹éœ€è¦å¯¹æ‰€æœ‰mä¸ªè®­ç»ƒæ ·æœ¬æ±‚å’Œã€‚</li> 
    </ul> </li> 
   <li> <p>Ideaï¼š</p> 
    <ul> 
     <li>å°†æ¢¯åº¦ä¸‹é™ä¸ä»£ä»·å‡½æ•°ç›¸ç»“åˆ</li> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190729202443605.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
   <li> <p>The result:</p> 
    <ul> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084257310.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
     <li>This is the result after derivation. And we can find that comparing with the first formula , the second has a x term at the end . It does tell us something that the variable is para0 and para1 , not the x or y . Although they looks more like variables.</li> 
     <li>After derivation , for the first formula we have nothing left , for the second , we just has a constant term(å¸¸æ•°é¡¹) x left .</li> 
    </ul> </li> 
   <li> <p>Finallyï¼š</p> 
    <ul> 
     <li>We have other ways to computing Jï¼ˆthetaï¼‰ï¼š</li> 
     <li>æ­£è§„æ–¹ç¨‹(normal equations)ï¼Œå®ƒå¯ä»¥åœ¨ä¸éœ€è¦å¤šæ­¥æ¢¯åº¦ä¸‹é™çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½è§£å‡ºä»£ä»·å‡½æ•°çš„æœ€å°å€¼ã€‚</li> 
     <li>å®é™…ä¸Šåœ¨æ•°æ®é‡è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œæ¢¯åº¦ä¸‹é™æ³•æ¯”æ­£è§„æ–¹ç¨‹è¦æ›´é€‚ç”¨ä¸€äº›ã€‚</li> 
    </ul> </li> 
  </ul> 
  <h2><a id="Part_3_Linear_Algebra_135"></a>Part 3 ï¼šçº¿æ€§ä»£æ•°(Linear Algebra)</h2> 
  <blockquote> 
   <p>çŸ©é˜µå’Œå‘é‡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼æ¥ç»„ç»‡å¤§é‡çš„æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å½“æˆ‘ä»¬å¤„ç†å·¨å¤§çš„è®­ç»ƒé›†æ—¶ï¼Œå®ƒçš„ä¼˜åŠ¿å°±æ›´åŠ çš„æ˜æ˜¾ã€‚</p> 
  </blockquote> 
  <ol start="9"> 
   <li>Matrices and vectors (çŸ©é˜µå’Œå‘é‡)</li> 
  </ol> 
  <ul> 
   <li> <p>Matrix:</p> 
    <ul> 
     <li>Rectangular array of numbers.</li> 
     <li>äºŒç»´æ•°ç»„</li> 
    </ul> </li> 
   <li> <p>The dimension(ç»´åº¦) of the matrix :</p> 
    <ul> 
     <li>number of row * number of columns</li> 
    </ul> </li> 
   <li> <p>Matrix Elements :</p> 
    <ul> 
     <li>How to refer to the element<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084309947.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
   <li> <p>Vector:</p> 
    <ul> 
     <li>An n*1 matrix . ( n dimensional vector. nç»´å‘é‡)</li> 
     <li>How to refer to the element in it</li> 
     <li> <pre><code>   ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190726084319572.png)
</code></pre> </li> 
     <li>We use 1-indexed vector</li> 
    </ul> </li> 
   <li> <p>Cvention:</p> 
    <ul> 
     <li>We use upper case to refer to the matrix and lower case refer to the vector.</li> 
    </ul> </li> 
  </ul> 
  <ol start="10"> 
   <li>Addition and scalar multiplication (åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•)</li> 
  </ol> 
  <ul> 
   <li>è¡Œåˆ—æ•°ç›¸ç­‰æ‰èƒ½è¿›è¡ŒåŠ æ³•è¿ç®—ï¼š 
    <ul> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730110510280.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
   <li>æ ‡é‡ä¸çŸ©é˜µçš„ä¹˜æ³•ï¼š 
    <ul> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019073011062270.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
  </ul> 
  <ol start="11"> 
   <li>Matrix-vector multiplication(çŸ©é˜µ-å‘é‡ä¹˜æ³•)</li> 
  </ol> 
  <p>Detailsï¼š</p> 
  <ul> 
   <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084403420.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
   <li><strong>Aâ€™s row number must be equal with Bâ€™s column number .</strong></li> 
  </ul> 
  <ol start="12"> 
   <li>Matrix-matrix multiplication(çŸ©é˜µ-çŸ©é˜µä¹˜æ³•)</li> 
  </ol> 
  <ul> 
   <li>Detailsï¼š 
    <ul> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084353221.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
     <li>Just as matrix-vector , the most important thing is to make sure <strong>Aâ€™s columns number is equal with Bâ€™s rows number</strong> .</li> 
     <li>Câ€™s columns number is equal with Bâ€™s .</li> 
     <li>n fact , before multiply between <strong>Matrixs</strong> , we can divide B into several vectors by columns, and then we can make the <strong>Matrix- Vector</strong> multiplication.</li> 
     <li>A column of <strong>B</strong> corresponding a hypothesis , and a column of <strong>C</strong> corresponding a set of prediction .</li> 
    </ul> </li> 
  </ul> 
  <ol start="13"> 
   <li>Matrix multiplication properties(çŸ©é˜µä¹˜æ³•ç‰¹å¾)</li> 
  </ol> 
  <ul> 
   <li> <p>Properties :</p> 
    <ul> 
     <li>Do not enjoy commutative property(ä¸éµå¾ªäº¤æ¢å¾‹)</li> 
     <li>Enjoy the associative property(éµå¾ªç»“åˆå¾‹)</li> 
    </ul> </li> 
   <li> <p>Identity Matrix(å•ä½çŸ©é˜µ):</p> 
    <ul> 
     <li> <pre><code>   ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/20190726084424308.png)
</code></pre> </li> 
     <li> <p>Featureï¼š<br> å®ƒæ˜¯ä¸ªæ–¹é˜µï¼Œä»å·¦ä¸Šè§’åˆ°å³ä¸‹è§’çš„å¯¹è§’çº¿ï¼ˆç§°ä¸ºä¸»å¯¹è§’çº¿ï¼‰ä¸Šçš„å…ƒç´ å‡ä¸º1ã€‚é™¤æ­¤ä»¥å¤–å…¨éƒ½ä¸º0ã€‚å®ƒçš„ä½œç”¨å°±åƒæ˜¯ä¹˜æ³•ä¸­çš„1ã€‚</p> </li> 
     <li> <p>Usageï¼š<br> <i> is the identity matrix .<br> A(m , n) * I(n , n) = I(m , m) * A(m , n)<br> M1â€™s column number should be equal with M2â€™s row number .</i></p> </li> 
    </ul> </li> 
  </ul> 
  <ol start="14"> 
   <li>Matrix inverse operation(çŸ©é˜µçš„é€†è¿ç®—)</li> 
  </ol> 
  <ul> 
   <li>Attentionï¼š 
    <ul> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084448585.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
     <li>Only square matrix have inverse .</li> 
     <li>Not all number have inverse , such as 0 .</li> 
    </ul> </li> 
  </ul> 
  <ol start="15"> 
   <li>Matrix transpose operation(çŸ©é˜µçš„è½¬ç½®è¿ç®—)</li> 
  </ol> 
  <ul> 
   <li>Definition: 
    <ul> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084457891.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
   <li>Official definition:<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084516645.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> å³å°†Aä¸Bçš„x , yåæ ‡äº’æ¢</li> 
   <li>çŸ©é˜µè½¬ç½®çš„åŸºæœ¬æ€§è´¨ï¼š 
    <ul> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730121355322.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
  </ul> 
  <h2><a id="Part_4Linear_Regression_with_Multiple_Variables_213"></a>Part 4ï¼šå¤šå˜é‡çº¿æ€§å›å½’(Linear Regression with Multiple Variables)</h2> 
  <blockquote> 
   <p>åœ¨ä¹‹å‰çš„éƒ¨åˆ†ä¸­ï¼Œå­¦ä¹ äº†<strong>å•å˜é‡</strong>/ç‰¹å¾çš„å›å½’æ¨¡å‹ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†è¿›è¡Œæ·±å…¥ï¼Œå°†å˜é‡çš„æ•°é‡å¢åŠ ï¼Œè®¨è®ºå¤šå˜é‡çš„çº¿æ€§å›å½’é—®é¢˜ã€‚</p> 
  </blockquote> 
  <ol start="16"> 
   <li>Multiple feature(å¤šåŠŸèƒ½)</li> 
  </ol> 
  <ul> 
   <li>Exampleï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730131929495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzUzNDcz,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
   <li> <h2><a id="_221"></a>å¯¹äºæ–°é—®é¢˜çš„æ³¨é‡Šï¼š</h2> </li> 
  </ul> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/201907301318227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzUzNDcz,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> 
  <ul> 
   <li>So the final prediction result is a vector too .</li> 
   <li>å¯ä»¥å‘ç°ï¼Œå¤šå˜é‡çº¿æ€§å›å½’å’Œå•å˜é‡çº¿æ€§å›å½’åŸºæœ¬æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„å˜åŒ–å°±æ˜¯<strong>theta</strong>å˜æˆäº†<strong>thetaé›†</strong>ï¼›å˜é‡<strong>x</strong>å˜æˆäº†<strong>xé›†</strong>ã€‚</li> 
  </ul> 
  <ol start="18"> 
   <li>Gradient descent for multiple variables(å¤šå…ƒæ¢¯åº¦ä¸‹é™æ³•)</li> 
  </ol> 
  <ul> 
   <li> <p>Ideaï¼š</p> 
    <ul> 
     <li>ä¸å•å˜é‡çº¿æ€§å›å½’ç±»ä¼¼ï¼Œåœ¨å¤šå˜é‡çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ„å»ºä¸€ä¸ªä»£ä»·å‡½æ•°ï¼Œåˆ™è¿™ä¸ªä»£ä»·å‡½æ•°æ˜¯æ‰€æœ‰å»ºæ¨¡è¯¯å·®çš„å¹³æ–¹å’Œ</li> 
     <li>ä»£ä»·å‡½æ•°ï¼š<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730132606595.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
     <li>æ¢¯åº¦ä¸‹é™æ³•åº”ç”¨åˆ°ä»£ä»·å‡½æ•°å¹¶æ±‚å¯¼ä¹‹åå¾—åˆ°ï¼š</li> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190730135254233.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
   <li> <p>Feature scaling(ç‰¹å¾ç¼©æ”¾):</p> 
    <ul> 
     <li>Purpose:<br> Make the gradient descent runs faster .<br> å°¤å…¶æ˜¯å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚</li> 
     <li>Idea:<br> Make sure number are in similar scale .</li> 
     <li>Usageï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084614924.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
     <li>The range limit ï¼š<br> Get every feature range from -3 to 3 or -1/3 to 1/3 . Thatâ€™s fine .<br> <s>In fact there is no problems , because the feature is a real number ! Thatâ€™s means it will never change .</s><br> x æ˜¯å˜é‡ï¼Œå¹¶ä¸”å®ƒæ˜¯å·²çŸ¥çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä»data set ä¸­ç›´æ¥è·å¾—ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å®ƒè¿›è¡Œç‰¹å¾ç¼©æ”¾ï¼›thetaæ˜¯æœªçŸ¥çš„ï¼Œæ˜¯æˆ‘ä»¬è¦æ±‚çš„ç›®æ ‡ã€‚</li> 
    </ul> </li> 
   <li> <p>Summaryï¼š</p> 
    <ul> 
     <li>ç‰¹å¾ç¼©æ”¾ç¼©æ”¾çš„æ˜¯ <strong>å·²çŸ¥çš„</strong> <strong>å˜é‡x</strong> ï¼Œè€Œä¸æ˜¯ <strong>æœªçŸ¥çš„</strong> <strong>å¸¸é‡theta</strong> ã€‚</li> 
     <li>è¿›è¡Œç‰¹å¾ç¼©æ”¾æ˜¯ä¸ºäº†ä¸ºäº†æé«˜æ¢¯åº¦ä¸‹é™çš„é€Ÿåº¦ã€‚</li> 
    </ul> </li> 
  </ul> 
  <ol start="18"> 
   <li>Learning rate <a> (å­¦ä¹ ç‡)</a></li> 
  </ol> 
  <ul> 
   <li>Ideaï¼š 
    <ul> 
     <li>If the gradient descent is working properly , then J should decrease after every iteration.</li> 
     <li>æ¢¯åº¦ä¸‹é™ç®—æ³•æ”¶æ•›æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°æ ¹æ®æ¨¡å‹çš„ä¸åŒè€Œä¸åŒï¼Œæˆ‘ä»¬ä¸èƒ½æå‰é¢„çŸ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è¿­ä»£æ¬¡æ•°å’Œä»£ä»·å‡½æ•°çš„å›¾è¡¨æ¥è§‚æµ‹ç®—æ³•åœ¨ä½•æ—¶è¶‹äºæ”¶æ•›ã€‚</li> 
    </ul> </li> 
   <li>Conventionï¼š 
    <ul> 
     <li>é€šå¸¸å¯ä»¥è€ƒè™‘çš„å­¦ä¹ ç‡&lt;a&gt;çš„æ•°å€¼ï¼š<br> <a>0.01ï¼›0.03ï¼›0.1ï¼›0.3ï¼›1ï¼›3ï¼›10ï¼›</a></li> 
    </ul> </li> 
   <li>Summary: 
    <ul> 
     <li>If &lt;a&gt; is too small ïƒ  slow move</li> 
     <li>If &lt;a&gt; is too big ïƒ  J may not decrease on every iteration .</li> 
    </ul> </li> 
  </ul> 
  <ol start="19"> 
   <li>Features and Polynomial regression(å¤šå˜é‡å’Œå¤šé¡¹å¼å›å½’)</li> 
  </ol> 
  <p>Ideaï¼š</p> 
  <ul> 
   <li>Linear regression å¹¶ä¸é€‚åº”æ‰€æœ‰çš„æ•°æ®æ¨¡å‹ï¼Œæœ‰æ—¶æˆ‘ä»¬éœ€è¦æ›²çº¿æ¥é€‚åº”æˆ‘ä»¬çš„æ•°æ®ï¼Œæ¯”å¦‚ä¸€ä¸ªäºŒæ¬¡æ–¹æ¨¡å‹ï¼š</li> 
   <li>é€šå¸¸æˆ‘ä»¬éœ€è¦å…ˆè§‚å¯Ÿæ•°æ®å†ç¡®å®šæœ€ç»ˆçš„æ¨¡å‹</li> 
  </ul> 
  <p>Attentionï¼š</p> 
  <ul> 
   <li>å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚</li> 
  </ul> 
  <ol start="20"> 
   <li>Normal equation(å¸¸è§„æ–¹ç¨‹)</li> 
  </ol> 
  <ul> 
   <li> <p>Ideaï¼š</p> 
    <ul> 
     <li>ä»æ•°å­¦ä¸Šæ¥çœ‹ï¼Œæƒ³è¦æ±‚å¾—Jï¼ˆtheta) çš„æœ€å°å€¼ï¼Œæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯å¯¹å®ƒè¿›è¡Œæ±‚å¯¼ï¼Œä»¤å®ƒçš„å¯¼æ•°=0ï¼Œå¾—å‡ºæ­¤æ—¶çš„thetaå°±æ˜¯æœ€ç»ˆçš„ç»“æœ</li> 
     <li>è€Œæ­£è§„æ–¹ç¨‹æ–¹æ³•çš„ç¡®å°±æ˜¯è¿™ä¹ˆå¹²çš„</li> 
    </ul> </li> 
   <li> <p>Attentionï¼š</p> 
    <ul> 
     <li>å¯¹äºé‚£äº›ä¸å¯é€†çš„çŸ©é˜µï¼ˆé€šå¸¸æ˜¯å› ä¸ºç‰¹å¾ä¹‹é—´ä¸ç‹¬ç«‹ï¼Œå¦‚åŒæ—¶åŒ…å«è‹±å°ºä¸ºå•ä½çš„å°ºå¯¸å’Œç±³ä¸ºå•ä½çš„å°ºå¯¸ä¸¤ä¸ªç‰¹å¾ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ç‰¹å¾æ•°é‡å¤§äºè®­ç»ƒé›†çš„æ•°é‡ï¼‰ï¼Œæ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯ä¸èƒ½ç”¨çš„ã€‚</li> 
     <li>è¿™å°±æ˜¯æ­£è§„æ–¹ç¨‹çš„å±€é™æ€§</li> 
    </ul> </li> 
  </ul> 
  <p>Usage:</p> 
  <ul> 
   <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084636872.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
   <li>Exampleï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084644911.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084651963.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084657492.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084702680.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
   <li></li> 
  </ul> 
  <ol start="21"> 
   <li>The Comparison of Gradient descent and Normal equation</li> 
  </ol> 
  <p>Gradient descent :</p> 
  <ul> 
   <li>Need to choose <a></a></li> 
   <li>Need many iterations</li> 
  </ul> 
  <p>Normal equation :</p> 
  <ul> 
   <li>Need to compute [xâ€™*x]^(-1) , So if n is large it will run slowly .</li> 
   <li>Do not fit every situations (åªé€‚ç”¨äºçº¿æ€§æ¨¡å‹ï¼Œä¸é€‚åˆé€»è¾‘å›å½’æ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹).</li> 
  </ul> 
  <p>Summary:</p> 
  <ul> 
   <li>åªè¦ç‰¹å¾å˜é‡çš„æ•°ç›®å¹¶ä¸å¤§ï¼Œæ ‡å‡†æ–¹ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è®¡ç®—å‚æ•°çš„æ›¿ä»£æ–¹æ³•ã€‚å…·ä½“åœ°è¯´ï¼Œåªè¦ç‰¹å¾å˜é‡æ•°é‡å°äº<strong>ä¸€ä¸‡</strong>ï¼Œæˆ‘é€šå¸¸ä½¿ç”¨æ ‡å‡†æ–¹ç¨‹æ³•ï¼Œè€Œä¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€‚</li> 
  </ul> 
  <ol start="22"> 
   <li>Normal equation ä¸å¯é€†æƒ…å†µ</li> 
  </ol> 
  <p>Ideaï¼š</p> 
  <ul> 
   <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190726084715505.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
   <li>ç¬¬ä¸€è¦é™ä½featuresä¹‹é—´çš„ç›¸å…³åº¦</li> 
   <li>ç¬¬äºŒè¦é€‚å½“çš„å‡å°‘features çš„æ•°é‡</li> 
  </ul> 
  <ol start="23"> 
   <li></li> 
  </ol> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019072821015284.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> Ideaï¼š</p> 
  <ul> 
   <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728210119668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzUzNDcz,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
  </ul> 
  <h2><a id="Part_5OctaveOctave_Tutorial_314"></a>Part 5ï¼šOctaveæ•™ç¨‹(Octave Tutorial)</h2> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/qq_44353473/article/details/97363394" rel="nofollow" data-token="88bea74dfd93ebd68322a0d5d11dd0f0">è¿™é‡Œæ˜¯ä¸€ä¸ªğŸ”—â€“&gt; OctaveåŸºæœ¬è¯­æ³•</a></p> 
  </blockquote> 
  <h2><a id="Part_6Logistic_Regression_320"></a>Part 6ï¼šé€»è¾‘å›å½’(Logistic Regression)</h2> 
  <ol start="28"> 
   <li>Vectorizationï¼ˆçŸ¢é‡åŒ–ï¼‰</li> 
  </ol> 
  <ul> 
   <li>Exampleï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728210349533.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> <strong>ä½¿ç”¨å‘é‡çš„è½¬ç½®æ¥æ±‚ä¸¤ä¸ªå‘é‡çš„å†…ç§¯ï¼Œå¯ä»¥éå¸¸æ–¹ä¾¿çš„å¤„ç†å¤§é‡çš„æ•°æ®ã€‚</strong><br> è¿™å°±æ˜¯æ‰€è°“çš„çŸ¢é‡åŒ–çš„çº¿æ€§å›å½’ã€‚</li> 
  </ul> 
  <ol start="29"> 
   <li>Classificationï¼ˆåˆ†ç±»é—®é¢˜ï¼‰</li> 
  </ol> 
  <ul> 
   <li>Convention in Classification problems:<br> 0 equal to empty ;<br> 1 means something is there.</li> 
   <li>Attention:<br> In classification problems , we want to predict &lt;ç¦»æ•£å€¼&gt;</li> 
  </ul> 
  <ol start="30"> 
   <li>Hypothesis Representationï¼ˆå‡è¯´è¡¨ç¤ºï¼‰</li> 
  </ol> 
  <ul> 
   <li> <p>Questionï¼š</p> 
    <ul> 
     <li>What kind of function we need to use to represent our hypothesis in the classification problems?</li> 
     <li>Linear function image may out of the range[-1,1]</li> 
    </ul> </li> 
   <li> <p>Idea:</p> 
    <ul> 
     <li>Make sure <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728210444682.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
   <li> <p>Import :</p> 
    <ul> 
     <li>Logistic regression(é€»è¾‘å›å½’)</li> 
    </ul> </li> 
   <li> <p>Hjypothesisï¼š <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728210509312.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> 
    <ul> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728210527479.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
   <li> <p>Statusï¼š</p> 
    <ul> 
     <li>The most popular machine learning algorithm.</li> 
     <li>ä¸€ç§éå¸¸å¼ºå¤§ï¼Œç”šè‡³å¯èƒ½ä¸–ç•Œä¸Šä½¿ç”¨æœ€å¹¿æ³›çš„ä¸€ç§åˆ†ç±»ç®—æ³•ã€‚</li> 
    </ul> </li> 
   <li> <p>Attentionï¼š<br> Although we called it â€˜regressionâ€™ , but in fact it is used in classification<br> Problems.</p> </li> 
   <li> <p>Logistic function: <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728210546708.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728210557389.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> </li> 
  </ul> 
  <ol start="31"> 
   <li>Decision boundary</li> 
  </ol> 
  <ul> 
   <li>å†³ç­–è¾¹ç•Œä¸æ˜¯è®­ç»ƒé›†çš„å±æ€§ï¼Œè€Œæ˜¯å‡è®¾æœ¬èº«ä»¥åŠå…¶å‚æ•°çš„å±æ€§ï¼›æ‰€ä»¥è¯´<br> åªè¦ç»™å®šäº†thetaé‚£ä¹ˆå†³ç­–è¾¹ç•Œå°±å·²ç»ç¡®å®šäº†ã€‚ï¼ˆ<strong>thetaé›†ç¡®å®šäº†ï¼Œé‚£ä¹ˆå‡½æ•°å°±ç¡®å®šäº†ï¼Œå†³ç­–è¾¹ç•Œè‡ªç„¶ä¹Ÿå°±è¢«ç¡®å®šäº†</strong>ï¼‰ 
    <ul> 
     <li>è®­ç»ƒé›†â€“&gt;ç”Ÿæˆthetaâ€“&gt;ç¡®å®šå†³ç­–è¾¹ç•Œ</li> 
    </ul> </li> 
  </ul> 
  <ol start="32"> 
   <li>Logistic Regression Cost functionï¼ˆé€»è¾‘å›å½’çš„ä»£ä»·å‡½æ•°ï¼‰</li> 
  </ol> 
  <ul> 
   <li> <p>Question:</p> 
    <ul> 
     <li>How to choose parameter thetaï¼Ÿ</li> 
     <li>å› ä¸ºæˆ‘ä»¬æ”¹å˜äº†ä»£ä»·å‡½æ•°çš„æ¨¡å‹<strong>h(theta)</strong> ,è¿™æ ·J(theta)-theta çš„å‡½æ•°å›¾åƒå°±ä¼šç”±è®¸å¤šä¸ªæå°å€¼ï¼Œè¿™å¯¹äºæˆ‘ä»¬æ±‚å¾—thetaæ˜¯ä¸åˆ©çš„</li> 
    </ul> </li> 
   <li> <p>Idea:</p> 
    <ul> 
     <li> <p>ä¸ºäº†ä¿è¯Cost function æ˜¯å‡¸å‡½æ•°ï¼Œæˆ‘ä»¬é€‰æ‹©æ›´æ¢ä»£ä»·å‡½æ•°ã€‚</p> </li> 
     <li> <p>åœ¨å½“æ—¶å­¦ä»£ä»·å‡½æ•°çš„æ—¶å€™å°±ç›´åˆ°ï¼Œä»£ä»·å‡½æ•°å¹¶ä¸åªæ˜¯æœ‰ä¸€ç§å½¢å¼ã€‚å®é™…ä¸Šä»£ä»·å‡½æ•°åªæ˜¯ä¸ºäº†è¡¨å¾è¯¯å·®çš„çš„å¤§å°ï¼Œæ‰€ä»¥åªè¦æ˜¯èƒ½å¤Ÿåˆç†çš„è¡¨ç¤ºè¯¯å·®çš„å‡½æ•°éƒ½å¯ä»¥å½“ä½œæ˜¯è¯¯å·®å‡½æ•°ã€‚</p> </li> 
     <li> <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728210621519.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> </li> 
     <li> <p>Simplified cost function:<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190728210634640.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> åŸºäºåªæœ‰y=1 || y=0 ä¸¤ç§æƒ…å†µ</p> </li> 
     <li> <p><strong>æ¨ªåæ ‡è¡¨ç¤ºçš„æ˜¯hï¼ˆx) ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„æ¨¡å‹å¾—å‡ºæ¥çš„ y å€¼ ï¼Œ çºµåæ ‡æ˜¯Cost function</strong>ï¼Œå¯¹äº y=1 å’Œ y=0 æœ‰ä¸åŒçš„å›¾åƒçš„ï¼Œè¿™ä¸¤ä¸ªå›¾åƒéƒ½æ˜¯åœ¨ä¸€ç«¯ç­‰äº0 ï¼Œ åœ¨å¦ä¸€ç«¯è¶‹äºæ— ç©·å¤§ã€‚<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190801090418619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzUzNDcz,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> </li> 
    </ul> </li> 
  </ul> 
  <ol start="33"> 
   <li>Logistic Regression Gradient Descentï¼ˆé€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™ï¼‰</li> 
  </ol> 
  <ul> 
   <li> <p>Ideaï¼š</p> 
    <ul> 
     <li>æˆ‘ä»¬å·²ç»æ‰¾åˆ°äº†åˆé€‚çš„Cost function äº† ï¼Œç°åœ¨å°±å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¢¯åº¦ä¸‹é™äº†</li> 
    </ul> </li> 
   <li> <p>Details:</p> 
    <ul> 
     <li>å¯¹äºæ¢¯åº¦ä¸‹é™ï¼Œæ— è®ºCost function æ˜¯ä»€ä¹ˆ ï¼Œå®ƒçš„ algorithmæ˜¯å§‹ç»ˆä¸å˜çš„<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190801091829676.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
     <li>æˆ‘ä»¬å°†Cost function å¸¦å…¥, ç»“æœå¦‚ä¸‹ï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019080109193942.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
    </ul> </li> 
   <li> <p>Attention:</p> 
    <ul> 
     <li>è™½ç„¶å¾—åˆ°çš„æ¢¯åº¦ä¸‹é™ç®—æ³•è¡¨é¢ä¸Šçœ‹ä¸Šå»ä¸çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸€æ ·ï¼Œä½†æ˜¯è¿™é‡Œçš„ hï¼ˆxï¼‰ä¸çº¿æ€§å›å½’ä¸­ä¸åŒï¼ˆæ­¤å¤„çš„æ˜¯é€»è¾‘å›å½’å‡½æ•°è€Œä¹‹å‰çš„æ˜¯çº¿æ€§å‡½æ•°ï¼‰æ‰€ä»¥å®é™…ä¸Šé€»è¾‘å‡½æ•°çš„æ¢¯åº¦ä¸‹é™ï¼Œè·Ÿçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™æ˜¯ä¸¤ä¸ªå®Œå…¨ä¸åŒçš„ä¸œè¥¿ã€‚</li> 
     <li>å¦å¤–ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•ä¹‹å‰ï¼Œè¿›è¡Œç‰¹å¾ç¼©æ”¾ä¾æ—§æ˜¯éå¸¸å¿…è¦çš„ã€‚</li> 
    </ul> </li> 
  </ul> 
  <blockquote> 
   <p>ä¸€äº›æ¢¯åº¦ä¸‹é™ç®—æ³•ä¹‹å¤–çš„é€‰æ‹©ï¼š é™¤äº†æ¢¯åº¦ä¸‹é™ç®—æ³•ä»¥å¤–ï¼Œè¿˜æœ‰ä¸€äº›å¸¸è¢«ç”¨æ¥ä»¤ä»£ä»·å‡½æ•°æœ€å°çš„ç®—æ³•ï¼Œè¿™äº›ç®—æ³•æ›´åŠ å¤æ‚å’Œä¼˜è¶Šï¼Œè€Œä¸”é€šå¸¸ä¸éœ€è¦äººå·¥é€‰æ‹©å­¦ä¹ ç‡ï¼Œé€šå¸¸æ¯”æ¢¯åº¦ä¸‹é™ç®—æ³•è¦æ›´åŠ å¿«é€Ÿã€‚è¿™äº›ç®—æ³•æœ‰ï¼šå…±è½­æ¢¯åº¦ï¼ˆConjugate Gradientï¼‰ï¼Œå±€éƒ¨ä¼˜åŒ–æ³•(Broyden fletcher goldfarb shann,BFGS)å’Œæœ‰é™å†…å­˜å±€éƒ¨ä¼˜åŒ–æ³•(LBFGS) ï¼Œfminuncæ˜¯ matlabå’Œoctave ä¸­éƒ½å¸¦çš„ä¸€ä¸ªæœ€å°å€¼ä¼˜åŒ–å‡½æ•°ï¼Œä½¿ç”¨æ—¶æˆ‘ä»¬éœ€è¦æä¾›ä»£ä»·å‡½æ•°å’Œæ¯ä¸ªå‚æ•°çš„æ±‚å¯¼ï¼Œä¸‹é¢æ˜¯ octave ä¸­ä½¿ç”¨ fminunc å‡½æ•°çš„ä»£ç ç¤ºä¾‹ï¼š</p> 
  </blockquote> 
  <pre><code>function [jVal, gradient] = costFunction(theta)
â€‹
    jVal = [...code to compute J(theta)...];
    gradient = [...code to compute derivative of J(theta)...];
    
end
    
options = optimset('GradObj', 'on', 'MaxIter', '100');
    
initialTheta = zeros(2,1);
    
[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);

%å¾ˆé†‰äººï¼Œè¿™è°ä½›å¾—äº†å•Š
</code></pre> 
  <ol start="34"> 
   <li>Advanced Optimizationï¼ˆé«˜çº§ä¼˜åŒ–ï¼‰</li> 
  </ol> 
  <h4><a id="_416"></a>é›¾é‡Œçœ‹èŠ±ã€‚ã€‚ã€‚</h4> 
  <ul> 
   <li>æ¢¯åº¦ä¸‹é™å¹¶ä¸æ˜¯æˆ‘ä»¬æ±‚thetaæœ€å¥½çš„æ–¹æ³•ï¼Œè¿˜æœ‰å…¶ä»–ä¸€äº›ç®—æ³•ï¼Œæ›´é«˜çº§ã€æ›´å¤æ‚ã€‚</li> 
   <li>å…±è½­æ¢¯åº¦æ³• BFGS (å˜å°ºåº¦æ³•) å’ŒL-BFGS (é™åˆ¶å˜å°ºåº¦æ³•) å°±æ˜¯å…¶ä¸­ä¸€äº›æ›´é«˜çº§çš„ä¼˜åŒ–ç®—æ³• 
    <ul> 
     <li>ä¼˜ç‚¹ï¼š<br> é€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨é€‰æ‹©å­¦ä¹ ç‡ ï¼›<br> ç®—æ³•çš„è¿è¡Œé€Ÿåº¦é€šå¸¸è¿œè¿œè¶…è¿‡æ¢¯åº¦ä¸‹é™ã€‚</li> 
    </ul> </li> 
   <li>Octaveä»£ç å®ç°ï¼š</li> 
  </ul> 
  <pre><code>%å®šä¹‰Cost function å‡½æ•°

function [jVal, gradient]=costFunction(theta)
    
ã€€ã€€jVal=(theta(1)-5)^2+(theta(2)-5)^2;
    
ã€€ã€€gradient=zeros(2,1);
    
ã€€ã€€gradient(1)=2*(theta(1)-5);
    
ã€€ã€€gradient(2)=2*(theta(2)-5);
    
end
</code></pre> 
  <ul> 
   <li>è°ƒç”¨è¯¥å‡½æ•°ï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190801114705217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzUzNDcz,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
  </ul> 
  <pre><code>ä½ è¦è®¾ç½®å‡ ä¸ªoptionsï¼Œè¿™ä¸ªoptionså˜é‡ä½œä¸ºä¸€ä¸ªæ•°æ®ç»“æ„å¯ä»¥å­˜å‚¨ä½ æƒ³è¦çš„optionsï¼Œ
æ‰€ä»¥ GradObj å’ŒOnï¼Œè¿™é‡Œè®¾ç½®æ¢¯åº¦ç›®æ ‡å‚æ•°ä¸ºæ‰“å¼€(on)ï¼Œè¿™æ„å‘³ç€ä½ ç°åœ¨ç¡®å®è¦ç»™è¿™
ä¸ªç®—æ³•æä¾›ä¸€ä¸ªæ¢¯åº¦ï¼Œç„¶åè®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ¯”æ–¹è¯´100ï¼Œæˆ‘ä»¬ç»™å‡ºä¸€ä¸ª çš„çŒœæµ‹åˆå§‹
å€¼ï¼Œå®ƒæ˜¯ä¸€ä¸ª2Ã—1çš„å‘é‡ï¼Œé‚£ä¹ˆè¿™ä¸ªå‘½ä»¤å°±è°ƒç”¨fminuncï¼Œè¿™ä¸ª@ç¬¦å·è¡¨ç¤ºæŒ‡å‘æˆ‘ä»¬åˆšåˆš
å®šä¹‰çš„costFunction å‡½æ•°çš„æŒ‡é’ˆã€‚å¦‚æœä½ è°ƒç”¨å®ƒï¼Œå®ƒå°±ä¼šä½¿ç”¨ä¼—å¤šé«˜çº§ä¼˜åŒ–ç®—æ³•ä¸­çš„ä¸€
ä¸ªï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥æŠŠå®ƒå½“æˆæ¢¯åº¦ä¸‹é™ï¼Œåªä¸è¿‡å®ƒèƒ½è‡ªåŠ¨é€‰æ‹©å­¦ä¹ é€Ÿç‡ï¼Œä½ ä¸éœ€è¦è‡ªå·±æ¥
åšã€‚ç„¶åå®ƒä¼šå°è¯•ä½¿ç”¨è¿™äº›é«˜çº§çš„ä¼˜åŒ–ç®—æ³•ï¼Œå°±åƒåŠ å¼ºç‰ˆçš„æ¢¯åº¦ä¸‹é™æ³•ï¼Œä¸ºä½ æ‰¾åˆ°æœ€ä½³
çš„å€¼ã€‚
</code></pre> 
  <blockquote> 
   <p>å­¦åˆ°çš„ä¸»è¦å†…å®¹æ˜¯ï¼šå†™ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒèƒ½è¿”å›ä»£ä»·å‡½æ•°å€¼ã€æ¢¯åº¦å€¼ï¼Œå› æ­¤è¦æŠŠè¿™ä¸ªåº”ç”¨åˆ°é€»è¾‘å›å½’ï¼Œæˆ–è€…ç”šè‡³çº¿æ€§å›å½’ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥æŠŠè¿™äº›ä¼˜åŒ–ç®—æ³•ç”¨äºçº¿æ€§å›å½’ï¼Œä½ éœ€è¦åšçš„å°±æ˜¯è¾“å…¥åˆé€‚çš„ä»£ç æ¥è®¡ç®—è¿™é‡Œçš„è¿™äº›ä¸œè¥¿ã€‚</p> 
  </blockquote> 
  <ol start="35"> 
   <li>Multiclass Classification_ One-vs-all ï¼ˆå¤šç±»åˆ«åˆ†ç±»ï¼šä¸€å¯¹å¤šï¼‰</li> 
  </ol> 
  <ul> 
   <li></li> 
  </ul> 
  <h2><a id="Part_7Regularization_460"></a>Part 7ï¼šæ­£åˆ™åŒ–(Regularization)</h2> 
  <blockquote> 
   <p>æœ‰æ—¶æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç®—æ³•åº”ç”¨åˆ°æŸäº›ç‰¹å®šçš„æœºå™¨å­¦ä¹ åº”ç”¨æ—¶ï¼Œé€šè¿‡å­¦ä¹ å¾—åˆ°çš„å‡è®¾å¯èƒ½èƒ½å¤Ÿéå¸¸å¥½åœ°é€‚åº”è®­ç»ƒé›†ï¼ˆä»£ä»·å‡½æ•°å¯èƒ½å‡ ä¹ä¸º0ï¼‰ï¼Œä½†æ˜¯å¯èƒ½ä¼šä¸èƒ½æ¨å¹¿åˆ°æ–°çš„æ•°æ®ã€‚è¿™å°±æ˜¯è¿‡æ‹Ÿåˆ(over-fitting)çš„é—®é¢˜ã€‚<br> æ­£åˆ™åŒ–(regularization)çš„æŠ€æœ¯ï¼Œå®ƒå¯ä»¥æ”¹å–„æˆ–è€…å‡å°‘è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</p> 
  </blockquote> 
  <ol start="36"> 
   <li>å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜çš„æ€è·¯</li> 
  </ol> 
  <ul> 
   <li>ä¸¢å¼ƒä¸€äº›ä¸èƒ½å¸®åŠ©æˆ‘ä»¬æ­£ç¡®é¢„æµ‹çš„ç‰¹å¾ã€‚å¯ä»¥æ˜¯æ‰‹å·¥é€‰æ‹©ä¿ç•™å“ªäº›ç‰¹å¾ï¼Œæˆ–è€…ä½¿ç”¨ä¸€äº›æ¨¡å‹é€‰æ‹©çš„ç®—æ³•æ¥å¸®å¿™ï¼ˆä¾‹å¦‚PCAï¼‰</li> 
   <li>æ­£åˆ™åŒ–ã€‚ ä¿ç•™æ‰€æœ‰çš„ç‰¹å¾ï¼Œä½†æ˜¯å‡å°‘å‚æ•°çš„å¤§å°ï¼ˆmagnitudeï¼‰ã€‚</li> 
  </ul> 
  <ol start="37"> 
   <li>å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜çš„ä»£ä»·å‡½æ•°</li> 
  </ol> 
  <ul> 
   <li>Ideaï¼š 
    <ul> 
     <li>è¿‡æ‹Ÿåˆé—®é¢˜çš„äº§ç”Ÿå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºé«˜æ¬¡é¡¹ã€‚æ‰€ä»¥ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯å¯¹é«˜æ­¤é¡¹è¿›è¡Œé™åˆ¶ï¼Œå…·ä½“åšæ³•å°±æ˜¯åœ¨ä»£ä»·å‡½æ•°é‡Œå¯¹ä»–ä»¬æ–½åŠ æƒ©ç½šã€‚</li> 
     <li>åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°è¿™äº›å‚æ•°thetaçš„å€¼ï¼Œè¿™å°±æ˜¯æ­£åˆ™åŒ–çš„åŸºæœ¬æ–¹æ³•ã€‚</li> 
    </ul> </li> 
   <li>ä¾‹å¦‚ï¼š<img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190802124341104.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
   <li>å‡å¦‚æˆ‘ä»¬æœ‰éå¸¸å¤šçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“å…¶ä¸­å“ªäº›ç‰¹å¾æˆ‘ä»¬è¦æƒ©ç½šï¼Œæˆ‘ä»¬å°†å¯¹æ‰€æœ‰çš„ç‰¹å¾è¿›è¡Œæƒ©ç½šï¼Œå¹¶ä¸”è®©ä»£ä»·å‡½æ•°æœ€ä¼˜åŒ–çš„è½¯ä»¶æ¥é€‰æ‹©è¿™äº›æƒ©ç½šçš„ç¨‹åº¦ã€‚è¿™æ ·çš„ç»“æœæ˜¯å¾—åˆ°äº†ä¸€ä¸ªè¾ƒä¸ºç®€å•çš„èƒ½é˜²æ­¢è¿‡æ‹Ÿåˆé—®é¢˜çš„å‡è®¾ï¼Œå°±åƒè¿™æ ·ï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190802124522145.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> å…¶ä¸­lambdaåˆç§°ä¸ºæ­£åˆ™åŒ–å‚æ•°ï¼ˆRguarization Parameterï¼‰ã€‚ æ³¨ï¼šæ ¹æ®æƒ¯ä¾‹ï¼Œæˆ‘ä»¬ä¸ theta0è¿›è¡Œæƒ©ç½šã€‚ 
    <ul> 
     <li>**lamnda **</li> 
    </ul> </li> 
  </ul> 
  <p>38.Regularized Linear Regression(æ­£åˆ™åŒ–çº¿æ€§å›å½’)</p> 
  <ul> 
   <li>æ­£åˆ™åŒ–çº¿æ€§å›å½’çš„ä»£ä»·å‡½æ•°ä¸ºï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190802131332288.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
   <li>å¦‚æœæˆ‘ä»¬è¦ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ä»¤è¿™ä¸ªä»£ä»·å‡½æ•°æœ€å°åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¯¹theta0è¿›è¡Œæ­£åˆ™åŒ–ï¼Œæ‰€ä»¥æ¢¯åº¦ä¸‹é™ç®—æ³•å°†åˆ†ç±»è®¨è®ºï¼Œå¯¹theta0ç‰¹æ®Šå¤„ç†ã€‚</li> 
   <li>æˆ‘ä»¬å¯¹æ–¹ç¨‹è¿›è¡Œæ‹†è§£ï¼Œå¯ä»¥çœ‹å‡ºï¼Œå®é™…ä¸Šæ­£åˆ™åŒ–çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å˜åŒ–åœ¨äºï¼Œæ¯æ¬¡éƒ½åœ¨åŸæœ‰ç®—æ³•æ›´æ–°è§„åˆ™çš„åŸºç¡€ä¸Šä»¤å€¼å‡å°‘äº†ä¸€ä¸ªé¢å¤–çš„å¾ˆå°çš„å€¼ã€‚<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019080213134470.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
   <li>æ­£è§„æ–¹ç¨‹æ¥æ±‚è§£æ­£åˆ™åŒ–çº¿æ€§å›å½’æ¨¡å‹ï¼š<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190802131351921.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li> 
  </ul> 
  <ol start="39"> 
   <li>Regularized Logistic Regression ï¼ˆæ­£åˆ™åŒ–é€»è¾‘å›å½’æ¨¡å‹ï¼‰</li> 
  </ol> 
  <ul> 
   <li>ä»¿ç…§çº¿æ€§å›å½’æ­£åˆ™åŒ–çš„æ€è·¯ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿®æ”¹ä»£ä»·å‡½æ•°ï¼Œæ·»åŠ ä¸€ä¸ªæ­£åˆ™åŒ–çš„è¡¨è¾¾å¼å³å¯ï¼š</li> 
  </ul> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019080213233365.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e44c3c0e64.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- è‡ªå®šä¹‰å¹¿å‘Š -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">æ›´å¤šç²¾å½©å†…å®¹</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">å›é¦–é¡µ</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d293c49e1e4bfe8f276695a5aa953300";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
