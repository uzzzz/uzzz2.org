<!DOCTYPE html>
<html amp lang="en-US" i-amphtml-layout="" i-amphtml-no-boilerplate="" transformed="self;v=1">
<head><meta charset="utf-8"><style amp-runtime="" i-amphtml-version="012111242025001">html{overflow-x:hidden!important}html.i-amphtml-fie{height:100%!important;width:100%!important}html:not([amp4ads]),html:not([amp4ads]) body{height:auto!important}html:not([amp4ads]) body{margin:0!important}body{-webkit-text-size-adjust:100%;-moz-text-size-adjust:100%;-ms-text-size-adjust:100%;text-size-adjust:100%}html.i-amphtml-singledoc.i-amphtml-embedded{-ms-touch-action:pan-y pinch-zoom;touch-action:pan-y pinch-zoom}html.i-amphtml-fie>body,html.i-amphtml-singledoc>body{overflow:visible!important}html.i-amphtml-fie:not(.i-amphtml-inabox)>body,html.i-amphtml-singledoc:not(.i-amphtml-inabox)>body{position:relative!important}html.i-amphtml-ios-embed-legacy>body{overflow-x:hidden!important;overflow-y:auto!important;position:absolute!important}html.i-amphtml-ios-embed{overflow-y:auto!important;position:static}#i-amphtml-wrapper{overflow-x:hidden!important;overflow-y:auto!important;position:absolute!important;top:0!important;left:0!important;right:0!important;bottom:0!important;margin:0!important;display:block!important}html.i-amphtml-ios-embed.i-amphtml-ios-overscroll,html.i-amphtml-ios-embed.i-amphtml-ios-overscroll>#i-amphtml-wrapper{-webkit-overflow-scrolling:touch!important}#i-amphtml-wrapper>body{position:relative!important;border-top:1px solid transparent!important}#i-amphtml-wrapper+body{visibility:visible}#i-amphtml-wrapper+body .i-amphtml-lightbox-element,#i-amphtml-wrapper+body[i-amphtml-lightbox]{visibility:hidden}#i-amphtml-wrapper+body[i-amphtml-lightbox] .i-amphtml-lightbox-element{visibility:visible}#i-amphtml-wrapper.i-amphtml-scroll-disabled,.i-amphtml-scroll-disabled{overflow-x:hidden!important;overflow-y:hidden!important}amp-instagram{padding:54px 0px 0px!important;background-color:#fff}amp-iframe iframe{box-sizing:border-box!important}[amp-access][amp-access-hide]{display:none}[subscriptions-dialog],body:not(.i-amphtml-subs-ready) [subscriptions-action],body:not(.i-amphtml-subs-ready) [subscriptions-section]{display:none!important}amp-experiment,amp-live-list>[update]{display:none}amp-list[resizable-children]>.i-amphtml-loading-container.amp-hidden{display:none!important}amp-list [fetch-error],amp-list[load-more] [load-more-button],amp-list[load-more] [load-more-end],amp-list[load-more] [load-more-failed],amp-list[load-more] [load-more-loading]{display:none}amp-list[diffable] div[role=list]{display:block}amp-story-page,amp-story[standalone]{min-height:1px!important;display:block!important;height:100%!important;margin:0!important;padding:0!important;overflow:hidden!important;width:100%!important}amp-story[standalone]{background-color:#000!important;position:relative!important}amp-story-page{background-color:#757575}amp-story .amp-active>div,amp-story .i-amphtml-loader-background{display:none!important}amp-story-page:not(:first-of-type):not([distance]):not([active]){transform:translateY(1000vh)!important}amp-autocomplete{position:relative!important;display:inline-block!important}amp-autocomplete>input,amp-autocomplete>textarea{padding:0.5rem;border:1px solid rgba(0,0,0,0.33)}.i-amphtml-autocomplete-results,amp-autocomplete>input,amp-autocomplete>textarea{font-size:1rem;line-height:1.5rem}[amp-fx^=fly-in]{visibility:hidden}amp-script[nodom],amp-script[sandboxed]{position:fixed!important;top:0!important;width:1px!important;height:1px!important;overflow:hidden!important;visibility:hidden}
/*# sourceURL=/css/ampdoc.css*/[hidden]{display:none!important}.i-amphtml-element{display:inline-block}.i-amphtml-blurry-placeholder{transition:opacity 0.3s cubic-bezier(0.0,0.0,0.2,1)!important;pointer-events:none}[layout=nodisplay]:not(.i-amphtml-element){display:none!important}.i-amphtml-layout-fixed,[layout=fixed][width][height]:not(.i-amphtml-layout-fixed){display:inline-block;position:relative}.i-amphtml-layout-responsive,[layout=responsive][width][height]:not(.i-amphtml-layout-responsive),[width][height][heights]:not([layout]):not(.i-amphtml-layout-responsive),[width][height][sizes]:not(img):not([layout]):not(.i-amphtml-layout-responsive){display:block;position:relative}.i-amphtml-layout-intrinsic,[layout=intrinsic][width][height]:not(.i-amphtml-layout-intrinsic){display:inline-block;position:relative;max-width:100%}.i-amphtml-layout-intrinsic .i-amphtml-sizer{max-width:100%}.i-amphtml-intrinsic-sizer{max-width:100%;display:block!important}.i-amphtml-layout-container,.i-amphtml-layout-fixed-height,[layout=container],[layout=fixed-height][height]:not(.i-amphtml-layout-fixed-height){display:block;position:relative}.i-amphtml-layout-fill,.i-amphtml-layout-fill.i-amphtml-notbuilt,[layout=fill]:not(.i-amphtml-layout-fill),body noscript>*{display:block;overflow:hidden!important;position:absolute;top:0;left:0;bottom:0;right:0}body noscript>*{position:absolute!important;width:100%;height:100%;z-index:2}body noscript{display:inline!important}.i-amphtml-layout-flex-item,[layout=flex-item]:not(.i-amphtml-layout-flex-item){display:block;position:relative;-ms-flex:1 1 auto;flex:1 1 auto}.i-amphtml-layout-fluid{position:relative}.i-amphtml-layout-size-defined{overflow:hidden!important}.i-amphtml-layout-awaiting-size{position:absolute!important;top:auto!important;bottom:auto!important}i-amphtml-sizer{display:block!important}@supports (aspect-ratio:1/1){i-amphtml-sizer.i-amphtml-disable-ar{display:none!important}}.i-amphtml-blurry-placeholder,.i-amphtml-fill-content{display:block;height:0;max-height:100%;max-width:100%;min-height:100%;min-width:100%;width:0;margin:auto}.i-amphtml-layout-size-defined .i-amphtml-fill-content{position:absolute;top:0;left:0;bottom:0;right:0}.i-amphtml-replaced-content,.i-amphtml-screen-reader{padding:0!important;border:none!important}.i-amphtml-screen-reader{position:fixed!important;top:0px!important;left:0px!important;width:4px!important;height:4px!important;opacity:0!important;overflow:hidden!important;margin:0!important;display:block!important;visibility:visible!important}.i-amphtml-screen-reader~.i-amphtml-screen-reader{left:8px!important}.i-amphtml-screen-reader~.i-amphtml-screen-reader~.i-amphtml-screen-reader{left:12px!important}.i-amphtml-screen-reader~.i-amphtml-screen-reader~.i-amphtml-screen-reader~.i-amphtml-screen-reader{left:16px!important}.i-amphtml-unresolved{position:relative;overflow:hidden!important}.i-amphtml-select-disabled{-webkit-user-select:none!important;-ms-user-select:none!important;user-select:none!important}.i-amphtml-notbuilt,[layout]:not(.i-amphtml-element),[width][height][heights]:not([layout]):not(.i-amphtml-element),[width][height][sizes]:not(img):not([layout]):not(.i-amphtml-element){position:relative;overflow:hidden!important;color:transparent!important}.i-amphtml-notbuilt:not(.i-amphtml-layout-container)>*,[layout]:not([layout=container]):not(.i-amphtml-element)>*,[width][height][heights]:not([layout]):not(.i-amphtml-element)>*,[width][height][sizes]:not([layout]):not(.i-amphtml-element)>*{display:none}amp-img:not(.i-amphtml-element)[i-amphtml-ssr]>img.i-amphtml-fill-content{display:block}.i-amphtml-notbuilt:not(.i-amphtml-layout-container),[layout]:not([layout=container]):not(.i-amphtml-element),[width][height][heights]:not([layout]):not(.i-amphtml-element),[width][height][sizes]:not(img):not([layout]):not(.i-amphtml-element){color:transparent!important;line-height:0!important}.i-amphtml-ghost{visibility:hidden!important}.i-amphtml-element>[placeholder],[layout]:not(.i-amphtml-element)>[placeholder],[width][height][heights]:not([layout]):not(.i-amphtml-element)>[placeholder],[width][height][sizes]:not([layout]):not(.i-amphtml-element)>[placeholder]{display:block;line-height:normal}.i-amphtml-element>[placeholder].amp-hidden,.i-amphtml-element>[placeholder].hidden{visibility:hidden}.i-amphtml-element:not(.amp-notsupported)>[fallback],.i-amphtml-layout-container>[placeholder].amp-hidden,.i-amphtml-layout-container>[placeholder].hidden{display:none}.i-amphtml-layout-size-defined>[fallback],.i-amphtml-layout-size-defined>[placeholder]{position:absolute!important;top:0!important;left:0!important;right:0!important;bottom:0!important;z-index:1}amp-img[i-amphtml-ssr]:not(.i-amphtml-element)>[placeholder]{z-index:auto}.i-amphtml-notbuilt>[placeholder]{display:block!important}.i-amphtml-hidden-by-media-query{display:none!important}.i-amphtml-element-error{background:red!important;color:#fff!important;position:relative!important}.i-amphtml-element-error:before{content:attr(error-message)}i-amp-scroll-container,i-amphtml-scroll-container{position:absolute;top:0;left:0;right:0;bottom:0;display:block}i-amp-scroll-container.amp-active,i-amphtml-scroll-container.amp-active{overflow:auto;-webkit-overflow-scrolling:touch}.i-amphtml-loading-container{display:block!important;pointer-events:none;z-index:1}.i-amphtml-notbuilt>.i-amphtml-loading-container{display:block!important}.i-amphtml-loading-container.amp-hidden{visibility:hidden}.i-amphtml-element>[overflow]{cursor:pointer;position:relative;z-index:2;visibility:hidden;display:initial;line-height:normal}.i-amphtml-layout-size-defined>[overflow]{position:absolute}.i-amphtml-element>[overflow].amp-visible{visibility:visible}template{display:none!important}.amp-border-box,.amp-border-box *,.amp-border-box :after,.amp-border-box :before{box-sizing:border-box}amp-pixel{display:none!important}amp-analytics,amp-auto-ads,amp-story-auto-ads{position:fixed!important;top:0!important;width:1px!important;height:1px!important;overflow:hidden!important;visibility:hidden}html.i-amphtml-fie>amp-analytics{position:initial!important}[visible-when-invalid]:not(.visible),form [submit-error],form [submit-success],form [submitting]{display:none}amp-accordion{display:block!important}@media (min-width:1px){:where(amp-accordion>section)>:first-child{margin:0;background-color:#efefef;padding-right:20px;border:1px solid #dfdfdf}:where(amp-accordion>section)>:last-child{margin:0}}amp-accordion>section{float:none!important}amp-accordion>section>*{float:none!important;display:block!important;overflow:hidden!important;position:relative!important}amp-accordion,amp-accordion>section{margin:0}amp-accordion:not(.i-amphtml-built)>section>:last-child{display:none!important}amp-accordion:not(.i-amphtml-built)>section[expanded]>:last-child{display:block!important}
/*# sourceURL=/css/ampshared.css*/</style><meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1"><title>deeplabv3+二：详细代码解读 data generator 数据生成器 - 有组织在!</title><link rel="preconnect" href="https://cdn.ampproject.org"><link rel="preload" as="script" href="https://cdn.ampproject.org/v0.js"><script async="" src="https://cdn.ampproject.org/v0.js"></script><style amp-custom="">html{background:#0a89c0}body{background:#fff;color:#353535;font-family:Georgia,"Times New Roman",Times,Serif;font-weight:300;line-height:1.75em}p,ul{margin:0 0 1em;padding:0}a,a:visited{color:#0a89c0}a:hover,a:active,a:focus{color:#353535}.amp-wp-meta,.amp-wp-header div,.amp-wp-title,.amp-wp-tax-category,.amp-wp-footer p,.back-to-top{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI","Roboto","Oxygen-Sans","Ubuntu","Cantarell","Helvetica Neue",sans-serif}.amp-wp-header{background-color:#0a89c0}.amp-wp-header div{color:#fff;font-size:1em;font-weight:400;margin:0 auto;max-width:calc(840px - 32px);padding:.875em 16px;position:relative}.amp-wp-header a{color:#fff;text-decoration:none}.amp-wp-header .amp-wp-site-icon{background-color:#fff;border:1px solid #fff;border-radius:50%;position:absolute;right:18px;top:10px}.amp-wp-article{color:#353535;font-weight:400;margin:1.5em auto;max-width:840px;overflow-wrap:break-word;word-wrap:break-word}.amp-wp-article-header{align-items:center;align-content:stretch;display:flex;flex-wrap:wrap;justify-content:space-between;margin:1.5em 16px 0}.amp-wp-title{color:#353535;display:block;flex:1 0 100%;font-weight:900;margin:0 0 .625em;width:100%}.amp-wp-meta{color:#696969;display:inline-block;flex:2 1 50%;font-size:.875em;line-height:1.5em;margin:0 0 1.5em;padding:0}.amp-wp-article-header .amp-wp-meta:last-of-type{text-align:right}.amp-wp-article-header .amp-wp-meta:first-of-type{text-align:left}.amp-wp-byline amp-img,.amp-wp-byline .amp-wp-author{display:inline-block;vertical-align:middle}.amp-wp-byline amp-img{border:1px solid #0a89c0;border-radius:50%;position:relative;margin-right:6px}.amp-wp-posted-on{text-align:right}.amp-wp-article-content{margin:0 16px}.amp-wp-article-content ul{margin-left:1em}.amp-wp-article-content amp-img{margin:0 auto}.amp-wp-article-footer .amp-wp-meta{display:block}.amp-wp-tax-category{color:#696969;font-size:.875em;line-height:1.5em;margin:1.5em 16px}.amp-wp-footer{border-top:1px solid #c2c2c2;margin:calc(1.5em - 1px) 0 0}.amp-wp-footer div{margin:0 auto;max-width:calc(840px - 32px);padding:1.25em 16px 1.25em;position:relative}.amp-wp-footer h2{font-size:1em;line-height:1.375em;margin:0 0 .5em}.amp-wp-footer p{color:#696969;font-size:.8em;line-height:1.5em;margin:0 85px 0 0}.amp-wp-footer a{text-decoration:none}.back-to-top{bottom:1.275em;font-size:.8em;font-weight:600;line-height:2em;position:absolute;right:16px}.htmledit_views span[lang]{font-style:italic}.htmledit_views{font-family:-apple-system,SF UI Text,Arial,PingFang SC,Hiragino Sans GB,Microsoft YaHei,WenQuanYi Micro Hei,sans-serif,SimHei,SimSun}.htmledit_views a>amp-img{padding:1px;margin:1px;border:none;outline:#0782c1 solid 1px}.htmledit_views p{font-size:16px;color:#4d4d4d;font-weight:400;line-height:26px;margin:0 0 16px;overflow-x:auto}.htmledit_views amp-img{max-width:100%}.htmledit_views strong,.htmledit_views strong span{font-weight:700}.htmledit_views *{box-sizing:border-box}.htmledit_views h1,.htmledit_views h2,.htmledit_views h3{color:#4f4f4f;margin:8px 0 16px;font-weight:700}.htmledit_views ul{margin:0 0 24px;padding:0;font-size:16px}.htmledit_views ul li{list-style-type:disc;margin:8px 0 0 32px}.htmledit_views h1{font-size:28px;line-height:36px}.htmledit_views h2{font-size:24px;line-height:32px}.htmledit_views h3{font-size:22px;line-height:30px}.htmledit_views hr{margin:24px 0;border:none;border-bottom:solid #ccc 1px}.htmledit_views pre{white-space:pre-wrap;word-wrap:break-word;margin:0 0 24px;overflow-x:auto;padding:8px}.htmledit_views pre{font-family:Consolas,Inconsolata,Courier,monospace;font-size:14px;line-height:22px;color:#000}.htmledit_views pre code,.htmledit_views pre code div,.htmledit_views pre code span{font-family:"Source Code Pro","DejaVu Sans Mono","Ubuntu Mono","Anonymous Pro","Droid Sans Mono",Menlo,Monaco,Consolas,Inconsolata,Courier,monospace,"PingFang SC","Microsoft YaHei",sans-serif}.htmledit_views code{border-radius:4px}.htmledit_views a{color:#4ea1db;text-decoration:none}.htmledit_views a:focus,.htmledit_views a:hover{color:#ca0c16}.htmledit_views a:visited{color:#6795b5}.htmledit_views pre code{display:block;line-height:22px;overflow-x:auto;white-space:pre;word-wrap:normal;border-radius:4px;padding:8px}.htmledit_views pre code:not(.hljs){background-color:#f3f4f5}.htmledit_views pre code,.htmledit_views pre code div,.htmledit_views pre code span{font-size:14px}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-5569a49{margin-left:80px}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-b5fbaa3{margin-left:40px}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-7abfdd6{margin-left:0px}

/*# sourceURL=amp-custom.css */</style><script type="application/ld+json" class="yoast-schema-graph yoast-schema-graph--main">{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://uzzz.org/#website","url":"https://uzzz.org/","name":"\u6709\u7ec4\u7ec7\u5728!","potentialAction":{"@type":"SearchAction","target":"https://uzzz.org/?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"WebPage","@id":"https://uzzz.org/article/2535/#webpage","url":"https://uzzz.org/article/2535/","inLanguage":"en-US","name":"deeplabv3+\u4e8c\uff1a\u8be6\u7ec6\u4ee3\u7801\u89e3\u8bfb data generator \u6570\u636e\u751f\u6210\u5668 - \u6709\u7ec4\u7ec7\u5728!","isPartOf":{"@id":"https://uzzz.org/#website"},"datePublished":"2019-07-23T07:50:45+00:00","dateModified":"2019-07-23T07:50:45+00:00","author":{"@id":"https://uzzz.org/#/schema/person/29673f1347b0abda5882803c72ee5a3f"}},{"@type":["Person"],"@id":"https://uzzz.org/#/schema/person/29673f1347b0abda5882803c72ee5a3f","name":"fandyvon","sameAs":[]}]}</script><link rel="canonical" href="https://uzzz.org/article/2535/"></head>

<body class="">

<header id="top" class="amp-wp-header">
	<div>
		<a href="https://uzzz.org/">
										<amp-img src="https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png" width="32" height="32" class="amp-wp-site-icon i-amphtml-layout-fixed i-amphtml-layout-size-defined" style="width:32px;height:32px;" i-amphtml-layout="fixed"></amp-img>
						<span class="amp-site-title">
				有组织在!			</span>
		</a>

					</div>
</header>

<article class="amp-wp-article">
	<header class="amp-wp-article-header">
		<h1 class="amp-wp-title">deeplabv3+二：详细代码解读 data generator 数据生成器</h1>
			<div class="amp-wp-meta amp-wp-byline">
					<amp-img src="https://secure.gravatar.com/avatar/e786821a74ef0467825a7d60183307bc?s=24&d=mm&r=g" alt="fandyvon" width="24" height="24" layout="fixed" class="i-amphtml-layout-fixed i-amphtml-layout-size-defined" style="width:24px;height:24px;" i-amphtml-layout="fixed"></amp-img>
				<span class="amp-wp-author author vcard">fandyvon</span>
	</div>
<div class="amp-wp-meta amp-wp-posted-on">
	<time datetime="2019-07-23T15:50:45+00:00">
		2 years ago	</time>
</div>
	</header>

	
	<div class="amp-wp-article-content">
		<div id="article_content" class="article_content clearfix">
 <br>
 
 
 
<div class="htmledit_views" id="content_views">
<p>3+支持三种数据库，voc2012，cityscapes，ade20k，</p>
<p>代码文件夹</p>
<p>-deeplab</p>
<p>    -datasets</p>
<p>         -data_generator.py</p>
<p>在开始之前，始终记住，网络模型的输入是非常简单的image，规格化到[-1,1]或[0,1]，或者数据扩增(水平翻转，随机裁剪，明暗变化，模糊)，以及一个实施了相同数据扩增的label（毕竟需要pixel对上），test的话只需要一个image。是非常简单的数据格式，也许程序员会为了存储的压缩量以及读取处理的速度（指的就是使用tf.example 与 tf.record）写复杂的代码，但是最终的结果始终都是很简单的。</p>
<p>觉得自己一定要先搞清楚tf.example 与tf.record:<a href="https://zhuanlan.zhihu.com/p/33223782" rel="nofollow" data-token="998ab165948c6442333241c120b96d4f">https://zhuanlan.zhihu.com/p/33223782</a></p>
<p> </p>
<p id="main-toc"><strong>目录</strong></p>
<p id="%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E6%9E%90-toc" data-amp-original-style="margin-left:0px;" class="amp-wp-7abfdd6"><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E6%9E%90" rel="nofollow" data-token="cb14a065ce6de53cf5b34f7425e9c57c">数据库分析</a></p>
<p id="%E4%BB%A3%E7%A0%81%E9%87%8D%E7%82%B9%E7%B1%BBDataset-toc" data-amp-original-style="margin-left:0px;" class="amp-wp-7abfdd6"><a href="#%E4%BB%A3%E7%A0%81%E9%87%8D%E7%82%B9%E7%B1%BBDataset" rel="nofollow" data-token="6a0102806ffd6f6b6d37e289026f9d43">代码重点类Dataset</a></p>
<p id="1.%E6%96%B9%E6%B3%95_parse_function()-toc" data-amp-original-style="margin-left:40px;" class="amp-wp-b5fbaa3"><a href="#1.%E6%96%B9%E6%B3%95_parse_function()" rel="nofollow" data-token="d080b9446123d1258f85a38c9ac7955b">1.方法_parse_function()</a></p>
<p id="2.%20%E6%96%B9%E6%B3%95_preprocess_image()-toc" data-amp-original-style="margin-left:40px;" class="amp-wp-b5fbaa3"><a href="#2.%20%E6%96%B9%E6%B3%95_preprocess_image()" rel="nofollow" data-token="effa3c1d1d83b1e8ef5d9ff79dfa67c6">2. 方法_preprocess_image()</a></p>
<p id="2.1%20input_preprocess%E7%9A%84preprocess_image_and_label%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D-toc" data-amp-original-style="margin-left:80px;" class="amp-wp-5569a49"><a href="#2.1%20input_preprocess%E7%9A%84preprocess_image_and_label%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D" rel="nofollow" data-token="2110a000043d3d14a38c6470cbde40fe">2.1 input_preprocess的preprocess_image_and_label方法介绍</a></p>
<p id="3.%E6%96%B9%E6%B3%95%20_get_all_files(self)%3A-toc" data-amp-original-style="margin-left:40px;" class="amp-wp-b5fbaa3"><a href="#3.%E6%96%B9%E6%B3%95%20_get_all_files(self)%3A" rel="nofollow" data-token="3195f17cd637751b46bb9bed9c74f1d9">3.方法 _get_all_files(self):</a></p>
<p id="4.%E6%96%B9%E6%B3%95%20get_one_shot_iterator(self)-toc" data-amp-original-style="margin-left:40px;" class="amp-wp-b5fbaa3"><a href="#4.%E6%96%B9%E6%B3%95%20get_one_shot_iterator(self)" rel="nofollow" data-token="d624b6a85f889f4ee9653ae3d0ff3381">4.方法 get_one_shot_iterator(self)</a></p>
<p id="class_tfrecorddataset-toc" data-amp-original-style="margin-left:40px;" class="amp-wp-b5fbaa3"><a href="#class_tfrecorddataset" rel="nofollow" data-token="0011cfd6b7fa5c5ba5e343621b54c052">Class TFRecordDataset</a></p>
<p id="%E4%BB%A3%E7%A0%81%E4%BD%BF%E7%94%A8%E6%98%AF%E5%9C%A8train.py%E9%87%8C%E9%9D%A2%EF%BC%9A-toc" data-amp-original-style="margin-left:0px;" class="amp-wp-7abfdd6"><a href="#%E4%BB%A3%E7%A0%81%E4%BD%BF%E7%94%A8%E6%98%AF%E5%9C%A8train.py%E9%87%8C%E9%9D%A2%EF%BC%9A" rel="nofollow" data-token="24adfdff0dce3ff060cd80e70311ac1b">代码使用是在train.py里面：</a></p>
<hr id="hr-toc">
<p>代码：先放代码，你可以尝试自己看，看得懂就不用往下翻浪费时间了。</p>
<pre class="has">
<code class="language-python"># Copyright 2018 The TensorFlow Authors All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Wrapper for providing semantic segmentaion data.

The SegmentationDataset class provides both images and annotations (semantic
segmentation and/or instance segmentation) for TensorFlow. Currently, we
support the following datasets:

1. PASCAL VOC 2012 (http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).

PASCAL VOC 2012 semantic segmentation dataset annotates 20 foreground objects
(e.g., bike, person, and so on) and leaves all the other semantic classes as
one background class. The dataset contains 1464, 1449, and 1456 annotated
images for the training, validation and test respectively.

2. Cityscapes dataset (https://www.cityscapes-dataset.com)

The Cityscapes dataset contains 19 semantic labels (such as road, person, car,
and so on) for urban street scenes.

3. ADE20K dataset (http://groups.csail.mit.edu/vision/datasets/ADE20K)

The ADE20K dataset contains 150 semantic labels both urban street scenes and
indoor scenes.

References:
  M. Everingham, S. M. A. Eslami, L. V. Gool, C. K. I. Williams, J. Winn,
  and A. Zisserman, The pascal visual object classes challenge a retrospective.
  IJCV, 2014.

  M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
  U. Franke, S. Roth, and B. Schiele, "The cityscapes dataset for semantic urban
  scene understanding," In Proc. of CVPR, 2016.

  B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, A. Torralba, "Scene Parsing
  through ADE20K dataset", In Proc. of CVPR, 2017.
"""

import collections
import os
import tensorflow as tf
from deeplab import common
from deeplab import input_preprocess

# Named tuple to describe the dataset properties.
DatasetDescriptor = collections.namedtuple(
    'DatasetDescriptor',
    [
        'splits_to_sizes',  # Splits of the dataset into training, val and test.
        'num_classes',  # Number of semantic classes, including the
                        # background class (if exists). For example, there
                        # are 20 foreground classes + 1 background class in
                        # the PASCAL VOC 2012 dataset. Thus, we set
                        # num_classes=21.
        'ignore_label',  # Ignore label value.
    ])

_CITYSCAPES_INFORMATION = DatasetDescriptor(
    splits_to_sizes={
        'train': 2975,
        'val': 500,
    },
    num_classes=19,
    ignore_label=255,
)

_PASCAL_VOC_SEG_INFORMATION = DatasetDescriptor(
    splits_to_sizes={
        'train': 1464,
        'train_aug': 10582,
        'trainval': 2913,
        'val': 1449,
    },
    num_classes=21,
    ignore_label=255,
)

_ADE20K_INFORMATION = DatasetDescriptor(
    splits_to_sizes={
        'train': 20210,  # num of samples in images/training
        'val': 2000,  # num of samples in images/validation
    },
    num_classes=151,
    ignore_label=0,
)

_DATASETS_INFORMATION = {
    'cityscapes': _CITYSCAPES_INFORMATION,
    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
    'ade20k': _ADE20K_INFORMATION,
}

# Default file pattern of TFRecord of TensorFlow Example.
_FILE_PATTERN = '%s-*'


def get_cityscapes_dataset_name():
  return 'cityscapes'


class Dataset(object):
  """Represents input dataset for deeplab model."""

  def __init__(self,
               dataset_name,
               split_name,
               dataset_dir,
               batch_size,
               crop_size,
               min_resize_value=None,
               max_resize_value=None,
               resize_factor=None,
               min_scale_factor=1.,
               max_scale_factor=1.,
               scale_factor_step_size=0,
               model_variant=None,
               num_readers=1,
               is_training=False,
               should_shuffle=False,
               should_repeat=False):
    """Initializes the dataset.

    Args:
      dataset_name: Dataset name.
      split_name: A train/val Split name.
      dataset_dir: The directory of the dataset sources.
      batch_size: Batch size.
      crop_size: The size used to crop the image and label.
      min_resize_value: Desired size of the smaller image side.
      max_resize_value: Maximum allowed size of the larger image side.
      resize_factor: Resized dimensions are multiple of factor plus one.
      min_scale_factor: Minimum scale factor value.
      max_scale_factor: Maximum scale factor value.
      scale_factor_step_size: The step size from min scale factor to max scale
        factor. The input is randomly scaled based on the value of
        (min_scale_factor, max_scale_factor, scale_factor_step_size).
      model_variant: Model variant (string) for choosing how to mean-subtract
        the images. See feature_extractor.network_map for supported model
        variants.
      num_readers: Number of readers for data provider.
      is_training: Boolean, if dataset is for training or not.
      should_shuffle: Boolean, if should shuffle the input data.
      should_repeat: Boolean, if should repeat the input data.

    Raises:
      ValueError: Dataset name and split name are not supported.
    """
    if dataset_name not in _DATASETS_INFORMATION:
      raise ValueError('The specified dataset is not supported yet.')
    self.dataset_name = dataset_name

    splits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes

    if split_name not in splits_to_sizes:
      raise ValueError('data split name %s not recognized' % split_name)

    if model_variant is None:
      tf.logging.warning('Please specify a model_variant. See '
                         'feature_extractor.network_map for supported model '
                         'variants.')

    self.split_name = split_name
    self.dataset_dir = dataset_dir
    self.batch_size = batch_size
    self.crop_size = crop_size
    self.min_resize_value = min_resize_value
    self.max_resize_value = max_resize_value
    self.resize_factor = resize_factor
    self.min_scale_factor = min_scale_factor
    self.max_scale_factor = max_scale_factor
    self.scale_factor_step_size = scale_factor_step_size
    self.model_variant = model_variant
    self.num_readers = num_readers
    self.is_training = is_training
    self.should_shuffle = should_shuffle
    self.should_repeat = should_repeat

    self.num_of_classes = _DATASETS_INFORMATION[self.dataset_name].num_classes
    self.ignore_label = _DATASETS_INFORMATION[self.dataset_name].ignore_label

  def _parse_function(self, example_proto):
    """Function to parse the example proto.

    Args:
      example_proto: Proto in the format of tf.Example.

    Returns:
      A dictionary with parsed image, label, height, width and image name.

    Raises:
      ValueError: Label is of wrong shape.
    """

    # Currently only supports jpeg and png.
    # Need to use this logic because the shape is not known for
    # tf.image.decode_image and we rely on this info to
    # extend label if necessary.
    def _decode_image(content, channels):
      return tf.cond(
          tf.image.is_jpeg(content),
          lambda: tf.image.decode_jpeg(content, channels),
          lambda: tf.image.decode_png(content, channels))

    features = {
        'image/encoded':
            tf.FixedLenFeature((), tf.string, default_value=''),
        'image/filename':
            tf.FixedLenFeature((), tf.string, default_value=''),
        'image/format':
            tf.FixedLenFeature((), tf.string, default_value='jpeg'),
        'image/height':
            tf.FixedLenFeature((), tf.int64, default_value=0),
        'image/width':
            tf.FixedLenFeature((), tf.int64, default_value=0),
        'image/segmentation/class/encoded':
            tf.FixedLenFeature((), tf.string, default_value=''),
        'image/segmentation/class/format':
            tf.FixedLenFeature((), tf.string, default_value='png'),
    }

    parsed_features = tf.parse_single_example(example_proto, features)

    image = _decode_image(parsed_features['image/encoded'], channels=3)

    label = None
    if self.split_name != common.TEST_SET:
      label = _decode_image(
          parsed_features['image/segmentation/class/encoded'], channels=1)

    image_name = parsed_features['image/filename']
    if image_name is None:
      image_name = tf.constant('')

    sample = {
        common.IMAGE: image,
        common.IMAGE_NAME: image_name,
        common.HEIGHT: parsed_features['image/height'],
        common.WIDTH: parsed_features['image/width'],
    }

    if label is not None:
      if label.get_shape().ndims == 2:
        label = tf.expand_dims(label, 2)
      elif label.get_shape().ndims == 3 and label.shape.dims[2] == 1:
        pass
      else:
        raise ValueError('Input label shape must be [height, width], or '
                         '[height, width, 1].')

      label.set_shape([None, None, 1])

      sample[common.LABELS_CLASS] = label

    return sample

  def _preprocess_image(self, sample):
    """Preprocesses the image and label.

    Args:
      sample: A sample containing image and label.

    Returns:
      sample: Sample with preprocessed image and label.

    Raises:
      ValueError: Ground truth label not provided during training.
    """
    image = sample[common.IMAGE]
    label = sample[common.LABELS_CLASS]

    original_image, image, label = input_preprocess.preprocess_image_and_label(
        image=image,
        label=label,
        crop_height=self.crop_size[0],
        crop_width=self.crop_size[1],
        min_resize_value=self.min_resize_value,
        max_resize_value=self.max_resize_value,
        resize_factor=self.resize_factor,
        min_scale_factor=self.min_scale_factor,
        max_scale_factor=self.max_scale_factor,
        scale_factor_step_size=self.scale_factor_step_size,
        ignore_label=self.ignore_label,
        is_training=self.is_training,
        model_variant=self.model_variant)

    sample[common.IMAGE] = image

    if not self.is_training:
      # Original image is only used during visualization.
      sample[common.ORIGINAL_IMAGE] = original_image

    if label is not None:
      sample[common.LABEL] = label

    # Remove common.LABEL_CLASS key in the sample since it is only used to
    # derive label and not used in training and evaluation.
    sample.pop(common.LABELS_CLASS, None)

    return sample

  def get_one_shot_iterator(self):
    """Gets an iterator that iterates across the dataset once.

    Returns:
      An iterator of type tf.data.Iterator.
    """

    files = self._get_all_files()

    dataset = (
        tf.data.TFRecordDataset(files, num_parallel_reads=self.num_readers)
        .map(self._parse_function, num_parallel_calls=self.num_readers)
        .map(self._preprocess_image, num_parallel_calls=self.num_readers))

    if self.should_shuffle:
      dataset = dataset.shuffle(buffer_size=100)

    if self.should_repeat:
      dataset = dataset.repeat()  # Repeat forever for training.
    else:
      dataset = dataset.repeat(1)

    dataset = dataset.batch(self.batch_size).prefetch(self.batch_size)
    return dataset.make_one_shot_iterator()

  def _get_all_files(self):
    """Gets all the files to read data from.

    Returns:
      A list of input files.
    """
    file_pattern = _FILE_PATTERN
    file_pattern = os.path.join(self.dataset_dir,
                                file_pattern % self.split_name)
    return tf.gfile.Glob(file_pattern)
</code></pre>
<h1 id="%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E6%9E%90">数据库分析</h1>
<p>声明了：namedtuple，新建了一种数据类型，格式：一般来讲splits_to_sizes这个属性是数据的图片的split，集合，train,train_aug,trainval,val这种。可以告诉model你有多少图片在训练模式，或者是与训练模式要完成。</p>
<pre class="has">
<code class="language-python">DatasetDescriptor = collections.namedtuple(
    'DatasetDescriptor',
    [
        'splits_to_sizes',  # Splits of the dataset into training, val and test.
        'num_classes',  # Number of semantic classes, including the
                        # background class (if exists). For example, there
                        # are 20 foreground classes + 1 background class in
                        # the PASCAL VOC 2012 dataset. Thus, we set
                        # num_classes=21.
        'ignore_label',  # Ignore label value.
    ])
</code></pre>
<p>举个例子：voc数据用这个数据类型DatasetDescriptor来声明这个数据库的一些信息训练图片1464张，val1449张，一共有21类，包括背景类，255是那个白边，也就是未标注类，所以是不计入损失函数的。</p>
<pre class="has">
<code class="language-python">_PASCAL_VOC_SEG_INFORMATION = DatasetDescriptor(
    splits_to_sizes={
        'train': 1464,
        'train_aug': 10582,
        'trainval': 2913,
        'val': 1449,
    },
    num_classes=21,
    ignore_label=255,
)</code></pre>
<p>代码中一共有三个数据库建立了DataDescriptor 信息，字典如下</p>
<pre class="has">
<code class="language-python">_DATASETS_INFORMATION = {
    'cityscapes': _CITYSCAPES_INFORMATION,
    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
    'ade20k': _ADE20K_INFORMATION,
}</code></pre>
<p>如果提供的数据库不属于已建立的，会出错，所以自己单独的数据需要建立相同得到generator。</p>
<h1 id="%E4%BB%A3%E7%A0%81%E9%87%8D%E7%82%B9%E7%B1%BBDataset">代码重点类Dataset</h1>
<p>这个代码中最重要的是Dataset这个类。</p>
<p> </p>
<p>这个类初始化需要传入很多参数，代码的注释写的很详细。其中model_variant必须传入，否则就会raise 错误。</p>
<p>model_variant这里的解释：</p>
<p>“</p>
<pre class="has">
<code class="language-html">model_variant: Model variant (string) for choosing how to mean-subtract
  the images. See feature_extractor.network_map for supported model
  variants.</code></pre>
<p>”</p>
<p>有时候数据处理需要减去mean的值那么对于图片就是255/2，把数据处理成[-1,1]</p>
<p>毕竟如果不减去那就是[0,1]规格化，feature_extractor在core文件夹下。</p>
<p> </p>
<p>还有另外的两个属性初始化来自于数据库信息，DataGenerator</p>
<pre class="has">
<code class="language-python">    self.num_of_classes = _DATASETS_INFORMATION[self.dataset_name].num_classes
    self.ignore_label = _DATASETS_INFORMATION[self.dataset_name].ignore_label</code></pre>
<p>再来说方法：</p>
<ul>
<li>_parse_function(self,example_proto)</li>
<li>_preprocess_image(self, sample)</li>
<li>get_one_shot_iterator(self)</li>
<li>_get_all_files(self)</li>
</ul>
<h2 id="1.%E6%96%B9%E6%B3%95_parse_function()">1.方法_parse_function()</h2>
<p>其中_parse_function 是解析tf.example 一种tf特有的数据格式到字典的。返回的是图片的所有信息。</p>
<pre class="has">
<code class="language-html">"""Function to parse the example proto.

Args:
  example_proto: Proto in the format of tf.Example.

Returns:
  A dictionary with parsed image, label, height, width and image name.

Raises:
  ValueError: Label is of wrong shape.
"""
</code></pre>
<p>该函数还包含了另一个函数：</p>
<pre class="has">
<code class="language-python">    def _decode_image(content, channels):
      return tf.cond(
          tf.image.is_jpeg(content),
          lambda: tf.image.decode_jpeg(content, channels),
          lambda: tf.image.decode_png(content, channels))</code></pre>
<p>将encode的图片解析uint8的tensor。所以网络的输入依旧是熟知的数据类型float，虽然tf官方为了目前没有看到作用的原因转换了下数据结构（不转换正常yeild），最终的generator又转了回来。</p>
<p>features提供了tf.example（可以认为也像字典那样存储，有key值）转成正常python字典的键值：</p>
<pre class="has">
<code class="language-python">features = {
        'image/encoded':
            tf.FixedLenFeature((), tf.string, default_value=''),
        'image/filename':
            tf.FixedLenFeature((), tf.string, default_value=''),
        'image/format':
            tf.FixedLenFeature((), tf.string, default_value='jpeg'),
        'image/height':
            tf.FixedLenFeature((), tf.int64, default_value=0),
        'image/width':
            tf.FixedLenFeature((), tf.int64, default_value=0),
        'image/segmentation/class/encoded':
            tf.FixedLenFeature((), tf.string, default_value=''),
        'image/segmentation/class/format':
            tf.FixedLenFeature((), tf.string, default_value='png'),
    }</code></pre>
<p>函数的参数example_proto就是待转换的被存储成example格式的图片数据，</p>
<p>利用</p>
<pre class="has">
<code class="language-python">parsed_features = tf.parse_single_example(example_proto, features)</code></pre>
<p>tf的函数parse_single_example就是将tf.example按照features格式解析的函数，那么还有一个问题，那么整体的打包解包格式到底是什么样的。</p>
<p>比如图片segmentation任务，feature包括image以及他的label作为网络输入。</p>
<p>那么一个uint8的图片会收到怎么样的处理。</p>
<p>打包过程：uint8->encoded_image->tf.example->存储tf.record</p>
<p>解包过程：读取tf.record->tf.example->encoded_image->uint8</p>
<p>所以parse_single_example 的输出是一个encode_image,需要使用上面提到的_decode_iamge转换。</p>
<p>这就是image的处理</p>
<p>对于label，测试数据是没有label的所以label=None</p>
<p>如果不是测试数据，那么用和image一样的方法来解析label, 如果你注意到了common，我直接把common.TEST_SET放这：</p>
<pre class="has">
<code class="language-html"># Test set name.
TEST_SET = 'test'</code></pre>
<p>明白了吧。</p>
<pre class="has">
<code class="language-python">    if self.split_name != common.TEST_SET:
      label = _decode_image(
          parsed_features['image/segmentation/class/encoded'], channels=1)</code></pre>
<p>就是比较奇怪，为什么不将_decode_image直接封装到decode_image中，而是要用户自己先做判断再挑选方法。可能下面的注释是解释，对于_decode_image的这段叙述：</p>
<pre class="has">
<code class="language-html"># Currently only supports jpeg and png.
# Need to use this logic because the shape is not known for
# tf.image.decode_image and we rely on this info to
# extend label if necessary.</code></pre>
<p>说是因为image shape未知，目前还没看到为什么用了这个条件语句shape就已知了。</p>
<p>另外features明明包含了</p>
<pre class="has">
<code class="language-html">'image/height':
    tf.FixedLenFeature((), tf.int64, default_value=0),
'image/width':
    tf.FixedLenFeature((), tf.int64, default_value=0),</code></pre>
<p>这里有个比较重要的东西，就是label的shape</p>
<p>label必须是三维的[512,512,1]，也就是说：</p>
<pre class="has">
<code class="language-html">'Input label shape must be [height, width], or '
                 '[height, width, 1].'</code></pre>
<pre class="has">
<code class="language-python">    if label is not None:
      if label.get_shape().ndims == 2:
        label = tf.expand_dims(label, 2)
      elif label.get_shape().ndims == 3 and label.shape.dims[2] == 1:
        pass
      else:
        raise ValueError('Input label shape must be [height, width], or '
                         '[height, width, 1].')</code></pre>
<p>最终返回的sample是一个字典：其中common表示的string我都直接给你显示出来了。common没什么神奇的，就是一个常数映射IMAGE=’image’</p>
<p>这样写代码的时候能够保持一致，并且使用Tab补全。</p>
<pre class="has">
<code class="language-python">    sample = {
        common.IMAGE: image,
        common.IMAGE_NAME: image_name,
        common.HEIGHT: parsed_features['image/height'],
        common.WIDTH: parsed_features['image/width'],
    }
      sample[common.LABELS_CLASS] = label
##import common
# Semantic segmentation item names.
LABELS_CLASS = 'labels_class'
IMAGE = 'image'
HEIGHT = 'height'
WIDTH = 'width'
IMAGE_NAME = 'image_name'
LABEL = 'label'
ORIGINAL_IMAGE = 'original_image'</code></pre>
<h2 id="2.%20%E6%96%B9%E6%B3%95_preprocess_image()">2. 方法_preprocess_image()</h2>
<pre class="has">
<code class="language-html">"""Preprocesses the image and label.

Args:
  sample: A sample containing image and label.

Returns:
  sample: Sample with preprocessed image and label.

Raises:
  ValueError: Ground truth label not provided during training.
"""</code></pre>
<p>该方法预处理image和label</p>
<p>sample就是我们刚才使用_parse_function返回的那个sample字典</p>
<p>首先将待预处理的label与image从sample中取出：</p>
<pre class="has">
<code class="language-python">    image = sample[common.IMAGE]
    label = sample[common.LABELS_CLASS]</code></pre>
<p>然后调用一个函数直接处理image和label</p>
<pre class="has">
<code class="language-python">    original_image, image, label = input_preprocess.preprocess_image_and_label(
        image=image,
        label=label,
        crop_height=self.crop_size[0],
        crop_width=self.crop_size[1],
        min_resize_value=self.min_resize_value,
        max_resize_value=self.max_resize_value,
        resize_factor=self.resize_factor,
        min_scale_factor=self.min_scale_factor,
        max_scale_factor=self.max_scale_factor,
        scale_factor_step_size=self.scale_factor_step_size,
        ignore_label=self.ignore_label,
        is_training=self.is_training,
        model_variant=self.model_variant)</code></pre>
<h3 id="2.1%20input_preprocess%E7%9A%84preprocess_image_and_label%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D">2.1 input_preprocess的preprocess_image_and_label方法介绍</h3>
<pre class="has">
<code class="language-html">"""Preprocesses the image and label.

Args:
  image: Input image.
  label: Ground truth annotation label.
  crop_height: The height value used to crop the image and label.
  crop_width: The width value used to crop the image and label.
  min_resize_value: Desired size of the smaller image side.
  max_resize_value: Maximum allowed size of the larger image side.
  resize_factor: Resized dimensions are multiple of factor plus one.
  min_scale_factor: Minimum scale factor value.
  max_scale_factor: Maximum scale factor value.
  scale_factor_step_size: The step size from min scale factor to max scale
    factor. The input is randomly scaled based on the value of
    (min_scale_factor, max_scale_factor, scale_factor_step_size).
  ignore_label: The label value which will be ignored for training and
    evaluation.
  is_training: If the preprocessing is used for training or not.
  model_variant: Model variant (string) for choosing how to mean-subtract the
    images. See feature_extractor.network_map for supported model variants.

Returns:
  original_image: Original image (could be resized).
  processed_image: Preprocessed image.
  label: Preprocessed ground truth segmentation label.

Raises:
  ValueError: Ground truth label not provided during training.
"""</code></pre>
<p>这个方法包含了图片大小的转换，随机放缩图片，以均值扩增，随机裁剪，随机左右翻转。更多的细节需要结合试验，才能知道具体的预处理效果。这里我们只需要知道返回的image , label都是处理好的就可以了。</p>
<p> </p>
<p>  注意返回值中的original_image并不是原始大小的，可能会被resize，这个图片只是在visualize展示用的。LABEL_CLASS这个键值由于是预处理的输入，就是个梯子，真正的label键值是LABEL，所以最终sample的内容</p>
<pre class="has">
<code class="language-python">    sample[common.IMAGE] = image

    if not self.is_training:
      # Original image is only used during visualization.
      sample[common.ORIGINAL_IMAGE] = original_image

    if label is not None:
      sample[common.LABEL] = label
    # Remove common.LABEL_CLASS key in the sample since it is only used to
    # derive label and not used in training and evaluation.这里就把LABELS_CLASS踢出去了
    sample.pop(common.LABELS_CLASS, None)</code></pre>
<p> </p>
<h2 id="3.%E6%96%B9%E6%B3%95%20_get_all_files(self)%3A">3.方法 _get_all_files(self):</h2>
<pre class="has">
<code class="language-html">"""Gets all the files to read data from.

Returns:
  A list of input files.
"""</code></pre>
<pre class="has">
<code>    file_pattern = _FILE_PATTERN
    file_pattern = os.path.join(self.dataset_dir,
                                file_pattern % self.split_name)
    return tf.gfile.Glob(file_pattern)</code></pre>
<pre class="has">
<code class="language-html">Glob的定义是

A list of strings containing filenames that match the given pattern(s).
</code></pre>
<p>一般情况下，代码是这么写的：</p>
<pre class="has">
<code class="language-python">filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]
dataset = tf.data.TFRecordDataset(filenames)</code></pre>
<p>这个调试一下会看的更清楚。这里的解释也没什么内容：<a href="https://zhuanlan.zhihu.com/p/31536538" rel="nofollow" data-token="dcf8abe9b0581959aa22774f61c1a4f5">https://zhuanlan.zhihu.com/p/31536538</a></p>
<p>就注释来看，这里获取了所有的数据地址，我们截图中的那些.tfrecord文件。但还是没有读取。</p>
<h2 id="4.%E6%96%B9%E6%B3%95%20get_one_shot_iterator(self)">4.方法 get_one_shot_iterator(self)</h2>
<pre class="has">
<code class="language-html">"""Gets an iterator that iterates across the dataset once.

Returns:
  An iterator of type tf.data.Iterator.
"""</code></pre>
<p>这个方法建立了一个迭代器。类型是</p>
<pre class="has">
<code class="language-html">tf.data.Iterator.</code></pre>
<p> </p>
<pre class="has">
<code class="language-python">    files = self._get_all_files()

    dataset = (
        tf.data.TFRecordDataset(files, num_parallel_reads=self.num_readers)
        .map(self._parse_function, num_parallel_calls=self.num_readers)
        .map(self._preprocess_image, num_parallel_calls=self.num_readers))
</code></pre>
<p>TFRecordDataset官方解释：<a href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset" rel="nofollow" data-token="5855d93549c4659fbf5e054445f74c6d">https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset</a></p>
<h2 id="class_tfrecorddataset">Class <code>TFRecordDataset</code></h2>
<p>A <code>Dataset</code> comprising records from one or more TFRecord files.</p>
<p>那么Dataset又是什么，map()和他怎么用的？注意这里的Dataset指的是tf.data.Dataset，而不是我们自己生成的。很简单，TFRecordDataset建立的Dataset实例。我们这个压根还没有建立实例呢。建立实例要到最后。你看我从来没提过我们自己建立的Dataset这个类对吧，因为目前还没用到。</p>
<p>Dataset是个实例，其定义,详细可以到官网：</p>
<p><a href="https://www.tensorflow.org/guide/datasets?hl=zh-CN" rel="nofollow" data-token="cb88393322f29eaeff637ca629bcc337">https://www.tensorflow.org/guide/datasets?hl=zh-CN</a>查看，是个很常用的官方提供的数据处理Class。</p>
<p>简单来讲Dataset就是一个数据库，包含了若干个样本，每个样本包含了image和label（对于图片分割任务来讲）</p>
<p>那个TFRecordDataset就是一个reord类型的数据库。而map方法就是将里面的function应用于Dataset当中的每个元素（样本）。</p>
<p> </p>
<p>那么这个一长串的代码到底干了什么。</p>
<pre class="has">
<code class="language-python">tf.data.TFRecordDataset(files, num_parallel_reads=self.num_readers)</code></pre>
<p>使用TFRecordDataset 的init方法，从files包含的一个list文件名中建立了一个Dataset，只不过里面存的都是tfrecord类型的元素。</p>
<p> </p>
<p>然后用</p>
<pre class="has">
<code class="language-python">.map(self._parse_function, num_parallel_calls=self.num_readers)</code></pre>
<p>指定的_parse_function方法将tfrecord类型的元素转换成字典sample，此刻的image，以及label都已经转换成了常见的数据类型。</p>
<p>最后用：</p>
<pre class="has">
<code class="language-python">map(self._preprocess_image, num_parallel_calls=self.num_readers))</code></pre>
<p>指定的_preprocess_image方法将sample字典中的image label做预处理。</p>
<p> </p>
<p>最后返回了一个迭代器。</p>
<h1 id="%E4%BB%A3%E7%A0%81%E4%BD%BF%E7%94%A8%E6%98%AF%E5%9C%A8train.py%E9%87%8C%E9%9D%A2%EF%BC%9A">代码使用是在train.py里面：</h1>
<pre class="has">
<code class="language-python">      dataset = data_generator.Dataset(
          dataset_name=FLAGS.dataset,
          split_name=FLAGS.train_split,
          dataset_dir=FLAGS.dataset_dir,
          batch_size=clone_batch_size,
          crop_size=[int(sz) for sz in FLAGS.train_crop_size],
          min_resize_value=FLAGS.min_resize_value,
          max_resize_value=FLAGS.max_resize_value,
          resize_factor=FLAGS.resize_factor,
          min_scale_factor=FLAGS.min_scale_factor,
          max_scale_factor=FLAGS.max_scale_factor,
          scale_factor_step_size=FLAGS.scale_factor_step_size,
          model_variant=FLAGS.model_variant,
          num_readers=2,
          is_training=True,
          should_shuffle=True,
          should_repeat=True)

      train_tensor, summary_op = _train_deeplab_model(
          dataset.get_one_shot_iterator(), dataset.num_of_classes,
          dataset.ignore_label)
def _train_deeplab_model(iterator, num_of_classes, ignore_label):</code></pre>
<p>实例化一个dataset，在利用get_one_shot_iterator()方法返回一个迭代器，提供了类别数，比如voc2012就是21类，</p>
<p>以及忽略的标签，比如voc2012就是255.</p>
<h1>总结</h1>
<p>这个类的名字Dataset不是随便取的，这个类行使的功能如同Dataset提供了一个sequential，就是一串的可以不断喂给网络的不消耗大量内存和cpu的数据流，也就是最终我们说的迭代器iterator。</p>
<p>传递给训练function。</p>
<p> </p>
</div>
</div>
	</div>

	<footer class="amp-wp-article-footer">
			<div class="amp-wp-meta amp-wp-tax-category">
		Categories: <a href="https://uzzz.org/category/deeplearning/" rel="category tag">DeepLearning</a>, <a href="https://uzzz.org/category/rengongzhineng/" rel="category tag">人工智能</a>	</div>

	</footer>
</article>

<footer class="amp-wp-footer">
	<div>
		<h2>有组织在!</h2>
		<a href="#top" class="back-to-top">Back to top</a>
	</div>
</footer>




</body></html>
