<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>论文学习 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/lunwenxuexi/feed" rel="self" type="application/rss+xml" />
	<link>http://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Sat, 16 Mar 2019 08:16:39 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>论文学习 &#8211; 有组织在!</title>
	<link>http://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>DCN:Deep &#038; Cross Network for Ad Click Predictions简介</title>
		<link>https://uzzz.org/article/1604.html</link>
				<pubDate>Sat, 16 Mar 2019 08:16:39 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[论文学习]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1604.html</guid>
				<description><![CDATA[Deep &#38; Cross Network for Ad Click Predictions 摘要 作者起草了DCN，该网络可以保持DNN的优点（隐式地生成特征之间的交互），同时又利用交叉网络来对特征进行显式的交叉计算。这也不要求手工的特征工程，同时只是在DNN的基础上加了一些可容忍的复杂度。实验证明DCN已经在CTR预估与分类问题上超过了sota。 介绍 对于web伸缩型的推荐系统，因为其产生的数据较为稀疏，对于线性模型来说已经不太好处理了。因此交叉特征变得很重要，但是这经常要求我们手动特征工程，为了减少这方面的工作，交叉网络应运而生。同时联合DNN，发挥两者的共同优势。 嵌入和堆叠层 对于离散数据，一般处理时会被编码成one-hot向量，对于实际应用中维度会非常高，因此使用 来将这些离散特征转换成实数值的稠密向量，最后将嵌入向量和连续特征向量堆叠在一起形成一个向量。 交叉网络 对于每层的计算，使用下述公式： 一层交叉层的可视化如下图所示： 该网络可以使交叉特征的次数随着层数的增加而不断变大，对于l层其最高多项式次数为l+1。 计算的时间与空间复杂度都是线性，因此DCN的效率与DNN是一个量级的。 深度网络 深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式： 链接层 将两个网络的输出联合，送进标准的logits层。 正则化的log loss函数： 实验 比较DCN，DC，DNN，FM，LR模型的最好的logloss 比较实现对应的logloss，DNN和DCN需要的参数数量 在固定参数下实现最好的logloss所需要的内存 在层数与结点一致的情况下，比较DNN与DCN的logloss(X 1 0 − 2 10^{-2} 10−2)差距，负值代表DCN表现好于DNN 最后展示了不同设置的变化趋势。 可以参考我的github来看看源代码，如有错误，欢迎交流。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h2><a id="Deep__Cross_Network_for_Ad_Click_Predictions_0"></a>Deep &amp; Cross Network for Ad Click Predictions</h2>
<h3><a id="_2"></a>摘要</h3>
<p>作者起草了DCN，该网络可以保持DNN的优点（隐式地生成特征之间的交互），同时又利用交叉网络来对特征进行显式的交叉计算。这也不要求手工的特征工程，同时只是在DNN的基础上加了一些可容忍的复杂度。实验证明DCN已经在CTR预估与分类问题上超过了sota。</p>
<h3><a id="_6"></a>介绍</h3>
<p>对于web伸缩型的推荐系统，因为其产生的数据较为稀疏，对于线性模型来说已经不太好处理了。因此交叉特征变得很重要，但是这经常要求我们手动特征工程，为了减少这方面的工作，交叉网络应运而生。同时联合DNN，发挥两者的共同优势。</p>
<h3><a id="_10"></a>嵌入和堆叠层</h3>
<p>对于离散数据，一般处理时会被编码成one-hot向量，对于实际应用中维度会非常高，因此使用<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161332569.png" alt="在这里插入图片描述"></p>
<p>来将这些离散特征转换成实数值的稠密向量，最后将嵌入向量和连续特征向量堆叠在一起形成一个向量。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161342759.png" alt="在这里插入图片描述"></p>
<h3><a id="_19"></a>交叉网络</h3>
<p>对于每层的计算，使用下述公式：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019031616135250.png" alt="在这里插入图片描述"></p>
<p>一层交叉层的可视化如下图所示：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161400699.png" alt="在这里插入图片描述"></p>
<p>该网络可以使交叉特征的次数随着层数的增加而不断变大，对于l层其最高多项式次数为l+1。</p>
<p>计算的时间与空间复杂度都是线性，因此DCN的效率与DNN是一个量级的。</p>
<h3><a id="_33"></a>深度网络</h3>
<p>深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161416355.png" alt="在这里插入图片描述"></p>
<h3><a id="_39"></a>链接层</h3>
<p>将两个网络的输出联合，送进标准的logits层。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161424323.png" alt="在这里插入图片描述"></p>
<p>正则化的log loss函数：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161434957.png" alt="在这里插入图片描述"></p>
<h3><a id="_49"></a>实验</h3>
<ol>
<li>
<p>比较DCN，DC，DNN，FM，LR模型的最好的logloss</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161443600.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>比较实现对应的logloss，DNN和DCN需要的参数数量</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019031616145798.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>在固定参数下实现最好的logloss所需要的内存</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161512312.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>在层数与结点一致的情况下，比较DNN与DCN的logloss(X<span class="katex--inline"><span class="katex"><span class="katex-mathml"></p>
<math>
         <semantics><br />
          <mrow><br />
           <mn><br />
            1<br />
           </mn><br />
           <msup><br />
            <mn><br />
             0<br />
            </mn><br />
            <mrow><br />
             <mo><br />
              −<br />
             </mo><br />
             <mn><br />
              2<br />
             </mn><br />
            </mrow><br />
           </msup><br />
          </mrow><br />
          <annotation encoding="application/x-tex"><br />
           10^{-2}<br />
          </annotation><br />
         </semantics><br />
        </math>
<p></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span>)差距，负值代表DCN表现好于DNN</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161528807.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>最后展示了不同设置的变化趋势。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161542428.png" alt="在这里插入图片描述"></p>
</li>
</ol>
<p>可以参考我的<a href="https://github.com/loserChen/TensorFlow-In-Practice" rel="nofollow" data-token="6687502f53a5f6339d52280282678380">github</a>来看看源代码，如有错误，欢迎交流。</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>DCN:Deep &#038; Cross Network for Ad Click Predictions简介</title>
		<link>https://uzzz.org/article/1672.html</link>
				<pubDate>Sat, 16 Mar 2019 08:16:39 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[论文学习]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1672.html</guid>
				<description><![CDATA[Deep &#38; Cross Network for Ad Click Predictions 摘要 作者起草了DCN，该网络可以保持DNN的优点（隐式地生成特征之间的交互），同时又利用交叉网络来对特征进行显式的交叉计算。这也不要求手工的特征工程，同时只是在DNN的基础上加了一些可容忍的复杂度。实验证明DCN已经在CTR预估与分类问题上超过了sota。 介绍 对于web伸缩型的推荐系统，因为其产生的数据较为稀疏，对于线性模型来说已经不太好处理了。因此交叉特征变得很重要，但是这经常要求我们手动特征工程，为了减少这方面的工作，交叉网络应运而生。同时联合DNN，发挥两者的共同优势。 嵌入和堆叠层 对于离散数据，一般处理时会被编码成one-hot向量，对于实际应用中维度会非常高，因此使用 来将这些离散特征转换成实数值的稠密向量，最后将嵌入向量和连续特征向量堆叠在一起形成一个向量。 交叉网络 对于每层的计算，使用下述公式： 一层交叉层的可视化如下图所示： 该网络可以使交叉特征的次数随着层数的增加而不断变大，对于l层其最高多项式次数为l+1。 计算的时间与空间复杂度都是线性，因此DCN的效率与DNN是一个量级的。 深度网络 深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式： 链接层 将两个网络的输出联合，送进标准的logits层。 正则化的log loss函数： 实验 比较DCN，DC，DNN，FM，LR模型的最好的logloss 比较实现对应的logloss，DNN和DCN需要的参数数量 在固定参数下实现最好的logloss所需要的内存 在层数与结点一致的情况下，比较DNN与DCN的logloss(X 1 0 − 2 10^{-2} 10−2)差距，负值代表DCN表现好于DNN 最后展示了不同设置的变化趋势。 可以参考我的github来看看源代码，如有错误，欢迎交流。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h2><a id="Deep__Cross_Network_for_Ad_Click_Predictions_0"></a>Deep &amp; Cross Network for Ad Click Predictions</h2>
<h3><a id="_2"></a>摘要</h3>
<p>作者起草了DCN，该网络可以保持DNN的优点（隐式地生成特征之间的交互），同时又利用交叉网络来对特征进行显式的交叉计算。这也不要求手工的特征工程，同时只是在DNN的基础上加了一些可容忍的复杂度。实验证明DCN已经在CTR预估与分类问题上超过了sota。</p>
<h3><a id="_6"></a>介绍</h3>
<p>对于web伸缩型的推荐系统，因为其产生的数据较为稀疏，对于线性模型来说已经不太好处理了。因此交叉特征变得很重要，但是这经常要求我们手动特征工程，为了减少这方面的工作，交叉网络应运而生。同时联合DNN，发挥两者的共同优势。</p>
<h3><a id="_10"></a>嵌入和堆叠层</h3>
<p>对于离散数据，一般处理时会被编码成one-hot向量，对于实际应用中维度会非常高，因此使用<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161332569.png" alt="在这里插入图片描述"></p>
<p>来将这些离散特征转换成实数值的稠密向量，最后将嵌入向量和连续特征向量堆叠在一起形成一个向量。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161342759.png" alt="在这里插入图片描述"></p>
<h3><a id="_19"></a>交叉网络</h3>
<p>对于每层的计算，使用下述公式：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019031616135250.png" alt="在这里插入图片描述"></p>
<p>一层交叉层的可视化如下图所示：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161400699.png" alt="在这里插入图片描述"></p>
<p>该网络可以使交叉特征的次数随着层数的增加而不断变大，对于l层其最高多项式次数为l+1。</p>
<p>计算的时间与空间复杂度都是线性，因此DCN的效率与DNN是一个量级的。</p>
<h3><a id="_33"></a>深度网络</h3>
<p>深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161416355.png" alt="在这里插入图片描述"></p>
<h3><a id="_39"></a>链接层</h3>
<p>将两个网络的输出联合，送进标准的logits层。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161424323.png" alt="在这里插入图片描述"></p>
<p>正则化的log loss函数：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161434957.png" alt="在这里插入图片描述"></p>
<h3><a id="_49"></a>实验</h3>
<ol>
<li>
<p>比较DCN，DC，DNN，FM，LR模型的最好的logloss</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161443600.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>比较实现对应的logloss，DNN和DCN需要的参数数量</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019031616145798.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>在固定参数下实现最好的logloss所需要的内存</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161512312.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>在层数与结点一致的情况下，比较DNN与DCN的logloss(X<span class="katex--inline"><span class="katex"><span class="katex-mathml"></p>
<math>
         <semantics><br />
          <mrow><br />
           <mn><br />
            1<br />
           </mn><br />
           <msup><br />
            <mn><br />
             0<br />
            </mn><br />
            <mrow><br />
             <mo><br />
              −<br />
             </mo><br />
             <mn><br />
              2<br />
             </mn><br />
            </mrow><br />
           </msup><br />
          </mrow><br />
          <annotation encoding="application/x-tex"><br />
           10^{-2}<br />
          </annotation><br />
         </semantics><br />
        </math>
<p></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span>)差距，负值代表DCN表现好于DNN</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161528807.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>最后展示了不同设置的变化趋势。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161542428.png" alt="在这里插入图片描述"></p>
</li>
</ol>
<p>可以参考我的<a href="https://github.com/loserChen/TensorFlow-In-Practice" rel="nofollow" data-token="6687502f53a5f6339d52280282678380">github</a>来看看源代码，如有错误，欢迎交流。</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
