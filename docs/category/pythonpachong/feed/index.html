<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>python爬虫 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/pythonpachong/feed" rel="self" type="application/rss+xml" />
	<link>http://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Wed, 26 Jun 2019 15:05:52 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>python爬虫 &#8211; 有组织在!</title>
	<link>http://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>反反爬虫相关机制（面试必问，后续陆续添加）</title>
		<link>https://uzzz.org/article/933.html</link>
				<pubDate>Wed, 26 Jun 2019 15:05:52 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[python爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/933.html</guid>
				<description><![CDATA[通常防止爬虫被反主要有以下几个策略： 一.BAN IP 原因：某一个某一个时刻IP访问量特别特别大 ，或者是超出正常用户使用权限，导致服务器会偶尔把该IP放入黑名单 ，过一段时间再将其放出来 解决办法：分布式爬虫（分布式【分散url的手动分布式，以及框架分布式】）以及购买代理IP（Tor代理~~能买暗网代理），转化成app有的也有效 二.BAN USERAGENT 原因：爬虫请求头就是默认的 python-requests/2.18.4 等等类型在headers的数据包，会直接拒绝访问，返回403错误 解决办法：伪装浏览器头 ，添加其请求头，再发送请求 三.BAN COOKIES 原因：服务器对每一个访问网页的人都set-cookie，给其一个cookies，当该cookies访问超过某一个阀值（请求次数或者timeout值）时就BAN掉该COOKIE 解决办法： 1,控制访问速度 , 2,在某宝上买多个账号，生成多个cookies，在每一次访问时带上cookies 3,手动获取页面返回cookies 4，解析js拿到其js生成的cookies再带入请求（超级稳） 5，再没有遇到逆天js加载情况下无所不能的selenuim 案例：亚马逊等大型电商平台，马蜂窝 四.验证码验证 原因：当某一用户访问次数过多后，就自动让请求跳转到一个验证码页面，只有在输入正确的验证码之后才能继续访问网站 解决办法：python可以通过一些第三方库如(pytesser,PIL)来对验证码进行处理，识别出正确的验证码，复杂的验证码可以通过机器学习让爬虫自动识别复杂验证码，让程序自动识别验证码并自动输入验证码继续抓取 ，还有最终手段打码平台（兔宝贝，超级鹰等）打码平台99%的能搞定，那1%也可以用打码平台的人工打码搞定。哈哈 短信验证：易码（专业手机短信验证好几年） 二维码验证：打码平台有支持扫码（神一样的验证，我没有遇到过，但听说过） 案例：淘宝，12306 五.javascript渲染 原因：网页开发者将重要信息放在网页中但不写入html标签中，而浏览器会自动渲染&#60;script&#62;标签的js代码将信息展现在浏览器当中，而爬虫是不具备执行js代码的能力，所以无法将js事件产生的信息读取出来 解决办法： 1，通过分析提取script中的js代码来通过正则匹配提取信息内容或通过webdriver+phantomjs直接进行无头浏览器渲染网页。 2，通过解析js获取正确的资源地址 案例：前程无忧网 ，信用中国 六.ajax异步传输 原因：访问网页的时候服务器将网页框架返回给客户端，在与客户端交互的过程中通过异步ajax技术传输数据包到客户端，呈现在网页上，爬虫直接抓取的话信息为空 解决办法：通过fiddler或是wireshark抓包分析ajax请求的界面，然后自己通过规律仿造服务器构造一个请求访问服务器得到返回的真实数据包。 案例：拉勾网 打开拉勾网的某一个工作招聘页，可以看到许许多多的招聘信息数据，点击下一页后发现页面框架不变化，url地址不变，而其中的每个招聘数据发生了变化，通过chrome开发者工具抓包找到了一个叫请求了一个叫做http://www.lagou.com/zhaopin/Java/2/?filterOption=3的网页，打开改网页发现为第二页真正的数据源，通过仿造请求可以抓取每一页的数据。 &#160;]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<h3>通常防止爬虫被反主要有以下几个策略：</h3>
<h3><em>一.BAN IP</em></h3>
<p><u>原因：</u>某一个某一个时刻IP访问量特别特别大 ，或者是超出正常用户使用权限，导致服务器会偶尔把该IP放入黑名单 ，过一段时间再将其放出来</p>
<p><u>解决办法：</u>分布式爬虫（分布式【分散url的手动分布式，以及框架分布式】）以及购买代理IP（Tor代理~~能买<a href="https://cloud.tencent.com/developer/news/313473" rel="nofollow" data-token="92db766e7922b555c7acd27ff1e2b40e">暗网代理</a>），转化成app有的也有效</p>
<h3><strong>二.BAN USERAGENT</strong></h3>
<p><u>原因：</u>爬虫请求头就是默认的 python-requests/2.18.4 等等类型在headers的数据包，会直接拒绝访问，返回403错误</p>
<p><u>解决办法：伪装</u>浏览器头 ，添加其请求头，再发送请求</p>
<h3><strong>三.BAN COOKIES</strong></h3>
<p><u>原因：</u>服务器对每一个访问网页的人都set-cookie，给其一个cookies，当该cookies访问超过某一个阀值（请求次数或者timeout值）时就BAN掉该COOKIE</p>
<p><u>解决办法：</u></p>
<p>1,控制访问速度 ,</p>
<p>2,在某宝上买多个账号，生成多个cookies，在每一次访问时带上cookies</p>
<p>3,手动获取页面返回cookies</p>
<p>4，解析js拿到其js生成的cookies再带入请求（超级稳）</p>
<p>5，再没有遇到逆天js加载情况下无所不能的selenuim</p>
<p>案例：亚马逊等大型电商平台，马蜂窝</p>
<h3><strong>四.验证码验证</strong></h3>
<p><u>原因：</u>当某一用户访问次数过多后，就自动让请求跳转到一个验证码页面，只有在输入正确的验证码之后才能继续访问网站</p>
<p>解决办法：python可以通过一些第三方库如(pytesser,PIL)来对验证码进行处理，识别出正确的验证码，复杂的验证码可以通过机器学习让爬虫自动识别复杂验证码，让程序自动识别验证码并自动输入验证码继续抓取 ，还有最终手段打码平台（兔宝贝，超级鹰等）打码平台99%的能搞定，那1%也可以用打码平台的人工打码搞定。哈哈</p>
<p>短信验证：易码（专业手机短信验证好几年）</p>
<p>二维码验证：打码平台有支持扫码（神一样的验证，我没有遇到过，但听说过）</p>
<p>案例：淘宝，12306</p>
<h3><strong>五.javascript渲染</strong></h3>
<p><u>原因：</u>网页开发者将重要信息放在网页中但不写入html标签中，而浏览器会自动渲染&lt;script&gt;标签的js代码将信息展现在浏览器当中，而爬虫是不具备执行js代码的能力，所以无法将js事件产生的信息读取出来</p>
<p>解决办法：</p>
<p>1，通过分析提取script中的js代码来通过正则匹配提取信息内容或通过webdriver+phantomjs直接进行无头浏览器渲染网页。</p>
<p>2，通过解析js获取正确的资源地址</p>
<p>案例：前程无忧网 ，信用中国</p>
<h3><strong>六.ajax异步传输</strong></h3>
<p><u>原因：</u>访问网页的时候服务器将网页框架返回给客户端，在与客户端交互的过程中通过异步ajax技术传输数据包到客户端，呈现在网页上，爬虫直接抓取的话信息为空</p>
<p>解决办法：通过fiddler或是wireshark抓包分析ajax请求的界面，然后自己通过规律仿造服务器构造一个请求访问服务器得到返回的真实数据包。</p>
<p>案例：拉勾网</p>
<p>打开拉勾网的某一个工作招聘页，可以看到许许多多的招聘信息数据，点击下一页后发现页面框架不变化，url地址不变，而其中的每个招聘数据发生了变化，通过chrome开发者工具抓包找到了一个叫请求了一个叫做<a href="http://www.lagou.com/zhaopin/Java/2/?filterOption=3" rel="nofollow" data-token="57be332e22bc9fee020d07b1cf959651">http://www.lagou.com/zhaopin/Java/2/?filterOption=3</a>的网页，打开改网页发现为第二页真正的数据源，通过仿造请求可以抓取每一页的数据。</p>
<p>&nbsp;</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>“深网” &#038;&#038; “暗网”</title>
		<link>https://uzzz.org/article/742.html</link>
				<pubDate>Sat, 13 Apr 2019 08:40:36 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[python爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/742.html</guid>
				<description><![CDATA[深网是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可以抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。 暗网，也被称为 Darknet 或 dark Internet，完全是另一种“怪兽”。它们也建立在已有的网络基础上，但是使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个信息交换的安全隧道。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<ul>
<li>
<p>深网是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可以抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。</p>
</li>
<li>
<p>暗网，也被称为 Darknet 或 dark Internet，完全是另一种“怪兽”。它们也建立在已有的网络基础上，但是使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个信息交换的安全隧道。</p>
</li>
</ul></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>“深网” &#038;&#038; “暗网”</title>
		<link>https://uzzz.org/article/1349.html</link>
				<pubDate>Sat, 13 Apr 2019 08:40:36 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[python爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1349.html</guid>
				<description><![CDATA[深网是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可以抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。 暗网，也被称为 Darknet 或 dark Internet，完全是另一种“怪兽”。它们也建立在已有的网络基础上，但是使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个信息交换的安全隧道。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<ul>
<li>
<p>深网是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可以抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。</p>
</li>
<li>
<p>暗网，也被称为 Darknet 或 dark Internet，完全是另一种“怪兽”。它们也建立在已有的网络基础上，但是使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个信息交换的安全隧道。</p>
</li>
</ul></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>scrapy中间件 部署 日志</title>
		<link>https://uzzz.org/article/907.html</link>
				<pubDate>Sun, 25 Nov 2018 11:57:37 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[python爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/907.html</guid>
				<description><![CDATA[IP代理： 抓取网上免费代理，测试 代理供应商提供的代理（收费） ADSL拨号，每次重新拨号会更换本地IP，但是会有1~3秒延迟 VPN/VPS 虚拟主机（翻墙爬取国外网站） Tor网络（暗网） 洋葱浏览器 自动生成user-agent pip install fake_useragent 导入: from fake_useragent import UserAgent ua_obj = UserAgent() ua_obj.ie ua_obj.chrome ua_obj.random 如果有重复图片、文件，保存到本地只有一份，后续改名只能成功一次，后面再改名。 用商品名称做为图片名保存，如果图片名里有&#8221;/&#8221;，则保存时会当作路径结点使用。 file_name = “Huawei Mate20 Pro 8GB/128GB 月光灰” if “/” in file_name: file_name.replace(&#8220;/&#8221;, “-”) 模拟登陆： 直接发送账户密码的POST请求，记录cookie，再发送其他页面的请求 先发送登录页面的get请求，获取登录参数，再发送登录的post请求，提交账户密码和登录参数，并记录cookie，再发送其他页面的请求 直接将cookies保存在请求报头里，直接发送附带登录状态的请求，获取页面。 Scrapyd远程部署和执行爬虫、停止爬虫、监控爬虫运行状态 服务端: 安装客户端和服务器端的工具： 端口: 6800 客户端：pip install scrapyd-client 服务器端：pip install scrapyd 服务器端开启scrapyd服务（提供一个监听6800端口的web） 修改 default_scrapyd.conf 配置文件里的 bind_address 为 0.0.0.0 再开启服务 ubuntu: $ scrapyd 以下全部是客户端的操作： 修改scrapy项目的scrapy.cfg文件，添加 配置名称和url [deploy:scrapyd_Tencent3] url = http://192.168.37.80:6800 将项目部署到指定scrapyd服务器上（每次本地有任何变动，必须重新部署一次） scrapyd-deploy scrapyd_Tencent3 -p Tencent3 启动指定 scrapyd服务上的 指定项目的 指定爬虫（会生成该爬虫的jobid值，用于区分） curl http://192.168.37.80:6800/schedule.json -d project=Tencent3 -d spider=tencent_crawl 停止指定 scrapyd服务上的 指定项目的 指定爬虫 curl http://192.168.37.80:6800/cancel.json -d project=Tencent3 -d job=jobid值 scrapyd-web]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p><strong>IP代理：</strong></p>
<ol>
<li>抓取网上免费代理，测试</li>
<li>代理供应商提供的代理（收费）</li>
<li>ADSL拨号，每次重新拨号会更换本地IP，但是会有1~3秒延迟</li>
<li>VPN/VPS 虚拟主机（翻墙爬取国外网站）</li>
<li>Tor网络（暗网） 洋葱浏览器</li>
</ol>
<p><strong>自动生成user-agent</strong></p>
<blockquote>
<p>pip install fake_useragent<br /> <code>导入</code>:<br /> from fake_useragent import UserAgent</p>
</blockquote>
<p>ua_obj = UserAgent()<br /> ua_obj.ie<br /> ua_obj.chrome<br /> ua_obj.random</p>
<ol>
<li>
<p>如果有重复图片、文件，保存到本地只有一份，后续改名只能成功一次，后面再改名。</p>
</li>
<li>
<p>用商品名称做为图片名保存，如果图片名里有&#8221;/&#8221;，则保存时会当作路径结点使用。<br /> file_name = “Huawei Mate20 Pro 8GB/128GB 月光灰”<br /> if “/” in file_name:<br /> file_name.replace(&#8220;/&#8221;, “-”)</p>
</li>
</ol>
<h2><a id="_31"></a>模拟登陆：</h2>
<ol>
<li>直接发送账户密码的POST请求，记录cookie，再发送其他页面的请求</li>
<li>先发送登录页面的get请求，获取登录参数，再发送登录的post请求，提交账户密码和登录参数，并记录cookie，再发送其他页面的请求</li>
<li>直接将cookies保存在请求报头里，直接发送附带登录状态的请求，获取页面。</li>
</ol>
<h2><a id="Scrapyd_42"></a>Scrapyd远程部署和执行爬虫、停止爬虫、监控爬虫运行状态</h2>
<p><strong>服务端:</strong></p>
<ol>
<li>
<p>安装客户端和服务器端的工具：<br /> 端口: <code>6800</code><br /> 客户端：<code>pip install scrapyd-client</code><br /> 服务器端：<code>pip install scrapyd</code></p>
</li>
<li>
<p>服务器端开启<code>scrapyd</code>服务（提供一个监听6800端口的web）<br /> 修改 <code>default_scrapyd.conf</code> 配置文件里的 bind_address 为 0.0.0.0<br /> 再开启服务<br /> ubuntu: $ scrapyd</p>
</li>
</ol>
<p><strong>以下全部是客户端的操作：</strong></p>
<ol start="3">
<li>
<p>修改scrapy项目的<code>scrapy.cfg</code>文件，添加 配置名称和url<br /> [deploy:scrapyd_Tencent3]<br /> url = <a href="http://192.168.37.80:6800" rel="nofollow" data-token="e6e28e3085ac3cf5d0d0dacec34acaa0">http://192.168.37.80:6800</a></p>
</li>
<li>
<p>将项目部署到指定scrapyd服务器上（每次本地有任何变动，必须重新部署一次）<br /> scrapyd-deploy scrapyd_Tencent3 -p Tencent3</p>
</li>
<li>
<p>启动指定 scrapyd服务上的 指定项目的 指定爬虫（会生成该爬虫的jobid值，用于区分）<br /> curl <a href="http://192.168.37.80:6800/schedule.json" rel="nofollow" data-token="16774b6201924cf56355933a41a937be">http://192.168.37.80:6800/schedule.json</a> -d project=Tencent3 -d spider=tencent_crawl</p>
</li>
<li>
<p>停止指定 scrapyd服务上的 指定项目的 指定爬虫<br /> curl <a href="http://192.168.37.80:6800/cancel.json" rel="nofollow" data-token="827fa24946d8387f353aa1d9c2362b60">http://192.168.37.80:6800/cancel.json</a> -d project=Tencent3 -d job=jobid值</p>
</li>
</ol>
<p>scrapyd-web</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
