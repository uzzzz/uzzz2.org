<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>图计算 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/tujisuan/feed" rel="self" type="application/rss+xml" />
	<link>http://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Thu, 06 Dec 2018 13:21:08 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>图计算 &#8211; 有组织在!</title>
	<link>http://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>图计算论文笔记&#8211;Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<link>https://uzzz.org/article/1706.html</link>
				<pubDate>Thu, 06 Dec 2018 13:21:08 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[图计算]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1706.html</guid>
				<description><![CDATA[Graph Convolutional Neural Networks for Web-Scale Recommender Systems abstract introduction related work method problem setup model architecture Forward propagation algorithm Importance-based neighborhoods Stacking convolutions model training Multi-GPU training with large minibatches Producer-consumer minibatch construction. Sampling negative items Node Embeddings via MapReduce 总结 abstract 在大规模数据上使用GCN做数据挖掘 introduction a random-walk-based GCN–PinSage 处理 3 billion nodes and 18 billion edges的图 为了在大规模图上运行，使用了On-the-fly convolutions；Producer-consumer minibatch construction； Efficient MapReduce inference 同时，使用的减少网络的复杂度的方法：Constructing convolutions via random walks；Importance pooling 使用Curriculumtraining来学习 related work method GCN的方法：一个节点可以形成一个local network，对很多个节点的local network进行GCN，这样，GCN网络的权重被每个network共享。 problem setup 将Pinterest上的pin和board组成二分图 pin可以看成item，board可以看成用户定义的contexts 或 collections。同时pin/item有information，有text和image features。 embedding item 考虑了item的information，也就是text和image，同时也考虑了网络的结构 目的是为了embedding pin也就是item，来通过相似的item推荐（nearest neighbor lookup，given a pin, find related pins），或者是来ranking这些item。 model architecture localized convolution operation：首先输入node feature，然后学习NN，来transform和aggregate图的feature，从而得到node的embedding。 Forward propagation algorithm 这个算法是整个图的圈出来的部分： Importance-based neighborhoods 如何确定用户的邻居？ 之前的GCN中是通过k-hot， 本片论文使用random work找到top T nodes 这里的方法是参考了一篇论文。 Stacking convolutions 在算法1 中使用层叠，也就是当前的点的表达用之前得到的表达，也就是点u的邻居节点不再使用初始值，而是使用通过那个邻居节点的邻居节点得到的向量来进行输入。也就是下图红色方框圈出的部分： Note that the model parameters in Algorithm 1 (Q, q, W, and w) are shared across the nodes but differ between layers. 注意，其中的参数是在同层的点中share的，不同层不share。 也就是我们要学习的参数有： (Q(k),q(k),W(k),w(k),∀k ∈ {1,…,K}) k表示第k层。 model training 目标函数max-margin ranking loss，其实就是hinge loss（svm的loss） 建立一个set L 里面是pair（q，i），q和i是相关的，使i被更好的推荐给query item q。也就是使用网络embedding出的q和i的向量要比较相似。 loss function： 解释一下变种的hinge loss： l(y,y′)=max(0,m−y+y′) 其中，y是正样本的得分，y’是负样本的得分，m是margin（自己选一个数） 即我们希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。 我们可以看到，如果证样本的得分较大，副样本的得分较小，在max后，得到0，相反得到m−y+y′，因为m−y+y′&#62;0，并且要min loss，所以在两个得分相差不大或者负样本得分高的时候进行更新。 Multi-GPU training with large minibatches Producer-consumer minibatch construction. Sampling negative items Node Embeddings via]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<div class="toc">
<h3>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</h3>
<ul>
<li><a href="#abstract_2" rel="nofollow" data-token="71936de71c86fba194524a83123debe3">abstract</a></li>
<li><a href="#introduction_5" rel="nofollow" data-token="1bb1e8f5bd351e71fddb2d8e001d3ef5">introduction</a></li>
<li><a href="#related_work_11" rel="nofollow" data-token="1b8ce216f53bb0e4a956cdf095f7fb0a">related work</a></li>
<li><a href="#method_13" rel="nofollow" data-token="27f6a7883ac31c844e2649a2e93ef9e1">method</a></li>
<ul>
<li><a href="#problem_setup_17" rel="nofollow" data-token="451aac041df9dad1182f77cc4ee452f3">problem setup</a></li>
<li><a href="#model_architecture_23" rel="nofollow" data-token="34670093421c08007162afd3c41db238">model architecture</a></li>
<ul>
<li><a href="#Forward_propagation_algorithm_25" rel="nofollow" data-token="3b7fb45992b2a969c28e0aaabeadc238">Forward propagation algorithm</a></li>
<li><a href="#Importancebased_neighborhoods_30" rel="nofollow" data-token="4934a57f79400a894fa83b0d5a78c4df">Importance-based neighborhoods</a></li>
<li><a href="#Stacking_convolutions_36" rel="nofollow" data-token="aa464ced07fecdb719d5b82aacd1c092">Stacking convolutions</a></li>
</ul>
<li><a href="#model_training_45" rel="nofollow" data-token="83d6415a7abaaf825c14e402132d9c13">model training</a></li>
<ul>
<li><a href="#MultiGPU_training_with_large_minibatches_58" rel="nofollow" data-token="e9404979ecb6776b8007611e702c32ca">Multi-GPU training with large minibatches</a></li>
<li><a href="#Producerconsumer_minibatch_construction_59" rel="nofollow" data-token="33e005e93bd82ae8ee5c1d46dba11f58">Producer-consumer minibatch construction.</a></li>
<li><a href="#Sampling_negative_items_60" rel="nofollow" data-token="7e97beadb81ad93076a2eba863675830">Sampling negative items</a></li>
</ul>
<li><a href="#Node_Embeddings_via_MapReduce_61" rel="nofollow" data-token="4aa646c69c5e144a56cf0e8ac7385c54">Node Embeddings via MapReduce</a></li>
</ul>
<li><a href="#_64" rel="nofollow" data-token="62ed42c22ef1bd199a47b6d4e5414311">总结</a></li>
</ul></div>
</p>
<h1><a id="abstract_2"></a>abstract</h1>
<p>在大规模数据上使用GCN做数据挖掘</p>
<h1><a id="introduction_5"></a>introduction</h1>
<ul>
<li>a random-walk-based GCN–PinSage 处理 3 billion nodes and 18 billion edges的图</li>
<li>为了在大规模图上运行，使用了On-the-fly convolutions；Producer-consumer minibatch construction； Efficient MapReduce inference</li>
<li>同时，使用的减少网络的复杂度的方法：Constructing convolutions via random walks；Importance pooling</li>
<li>使用Curriculumtraining来学习</li>
</ul>
<h1><a id="related_work_11"></a>related work</h1>
<h1><a id="method_13"></a>method</h1>
<p>GCN的方法：一个节点可以形成一个local network，对很多个节点的local network进行GCN，这样，GCN网络的权重被每个network共享。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2018120620332672.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2><a id="problem_setup_17"></a>problem setup</h2>
<ul>
<li>将Pinterest上的pin和board组成二分图</li>
<li>pin可以看成item，board可以看成用户定义的contexts 或 collections。同时pin/item有information，有text和image features。</li>
<li>embedding item 考虑了item的information，也就是text和image，同时也考虑了网络的结构</li>
<li>目的是为了embedding pin也就是item，来通过相似的item推荐（nearest neighbor lookup，given a pin, find related pins），或者是来ranking这些item。</li>
</ul>
<h2><a id="model_architecture_23"></a>model architecture</h2>
<p>localized convolution operation：首先输入node feature，然后学习NN，来transform和aggregate图的feature，从而得到node的embedding。</p>
<h3><a id="Forward_propagation_algorithm_25"></a>Forward propagation algorithm</h3>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206204948684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 这个算法是整个图的圈出来的部分：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206205046621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3><a id="Importancebased_neighborhoods_30"></a>Importance-based neighborhoods</h3>
<p>如何确定用户的邻居？<br /> 之前的GCN中是通过k-hot，<br /> 本片论文使用random work找到top T nodes<br /> 这里的方法是参考了一篇论文。</p>
<h3><a id="Stacking_convolutions_36"></a>Stacking convolutions</h3>
<p>在算法1 中使用层叠，也就是当前的点的表达用之前得到的表达，也就是点u的邻居节点不再使用初始值，而是使用通过那个邻居节点的邻居节点得到的向量来进行输入。也就是下图红色方框圈出的部分：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206210028659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> Note that the model parameters in Algorithm 1 (Q, q, W, and w) are shared across the nodes but differ between layers.<br /> 注意，其中的参数是在同层的点中share的，不同层不share。<br /> 也就是我们要学习的参数有：<br /> (Q(k),q(k),W(k),w(k),∀k ∈ {1,…,K}) k表示第k层。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206210521608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RyYXZhbHNjeA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2><a id="model_training_45"></a>model training</h2>
<ul>
<li>目标函数max-margin ranking loss，其实就是hinge loss（svm的loss）</li>
<li>建立一个set L 里面是pair（q，i），q和i是相关的，使i被更好的推荐给query item q。也就是使用网络embedding出的q和i的向量要比较相似。</li>
</ul>
<p>loss function：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181206211046398.png" alt="在这里插入图片描述"></p>
<p>解释一下变种的hinge loss：<br /> l(y,y′)=max(0,m−y+y′)<br /> 其中，y是正样本的得分，y’是负样本的得分，m是margin（自己选一个数）<br /> 即我们希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。<br /> 我们可以看到，如果证样本的得分较大，副样本的得分较小，在max后，得到0，相反得到m−y+y′，因为m−y+y′&gt;0，并且要min loss，所以在两个得分相差不大或者负样本得分高的时候进行更新。</p>
<h3><a id="MultiGPU_training_with_large_minibatches_58"></a>Multi-GPU training with large minibatches</h3>
<h3><a id="Producerconsumer_minibatch_construction_59"></a>Producer-consumer minibatch construction.</h3>
<h3><a id="Sampling_negative_items_60"></a>Sampling negative items</h3>
<h2><a id="Node_Embeddings_via_MapReduce_61"></a>Node Embeddings via MapReduce</h2>
<p>以上这些就不再写了，可以看论文，都是描述性的话，总体就是如何快速计算大数据。</p>
<h1><a id="_64"></a>总结</h1>
<p>我觉得这个论文的方法和我之前做推荐系统的时候用的random work+skip gram 很像。。。</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
