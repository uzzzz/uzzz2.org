<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>图像检索Image Retrieval &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/tuxiangjiansuoimage-retrieval/feed" rel="self" type="application/rss+xml" />
	<link>http://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Wed, 23 Nov 2016 06:44:06 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>图像检索Image Retrieval &#8211; 有组织在!</title>
	<link>http://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>图像检索系列一：Deep Learning of Binary Hash Codes for Fast Image Retrieval</title>
		<link>https://uzzz.org/article/1715.html</link>
				<pubDate>Wed, 23 Nov 2016 06:44:06 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[caffe学习]]></category>
		<category><![CDATA[图像检索Image Retrieval]]></category>
		<category><![CDATA[深度学习Deep Learning]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1715.html</guid>
				<description><![CDATA[Deep Learning of Binary Hash Codes for Fast Image Retrieval 这篇文章发表在2015CVPR workshop 文章链接：http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W03/papers/Lin_Deep_Learning_of_2015_CVPR_paper.pdf 代码链接：https://github.com/kevinlin311tw/caffe-cvprw15 图一 算法框架流程 这篇文章的想法很巧妙，在一个深层CNN的最后一个全连接层（fc8）和倒数第二个全连接层（fc7）之间加了一层全连接隐层，就是图一中绿色的latent layer （H）。这样一来，既可以得到深层的CNN特征，文中主要用的是fc7的特征，还可以得到二分的哈希编码，即来自H。这个隐层H不仅是对fc7的一个特征概括，而且是一个连接CNN网络的中层特征与高层特征的桥梁。 1. Domain Adaption 为了让一个网络能够对某一类物体高鲁棒，即target domain adaption，用一类主题目标数据集来整定(fine-tune)整个网络。fc8的节点数由目标类别数决定，H的节点数在文中有两种尝试：48和128。这两个层在fine-tune时，是随机初始化的，其中H的初始化参考了LSH[1]的方法，即通过随机映射来构造哈希位。通过这样训练，得到的网络能够产生对特定物体的描述子以及对应的哈希编码。 2. Image Retrieval 主要提出了一种从粗糙到细致的检索方案（coarse-to-fine）。H层首先被二值化： 粗糙检索是用H层的二分哈希码，相似性用hamming距离衡量。待检索图像设为I，将I和所有的图像的对应H层编码进行比对后，选择出hamming距离小于一个阈值的m个构成一个池，其中包含了这m个比较相似的图像。 细致检索则用到的是fc7层的特征，相似性用欧氏距离衡量。距离越小，则越相似。从粗糙检索得到的m个图像池中选出最相似的前k个图像作为最后的检索结果。每两张图128维的H层哈希码距离计算速度是0.113ms，4096维的fc7层特征的距离计算需要109.767ms，因此可见二值化哈希码检索的速度优势。 3. 实验结果 作者在MINIST，CIFAR-10，YAHOO-1M三个数据集上做了实验，并且在分类和检索上都做了实验，结果都很不错，特别是在CIFAR-10上图像检索的精度有30%的提升。 （1）MINIST 左边第一列是待检索图像，右边是48和128位H层节点分别得到的结果。可以看到检索出的数字都是正确的，并且在这个数据集上48位的效果更好，128位的太高，容易引起过拟合。 （2）CIFAR-10 在这个数据集上128位的H层节点比48位的效果更好，比如128检索出更多的马头，而48位的更多的全身的马。 （3）YAHOO-1M 作者在这个数据集上比较了只用fc7,只用H和同时用两者（粗糙到细致）的结果，实验结果表明是两者都用的效果更好。 可以看到如果只用alexnet而不进行fine-tune的话，检索出的结果精度很低。 4. 总结 这个方法整篇文章看下来给人的感觉比较工程，全篇讲理论和方法的部分很少，几乎没有什么数学公式，但是效果好，这个最重要。想法很简单，但是很巧妙，值得学习。代码已经开源，准备尝试。 [1] Gionis A, Indyk P, Motwani R. Similarity search in high dimensions via hashing[C]//VLDB. 1999, 99(6): 518-529.]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p>Deep Learning of Binary Hash Codes for Fast Image Retrieval 这篇文章发表在2015CVPR workshop</p>
<p>文章链接：<a href="http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W03/papers/Lin_Deep_Learning_of_2015_CVPR_paper.pdf" rel="nofollow" data-token="925f534a6179f5aeb33a97507bdb6a84">http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W03/papers/Lin_Deep_Learning_of_2015_CVPR_paper.pdf</a></p>
<p>代码链接：<a href="https://github.com/kevinlin311tw/caffe-cvprw15" rel="nofollow" data-token="865bf79e90efcb12774b7e26b0df400c">https://github.com/kevinlin311tw/caffe-cvprw15</a></p>
<p style="text-align:center;"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123144809544" alt=""></p>
<p style="text-align:center;">图一 算法框架流程</p>
<p>这篇文章的想法很巧妙，在一个深层CNN的最后一个全连接层（fc8）和倒数第二个全连接层（fc7）之间加了一层全连接隐层，就是图一中绿色的latent layer （H）。这样一来，既可以得到深层的CNN特征，文中主要用的是fc7的特征，还可以得到二分的哈希编码，即来自H。这个隐层H不仅是对fc7的一个特征概括，而且是一个连接CNN网络的中层特征与高层特征的桥梁。</p>
<p><span style="color:#3366ff;">1. Domain Adaption</span></p>
<p>为了让一个网络能够对某一类物体高鲁棒，即target domain adaption，用一类主题目标数据集来整定(fine-tune)整个网络。fc8的节点数由目标类别数决定，H的节点数在文中有两种尝试：48和128。这两个层在fine-tune时，是随机初始化的，其中H的初始化参考了LSH[1]的方法，即通过随机映射来构造哈希位。通过这样训练，得到的网络能够产生对特定物体的描述子以及对应的哈希编码。</p>
<p></p>
<p><span style="color:#3366ff;">2. Image Retrieval</span></p>
<p>主要提出了一种从粗糙到细致的检索方案（coarse-to-fine）。H层首先被二值化：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123152530221" alt=""></p>
<p>粗糙检索是用H层的二分哈希码，相似性用hamming距离衡量。待检索图像设为I，将I和所有的图像的对应H层编码进行比对后，选择出hamming距离小于一个阈值的m个构成一个池，其中包含了这m个比较相似的图像。</p>
<p>细致检索则用到的是fc7层的特征，相似性用欧氏距离衡量。距离越小，则越相似。从粗糙检索得到的m个图像池中选出最相似的前k个图像作为最后的检索结果。每两张图128维的H层哈希码距离计算速度是0.113ms，4096维的fc7层特征的距离计算需要109.767ms，因此可见二值化哈希码检索的速度优势。</p>
<p></p>
<p><span style="color:#3366ff;">3. 实验结果</span></p>
<p>作者在MINIST，CIFAR-10，YAHOO-1M三个数据集上做了实验，并且在分类和检索上都做了实验，结果都很不错，特别是在CIFAR-10上图像检索的精度有30%的提升。</p>
<p>（1）MINIST</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123153630259" alt=""></p>
<p>左边第一列是待检索图像，右边是48和128位H层节点分别得到的结果。可以看到检索出的数字都是正确的，并且在这个数据集上48位的效果更好，128位的太高，容易引起过拟合。</p>
<p>（2）CIFAR-10</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123153925066" alt=""></p>
<p>在这个数据集上128位的H层节点比48位的效果更好，比如128检索出更多的马头，而48位的更多的全身的马。</p>
<p>（3）YAHOO-1M</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123154147429" alt=""></p>
<p>作者在这个数据集上比较了只用fc7,只用H和同时用两者（粗糙到细致）的结果，实验结果表明是两者都用的效果更好。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20161123154347949" alt=""></p>
<p>可以看到如果只用alexnet而不进行fine-tune的话，检索出的结果精度很低。</p>
<p></p>
<p><span style="color:#3366ff;">4. 总结</span></p>
<p>这个方法整篇文章看下来给人的感觉比较工程，全篇讲理论和方法的部分很少，几乎没有什么数学公式，但是效果好，这个最重要。想法很简单，但是很巧妙，值得学习。代码已经开源，准备尝试。</p>
<p><span style="color:rgb(34,34,34);font-family:Arial, sans-serif;font-size:13px;line-height:16.12px;">[1] Gionis A, Indyk P, Motwani R. Similarity search in high dimensions via hashing[C]//VLDB. 1999, 99(6): 518-529.</span></p>
</p></div>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
