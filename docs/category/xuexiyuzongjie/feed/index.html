<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>学习与总结 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/xuexiyuzongjie/feed" rel="self" type="application/rss+xml" />
	<link>http://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Sat, 03 Jun 2017 03:21:01 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>学习与总结 &#8211; 有组织在!</title>
	<link>http://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Stanford CS224n: Natural Language Processing with Deep Learning 课程笔记</title>
		<link>https://uzzz.org/article/1598.html</link>
				<pubDate>Sat, 03 Jun 2017 03:21:01 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[学习与总结]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1598.html</guid>
				<description><![CDATA[Stanford CS224n: Natural Language Processing with Deep Learning&#160; 课程主页：http://web.stanford.edu/class/cs224n/ 已完成的课程作业代码：https://github.com/xingjian-f/Standford-cs224n-assignments 一些笔记： lec1&#160; 1 什么是自然语言处理？&#160; 让机器能够“理解”自然语言，从而能够做一些有意义的任务。&#160; 2 人类语言有什么特点？&#160; 语言不是大自然的产物，是人造的，所以处理它和处理视觉、声音会有很多不同。&#160; 语言是离散的。但人类传递语言的载体，图像、语音是连续的。不同的载体对应的语言含义却是不变的。&#160; 3 自然语言处理为什么很难？&#160; nlp的难点在于歧义性和，需要推理性（人类说话有时不是完整的表达，而需要听者的推理）&#160; 4 （词汇形态学，语法，语义，篇章）各个层级的表示形式？&#160; 向量 lec2&#160; 1 怎样表示一个词的含义？&#160; （1）使用分类学的方法，例如WordNet。离散表示，one-hot。&#160; （2）分布式表示 lec3&#160; 1 Word2Vec 和 Glove 训练后，都得到了两组词向量，这两组要怎么用呢？&#160; 经验主义，把它们叠加。不过最好先自己试一试各种情况。 2 怎么评估词向量？&#160; 词的相似度，类比性，之类的测试集，或者作为下游任务的输入，评估下游任务的好坏。&#160; 后者更“实际”，但评估消耗时间更长。 lec8&#160; 1 RNN中为什么梯度消失或爆炸会带来问题？&#160; 爆炸导致导数无法评估，clip解决。&#160; 消失导致“远”一点的参数几乎不更新，对角线初始化，relu，LSTM，GRU解决。 2 神经翻译模型中使用RNN有哪些trick？&#160; 1 encoder 和 decoder 的参数不共享。&#160; 2 decoder中，把上一个隐层，encoder最后一层，上一个输出结果都最为当前隐层的输入。&#160; 3 多层RNN。&#160; 4 双向RNN。&#160; （以上都是想办法增加参数）&#160; 5 GRU 和 LSTM lec11&#160; 1 LSTM 和 RNN记忆力的比较&#160; RNN最多记8-10步，LSTM可以到100步。（实验性结论） 2 使用LSTM的trick&#160; 1 把forget gate 的 bias 初始化到1&#160; 2 只在垂直方向上使用dropout&#160; 3 ensemble，使用8-10个网络，然后平均它们的预测结果，一般有少量的提升！ 3 什么是 scheduled sampling？&#160; RNN下一步的输入中，随机的给ground truth 或 预测结果。 4 什么是BLEU？&#160; 共有n-gram数目（n经常取4），并惩罚预测长度小于真实长度。作为机器翻译的评价指标，开始时和人类评分很match，现在已经被玩坏了。所以google等宣称达到人类水平的翻译结果，其实和真实人类的水平还有很多差距。 lec12&#160; 1 NLP中CNN有什么用？怎么用？&#160; CNN可以比RNN更能捕获短语向量，没有RNN的梯度消失问题，GPU更友好。&#160; 可以用不同size的卷积来捕获不同长度的短语向量。&#160; pooling层，尤其是max-over-time pooling， 可以得到这个卷积提取到的某个激活值最大的特征，（即找到了某个短语）。&#160; 使用两套预训练词向量，一套参与训练，一套不参与。&#160; （character based model，经验主义，一般效果会提升，但训练时间更长，因为输入长度变的更长了。似乎中文很适合？我在中文文本分类任务中，也发现char based 效果比 word based效果要好一些。） 2 NLP任务中不同模型的比较&#160; 1 Bag of Vectors：对于简单的分类任务来说，是很好的base line。多加几层效果会更好！&#160; 2 Window Model：对于不需要宽的视野的分类任务来说，比较好。&#160; 3 CNNs：分类任务上效果好，但很难给出直观解释，在GPU上容易并行。&#160; 4 RNNs：对于分类任务不是最好的，比CNN慢，可以做序列tagging he classification。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">Stanford CS224n: Natural Language Processing with Deep Learning&nbsp;<br /> 课程主页：<a href="http://web.stanford.edu/class/cs224n/" rel="nofollow" style="color:rgb(51,102,153);" data-token="338db12b9ec51d24c4fd0c2789980c7c">http://web.stanford.edu/class/cs224n/</a></span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">已完成的课程作业代码：<a href="https://github.com/xingjian-f/Standford-cs224n-assignments" rel="nofollow" style="color:rgb(51,102,153);" data-token="97a26fc2a0609a0ff2787c160824f6c8">https://github.com/xingjian-f/Standford-cs224n-assignments</a></span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">一些笔记：</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec1</span>&nbsp;</strong><br /> 1 什么是自然语言处理？&nbsp;<br /> 让机器能够“理解”自然语言，从而能够做一些有意义的任务。&nbsp;<br /> 2 人类语言有什么特点？&nbsp;<br /> 语言不是大自然的产物，是人造的，所以处理它和处理视觉、声音会有很多不同。&nbsp;<br /> 语言是离散的。但人类传递语言的载体，图像、语音是连续的。不同的载体对应的语言含义却是不变的。&nbsp;<br /> 3 自然语言处理为什么很难？&nbsp;<br /> nlp的难点在于歧义性和，需要推理性（人类说话有时不是完整的表达，而需要听者的推理）&nbsp;<br /> 4 （词汇形态学，语法，语义，篇章）各个层级的表示形式？&nbsp;<br /> 向量</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec2</span>&nbsp;</strong><br /> 1 怎样表示一个词的含义？&nbsp;<br /> （1）使用分类学的方法，例如WordNet。离散表示，one-hot。&nbsp;<br /> （2）分布式表示</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><span><strong>lec3</strong></span>&nbsp;<br /> 1 Word2Vec 和 Glove 训练后，都得到了两组词向量，这两组要怎么用呢？&nbsp;<br /> 经验主义，把它们叠加。不过最好先自己试一试各种情况。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">2 怎么评估词向量？&nbsp;<br /> 词的相似度，类比性，之类的测试集，或者作为下游任务的输入，评估下游任务的好坏。&nbsp;<br /> 后者更“实际”，但评估消耗时间更长。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec8</span>&nbsp;</strong><br /> 1 RNN中为什么梯度消失或爆炸会带来问题？&nbsp;<br /> 爆炸导致导数无法评估，clip解决。&nbsp;<br /> 消失导致“远”一点的参数几乎不更新，对角线初始化，relu，LSTM，GRU解决。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">2 神经翻译模型中使用RNN有哪些trick？&nbsp;<br /> 1 encoder 和 decoder 的参数不共享。&nbsp;<br /> 2 decoder中，把上一个隐层，encoder最后一层，上一个输出结果都最为当前隐层的输入。&nbsp;<br /> 3 多层RNN。&nbsp;<br /> 4 双向RNN。&nbsp;<br /> （以上都是想办法增加参数）&nbsp;<br /> 5 GRU 和 LSTM</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec11</span>&nbsp;</strong><br /> 1 LSTM 和 RNN记忆力的比较&nbsp;<br /> RNN最多记8-10步，LSTM可以到100步。（实验性结论）</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">2 使用LSTM的trick&nbsp;<br /> 1 把forget gate 的 bias 初始化到1&nbsp;<br /> 2 只在垂直方向上使用dropout&nbsp;<br /> 3 ensemble，使用8-10个网络，然后平均它们的预测结果，一般有少量的提升！</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">3 什么是 scheduled sampling？&nbsp;<br /> RNN下一步的输入中，随机的给ground truth 或 预测结果。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">4 什么是BLEU？&nbsp;<br /> 共有n-gram数目（n经常取4），并惩罚预测长度小于真实长度。作为机器翻译的评价指标，开始时和人类评分很match，现在已经被玩坏了。所以google等宣称达到人类水平的翻译结果，其实和真实人类的水平还有很多差距。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec12</span>&nbsp;</strong><br /> 1 NLP中CNN有什么用？怎么用？&nbsp;<br /> CNN可以比RNN更能捕获短语向量，没有RNN的梯度消失问题，GPU更友好。&nbsp;<br /> 可以用不同size的卷积来捕获不同长度的短语向量。&nbsp;<br /> pooling层，尤其是max-over-time pooling， 可以得到这个卷积提取到的某个激活值最大的特征，（即找到了某个短语）。&nbsp;<br /> 使用两套预训练词向量，一套参与训练，一套不参与。&nbsp;<br /> （character based model，经验主义，一般效果会提升，但训练时间更长，因为输入长度变的更长了。似乎中文很适合？我在中文文本分类任务中，也发现char based 效果比 word based效果要好一些。）</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">2 NLP任务中不同模型的比较&nbsp;<br /> 1 Bag of Vectors：对于简单的分类任务来说，是很好的base line。多加几层效果会更好！&nbsp;<br /> 2 Window Model：对于不需要宽的视野的分类任务来说，比较好。&nbsp;<br /> 3 CNNs：分类任务上效果好，但很难给出直观解释，在GPU上容易并行。&nbsp;<br /> 4 RNNs：对于分类任务不是最好的，比CNN慢，可以做序列tagging he classification。</span></p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Stanford CS224n: Natural Language Processing with Deep Learning 课程笔记</title>
		<link>https://uzzz.org/article/1666.html</link>
				<pubDate>Sat, 03 Jun 2017 03:21:01 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[学习与总结]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1666.html</guid>
				<description><![CDATA[Stanford CS224n: Natural Language Processing with Deep Learning&#160; 课程主页：http://web.stanford.edu/class/cs224n/ 已完成的课程作业代码：https://github.com/xingjian-f/Standford-cs224n-assignments 一些笔记： lec1&#160; 1 什么是自然语言处理？&#160; 让机器能够“理解”自然语言，从而能够做一些有意义的任务。&#160; 2 人类语言有什么特点？&#160; 语言不是大自然的产物，是人造的，所以处理它和处理视觉、声音会有很多不同。&#160; 语言是离散的。但人类传递语言的载体，图像、语音是连续的。不同的载体对应的语言含义却是不变的。&#160; 3 自然语言处理为什么很难？&#160; nlp的难点在于歧义性和，需要推理性（人类说话有时不是完整的表达，而需要听者的推理）&#160; 4 （词汇形态学，语法，语义，篇章）各个层级的表示形式？&#160; 向量 lec2&#160; 1 怎样表示一个词的含义？&#160; （1）使用分类学的方法，例如WordNet。离散表示，one-hot。&#160; （2）分布式表示 lec3&#160; 1 Word2Vec 和 Glove 训练后，都得到了两组词向量，这两组要怎么用呢？&#160; 经验主义，把它们叠加。不过最好先自己试一试各种情况。 2 怎么评估词向量？&#160; 词的相似度，类比性，之类的测试集，或者作为下游任务的输入，评估下游任务的好坏。&#160; 后者更“实际”，但评估消耗时间更长。 lec8&#160; 1 RNN中为什么梯度消失或爆炸会带来问题？&#160; 爆炸导致导数无法评估，clip解决。&#160; 消失导致“远”一点的参数几乎不更新，对角线初始化，relu，LSTM，GRU解决。 2 神经翻译模型中使用RNN有哪些trick？&#160; 1 encoder 和 decoder 的参数不共享。&#160; 2 decoder中，把上一个隐层，encoder最后一层，上一个输出结果都最为当前隐层的输入。&#160; 3 多层RNN。&#160; 4 双向RNN。&#160; （以上都是想办法增加参数）&#160; 5 GRU 和 LSTM lec11&#160; 1 LSTM 和 RNN记忆力的比较&#160; RNN最多记8-10步，LSTM可以到100步。（实验性结论） 2 使用LSTM的trick&#160; 1 把forget gate 的 bias 初始化到1&#160; 2 只在垂直方向上使用dropout&#160; 3 ensemble，使用8-10个网络，然后平均它们的预测结果，一般有少量的提升！ 3 什么是 scheduled sampling？&#160; RNN下一步的输入中，随机的给ground truth 或 预测结果。 4 什么是BLEU？&#160; 共有n-gram数目（n经常取4），并惩罚预测长度小于真实长度。作为机器翻译的评价指标，开始时和人类评分很match，现在已经被玩坏了。所以google等宣称达到人类水平的翻译结果，其实和真实人类的水平还有很多差距。 lec12&#160; 1 NLP中CNN有什么用？怎么用？&#160; CNN可以比RNN更能捕获短语向量，没有RNN的梯度消失问题，GPU更友好。&#160; 可以用不同size的卷积来捕获不同长度的短语向量。&#160; pooling层，尤其是max-over-time pooling， 可以得到这个卷积提取到的某个激活值最大的特征，（即找到了某个短语）。&#160; 使用两套预训练词向量，一套参与训练，一套不参与。&#160; （character based model，经验主义，一般效果会提升，但训练时间更长，因为输入长度变的更长了。似乎中文很适合？我在中文文本分类任务中，也发现char based 效果比 word based效果要好一些。） 2 NLP任务中不同模型的比较&#160; 1 Bag of Vectors：对于简单的分类任务来说，是很好的base line。多加几层效果会更好！&#160; 2 Window Model：对于不需要宽的视野的分类任务来说，比较好。&#160; 3 CNNs：分类任务上效果好，但很难给出直观解释，在GPU上容易并行。&#160; 4 RNNs：对于分类任务不是最好的，比CNN慢，可以做序列tagging he classification。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">Stanford CS224n: Natural Language Processing with Deep Learning&nbsp;<br /> 课程主页：<a href="http://web.stanford.edu/class/cs224n/" rel="nofollow" style="color:rgb(51,102,153);" data-token="338db12b9ec51d24c4fd0c2789980c7c">http://web.stanford.edu/class/cs224n/</a></span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">已完成的课程作业代码：<a href="https://github.com/xingjian-f/Standford-cs224n-assignments" rel="nofollow" style="color:rgb(51,102,153);" data-token="97a26fc2a0609a0ff2787c160824f6c8">https://github.com/xingjian-f/Standford-cs224n-assignments</a></span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">一些笔记：</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec1</span>&nbsp;</strong><br /> 1 什么是自然语言处理？&nbsp;<br /> 让机器能够“理解”自然语言，从而能够做一些有意义的任务。&nbsp;<br /> 2 人类语言有什么特点？&nbsp;<br /> 语言不是大自然的产物，是人造的，所以处理它和处理视觉、声音会有很多不同。&nbsp;<br /> 语言是离散的。但人类传递语言的载体，图像、语音是连续的。不同的载体对应的语言含义却是不变的。&nbsp;<br /> 3 自然语言处理为什么很难？&nbsp;<br /> nlp的难点在于歧义性和，需要推理性（人类说话有时不是完整的表达，而需要听者的推理）&nbsp;<br /> 4 （词汇形态学，语法，语义，篇章）各个层级的表示形式？&nbsp;<br /> 向量</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec2</span>&nbsp;</strong><br /> 1 怎样表示一个词的含义？&nbsp;<br /> （1）使用分类学的方法，例如WordNet。离散表示，one-hot。&nbsp;<br /> （2）分布式表示</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><span><strong>lec3</strong></span>&nbsp;<br /> 1 Word2Vec 和 Glove 训练后，都得到了两组词向量，这两组要怎么用呢？&nbsp;<br /> 经验主义，把它们叠加。不过最好先自己试一试各种情况。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">2 怎么评估词向量？&nbsp;<br /> 词的相似度，类比性，之类的测试集，或者作为下游任务的输入，评估下游任务的好坏。&nbsp;<br /> 后者更“实际”，但评估消耗时间更长。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec8</span>&nbsp;</strong><br /> 1 RNN中为什么梯度消失或爆炸会带来问题？&nbsp;<br /> 爆炸导致导数无法评估，clip解决。&nbsp;<br /> 消失导致“远”一点的参数几乎不更新，对角线初始化，relu，LSTM，GRU解决。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">2 神经翻译模型中使用RNN有哪些trick？&nbsp;<br /> 1 encoder 和 decoder 的参数不共享。&nbsp;<br /> 2 decoder中，把上一个隐层，encoder最后一层，上一个输出结果都最为当前隐层的输入。&nbsp;<br /> 3 多层RNN。&nbsp;<br /> 4 双向RNN。&nbsp;<br /> （以上都是想办法增加参数）&nbsp;<br /> 5 GRU 和 LSTM</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec11</span>&nbsp;</strong><br /> 1 LSTM 和 RNN记忆力的比较&nbsp;<br /> RNN最多记8-10步，LSTM可以到100步。（实验性结论）</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">2 使用LSTM的trick&nbsp;<br /> 1 把forget gate 的 bias 初始化到1&nbsp;<br /> 2 只在垂直方向上使用dropout&nbsp;<br /> 3 ensemble，使用8-10个网络，然后平均它们的预测结果，一般有少量的提升！</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">3 什么是 scheduled sampling？&nbsp;<br /> RNN下一步的输入中，随机的给ground truth 或 预测结果。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">4 什么是BLEU？&nbsp;<br /> 共有n-gram数目（n经常取4），并惩罚预测长度小于真实长度。作为机器翻译的评价指标，开始时和人类评分很match，现在已经被玩坏了。所以google等宣称达到人类水平的翻译结果，其实和真实人类的水平还有很多差距。</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;"><strong><span>lec12</span>&nbsp;</strong><br /> 1 NLP中CNN有什么用？怎么用？&nbsp;<br /> CNN可以比RNN更能捕获短语向量，没有RNN的梯度消失问题，GPU更友好。&nbsp;<br /> 可以用不同size的卷积来捕获不同长度的短语向量。&nbsp;<br /> pooling层，尤其是max-over-time pooling， 可以得到这个卷积提取到的某个激活值最大的特征，（即找到了某个短语）。&nbsp;<br /> 使用两套预训练词向量，一套参与训练，一套不参与。&nbsp;<br /> （character based model，经验主义，一般效果会提升，但训练时间更长，因为输入长度变的更长了。似乎中文很适合？我在中文文本分类任务中，也发现char based 效果比 word based效果要好一些。）</span></p>
<p style="color:rgb(51,51,51);font-family:'microsoft yahei';"> <span style="font-size:18px;">2 NLP任务中不同模型的比较&nbsp;<br /> 1 Bag of Vectors：对于简单的分类任务来说，是很好的base line。多加几层效果会更好！&nbsp;<br /> 2 Window Model：对于不需要宽的视野的分类任务来说，比较好。&nbsp;<br /> 3 CNNs：分类任务上效果好，但很难给出直观解释，在GPU上容易并行。&nbsp;<br /> 4 RNNs：对于分类任务不是最好的，比CNN慢，可以做序列tagging he classification。</span></p>
</p></div>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
